<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.RO](#cs.RO) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GIFSplat: Generative Prior-Guided Iterative Feed-Forward 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2602.22571)
*Tianyu Chen,Wei Xiang,Kang Han,Yu Lu,Di Wu,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: GIFSplat提出了一种全新的3D高斯点云快速重建方法，在保持极快推理速度的同时，带来了更高质量的重建效果，尤其适用于少量、无姿态的输入视角。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法中，逐场景优化虽可获得高质量结果，但推理慢且对稀疏视角不稳定；而前馈式方法尽管快，但在新领域或引入生成先验时容易丢失速度优势。作者旨在突破前馈式重建的性能瓶颈，并让其能更好地结合生成先验、提升泛化能力。

Method: GIFSplat采用纯前馈、迭代式的3D高斯点云细化框架，每次前向残差更新都根据渲染证据逐步改善场景。通过从改进渲染结果提取高斯级生成先验信号，无需反向传播和额外扩展观测集，实现了带生成先验的逐场景自适应。

Result: GIFSplat在DL3DV、RealEstate10K和DTU数据集上，优于当前前馈基线，将PSNR提升最高达2.1dB，始终维持秒级推理速度，并且无需已知相机位姿或测试时梯度优化。

Conclusion: GIFSplat在解放3D重建速度和质量的权衡方面取得突破，首次实现了无需姿态、测试端无梯度优化、秒级推理下，具备生成先验适应能力和高重建质量的3D点云恢复。

Abstract: Feed-forward 3D reconstruction offers substantial runtime advantages over per-scene optimization, which remains slow at inference and often fragile under sparse views. However, existing feed-forward methods still have potential for further performance gains, especially for out-of-domain data, and struggle to retain second-level inference time once a generative prior is introduced. These limitations stem from the one-shot prediction paradigm in existing feed-forward pipeline: models are strictly bounded by capacity, lack inference-time refinement, and are ill-suited for continuously injecting generative priors. We introduce GIFSplat, a purely feed-forward iterative refinement framework for 3D Gaussian Splatting from sparse unposed views. A small number of forward-only residual updates progressively refine current 3D scene using rendering evidence, achieve favorable balance between efficiency and quality. Furthermore, we distill a frozen diffusion prior into Gaussian-level cues from enhanced novel renderings without gradient backpropagation or ever-increasing view-set expansion, thereby enabling per-scene adaptation with generative prior while preserving feed-forward efficiency. Across DL3DV, RealEstate10K, and DTU, GIFSplat consistently outperforms state-of-the-art feed-forward baselines, improving PSNR by up to +2.1 dB, and it maintains second-scale inference time without requiring camera poses or any test-time gradient optimization.

</details>


### [2] [Don't let the information slip away](https://arxiv.org/abs/2602.22595)
*Taozhe Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的目标检测模型Association DETR，通过结合背景信息提升检测性能，并在COCO val2017数据集上取得了领先的结果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的目标检测模型（如YOLO系列和DETR系列）在性能上虽有提升，但普遍忽视了背景信息对目标检测的辅助作用。例如，车辆通常出现在道路等特定场景中，因此利用背景信息有助于提高检测准确率。

Method: 提出了Association DETR模型，区别于以往方法，它显式地结合并利用了图像中的背景上下文信息，以增强对目标物体的检测能力。

Result: 该模型在COCO val2017数据集上实现了SOTA表现，优于现有的YOLO和RT-DETR等主流目标检测方法。

Conclusion: 充分利用背景信息能够显著提升目标检测的效果，Association DETR为目标检测领域提供了一种新思路。

Abstract: Real-time object detection has advanced rapidly in recent years. The YOLO series of detectors is among the most well-known CNN-based object detection models and cannot be overlooked. The latest version, YOLOv26, was recently released, while YOLOv12 achieved state-of-the-art (SOTA) performance with 55.2 mAP on the COCO val2017 dataset. Meanwhile, transformer-based object detection models, also known as DEtection TRansformer (DETR), have demonstrated impressive performance. RT-DETR is an outstanding model that outperformed the YOLO series in both speed and accuracy when it was released. Its successor, RT-DETRv2, achieved 53.4 mAP on the COCO val2017 dataset. However, despite their remarkable performance, all these models let information to slip away. They primarily focus on the features of foreground objects while neglecting the contextual information provided by the background. We believe that background information can significantly aid object detection tasks. For example, cars are more likely to appear on roads rather than in offices, while wild animals are more likely to be found in forests or remote areas rather than on busy streets. To address this gap, we propose an object detection model called Association DETR, which achieves state-of-the-art results compared to other object detection models on the COCO val2017 dataset.

</details>


### [3] [BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model](https://arxiv.org/abs/2602.22596)
*Yuci Han,Charles Toth,John E. Anderson,William J. Shuart,Alper Yilmaz*

Main category: cs.CV

TL;DR: BetterScene是一种通过极度稀疏、无约束的照片提升真实场景新视图合成（NVS）质量的方法，改进了现有扩散模型在细节一致性和伪影方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有NVS方法在细节一致性与去伪影方面仍有不足，尤其是在使用较少图片进行新视图合成时效果不理想，多依赖已训练扩散先验且只微调UNet模块，限制了性能提升。

Method: 提出BetterScene，利用预训练大规模的Stable Video Diffusion (SVD)模型为主干，针对其变分自编码器（VAE）模块引入时序等变正则和视觉基础模型对齐表征，并结合3D 高斯泼洒（3DGS）模型渲染连续特征，通过SVD增强器生成无伪影、一致的新视图。

Result: 在DL3DV-10K等高难度数据集上进行评测，展示了优于现有最先进方法的性能。

Conclusion: BetterScene有效提升了基于稀疏、不受限照片的新视图合成质量，在细节一致性、去伪影和泛化能力方面均优于之前方法。

Abstract: We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to mitigate artifacts and recover view-consistent details at inference time. Conventional methods have developed similar diffusion-based solutions to address these challenges of novel view synthesis. Despite significant improvements, these methods typically rely on off-the-shelf pretrained diffusion priors and fine-tune only the UNet module while keeping other components frozen, which still leads to inconsistent details and artifacts even when incorporating geometry-aware regularizations like depth or semantic conditions. To address this, we investigate the latent space of the diffusion model and introduce two components: (1) temporal equivariance regularization and (2) vision foundation model-aligned representation, both applied to the variational autoencoder (VAE) module within the SVD pipeline. BetterScene integrates a feed-forward 3D Gaussian Splatting (3DGS) model to render features as inputs for the SVD enhancer and generate continuous, artifact-free, consistent novel views. We evaluate on the challenging DL3DV-10K dataset and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [4] [LoR-LUT: Learning Compact 3D Lookup Tables via Low-Rank Residuals](https://arxiv.org/abs/2602.22607)
*Ziqi Zhao,Abhijit Mishra,Shounak Roychowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种低秩形式的3D查找表（LUT）生成方法LoR-LUT，以实现图像增强和风格迁移，兼具紧凑性和可解释性。新方法通过引入低秩残差修正，在不增加计算复杂度的情况下提升了感知质量，并提供了交互式可视化工具LoR-LUT Viewer。


<details>
  <summary>Details</summary>
Motivation: 传统3D-LUT方法通常依赖密集张量作为基底LUT，不仅参数量大、计算复杂性高，而且缺乏易解释的结构。现有方法难以在保证高感知质量的同时维持模型精简，限制了其在实际应用中的推广，尤其是在设备资源受限的场景下。作者旨在解决这些问题。

Method: LoR-LUT利用了一套基底LUT与一组低秩张量形式的残差修正联合建模，通过低秩分解以显著减少参数量，并通过三线性插值实现快速推断。模型在MIT-Adobe FiveK数据集上进行训练，并配备可交互的LoR-LUT Viewer工具用于图像效果可视化和参数调节。

Result: LoR-LUT实现了与专家级修图软件相媲美的高感知保真度图像增强效果，模型体积不足1MB。与传统方法相比，LoR-LUT在参数量和计算复杂度不增加的前提下，显著提升了图像感知质量。同时，Viewer工具大幅提升了结果的可解释性和用户信心。

Conclusion: LoR-LUT为基于LUT的图像增强和风格迁移提供了更加紧凑、可解释且高效的方法。该方法兼具应用实用性和良好的用户体验，具有较高的实际推广价值和未来研究潜力。

Abstract: We present LoR-LUT, a unified low-rank formulation for compact and interpretable 3D lookup table (LUT) generation. Unlike conventional 3D-LUT-based techniques that rely on fusion of basis LUTs, which are usually dense tensors, our unified approach extends the current framework by jointly using residual corrections, which are in fact low-rank tensors, together with a set of basis LUTs. The approach described here improves the existing perceptual quality of an image, which is primarily due to the technique's novel use of residual corrections. At the same time, we achieve the same level of trilinear interpolation complexity, using a significantly smaller number of network, residual corrections, and LUT parameters. The experimental results obtained from LoR-LUT, which is trained on the MIT-Adobe FiveK dataset, reproduce expert-level retouching characteristics with high perceptual fidelity and a sub-megabyte model size. Furthermore, we introduce an interactive visualization tool, termed LoR-LUT Viewer, which transforms an input image into the LUT-adjusted output image, via a number of slidebars that control different parameters. The tool provides an effective way to enhance interpretability and user confidence in the visual results. Overall, our proposed formulation offers a compact, interpretable, and efficient direction for future LUT-based image enhancement and style transfer.

</details>


### [5] [Spectrally Distilled Representations Aligned with Instruction-Augmented LLMs for Satellite Imagery](https://arxiv.org/abs/2602.22613)
*Minh Kha Do,Wei Xiang,Kang Han,Di Wu,Khoa Phan,Yi-Ping Phoebe Chen,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: 本文提出SATtxt模型，在推理时仅需RGB输入，但能利用训练时学习到的多光谱信息，实现高效且具谱感知能力的视觉-语言理解。实验证明该方法在遥感数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基础模型在遥感领域应用受阻，主要由于多光谱数据的冗余与不对齐，以及文本编码器表达和对齐能力不足。同时，现实卫星系统往往只有RGB数据，迫切需要兼容RGB的高效方案。

Method: 该方法分为两步：第一，通过谱表征蒸馏，将多光谱教师模型中的先验知识转移到RGB学生模型；第二，利用指令增强的大语言模型，将蒸馏后的视觉空间和文本嵌入空间对齐，从而提升表达能力。

Result: 在EuroSAT、BigEarthNet和ForestNet三个遥感数据集上，SATtxt模型分别在零样本分类、检索和线性探查任务上较现有基线平均提升4.2%、5.9%、2.7%。

Conclusion: SATtxt实现了可扩展的仅需RGB输入但具备谱感知能力的视觉-语言基础模型，为遥感影像处理提供了高效新路径。

Abstract: Vision-language foundation models (VLFMs) promise zero-shot and retrieval understanding for Earth observation. While operational satellite systems often lack full multi-spectral coverage, making RGB-only inference highly desirable for scalable deployment, the adoption of VLFMs for satellite imagery remains hindered by two factors: (1) multi-spectral inputs are informative but difficult to exploit consistently due to band redundancy and misalignment; and (2) CLIP-style text encoders limit semantic expressiveness and weaken fine-grained alignment. We present SATtxt, a spectrum-aware VLFM that operates with RGB inputs only at inference while retaining spectral cues learned during training. Our framework comprises two stages. First, Spectral Representation Distillation transfers spectral priors from a frozen multi-spectral teacher to an RGB student via a lightweight projector. Second, Spectrally Grounded Alignment with Instruction-Augmented LLMs bridges the distilled visual space and an expressive LLM embedding space. Across EuroSAT, BigEarthNet, and ForestNet, SATtxt improves zero-shot classification on average by 4.2%, retrieval by 5.9%, and linear probing by 2.7% over baselines, showing an efficient path toward spectrum-aware vision-language learning for Earth observation. Project page: https://ikhado.github.io/sattxt/

</details>


### [6] [Coded-E2LF: Coded Aperture Light Field Imaging from Events](https://arxiv.org/abs/2602.22620)
*Tomoya Tsuchida,Keita Takahashi,Chihiro Tsutake,Toshiaki Fujii,Hajime Nagahara*

Main category: cs.CV

TL;DR: 本文提出了一种仅依赖事件相机和编码孔径的4维光场采集方法，无需强度图像，实现了硬件简化与精确重建。


<details>
  <summary>Details</summary>
Motivation: 现有利用事件相机的光场采集方法通常需要结合传统强度相机，增加了系统复杂性和硬件成本，限制了应用场景。本研究旨在探索用更简洁硬件直接从事件信息重建高精度光场。

Method: 设计了一套Coded-E2LF系统：采用编码孔径配合纯事件相机采集事件数据；理论上分析了编码中全黑模式的重要性；提出改进方法以提升基于纯事件数据的光场重建质量，并在真实系统上实现。

Result: 实验证明，在无强度图像参与情况下，仅靠事件流可以重建像素级精度的4维光场，涵盖真实3D场景，优于以往需强度信息的方法。

Conclusion: Coded-E2LF为事件相机光场成像提供了纯事件的新范式，简化硬件实现、降低成本，同时保持甚至提升重建精度。

Abstract: We propose Coded-E2LF (coded event to light field), a computational imaging method for acquiring a 4-D light field using a coded aperture and a stationary event-only camera. In a previous work, an imaging system similar to ours was adopted, but both events and intensity images were captured and used for light field reconstruction. In contrast, our method is purely event-based, which relaxes restrictions for hardware implementation. We also introduce several advancements from the previous work that enable us to theoretically support and practically improve light field reconstruction from events alone. In particular, we clarify the key role of a black pattern in aperture coding patterns. We finally implemented our method on real imaging hardware to demonstrate its effectiveness in capturing real 3-D scenes. To the best of our knowledge, we are the first to demonstrate that a 4-D light field with pixel-level accuracy can be reconstructed from events alone. Our software and supplementary video are available from our project website.

</details>


### [7] [CGSA: Class-Guided Slot-Aware Adaptation for Source-Free Object Detection](https://arxiv.org/abs/2602.22621)
*Boyang Dai,Zeng Fan,Zihao Qi,Meng Lou,Yizhou Yu*

Main category: cs.CV

TL;DR: CGSA首次将基于对象的学习引入源数据不可用的领域自适应目标检测中，通过层次化slot感知模块与类别引导对比，显著提升跨领域检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有源数据不可用的领域自适应目标检测方法主要关注伪标签或教师-学生框架，较少利用跨域数据中的对象层次结构信息，限制了适应能力。

Method: 提出CGSA框架，将对象感知学习引入DETR检测器，设计了层次Slot感知模块（HSA）用于提取对象先验，并通过类别引导Slot对比模块（CGSC）实现语义一致性与领域不变性。

Result: 在多个跨域数据集上，CGSA优于以往源数据不可用领域自适应目标检测方法，理论和实验证明其各模块和整体框架有效。

Conclusion: 基于对象中心设计的CGSA在隐私敏感的自适应场景中表现出色，验证了对象层次结构信息在领域自适应检测中的潜力。

Abstract: Source-Free Domain Adaptive Object Detection (SF-DAOD) aims to adapt a detector trained on a labeled source domain to an unlabeled target domain without retaining any source data. Despite recent progress, most popular approaches focus on tuning pseudo-label thresholds or refining the teacher-student framework, while overlooking object-level structural cues within cross-domain data. In this work, we present CGSA, the first framework that brings Object-Centric Learning (OCL) into SF-DAOD by integrating slot-aware adaptation into the DETR-based detector. Specifically, our approach integrates a Hierarchical Slot Awareness (HSA) module into the detector to progressively disentangle images into slot representations that act as visual priors. These slots are then guided toward class semantics via a Class-Guided Slot Contrast (CGSC) module, maintaining semantic consistency and prompting domain-invariant adaptation. Extensive experiments on multiple cross-domain datasets demonstrate that our approach outperforms previous SF-DAOD methods, with theoretical derivations and experimental analysis further demonstrating the effectiveness of the proposed components and the framework, thereby indicating the promise of object-centric design in privacy-sensitive adaptation scenarios. Code is released at https://github.com/Michael-McQueen/CGSA.

</details>


### [8] [CRAG: Can 3D Generative Models Help 3D Assembly?](https://arxiv.org/abs/2602.22629)
*Zeyu Jiang,Sihang Li,Siqi Tan,Chenyang Xu,Juexiao Zhang,Julia Galway-Witham,Xue Wang,Scott A. Williams,Radu Iovita,Chen Feng,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出将3D拼装问题视为拼装与生成的联合任务，提出CRAG方法，能够在预测零件位置的同时补全缺失几何，实现了对多样复杂零件3D拼装的高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有3D拼装方法仅将拼装视为姿态估计，缺乏结构与整体形态的推理，不能处理缺失零件的情况。作者受人类拼装启发，认为应结合结构推理与整体形状生成。

Method: 提出CRAG方法，将3D拼装建模为拼装与生成的联合过程：拼装过程提供零件级结构先验；生成过程则提供整体形状语境，并能补全缺失几何。方法既能预测零件位置，也能生成完整3D形状。

Result: 在包含不同几何体、零件数和缺失部分的实际对象上，方法显著超越现有方法，具备更好的泛化能力和拼装效果。

Conclusion: 拼装与生成过程互为补充，CRAG模型能够实现更智能的3D拼装，未来可广泛应用于复杂场景的自动三维建模。

Abstract: Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference. Inspired by this intuition, we reformulate 3D assembly as a joint problem of assembly and generation. We show that these two processes are mutually reinforcing: assembly provides part-level structural priors for generation, while generation injects holistic shape context that resolves ambiguities in assembly. Unlike prior methods that cannot synthesize missing geometry, we propose CRAG, which simultaneously generates plausible complete shapes and predicts poses for input parts. Extensive experiments demonstrate state-of-the-art performance across in-the-wild objects with diverse geometries, varying part counts, and missing pieces. Our code and models will be released.

</details>


### [9] [Plug, Play, and Fortify: A Low-Cost Module for Robust Multimodal Image Understanding Models](https://arxiv.org/abs/2602.22644)
*Siqi Lu,Wanying Xu,Yongbin Zheng,Wenting Luan,Peng Sun,Jianhang Yao*

Main category: cs.CV

TL;DR: 本文提出了一种高效方法，通过频域分析处理多模态模型在缺失模态下性能急剧下降的问题，所提方法能动态平衡不同模态的贡献，显著提升多种任务表现。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在某些模态缺失时性能大幅下降，主要原因是模型在学习时对部分模态存在隐含偏好，导致其他模态的特征未被充分优化。

Method: 作者提出了频率比率度量（FRM）来在频域衡量模态偏好，并基于此设计了多模态权重分配模块（MWAM），该模块可动态调整各模态在训练中的权重，促进均衡学习。

Result: 实验表明MWAM可无缝集成进不同结构（如CNN、ViT），在多种任务及模态组合下都带来稳定性能提升，对比主流方法进一步提升了处理缺失模态问题的效果。

Conclusion: MWAM模块不仅能优化基础模型表现，还能进一步提升现有SOTA方法的鲁棒性和效果，是处理缺失模态问题的一种有效补充。

Abstract: Missing modalities present a fundamental challenge in multimodal models, often causing catastrophic performance degradation. Our observations suggest that this fragility stems from an imbalanced learning process, where the model develops an implicit preference for certain modalities, leading to the under-optimization of others. We propose a simple yet efficient method to address this challenge. The central insight of our work is that the dominance relationship between modalities can be effectively discerned and quantified in the frequency domain. To leverage this principle, we first introduce a Frequency Ratio Metric (FRM) to quantify modality preference by analyzing features in the frequency domain. Guided by FRM, we then propose a Multimodal Weight Allocation Module, a plug-and-play component that dynamically re-balances the contribution of each branch during training, promoting a more holistic learning paradigm. Extensive experiments demonstrate that MWAM can be seamlessly integrated into diverse architectural backbones, such as those based on CNNs and ViTs. Furthermore, MWAM delivers consistent performance gains across a wide range of tasks and modality combinations. This advancement extends beyond merely optimizing the performance of the base model; it also manifests as further performance improvements to state-of-the-art methods addressing the missing modality problem.

</details>


### [10] [Interactive Medical-SAM2 GUI: A Napari-based semi-automatic annotation tool for medical images](https://arxiv.org/abs/2602.22649)
*Woojae Hong,Jong Ha Hwang,Jiyong Chung,Joongyeon Choi,Hyunngun Kim,Yong Hwy Kim*

Main category: cs.CV

TL;DR: 该论文介绍了一款基于Napari的开源交互式医学影像标注工具Medical-SAM2 GUI，可高效进行2D和3D医学影像的半自动标注，并支持体素级注释、标签传播、交互修正、批量导出等多功能操作。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像体素级标注是医学影像算法开发和验证的关键，但人工标注耗时且成本高，现有方法主要支持逐层交互，缺乏统一、便捷、适用于多病例的完整工作流。

Method: 基于Napari多维查看器，集成SAM2式掩码传播，将3D体积视为切片序列。通过框选/点选提示触发半自动标注，可在单一界面下进行批量病例导航、掩码传播、交互修正与定量导出。支持标准DICOM/NIfTI数据格式，支持保留图像几何、三维渲染和体积测量。

Result: 实现了支持多病例批量注释、交互编辑、标注矫正和定量分析的一体化本地工作流。显著提高3D医学影像的标注效率。

Conclusion: 该工具为临床和科研领域提供了高效、便捷的3D医学影像体素级半自动标注流程，促进医学影像AI相关算法的开发与验证。代码已开源，适合科研标注流程使用。

Abstract: Interactive Medical-SAM2 GUI is an open-source desktop application for semi-automatic annotation of 2D and 3D medical images. Built on the Napari multi-dimensional viewer, box/point prompting is integrated with SAM2-style propagation by treating a 3D volume as a slice sequence, enabling mask propagation from sparse prompts using Medical-SAM2 on top of SAM2. Voxel-level annotation remains essential for developing and validating medical imaging algorithms, yet manual labeling is slow and expensive for 3D scans, and existing integrations frequently emphasize per-slice interaction without providing a unified, cohort-oriented workflow for navigation, propagation, interactive correction, and quantitative export in a single local pipeline. To address this practical limitation, a local-first Napari workflow is provided for efficient 3D annotation across multiple studies using standard DICOM series and/or NIfTI volumes. Users can annotate cases sequentially under a single root folder with explicit proceed/skip actions, initialize objects via box-first prompting (including first/last-slice initialization for single-object propagation), refine predictions with point prompts, and finalize labels through prompt-first correction prior to saving. During export, per-object volumetry and 3D volume rendering are supported, and image geometry is preserved via SimpleITK. The GUI is implemented in Python using Napari and PyTorch, with optional N4 bias-field correction, and is intended exclusively for research annotation workflows. The code is released on the project page: https://github.com/SKKU-IBE/Medical-SAM2GUI/.

</details>


### [11] [Scaling Audio-Visual Quality Assessment Dataset via Crowdsourcing](https://arxiv.org/abs/2602.22659)
*Renyu Yang,Jian Jin,Lili Meng,Meiqin Liu,Yilin Wang,Balu Adsumilli,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种构建大规模、多样化音视频质量评估（AVQA）数据集的方法，并基于该方法推出了目前最大规模、最丰富的YT-NTU-AVQ音视频数据集。


<details>
  <summary>Details</summary>
Motivation: 现有AVQA数据集规模较小，内容和质量多样性不足，并且仅提供整体分数标签，限制了模型开发和多模态感知研究。

Method: 1）设计了基于众包的主观实验框架，突破实验室环境限制，实现了可靠的跨环境标注；2）采用系统性数据准备策略，确保质量等级和语义场景的广泛覆盖；3）扩展了数据注释，支持多模态感知机制及其与内容关系的研究。

Result: 通过实施上述方法，提出了YT-NTU-AVQ数据集，包含1620段用户生成的音视频序列，成为目前规模最大、内容最丰富的AVQA数据集，并已公开数据集和平台代码。

Conclusion: 提出的方法切实提升了AVQA数据集的规模与多样性，为后续的模型开发和多模态感知研究提供了坚实基础。

Abstract: Audio-visual quality assessment (AVQA) research has been stalled by limitations of existing datasets: they are typically small in scale, with insufficient diversity in content and quality, and annotated only with overall scores. These shortcomings provide limited support for model development and multimodal perception research. We propose a practical approach for AVQA dataset construction. First, we design a crowdsourced subjective experiment framework for AVQA, breaks the constraints of in-lab settings and achieves reliable annotation across varied environments. Second, a systematic data preparation strategy is further employed to ensure broad coverage of both quality levels and semantic scenarios. Third, we extend the dataset with additional annotations, enabling research on multimodal perception mechanisms and their relation to content. Finally, we validate this approach through YT-NTU-AVQ, the largest and most diverse AVQA dataset to date, consisting of 1,620 user-generated audio and video (A/V) sequences. The dataset and platform code are available at https://github.com/renyu12/YT-NTU-AVQ

</details>


### [12] [ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals](https://arxiv.org/abs/2602.22666)
*Xuelu Li,Zhaonan Wang,Xiaogang Wang,Lei Wu,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: 该论文提出ArtPro，一个自监督框架，通过自适应移动性提议实现高保真的多部件可动物体数字孪生重建。实验表明其在复杂多部件对象上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯渲染的自监督方法对初始分割极为敏感，依赖启发式聚类或预训练模型，导致复杂对象优化易陷入局部最优。

Method: ArtPro首先基于几何特征和运动先验进行过分割，生成带有运动假设的部件提议。在优化过程中，通过分析空间邻居的运动一致性动态合并部件提议，并通过碰撞感知机制避免运动学估计错误。

Result: 在合成与真实数据集上的大量实验表明，ArtPro在复杂多部件物体重建上表现出更高的鲁棒性、准确性和稳定性，显著超过现有方法。

Conclusion: ArtPro为复杂可动物体的数字孪生提供了稳健、自监督的高质量重建方案，对机器人操作和交互式仿真有重要意义。

Abstract: Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.

</details>


### [13] [Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes](https://arxiv.org/abs/2602.22667)
*Changqing Zhou,Yueru Luo,Han Zhang,Zeyu Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种适用于复杂室内环境的open-vocabulary三维占据预测方法，能够在不依赖固定语义标签的情况下实现精细的语义理解，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统三维占据预测方法主要基于固定语义分类标签，缺乏泛化能力，且现有open-vocabulary方法主要针对室外环境，难以适应室内场景的复杂几何和细粒度语义。本研究旨在提升室内环境下的三维占据与语义对齐能力。

Method: 提出仅基于几何信息（occupied/free）的弱监督训练方案，利用3D Language-Embedded Gaussians进行几何与语义的耦合表征。对几何部分，设计了全新基于不透明性与泊松过程的体聚合方法稳定训练；对语义对齐问题，提出逐步温度衰减机制缓解特征混杂问题，增强高斯-语言对齐。

Result: 在Occ-ScanNet数据集open-vocabulary设置下，该方法在IoU和mIoU指标上显著超越现有所有占据方法，分别达到了59.50（IoU）和21.05（mIoU），大幅领先于其他open-vocabulary方法。

Conclusion: 本文方法在弱监督和开放语义场景下实现了精细的室内三维几何与语义理解，为智能体感知赋能，具有更强泛化能力和实际应用价值。

Abstract: Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.

</details>


### [14] [GeoWorld: Geometric World Models](https://arxiv.org/abs/2602.23058)
*Zeyu Zhang,Danning Li,Ian Reid,Richard Hartley*

Main category: cs.CV

TL;DR: 本文提出GeoWorld，一种基于几何结构优化的能量世界模型，用于提升多步视觉规划中的表现，尤其在复杂任务和长期预测上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的能量世界模型在潜在空间的表征上仅考虑欧氏空间，忽视了状态间潜在的几何和层级结构，并且在长时序推理中准确率下降严重。为了解决这一点，亟需考虑几何结构的世界模型提升多步规划能力。

Method: 作者提出GeoWorld模型，将潜在表示通过超曲率JEPA从欧氏空间映射到双曲空间，有效保留了几何和层级关系。同时，提出了适用于双曲潜在空间的几何强化学习，用于稳定的能量优化和多步规划。

Result: 在CrossTask和COIN数据集上的实验表明，GeoWorld在3步规划上比最优的V-JEPA 2方法提升了3%的成功率，在4步规划上提升了2%。

Conclusion: 通过引入双曲结构和几何优化，GeoWorld模型显著提升了多步视觉规划的效果，为基于能量世界模型的长期推理提供了新途径。

Abstract: Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.

</details>


### [15] [SPMamba-YOLO: An Underwater Object Detection Network Based on Multi-Scale Feature Enhancement and Global Context Modeling](https://arxiv.org/abs/2602.22674)
*Guanghao Liao,Zhen Liu,Liyuan Cao,Yonghui Yang,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的水下目标检测网络SPMamba-YOLO，通过增强多尺度特征融合与引入全局上下文建模，有效提升了复杂水下环境下的小目标检测能力。


<details>
  <summary>Details</summary>
Motivation: 水下目标因光照衰减、颜色失真、背景杂乱、目标尺度小等问题，导致检测难度大。现有方法对多尺度特征聚合与全局信息建模不足，限制了检测性能。

Method: 提出了SPPELAN（空间金字塔池化增强层聚合网络）模块加强多尺度特征聚合和感受野扩展，PSA（金字塔分割注意力）机制提升特征区分能力，Mamba状态空间建模模块用于高效获取全局上下文信息。

Result: 在URPC2022数据集上，SPMamba-YOLO相比YOLOv8n在mAP@0.5上提升4.9%以上，特别在小目标和高密度目标检测上效果突出，且保持了检测精度与计算成本的平衡。

Conclusion: SPMamba-YOLO在水下目标检测上展现出优越性能，尤其适合复杂环境下的小尺度和密集目标检测，为水下视觉应用提供了有力的技术支持。

Abstract: Underwater object detection is a critical yet challenging research problem owing to severe light attenuation, color distortion, background clutter, and the small scale of underwater targets. To address these challenges, we propose SPMamba-YOLO, a novel underwater object detection network that integrates multi-scale feature enhancement with global context modeling. Specifically, a Spatial Pyramid Pooling Enhanced Layer Aggregation Network (SPPELAN) module is introduced to strengthen multi-scale feature aggregation and expand the receptive field, while a Pyramid Split Attention (PSA) mechanism enhances feature discrimination by emphasizing informative regions and suppressing background interference. In addition, a Mamba-based state space modeling module is incorporated to efficiently capture long-range dependencies and global contextual information, thereby improving detection robustness in complex underwater environments. Extensive experiments on the URPC2022 dataset demonstrate that SPMamba-YOLO outperforms the YOLOv8n baseline by more than 4.9\% in mAP@0.5, particularly for small and densely distributed underwater objects, while maintaining a favorable balance between detection accuracy and computational cost.

</details>


### [16] [FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time](https://arxiv.org/abs/2602.23115)
*David Dirnfeld,Fabien Delattre,Pedro Miraldo,Erik Learned-Miller*

Main category: cs.CV

TL;DR: 本论文提出了一种基于单位球面Hough变换的单目相机运动方向估计算法，兼顾精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前许多单目视觉定位方法在面对噪声和离群点时容易降低精度或计算量大，限制了实际应用。为此，本文致力于提升相机方向估计的鲁棒性与效率。

Method: 提出对单位球面（S(2)）上的Hough变换进行推广。具体做法是，首先提取帧间特征匹配，每对对应点生成一个大圆，然后使用Fibonacci网格对球面进行离散化，每个大圆对一系列方向投票。这样可信特征能一致投票正确的运动方向。

Result: 在三个公开数据集的实验中，该方法在准确率与效率上均表现优异，位于帕累托前沿。在SLAM实验中，通过该方法初始化相机朝向，降低了RMSE误差。

Conclusion: 提出的方法在高噪声和离群点条件下依旧高效、准确，兼顾实际应用需求，为单目视觉中相机方向估计提供了有力的新工具。

Abstract: Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.

</details>


### [17] [ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport](https://arxiv.org/abs/2602.22678)
*Quoc-Khang Tran,Minh-Thien Nguyen,Nguyen-Khang Pham*

Main category: cs.CV

TL;DR: ViCLIP-OT是专为越南语图文检索设计的新模型，引入了SIGROT损失提升跨模态一致性，在多个基准上优于现有方法，对低资源语言检索具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索模型大多针对高资源语言，低资源语言（如越南语）性能不足，限制了智能多媒体系统在这些语境下的应用。

Method: 本研究提出ViCLIP-OT，结合了CLIP风格的对比学习和相似性图正则化最优传输（SIGROT）损失，提升全局跨模态一致性并减小模态间差距。

Result: 在UIT-OpenViIC、KTVIC和Crossmodal-3600三大越南语基准上，ViCLIP-OT在域内与零样本场景下均优于CLIP和SigLIP。具体如UIT-OpenViIC数据集上Recall@K均值提升5.75个百分点，Crossmodal-3600零样本提升11.72个百分点。嵌入空间分析进一步验证了模态差距的缩小和对齐性的提升。

Conclusion: SIGROT损失的引入为低资源语言跨模态检索提供了高效、可扩展的解决方案，对于越南语及其它弱资源语言的智能多媒体系统具有重要应用价值。

Abstract: Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.

</details>


### [18] [Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking](https://arxiv.org/abs/2602.23172)
*Maximilian Luz,Rohit Mohan,Thomas Nürnberg,Yakov Miron,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: LaGS方法结合多视角的掩码分割与高效的高斯投影，将时空场景信息以稀疏3D高斯点进行融合并进行4D全景占有跟踪，在多个数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景机器人感知方法仅关注于粗糙的几何跟踪或细粒度结构，但往往缺少对完整时空（4D）及多类别（panoptic）占有的高效联合建模，且多视角信息的融合存在效率瓶颈。

Method: 提出Latent Gaussian Splatting (LaGS)方法，利用3D高斯作为稀疏场景表征，将多视角观测信息融合后，通过特征投射到体素网格。结合掩码分割头，实现端到端的4D全景占有跟踪。

Result: 在Occ3D nuScenes和Waymo数据集上，LaGS实现了4D全景占有跟踪任务的最新最优（SOTA）效果。

Conclusion: LaGS模型有效解决了多视角高效聚合和时空4D全景占有联合建模的难题，为机器人动态环境感知提供了新的高效方案，具有良好的工程和实用前景。

Abstract: Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.

</details>


### [19] [SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses](https://arxiv.org/abs/2602.22683)
*Zhuohang Jiang,Xu Yuan,Haohao Qu,Shanru Lin,Kanglong Liu,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了SUPERGLASSES，这是首个基于智能眼镜真实用户数据的视觉问答(VQA)基准，并以此分析传统视觉语言模型的不足，同时提出SUPERLENS模型实现了更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VQA数据集和模型多基于传统场景，难以真实反映智能眼镜在实际使用中的多模态交互和对象识别等挑战。亟需建立针对这种应用场景的真实数据集和更适配的方法。

Method: 作者收集了由智能眼镜设备采集的2,422组包含14类图片领域和8类查询类型的第一视角图片和问题，构建SUPERGLASSES基准，评估了26种VLM模型，并提出结合自动目标检测、问题解耦以及多模态网页搜索的SUPERLENS智能眼镜多模态代理模型。

Result: 基于SUPERGLASSES基准，现有26种代表性模型在VQA任务中表现出显著不足。SUPERLENS模型在该基准上达到业界最优表现，超越GPT-4o 2.19个百分点。

Conclusion: 面向智能眼镜VQA场景，仅依赖传统VLM模型并不能满足实际需求，需要构建特定场景的基准和解决方案，SUPERLENS方法为未来的开发指明了方向。

Abstract: The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.

</details>


### [20] [Motion-aware Event Suppression for Event Cameras](https://arxiv.org/abs/2602.23204)
*Roberto Pellerito,Nico Messikommer,Giovanni Cioffi,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 本文提出了首个运动感知事件抑制框架，可实时过滤独立运动物体(IMOs)和自运动引发的事件，在EVIMO基准上大幅领先，并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机生成大量运动相关事件，但很多事件是由无关的自运动或独立运动物体触发的，会影响后续视觉任务的准确性和效率。因此，迫切需要一种能实时抑制不必要运动事件的有效框架。

Method: 本方法创新性地联合了IMO分割与未来运动预测，实现了预判性过滤动态事件。架构轻量，能够在消费级GPU上以173 Hz速率、低于1 GB显存运行。用事件序列输入，模型同时输出分割与预测结果，从而提前抑制即将发生的动态事件。

Result: 在EVIMO基准上，分割准确率提升67%，推理速度提升53%。在下游应用上，结合Token Pruning的Vision Transformer推理加速83%，基于事件的视觉里程计（visual odometry）中轨迹误差降低13%。

Conclusion: 所提框架在高效、准确事件抑制方面显著优于现有方法，并能极大促进后续事件视觉任务的速度与精度。

Abstract: In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\% in segmentation accuracy while operating at a 53\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\%.

</details>


### [21] [No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings](https://arxiv.org/abs/2602.22689)
*Joonsung Jeon,Woo Jae Kim,Suhyeon Ha,Sooel Son,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 本文提出了MoFit方法，实现了无需文本注释即可对扩散模型进行成员推理攻击，以检测训练数据被记忆的程度，对图像生成模型隐私及知识产权保护具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前的潜变量扩散模型（如文本到图像生成）在高质量图像合成方面获得了巨大成功，但由于模型倾向于记忆训练数据，引发了严重的隐私和知识产权问题。传统的成员推理攻击虽然能检测模型是否记住特定样本，但大多需要已知的原始文本描述（caption），在现实中难以获得，使方法大打折扣。因此，需要一种不依赖图片文本描述的新方式来提升隐私审计能力。

Method: MoFit是一种无需caption的成员推理攻击框架。其流程包含两个主要阶段：（1）以模型为导向的代理优化：通过对原始图片加入扰动，优化出分布于模型生成流形上的代理图片，该区域被训练样本更充分覆盖；（2）代理驱动的embedding提取：从优化后的代理图片获得一个嵌入向量，将其作为错误条件输入测试模型。对于训练集图片，这个embedding会引发较大条件损失响应，而对未见过的图片（hold-outs）影响较小，从而实现有效区分。

Result: 大量实验覆盖多个数据集和扩散模型，结果表明MoFit显著优于依赖视觉-语言模型的基线方法，在没有caption的情况下，性能接近甚至媲美依赖caption的同类方法。

Conclusion: MoFit突破了过往对图片文本描述的依赖，为隐私审计和知识产权保护提供了更实用的技术手段，有助于提升扩散模型安全性和可审计性。

Abstract: Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.

</details>


### [22] [UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception](https://arxiv.org/abs/2602.23224)
*Mohammad Mahdavian,Gordon Tan,Binbin Xu,Yuan Ren,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出了一种面向机器人的统一、可扩展的多视角三维重建框架UniScale，能够灵活集成几何先验知识，通过单一模型实现摄像头参数、尺度不变深度、点云和场景度量尺度的联合估计，具有良好的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 视觉导航和机器人应用中，需要从图片序列中准确提取环境结构，为下游任务提供支持。传统3D重建在尺度估计和先验集成方面存在局限，缺乏统一且高效的解决方案。

Method: 设计了一个模块化的单网络架构，可联合推理摄像头内外参、尺度不变深度和点云、场景尺度等。当已知相机先验（如内参或姿态）时可直接整合进模型。可选的几何先验辅助，融合全局上下文和摄像头相关特征，无需从零训练，可利用已有模型的世界先验。

Result: 在多个基准数据集上评估，展现出较强泛化能力和环境适应一致性，能在已知或部分已知相机参数情况下进一步提升性能。

Conclusion: UniScale实现了鲁棒、高精度的度量尺度3D重建，并易于与现有机器人系统集成，尤其适合资源受限环境，代码在论文接收后开源。

Abstract: We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.

</details>


### [23] [GFRRN: Explore the Gaps in Single Image Reflection Removal](https://arxiv.org/abs/2602.22695)
*Yu Chen,Zewei He,Xingyu Liu,Zixuan Chen,Zheming Lu*

Main category: cs.CV

TL;DR: 本文提出了一种无语义鸿沟的新型单幅图像反射去除网络（GFRRN），通过对预训练模型微调和标签统一等策略，显著提升了SIRR任务表现，并超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前双流反射去除方法虽表现优异，但存在两大难题：一是预训练模型与去反射模型特征语义理解不一致，二是合成数据和真实数据的反射标注具有不一致性。本文旨在解决上述瓶颈，以提升模型的泛化能力和真实场景表现。

Method: 1) 采用高效参数微调（PEFT）策略，把可学习的Mona层集成进预训练模型，实现特征对齐；2) 设计标签生成器，统一合成与真实数据的反射标签；3) 提出高斯自适应频率学习模块G-AFLB，提高频率先验学习与融合能力；4) 新颖的动态代理注意力DAA，作为动态建模窗口间与窗口内显著性的替代模块。这些创新共同组成所提GFRRN网络结构。

Result: 大量实验结果表明，GFRRN在多个数据集上的表现均优于当前最先进的反射去除方法，无论是在定量指标还是可视化效果上都取得了最佳性能。

Conclusion: GFRRN有效消除了语义理解差异和标签不一致两大挑战，显著提升了单幅图像反射去除质量，为后续研究提供了更强的模型设计思路。

Abstract: Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.

</details>


### [24] [Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving](https://arxiv.org/abs/2602.23259)
*Jiangxin Sun,Feng Xue,Teng Long,Chang Liu,Jian-Fang Hu,Wei-Shi Zheng,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种无需专家示范指导的端到端自动驾驶方法RaWMPC，通过世界模型预测多种候选动作的后果，并显式评估风险，最终选取低风险动作，有效提升了模型在常见与长尾场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶主要依赖模仿学习，通过学习专家驾驶数据来决策。然而，这类方法在遇到未见过的、长尾场景时容易发生安全决策失误，根本原因在于模型缺乏对高风险行为后果的自主认知与规避能力。作者提出摆脱专家行为监督，面向泛化和安全的决策新范式。

Method: 提出RaWMPC框架，无需专家示范数据，通过世界模型预测多候选动作结果，并用显式风险评估机制选取低风险动作。同时设计风险感知交互策略，主动让世界模型学习高危动作导致的后果，并提出自评蒸馏方法，在推理时将世界模型的风险规避能力迁移到生成式动作提议网络。

Result: 大量实验表明，RaWMPC在常见场景和分布外场景（长尾、稀有）下均优于现有最先进方法，并拥有更强的决策可解释性。

Conclusion: RaWMPC为自动驾驶领域提供了一种不依赖专家动作监督的新思路，通过风险感知与世界模型联合优化，实现了更强泛化能力和安全性，为E2E自动驾驶发展带来了重要突破。

Abstract: With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of "only driving like the expert" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.

</details>


### [25] [UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects](https://arxiv.org/abs/2602.22712)
*Yuankai Chen,Kai Lin,Qihong Wu,Xinxuan Yang,Jiashuo Lai,Ruoen Chen,Haonan Shi,Minfan He,Meihua Wang*

Main category: cs.CV

TL;DR: 该论文提出了用于无人机图像小目标检测的UFO-DETR方法，综合提升检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有小目标检测方法主要依赖手工设计组件，且通用检测器难以适配无人机图像，导致在精度与复杂度间难以权衡。无人机图像中目标小、密集、尺度变化大，这对现有方法提出挑战。

Method: 本文提出端到端对象检测框架UFO-DETR，采用LSKNet作为主干网络以优化感受野并减少参数。同时，通过结合DAttention和AIFI模块灵活建模多尺度空间关系，提升多尺度检测性能。还提出DynFreq-C3模块，通过跨空间频率特征增强专门增强小目标检测能力。

Result: 实验结果表明，所提方法在检测精度和计算效率上相比RT-DETR-L有显著提升，特别适合无人机边缘计算场景。

Conclusion: UFO-DETR为无人机小目标检测提供了高效且优异的新方案，在实际应用中具有广泛前景。

Abstract: Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.

</details>


### [26] [AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation](https://arxiv.org/abs/2602.22740)
*Tongfei Chen,Shuo Yang,Yuguang Yang,Linlin Yang,Runtang Guo,Changbai Li,He Long,Chunyu Xie,Dawei Leng,Baochang Zhang*

Main category: cs.CV

TL;DR: 该论文提出一种新型训练策略（AML），通过显式估算像素级视觉-语言对齐性，并在优化过程中过滤对齐较差的区域，提升参考图像分割的性能。方法在RefCOCO数据集上取得了最新最优性能，同时提升了对不同描述和场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有参考图像分割方法在处理自然语言表达时，常因像素级视觉与语言语义对齐不充分，从而影响分割效果。作者希望克服这一瓶颈，更充分利用对齐信息，提升分割精度和模型鲁棒性。

Method: 提出Alignment-Aware Masked Learning（AML）训练策略，主要包括两步：1）显式估算图片中像素与语言表达之间的对齐程度；2）在优化阶段过滤掉对齐较差的像素区域，仅用可信的区域进行训练，减小噪声影响。

Result: 该方法在RefCOCO等主流参考图像分割数据集上刷新了最新最优性能；同时，在面对多样化自然语言描述和不同场景时表现出更好的鲁棒性。

Conclusion: AML方法有效提升了参考图像分割精度，证明了像素级视觉-语言对齐的重要性，并为后续视觉与语言结合的相关任务提供了新思路。

Abstract: Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios

</details>


### [27] [ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control](https://arxiv.org/abs/2602.22742)
*Akihisa Watanabe,Qing Yu,Edgar Simo-Serra,Kent Fujiwara*

Main category: cs.CV

TL;DR: 提出了一种无需训练、能精确控制空间约束且保证动作自然性的生成方法ProjFlow。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成方法要么依赖特定任务训练、要么求解优化慢，而且强制约束常导致动作不自然。需要训练自由、能精确满足约束且保持自然性的技术。

Method: 提出ProjFlow采样器，将许多动画任务建模为线性逆问题。核心贡献在于引入新的骨骼拓扑感知度量，使空间约束校正能在骨架全局一致分布，避免投影方法常见的不自然现象。对于关键帧稀疏等场景，引入带有“伪观测”随采样逐渐衰减的时变设定以填补长缺失。整个方法无需训练。

Result: ProjFlow在动作修补、2D转3D等代表性任务中，能精确满足空间约束，在动作自然性上与零样本基线持平或更优，并接近甚至赶超有监督训练方法。

Conclusion: ProjFlow实现了零样本、无需训练、精确空间控制的人体动作生成，且在动作自然性和效果上优于现有多数零样本方法，并与训练型方法具有竞争力。

Abstract: Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.

</details>


### [28] [SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation](https://arxiv.org/abs/2602.22745)
*Fengming Liu,Tat-Jen Cham,Chuanxia Zheng*

Main category: cs.CV

TL;DR: 本论文提出了SPATIALALIGN框架，可提升文本生成视频模型对动态空间关系的表现能力，显著增强了视频生成的空间约束对齐。


<details>
  <summary>Details</summary>
Motivation: 当前T2V生成器关注视觉美感却常忽视生成视频中的空间关系。本研究旨在解决文本描述与生成视频在动态空间关系不符的问题。

Method: 方法上，作者采用了零阶正则化的直接偏好优化（Direct Preference Optimization, DPO）对T2V模型进行微调，并设计了名为DSR-SCORE的几何度量指标，用于量化生成视频与提示文本中动态空间关系（DSR）的对齐度。此外还建立了包含多样DSR的文本-视频对数据集。

Result: 实验结果表明，经微调后的模型在生成满足空间关系上显著优于基线模型，有效提升了空间关系的表达准确性。

Conclusion: 本工作提出框架和度量方法为T2V生成空间关系一致性提供了新的思路和工具，有助于相关领域的进一步发展。

Abstract: Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.

</details>


### [29] [Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval](https://arxiv.org/abs/2602.22759)
*Yuan-Chih Chen,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的图像隐码恢复框架，实现了对图片篡改内容的检索和还原，并在新构建的数据集 ImageNet-S 上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前图像真伪性研究主要聚焦于deepfake检测和篡改区域定位，然而对于篡改内容信息的检索和还原问题关注较少。为促进事实恢复，作者试图填补这一空白。

Method: 提出了一种利用语义和感知信息的紧凑隐码表示，并通过多尺度向量量化和条件Transformer模块增强上下文推理。能同时支持后插水印和生成式水印范式下图片内容的检索和重建。

Result: 在作者新构建的ImageNet-S基准上，实验结果表明该方法在检索和重建任务中表现良好，且与多种水印流程兼容。

Conclusion: 该框架不仅能够检测和定位篡改，而且奠定了广泛适用的图像内容恢复基础，推进了图像真实性领域的发展。

Abstract: Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.

</details>


### [30] [Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning](https://arxiv.org/abs/2602.22791)
*Taishu Arashima,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督骨骼表征的轨迹预测方法，以增强在骨骼数据丢失情况下的鲁棒性，并在多种遮挡场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于遮挡等因素，现实场景中的骨骼数据往往存在关节缺失，这极大影响了人类轨迹预测的准确性。目前缺乏对这种骨骼信息缺失鲁棒性的针对性解决方案。

Method: 作者设计了一种基于自监督预训练的骨骼表征模型，通过masked autoencoding方法学习骨骼特征，结合到轨迹预测模型中，以提升其对缺失关节数据的鲁棒性。

Result: 实验结果表明，该方法在易受遮挡影响的场景下对缺失骨骼数据具有更好的鲁棒性，并且在关节缺失从无到中等程度的条件下持续优于基线模型。

Conclusion: 自监督的骨骼表征能有效提升轨迹预测模型在骨骼数据不完整情形下的性能，兼顾准确率和鲁棒性，具有实际应用前景。

Abstract: Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.

</details>


### [31] [GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation](https://arxiv.org/abs/2602.22800)
*Hanliang Du,Zhangji Lu,Zewei Cai,Qijian Tang,Qifeng Yu,Xiaoli Liu*

Main category: cs.CV

TL;DR: 提出了一种新的大气湍流图像恢复框架GSTurb，将光流引导的像素位移校正与高斯点云分布相结合，在合成及真实湍流数据集上证明了其卓越性能。


<details>
  <summary>Details</summary>
Motivation: 大气湍流常导致远距离成像中的像素偏移和模糊，严重影响图像质量，因此需要更高效的方法进行图像恢复。

Method: GSTurb方法首先利用光流对湍流引起的像素位移（tilt）进行校正，然后用高斯点云分布模型对非等方模糊进行建模。该方法用高斯参数联合表示像素位移和模糊，并在多帧图像中进行联合优化，实现图像高质量恢复。

Result: 在ATSyn-static数据集上，GSTurb方法实现了27.67 dB的PSNR和0.8735的SSIM，分别比最新方法提升4.5%和5.8%；在TSRWGAN Real-World和CLEAR等真实数据集上同样表现优异，定量和定性评分均优于现有方法。

Conclusion: GSTurb将光流引导的位移校正与高斯点云模糊建模有机结合，可明显提升大气湍流条件下的图像恢复效果，具备很强的实际应用前景。

Abstract: Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.

</details>


### [32] [PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning](https://arxiv.org/abs/2602.22809)
*Mingde Yao,Zhiyuan You,Tam-King Man,Menglu Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: PhotoAgent提出了一种自动化的图片编辑系统，通过美学规划实现无需逐步指令即可高质量编辑图片，并在多个基准上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的图片编辑方法效果受限于用户书写复杂、高质量指令的能力，增加了用户负担，因此需要一种能自动理解和分解编辑任务的智能系统。

Method: PhotoAgent将图片编辑建模为长时决策问题，结合树搜索与响应式执行，能根据用户审美意图，自动规划多步编辑任务，并通过记忆机制和视觉反馈不断优化结果。系统引入了UGC-Edit美学评估基准及审美奖励模型，用于真实场景下评估编辑效果。

Result: PhotoAgent在包含7000张照片的美学评估基准和1017张测试集上进行了大量实验。结果显示，该系统在遵循用户指令和提升视觉质量方面优于以往的基线方法。

Conclusion: PhotoAgent为图片编辑提供了自动化、高质量的解决方案，减轻了用户负担，并为未来自动图片编辑提供了新的评测工具和方向。

Abstract: With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.

</details>


### [33] [Face Time Traveller : Travel Through Ages Without Losing Identity](https://arxiv.org/abs/2602.22819)
*Purbayan Kar,Ayush Ghadiya,Vishal Chudasama,Pankaj Wasnik,C. V. Jawahar*

Main category: cs.CV

TL;DR: 本文提出Face Time Traveller (FaceTT) 框架，通过扩散模型实现高质量、人脸身份保持的年龄变换，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人脸年龄变换方法过度依赖数值化年龄，忽略生物和环境等复杂线索；并且在大年龄跨度下难以保持身份，还有关注机制僵化和逆向干预过程繁琐等问题。

Method: 1. 构建基于扩散模型的FaceTT框架。2. 提出面部属性感知提示细化（Face-Attribute-Aware Prompt Refinement），融合生物与环境年龄信息，实现上下文感知。3. 提出零调优角度逆转（Tuning-free Angular Inversion），高效将真实人脸映射到扩散潜空间，便于准确重建。4. 自适应注意力机制动态平衡语义年龄信息及身份保持。

Result: 在多个公开数据集及实际场景中，FaceTT在身份保持、背景保留和年龄变化真实性等方面均优于现有最先进方法。

Conclusion: FaceTT为人脸年龄变换带来高保真、身份一致和细粒度变化控制，克服了传统方法的主要局限，具备重要应用价值。

Abstract: Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.

</details>


### [34] [Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification](https://arxiv.org/abs/2602.22829)
*G. A. S. L Ranasinghe,J. A. S. T. Jayakody,M. C. L. De Silva,G. Thilakarathne,G. M. R. I. Godaliyadda,H. M. V. R. Herath,M. P. B. Ekanayake,S. K. Navaratnarajah*

Main category: cs.CV

TL;DR: 本论文提出利用多光谱成像(MSI)和机器学习方法，精准、低成本地预测土壤成分和美国农业部(USDA)土壤质地类别，实现高效野外应用。


<details>
  <summary>Details</summary>
Motivation: 传统的土壤质地分析方法费时、费力且成本较高，不适合大规模、常规的田间应用，而现有的部分传感替代技术又过于粗糙或过于昂贵。

Method: 开发了一套涵盖365 nm至940 nm共十三个波段的自制多光谱成像仪，并结合回归与分类机器学习模型，分别预测土壤的黏土、粉土及砂粒含量（回归），同时直接分类USDA的十二种质地类别，也通过USDA质地三角图间接分类质地。

Result: 在由黏土、砂土和粉土不同配比组成的混合样本上测试，成分预测R^2最高可达0.99，质地分类准确率超过99%。

Conclusion: 多光谱成像配合数据驱动模型能够实现土壤质地的精准、无损、可野外部署的检测，为岩土工程筛查和精准农业应用提供了有效新工具。

Abstract: Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.

</details>


### [35] [A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling](https://arxiv.org/abs/2602.22843)
*Chong Wang,Yabin Zhang,Yunhe Gao,Maya Varma,Clemence Mottez,Faidra Patsatzi,Jiaming Liu,Jin Long,Jean-Benoit Delbrouck,Sergios Gatidis,Akshay S. Chaudhari,Curtis P. Langlotz*

Main category: cs.CV

TL;DR: 该论文提出通过主动、精细化的数据筛选提升医学影像基础模型的训练效率和效果，而不是单纯扩大数据规模。作者提出的CheXficient模型仅利用约22.7%的数据和27.3%的算力预算即可达到甚至超越完全体模型的表现。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像基础模型依赖于大规模数据集进行预训练，但这带来了数据冗余、类别失衡和计算资源浪费等问题。作者希望通过更有效的数据利用方式突破当前“无差别大规模扩展”的固有瓶颈。

Method: 提出CheXficient模型，在预训练阶段主动筛选和优先选择更有信息量和多样性的样本，减少对过度表示类别的依赖，从而降低算力消耗，并提升模型泛化能力。实验在123万余对胸片和报告的数据集上，仅选用其中约22.7%的样本进行训练。

Result: CheXficient在消耗远低于全量模型的算力和数据情况下，在20项涵盖5种任务类型的基准上表现与全数据模型和同类大型模型相当或更优，尤其在处理长尾和稀有疾病条件下表现更出色。

Conclusion: 主动和有原则的数据筛选显著提升了医学影像大模型预训练的效率与效果，为该类模型的高效适应和实际部署提供了实践性建议。

Abstract: Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a "scale-at-all-costs" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.

</details>


### [36] [From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models](https://arxiv.org/abs/2602.22859)
*Hongrui Jia,Chaoya Jiang,Shikun Zhang,Wei Ye*

Main category: cs.CV

TL;DR: 本文提出了一种新的大规模多模态模型（LMM）持续训练方法DPE（诊断驱动的渐进进化），通过诊断模型弱点，动态调整训练数据，并指导数据生成，实现模型的稳定持续提升。


<details>
  <summary>Details</summary>
Motivation: 现有LMM训练依赖静态数据和固定流程，难以发现模型盲点，缺乏动态、针对性的强化。最近研究显示，基于测试的错误暴露和反馈修正优于重复练习，因此需要更具适应性和靶向性的训练范式。

Method: 提出DPE方法，采用螺旋式循环流程：每轮先诊断模型弱点，由多智能体利用网络搜索、图像编辑等工具对大规模未标注数据进行注释和质控，生成多样真实的样本；再根据失败案例动态调整数据比例，并引导智能体针对弱点生成定向数据，以强化弱项。

Result: 在Qwen3-VL-8B-Instruct和Qwen2.5-VL-7B-Instruct模型上，DPE方法在11项基准测试中实现持续、稳定提升，显示其在开放任务分布下可扩展性和有效性。

Conclusion: DPE为大模型的持续训练提供了一种新的可扩展范式，通过诊断-生成-强化循环，能够长期动态提升多模态模型的能力。相关代码、模型和数据已公开。

Abstract: As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.

</details>


### [37] [SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation](https://arxiv.org/abs/2602.22867)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: 论文提出SO3UFormer，突破现有球面Transformer模型对重力对齐的依赖，实现对任意旋转的鲁棒全景语义分割，在强SO(3)扰动环境下大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有全景语义分割模型普遍假设输入是重力对齐的（即拍摄时相机与地面平行），但实际中由于手持、无人机等设备的任意旋转，输入往往存在较大偏离，导致模型在发生旋转时分割性能显著下降。因此需要设计对旋转变换更鲁棒的方法。

Method: 提出SO3UFormer，包括：（1）去除绝对纬度编码，学习不依赖重力矢量的球面内在特征；（2）一致性球面注意力机制，解决球面上采样密度不均；（3）局部角度几何感知的位置机制，依赖切平面角度和离散Gauge池化，突破全球坐标轴依赖。训练阶段引入索引为基础的球面重采样和logit级别的SO(3)一致性正则化。

Result: 构建Pose35数据集，将标准Stanford2D3D随机旋转±35°；实验证明在充分SO(3)旋转下，现有主流方法SphereUFormer的mIoU降至25.26，而SO3UFormer在Pose35和全面SO(3)旋转下mIoU分别达72.03和70.67，显著优于对比方法。

Conclusion: SO3UFormer极大提升球面语义分割对任意三维旋转的鲁棒性，解决因实际拍摄产生的严重姿态扰动带来的模型性能崩溃问题，未来可广泛应用于各类全景感知场景。

Abstract: Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\pm 35^\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.

</details>


### [38] [Towards Multimodal Domain Generalization with Few Labels](https://arxiv.org/abs/2602.22917)
*Hongzhao Li,Hao Dong,Hualei Wan,Shupan Li,Mingliang Xu,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一个新的任务——半监督多模态领域泛化（SSMDG），并设计了统一的解决框架，有效提升了多模态模型在新领域和数据稀缺场景下的泛化能力。作者同时建立了首个SSMDG基准并在此上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 多模态模型要能够泛化到未知领域并在标签稀缺时降低标注成本，现有方法无法同时处理无标注数据、多领域和多模态带来的挑战。

Method: 提出了三大关键模块：1）基于多模态一致性的伪标签生成；2）对分歧样本设计专门正则化；3）跨模态原型对齐，用于提升领域和模态不变性，并兼容部分模态缺失。

Result: 作者建立了首个SSMDG基准，并且实验证明该方法在完整模态和模态缺失情况下都优于强劲对比方法。

Conclusion: 本文方法有效提升了半监督多模态领域泛化能力，有望降低实际应用中的标注需求，并为未来类似研究提供基准和代码支持。

Abstract: Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.

</details>


### [39] [Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins](https://arxiv.org/abs/2602.22919)
*Haofan Wu,Nay Aung,Theodoros N. Arvanitis,Joao A. C. Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Chain of Flow（COF）的新型框架，可通过单次心电周期ECG数据重建患者个性化的4D心脏结构和运动，实现完整的心脏数字孪生。


<details>
  <summary>Details</summary>
Motivation: 现有的心脏数字孪生技术多用于特定任务的预测，无法实现可操控、可用于广泛下游仿真的虚拟患者全心脏数字模型。

Method: COF框架将心脏磁共振（cine-CMR）与12导联心电图（ECG）数据结合，在训练阶段学习心脏几何、心电生理与动力学的统一表征，能够从单次心電周期直接生成4D心脏结构与运动。

Result: COF在多个群体测试中表现优异，可精确恢复心脏解剖结构、分腔功能及动态运动模式。重建的4D心脏还能用于体积测量、区域功能分析、虚拟 cine 合成等多项下游CDT任务。

Conclusion: COF方法使得仅凭ECG即可实现完整的4D个体化心脏重建，促进数字孪生心脏从单一预测工具向可生成、可操控的临床实用虚拟心脏模型的转变。

Abstract: A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.

</details>


### [40] [OSDaR-AR: Enhancing Railway Perception Datasets via Multi-modal Augmented Reality](https://arxiv.org/abs/2602.22920)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.CV

TL;DR: 论文提出了一种多模态增强现实框架，用于在真实铁路视频中插入虚拟物体，通过高精度的数据融合，生成高质量可用于智能交通系统感知任务的数据集，并发布了新数据集OSDaR-AR。


<details>
  <summary>Details</summary>
Motivation: 深度学习提升了交通系统的感知能力，但铁路领域因缺乏高质量标注数据，特别是安全关键的障碍物检测任务受限。虽然模拟器能生成数据，但存在‘虚实差距’，传统的数据增强方法又难以保证场景的时空一致性和真实性。

Method: 首先利用Unreal Engine 5结合LiDAR点云和INS/GNSS等多模态数据，将虚拟物体精确嵌入真实的铁路RGB序列。其次，设计了基于分割的INS/GNSS数据优化策略，提升虚拟物体与场景的融合度和增强序列的真实感。

Result: 通过对比实验，提出的方法在增强序列的现实度和时空一致性上显著优于传统方法，同时生成了新的铁路增强现实数据集OSDaR-AR。实验结果验证了所提策略的有效性。

Conclusion: 该工作为铁路智能感知系统的训练提供了高质量合成数据，显著缓解了数据稀缺问题，并且提出的多模态增强方法对现实任务具有实际应用前景。

Abstract: Although deep learning has significantly advanced the perception capabilities of intelligent transportation systems, railway applications continue to suffer from a scarcity of high-quality, annotated data for safety-critical tasks like obstacle detection. While photorealistic simulators offer a solution, they often struggle with the ``sim-to-real" gap; conversely, simple image-masking techniques lack the spatio-temporal coherence required to obtain augmented single- and multi-frame scenes with the correct appearance and dimensions. This paper introduces a multi-modal augmented reality framework designed to bridge this gap by integrating photorealistic virtual objects into real-world railway sequences from the OSDaR23 dataset. Utilizing Unreal Engine 5 features, our pipeline leverages LiDAR point-clouds and INS/GNSS data to ensure accurate object placement and temporal stability across RGB frames. This paper also proposes a segmentation-based refinement strategy for INS/GNSS data to significantly improve the realism of the augmented sequences, as confirmed by the comparative study presented in the paper. Carefully designed augmented sequences are collected to produce OSDaR-AR, a public dataset designed to support the development of next-generation railway perception systems. The dataset is available at the following page: https://syndra.retis.santannapisa.it/osdarar.html

</details>


### [41] [MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2602.22932)
*Wenhui Tan,Xiaoyi Yu,Jiaze Li,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Ruihua Song,Jian Luan*

Main category: cs.CV

TL;DR: 本文提出了一种联合优化采样器与多模态大语言模型(MLLM)的新方法MSJoE，用于高效理解长时视频，通过只选出关键帧来提升问答任务表现，并收集了新数据集验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在理解长视频面临计算资源消耗大和关键信息稀疏的困境。文章认为，长视频问答中，真正有用的信息集中于少量关键帧，因此亟需高效的帧选择与信息聚合方法。

Method: 方法包括(1)基于问题生成一组多视角查询；(2)用这些查询与冻结的CLIP模型进行交互，得到查询-帧相似度矩阵；(3)基于该矩阵由轻量级采样器预测关键帧的采样权重；(4)选出少量关键帧输入MLLM生成答案。采样器和MLLM通过强化学习联合训练，实现采样、推理与理解的协同优化。

Result: 提出了包含2.8K长视频和7K问答对的新数据集，广泛实验表明MSJoE使基础MLLM准确率提升8.0%，比最强对比方法高1.1%。

Conclusion: MSJoE证明了通过联合进化采样和MLLM模型，高效选帧可大幅提升长视频理解与问答任务表现，为长视频多模态理解提供了新范式。

Abstract: Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\% accuracy gain upon the base MLLM, and 1.1\% higher accuracy than strongest baseline method.

</details>


### [42] [pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation](https://arxiv.org/abs/2602.22938)
*Shentong Mo,Xufang Luo,Dongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的高效参数微调方法pMoE，通过多域专家的Prompt token和可学习分派器动态结合不同领域知识，在多种视觉迁移任务上实现了更优性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Prompt tuning方法通常只利用单一预训练模型的知识，未能结合多领域知识的互补性，限制了模型在不同任务场景下的泛化能力。

Method: 作者提出了混合专家的Prompt tuning方法（pMoE），它在Prompt层引入专家专属的Prompt token，并采用可学习的分派机制，在任务适应阶段动态分配各专家的贡献。该方法融合了多领域（如通用与医学）专家知识，在统一框架下提升模型适用性。

Result: 在47个适应任务（涵盖分类与分割，涉及通用与医学领域）上的大量实验表明，pMoE在性能提升和效率之间实现了最佳平衡，并显著优于现有方法。

Conclusion: pMoE有效整合了多领域知识，提高了模型在广泛视觉任务上的适应能力和效率，对提升视觉模型的通用性和实际应用价值具有重要意义。

Abstract: Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.

</details>


### [43] [Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings](https://arxiv.org/abs/2602.22941)
*Julian Ziegler,Daniel Matthes,Finn Gerdts,Patrick Frenzel,Torsten Warnke,Matthias Englert,Tina Koevari,Mirco Fuchs*

Main category: cs.CV

TL;DR: 本文提出了一种通用的视频分析框架，可自动从各类皮划艇竞速视频中提取速度、船位和划频等关键性能指标，精度接近GPS标准，无需人工标注或船载传感器。


<details>
  <summary>Details</summary>
Motivation: GPS虽为皮划艇运动分析的黄金标准，但普及率有限，且船体安装硬件有实际挑战。因此，亟需开发能够自动处理比赛视频、并提供可靠性能数据的解决方案，辅助教练与运动员优化训练和比赛策略。

Method: 本方法基于YOLOv8实现浮标及运动员检测，并利用已知浮标分布进行单应性估计；通过U-net模型校准运动员与船头间偏移，泛化了船位估算方式。结合光流跟踪增强多运动员艇型下的追踪鲁棒性。划频提取则可通过姿态估计或包围框信息实现。

Result: 与精英比赛的GPS数据对比，速度和划频提取的均方根相对误差（RRMSE）分别为0.020±0.011（相关系数0.956）和0.022±0.024（相关系数0.932），解决方案精度接近GPS。

Conclusion: 该自动化视频分析方法在不依赖昂贵硬件和人工标注的情况下，能为皮划艇教练和运动员提供准确、即时的运动表现反馈，有助于优化训练与比赛策略。

Abstract: Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.

</details>


### [44] [Cross-Task Benchmarking of CNN Architectures](https://arxiv.org/abs/2602.22945)
*Kamal Sherawat,Vikrant Bhati*

Main category: cs.CV

TL;DR: 本文比较了多种动态卷积神经网络（CNN）变体在图像分类、分割和时间序列分析任务中的表现，发现注意力机制和动态卷积方法在准确率和计算效率上优于传统CNN，其中ODConv对复杂图形效果最优。


<details>
  <summary>Details</summary>
Motivation: 深入探究不同类型的动态卷积和注意力机制如何提升CNN在多任务上的表现，为未来神经网络体系结构的设计提供参考。

Method: 基于ResNet-18架构，比较了五种CNN变体：普通CNN、基于硬注意力和软注意力（局部与全局）CNN，以及全向ODConv。通过在Tiny ImageNet、Pascal VOC和UCR时间序列数据集上实验，量化不同网络方法的表现。

Result: 实验结果表明，包含注意力机制和动态卷积的CNN变体在准确性、效率与计算性能上均优于传统CNN。ODConv在处理形态复杂的图像时表现尤为突出，各动态CNN能够通过自适应卷积核调节提升特征表示和任务泛化能力。

Conclusion: 动态CNN（特别是有注意力机制及ODConv）对不同类型数据表现更优，显示出在多模态数据和任务泛化上的潜力，为神经网络工程和结构设计指明了新方向。

Abstract: This project provides a comparative study of dynamic convolutional neural networks (CNNs) for various tasks, including image classification, segmentation, and time series analysis. Based on the ResNet-18 architecture, we compare five variants of CNNs: the vanilla CNN, the hard attention-based CNN, the soft attention-based CNN with local (pixel-wise) and global (image-wise) feature attention, and the omni-directional CNN (ODConv). Experiments on Tiny ImageNet, Pascal VOC, and the UCR Time Series Classification Archive illustrate that attention mechanisms and dynamic convolution methods consistently exceed conventional CNNs in accuracy, efficiency, and computational performance. ODConv was especially effective on morphologically complex images by being able to dynamically adjust to varying spatial patterns. Dynamic CNNs enhanced feature representation and cross-task generalization through adaptive kernel modulation. This project provides perspectives on advanced CNN design architecture for multiplexed data modalities and indicates promising directions in neural network engineering.

</details>


### [45] [ToProVAR: Efficient Visual Autoregressive Modeling via Tri-Dimensional Entropy-Aware Semantic Analysis and Sparsity Optimization](https://arxiv.org/abs/2602.22948)
*Jiayu Chen,Ruoyu Lin,Zihao Zheng,Jingxin Li,Maoliang Li,Guojie Luo,Xiang chen*

Main category: cs.CV

TL;DR: 提出了一种基于注意力熵的新优化框架ToProVAR，大幅提升视觉自回归(VAR)模型的生成速度，同时兼顾高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有VAR模型如FastVAR和SkipVAR在后期生成阶段效率极低，成为发展瓶颈，亟需新方法提高生成速度且不牺牲质量。

Method: 提出关注于注意力熵，通过其衡量模型语义投影，深入分析token、层、尺度三维稀疏性，并据此设计细粒度优化策略，从根本上提升生成效率。

Result: 在Infinity-2B和Infinity-8B上测试，ToProVAR实现最高3.4倍加速，且生成效果几乎无损，效率与质量均优于传统跳步方法。

Conclusion: ToProVAR为视觉自回归模型提供了一种新颖高效的优化思路，显著缓解了效率与质量难以兼顾的难题，具有较强应用前景。

Abstract: Visual Autoregressive(VAR) models enhance generation quality but face a critical efficiency bottleneck in later stages. In this paper, we present a novel optimization framework for VAR models that fundamentally differs from prior approaches such as FastVAR and SkipVAR. Instead of relying on heuristic skipping strategies, our method leverages attention entropy to characterize the semantic projections across different dimensions of the model architecture. This enables precise identification of parameter dynamics under varying token granularity levels, semantic scopes, and generation scales. Building on this analysis, we further uncover sparsity patterns along three critical dimensions-token, layer, and scale-and propose a set of fine-grained optimization strategies tailored to these patterns. Extensive evaluation demonstrates that our approach achieves aggressive acceleration of the generation process while significantly preserving semantic fidelity and fine details, outperforming traditional methods in both efficiency and quality. Experiments on Infinity-2B and Infinity-8B models demonstrate that ToProVAR achieves up to 3.4x acceleration with minimal quality loss, effectively mitigating the issues found in prior work. Our code will be made publicly available.

</details>


### [46] [OpenFS: Multi-Hand-Capable Fingerspelling Recognition with Implicit Signing-Hand Detection and Frame-Wise Letter-Conditioned Synthesis](https://arxiv.org/abs/2602.22949)
*Junuk Cha,Jihyeon Kim,Han-Mu Park*

Main category: cs.CV

TL;DR: 本文提出了OpenFS，一种用于手语指字识别与生成的开源方法，通过无需显式判断签字手、引入新的损失函数及数据生成器，大幅提升了手语指字识别的准确性和应用能力。


<details>
  <summary>Details</summary>
Motivation: 自动手语指字识别有助于缩小聋人和听人之间的沟通鸿沟，但目前存在签字手不明、损失函数不合适及词表外词(OOV)难题，现有方法难以有效应对实际复杂场景。

Method: 提出支持单手和多手输入的手语指字识别器，结合双层位置编码与“签字手关注损失”(SF loss)实现隐式签字手检测，并用单调对齐损失(MA loss)替代传统CTC损失，强化字母顺序与手势时序的对齐。此外，设计帧级字母条件生成器，能为OOV词合成逼真手语数据，建立新基准FSNeo。

Result: 实验表明，OpenFS在手语指字识别任务中达到了最先进性能，提出的识别器和生成器效果显著优于现有方法。

Conclusion: OpenFS有效解决了手语指字识别中的多项核心挑战，并为OOV词生成和基准测试带来了新工具，有望促进手语识别技术的实用化和研究进展。

Abstract: Fingerspelling is a component of sign languages in which words are spelled out letter by letter using specific hand poses. Automatic fingerspelling recognition plays a crucial role in bridging the communication gap between Deaf and hearing communities, yet it remains challenging due to the signing-hand ambiguity issue, the lack of appropriate training losses, and the out-of-vocabulary (OOV) problem. Prior fingerspelling recognition methods rely on explicit signing-hand detection, which often leads to recognition failures, and on a connectionist temporal classification (CTC) loss, which exhibits the peaky behavior problem. To address these issues, we develop OpenFS, an open-source approach for fingerspelling recognition and synthesis. We propose a multi-hand-capable fingerspelling recognizer that supports both single- and multi-hand inputs and performs implicit signing-hand detection by incorporating a dual-level positional encoding and a signing-hand focus (SF) loss. The SF loss encourages cross-attention to focus on the signing hand, enabling implicit signing-hand detection during recognition. Furthermore, without relying on the CTC loss, we introduce a monotonic alignment (MA) loss that enforces the output letter sequence to follow the temporal order of the input pose sequence through cross-attention regularization. In addition, we propose a frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words. This generator enables the construction of a new synthetic benchmark, called FSNeo. Through comprehensive experiments, we demonstrate that our approach achieves state-of-the-art performance in recognition and validate the effectiveness of the proposed recognizer and generator. Codes and data are available in: https://github.com/JunukCha/OpenFS.

</details>


### [47] [MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis](https://arxiv.org/abs/2602.22955)
*Feng Guo,Jiaxiang Liu,Yang Li,Qianqian Shi,Mingkun Xu*

Main category: cs.CV

TL;DR: 作者提出了MM-NeuroOnco，这是一个大规模、多模态、有丰富语义标注的脑肿瘤MRI诊断数据集，并建立了新的评测基准和模型，显著提升了多模态诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有公开脑肿瘤数据集在标注丰富性和诊断语义方面严重不足，难以支持模型生成更具临床解释性的推理。

Method: 1. 构建MM-NeuroOnco数据集，包含24,726张MRI切片和约20万个多模态指令，涵盖多种肿瘤亚型与成像模态。
2. 开发多模型协作的自动语义补全与质控流程，低成本生成高质量诊断语义标注。
3. 建立MM-NeuroOnco-Bench基准，引入人工标注和拒绝意识设置，提升评测公正性。
4. 基于新数据集提出NeuroOnco-GPT，对诊断任务进行微调。

Result: 在10个主流模型测评下，即便最强基线（Gemini 3 Flash）诊断相关问题的准确率仅为41.88%。微调后NeuroOnco-GPT在诊断问题上较基线模型提升了27个百分点。

Conclusion: MM-NeuroOnco数据集和基准能显著促进多模态、临床语义驱动的脑肿瘤诊断研究，提供了新的发展方向和挑战。

Abstract: Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco

</details>


### [48] [Can Agents Distinguish Visually Hard-to-Separate Diseases in a Zero-Shot Setting? A Pilot Study](https://arxiv.org/abs/2602.22959)
*Zihao Zhao,Frederik Hauke,Juliana De Castilhos,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型（MLLMs）在医学影像中零样本区分难以区分疾病的能力，并提出了一种对比判决多智能体框架，在两类医学诊断任务中取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前医学影像的大部分研究集中在自动化常规流程上，较少关注在没有临床上下文的信息下，区分表面特征极为相似而临床处理完全不同的疾病。针对这一难题，本文希望评估智能体在这种“零样本、难判别”场景下的能力，探索其潜力。

Method: 构建了基于多智能体的对比判决（contrastive adjudication）框架，并在两个极具挑战性的影像诊断任务（黑色素瘤vs.非典型痣、肺水肿vs.肺炎）上，对常见多模态大模型智能体性能进行基准评测。

Result: 实验表明，该多智能体框架在皮肤镜数据上的诊断准确率提高了11个百分点，并且在质性样本中的无依据结论有所减少。但总体性能仍未达到临床应用标准。

Conclusion: 尽管方法在受控环境下有所提升，但受限于人为标注不确定性和临床信息缺失，实际应用仍有很大距离。本研究为理解高混淆场景下智能体无监督诊断能力提供了初步见解。

Abstract: The rapid progress of multimodal large language models (MLLMs) has led to increasing interest in agent-based systems. While most prior work in medical imaging concentrates on automating routine clinical workflows, we study an underexplored yet clinically significant setting: distinguishing visually hard-to-separate diseases in a zero-shot setting. We benchmark representative agents on two imaging-only proxy diagnostic tasks, (1) melanoma vs. atypical nevus and (2) pulmonary edema vs. pneumonia, where visual features are highly confounded despite substantial differences in clinical management. We introduce a multi-agent framework based on contrastive adjudication. Experimental results show improved diagnostic performance (an 11-percentage-point gain in accuracy on dermoscopy data) and reduced unsupported claims on qualitative samples, although overall performance remains insufficient for clinical deployment. We acknowledge the inherent uncertainty in human annotations and the absence of clinical context, which further limit the translation to real-world settings. Within this controlled setting, this pilot study provides preliminary insights into zero-shot agent performance in visually confounded scenarios.

</details>


### [49] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: 本文提出了UCM框架，通过时间感知的位置编码和高效扩散Transformer，实现视频生成中长期一致性与精确相机控制的统一，大幅提升交互环境的模拟效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于视频生成的世界模型存在两个难点：1）场景多次访问时保持长期内容一致性；2）用户输入实现精准相机控制。现有3D重建法灵活性差，基于历史帧的方法控制力和一致性有限。

Method: 作者提出UCM框架，将长期记忆与相机控制统一于时间感知位置编码扭曲机制中，并设计高效的双流扩散Transformer用于高质量视频生成，同时采用基于点云渲染的数据扩增方法以增强对场景重访的训练。

Result: 在500K单目视频数据上训练，并在真实和合成基准测试中，UCM在长期场景一致性和相机可控性上显著优于现有方法，生成视频质量也更高。

Conclusion: UCM有效解决了世界模型中场景长期一致性和相机可控性的难题，为可控高一致性的视频生成提供了新方案，推动交互式环境仿真发展。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [50] [SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling](https://arxiv.org/abs/2602.23013)
*Camile Lendering,Erkut Akdag,Egor Bondarev*

Main category: cs.CV

TL;DR: 本文提出了一个无需训练的工业视觉异常检测方法SubspaceAD，只需少量正常图片通过主成分分析识别异常，且性能超过当前SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的工业视觉异常检测依赖复杂的后处理方式（如记忆库、辅助数据集、多模态模型调整等），即使在有强大的基础模型特征时也如此。因此，作者想检验在现代视觉基础模型下，是否可以用更简单方法达到甚至超越SOTA效果。

Method: SubspaceAD方法分两步：1）用冻结的DINOv2主干提取少量正常样本的patch特征；2）用PCA建模这些特征，获得正常变异的低维子空间。推理时通过重建残差判断异常，量化得分直观且有统计依据。

Result: 在MVTec-AD和VisA数据集上，无需训练、无提示调优和记忆库，SubspaceAD在one-shot和few-shot异常检测任务都优于已有方法（例如在MVTec-AD上图像/像素级AUROC分别为98.0%和97.6%）。

Conclusion: SubspaceAD以极简、训练自由的方式充分利用视觉基础模型特征，可解释且高效，显著优于当前复杂方法，为工业检测提供了新选择。

Abstract: Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.

</details>


### [51] [DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis](https://arxiv.org/abs/2602.23022)
*Xinglong Luo,Ao Luo,Zhengning Wang,Yueqi Yang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的图像对齐新框架DMAligner，通过生成特定视图实现更高质量、更鲁棒的图像对齐，明显优于传统的光流方法。


<details>
  <summary>Details</summary>
Motivation: 现有光流法容易受遮挡和光照变化影响，导致对齐质量下降，影响下游任务，因此需要更加鲁棒的新方法。

Method: 1）提出DMAligner扩散模型框架，通过生成式学习实现对齐导向的视图合成；2）引入动态感知掩码生成（DMP）模块，区分动态前景和静态背景，提升对复杂场景的适应能力；3）构建了新的DSIA数据集，包含大量室内外配对图像用于对齐任务。

Result: 在新构建的DSIA数据集及常用视频数据集上，DMAligner在定量与定性实验中表现优于现有方法。

Conclusion: DMAligner有效解决了光流法的局限性，在复杂场景下依然保持高对齐质量，为图像对齐任务提供了新的方向。

Abstract: Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.

</details>


### [52] [WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.23029)
*Tianyue Wang,Leigang Qu,Tianyu Yang,Xiangzhao Hao,Yifan Xu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: WISER提出了一种无需训练、融合T2I和I2I的多阶段ZS-CIR检索框架，通过“检索-验证-精炼”管线显著提升了多模态复合检索性能。


<details>
  <summary>Details</summary>
Motivation: ZS-CIR作为一种无需人工三元组标注的新型检索任务，现有方法面临T2I细节损失与I2I语义改动受限的困境，缺乏能兼顾多检索意图与复杂语义精确匹配的高效方法。

Method: 提出训练自由的WISER框架，结合编辑后的文本和图像进行广泛并行检索，并利用检索置信度自适应融合双路径，对不确定样本采用自反思生成建议持续精炼，显式建模意图与不确定性。

Result: 在CIRCO与CIRR等数据集上，WISER无训练下性能超越同类方法，mAP@5提升45%，Recall@1提升57%，甚至优于部分有训练的方案。

Conclusion: WISER凭借其创新性检索-验证-精炼流程，有效融合T2I和I2I优点，实现了更通用且高效的零样本复合检索，展现出极强的泛化性。

Abstract: Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a "retrieve-verify-refine" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.

</details>


### [53] [Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images](https://arxiv.org/abs/2602.23031)
*Zhangjian Ji,Huijia Yan,Shaotong Qiao,Kai Feng,Wei Wei*

Main category: cs.CV

TL;DR: 本文提出了一种基于空间拉普拉斯金字塔注意力和多尺度特征增强的小目标检测算法，在高分辨率遥感图像中实现优于传统方法的小目标检测效果。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的目标通常体积小、分布密集且不均匀，现有检测方法在高分辨率图像下难以高效检测小目标，亟需提升小目标的检测能力。

Method: 1）在ResNet-50各阶段引入空间拉普拉斯金字塔注意力（SLPA）模块，强化对重要局部区域的关注；2）在FPN的C5层侧路添加多尺度特征增强模块（MSFEM），丰富特征语义表达；3）在FPN特征融合时采用可变形卷积对齐上下层特征，提升特征表达质量。

Result: 在VisDrone和DOTA两个小目标检测基准数据集上，所提改进模型在小目标检测准确率上均优于原始算法，取得更优性能。

Conclusion: 通过SLPA、MSFEM以及特征对齐策略，本文方法有效提升了遥感图像中小目标的检测能力。

Abstract: Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model's semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model's ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.

</details>


### [54] [PackUV: Packed Gaussian UV Maps for 4D Volumetric Video](https://arxiv.org/abs/2602.23040)
*Aashish Rai,Angela Xing,Anushka Agarwal,Xiaoyan Cong,Zekun Li,Tao Lu,Aayush Prakash,Srinath Sridhar*

Main category: cs.CV

TL;DR: 本文提出了一种新型的4D高斯表示和处理方法PackUV，实现了高质量体积视频的高效重建、存储和流式传输，同时兼容传统视频编码，显著提升时序一致性和大规模场景处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的方法虽然重建质量高，但在长序列、时序一致性、剧烈运动和消隐情况下效果不佳，且输出难以与主流视频编码系统兼容，限制了体积视频的实际应用和大规模部署。

Method: 作者提出PackUV，利用多尺度UV atlas将所有高斯参数映射为结构化、紧凑的图像原生格式，配合PackUV-GS方法在UV域直接优化参数。通过流引导高斯标记和视频关键帧模块区分动态与静态高斯，实现大运动和消隐下的时序一致性。最终的UV atlas格式可直接与标准视频编解码（如FFV1）兼容。

Result: 实验基于新构建的PackUV-2B大规模数据集（50台同步摄像头，100段序列，20亿帧），结果显示PackUV在渲染保真度上显著优于现有方法，并能稳定处理长达30分钟的序列，保持高质量输出。

Conclusion: PackUV首次实现高质量4D体积视频在传统视频编码和流媒体体系下的无缝融合，大幅提升长时序、大场景体积视频的重建、存储与传输能力，为体积视频实用化铺平道路。

Abstract: Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications.
  We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure.
  To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.

</details>


### [55] [D-FINE-seg: Object Detection and Instance Segmentation Framework with multi-backend deployment](https://arxiv.org/abs/2602.23043)
*Argo Saakyan,Dmitry Solntsev*

Main category: cs.CV

TL;DR: 本文提出了基于D-FINE的实例分割方法D-FINE-seg，通过加入高效的掩码头、分割相关训练以及辅助掩码监督，实现了在保持较低延迟的同时提升实例分割准确率。此外，还构建了相关的模型端到端训练及推理全流程，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 虽然基于Transformer的实时目标检测已取得较好表现，但用于实例分割的相关研究相对较少。现有方法难以统一兼顾速度和分割精度，因此作者希望实现一个能兼顾实时性和实例分割性能的架构。

Method: 在D-FINE检测器基础上，提出了D-FINE-seg，通过增加轻量级掩码头、引入兼顾分割的训练策略（如box cropped BCE、dice mask损失、多任务和去噪掩码监督），并调整匈牙利匹配代价函数。同时，开发了支持ONNX、TensorRT及OpenVINO等端到端训练与优化推理的完整框架。

Result: 在TACO数据集上，D-FINE-seg在TensorRT FP16端到端推理下显著提升了F1分数，并保持了与现有SOTA模型Ultralytics YOLO26相当的推理延迟。

Conclusion: D-FINE-seg同时兼顾实例分割精度和实时性，适合工业实际应用，其端到端的训练推理流程和代码已全部开源，为相关领域提供了可复现、高效的解决方案。

Abstract: Transformer-based real-time object detectors achieve strong accuracy-latency trade-offs, and D-FINE is among the top-performing recent architectures. However, real-time instance segmentation with transformers is still less common. We present D-FINE-seg, an instance segmentation extension of D-FINE that adds: a lightweight mask head, segmentation-aware training, including box cropped BCE and dice mask losses, auxiliary and denoising mask supervision, and adapted Hungarian matching cost. On the TACO dataset, D-FINE-seg improves F1-score over Ultralytics YOLO26 under a unified TensorRT FP16 end-to-end benchmarking protocol, while maintaining competitive latency. Second contribution is an end-to-end pipeline for training, exporting, and optimized inference across ONNX, TensorRT, OpenVINO for both object detection and instance segmentation tasks. This framework is released as open-source under the Apache-2.0 license. GitHub repository - https://github.com/ArgoHA/D-FINE-seg.

</details>


### [56] [Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception](https://arxiv.org/abs/2602.23069)
*Yiding Sun,Jihua Zhu,Haozhe Cheng,Chaoyi Lu,Zhichuan Yang,Lin Chen,Yaonan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的点云视频（4D 点云）模型迁移范式PointATA，提升了3D到4D感知任务的迁移能力，有效缓解了4D数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 点云视频（4D）数据稀缺，严重限制了自监督4D模型的可拓展性。现有的从3D到4D模型迁移在实际中面临过拟合和模态差异（modality gap）两个关键挑战。

Method: 提出“Align then Adapt”（先对齐再适应，PointATA）两阶段迁移范式。第一阶段用最优传输理论度量并缓解3D/4D模态差异，训练point align embedder对齐特征分布；第二阶段通过point-video adapter和空间上下文编码器增强时序建模，在冻结3D骨干的情况下避免过拟合。整体可高效利用3D预训练模型进行4D任务。

Result: PointATA显著提升了参数效率与性能，在多个任务上达到或超越全面微调模型：3D动作识别97.21%准确率，4D动作分割提升8.7%，4D语义分割84.06%。

Conclusion: PointATA有效缓解了4D数据稀缺带来的问题，实现了参数高效的3D到4D知识迁移，在多项点云视频理解任务中表现优异。

Abstract: Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel "Align then Adapt" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \% accuracy on 3D action recognition, $+8.7 \%$ on 4 D action segmentation, and 84.06\% on 4D semantic segmentation.

</details>


### [57] [Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy](https://arxiv.org/abs/2602.23088)
*Matthew Sutton,Katrin Amunts,Timo Dickscheid,Christian Schiffer*

Main category: cs.CV

TL;DR: 本论文提出了一种无需成对图像-文本数据，通过标签关联图像与文本的方法，实现对脑组织切片显微图像的自然语言描述。


<details>
  <summary>Details</summary>
Motivation: 在医学和科研领域，特别是在脑组织细胞结构分析中，成对的图像-文本数据获取困难，阻碍了视觉基础模型与自然语言界面的结合。

Method: 作者提出了一种标签介导的方法，首先通过标签自动从相关文献中挖掘区域描述，将其作为合成的图像说明，并利用现有的视觉基础模型（CytoNet）与大语言模型结合，实现图像到文本的训练。

Result: 该方法针对57个脑区生成了可信的区域描述，并具备对未见区域的拒绝能力。对于已知标签区域，匹配准确率达90.6%；在标签隐藏的情况下，描述仍能以68.6%的准确率区分8个不同区域。

Conclusion: 弱标签介导的方法能够有效地将现有生物医学视觉模型与语言结合，为缺乏细致图文标注场景提供了一种实用的自然语言集成方案。

Abstract: Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.

</details>


### [58] [Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras](https://arxiv.org/abs/2602.23101)
*Paul Kielty,Timothy Hanley,Peter Corcoran*

Main category: cs.CV

TL;DR: 本文提出了一种名为LADS（Locally Adaptive Decay Surfaces）的新型事件相机表示方法，通过根据局部信号动态自适应调节时域衰减，有效提升了面部检测和特征点定位的精度和速度，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机的稀疏、异步输出不易直接用于神经网络。现有全局统一参数的时间表面表示法存在空间结构保留与运动模糊之间的权衡，难以同时兼顾静止区域细节与快速运动下的边缘清晰。本文旨在解决此瓶颈，实现更高精度和效率。

Method: 提出LADS方法，在事件相机生成的图像每个位置处，根据局部事件率、Laplacian-of-Gaussian响应、高频谱能量三种策略，动态自适应地调整时域衰减参数。通过在代表性公共数据集上，进行面部检测和特征点提取实验，与传统非自适应方法进行对比。

Result: LADS在面部检测和特征点定位任务下，相比传统方法，在30Hz情况下获得更高检测准确率和更低定位误差；在240Hz时减缓了高频率下的性能下降，表现优于现有30Hz条件下的最佳结果（如地标定位的标准化均值误差2.44%，人脸检测mAP50 0.966）。此外，LADS支持更轻量的网络，实现实时处理。

Conclusion: LADS证明了上下文相关时域整合对于类脑视觉系统的重要性，在高频实时人机交互应用中展现出极大潜力，为事件相机的实际应用与后续研究设定了新基准。

Abstract: Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.

</details>


### [59] [SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation](https://arxiv.org/abs/2602.23103)
*Fuhao Zhang,Lei Liu,Jialin Zhang,Ya-Nan Zhang,Nan Mu*

Main category: cs.CV

TL;DR: 本文提出SpectralMamba-UNet，将频域分解与Mamba模型结合，实现结构和纹理信息的有效分离与融合，提升了医学图像分割效果。


<details>
  <summary>Details</summary>
Motivation: 目前的医学图像分割亟需兼顾全局结构建模与细粒度边界细节，但现有Mamba类模型一维序列化导致局部空间连续性及高频细节捕捉能力不足。

Method: 提出Spectral Decomposition and Modeling（SDM）模块，采用离散余弦变换将特征分解为低频（结构、全局建模）与高频（边界、细节），结合频域Mamba及高频信息处理。同时引入频谱通道重加权（SCR）机制和频谱引导融合（SGF）模块以实现多尺度、频域感知的特征融合。

Result: 在五个公开医学图像分割基准测试上，不同模态与分割目标的任务中均取得了持续、显著提升，表现出较好的有效性与泛化能力。

Conclusion: SpectralMamba-UNet通过频域分解及有效融合，显著提升了医学图像分割性能，为结构和纹理信息的解耦建模提供了一种新范式。

Abstract: Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.

</details>


### [60] [WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2602.23114)
*Xudong Yan,Songhe Feng,Jiaxin Wang,Xin Su,Yi Jin*

Main category: cs.CV

TL;DR: 提出了一种新的方法，通过利用无监督数据的多模态知识和动态原型更新，解决组合零样本学习（CZSL）测试时标签空间分布转变的问题，并在四个基准数据集上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有CZSL方法在遇到未见属性-对象组合时通常性能明显下降，主要是由于测试集的标签空间发生了分布变化。研究者希望提升模型对这种分布转变的适应能力，避免性能损失。

Method: 1. 利用无监督数据，提取文本和视觉的多模态知识，并在测试时动态更新原型。2. 引入自适应原型更新权重，根据变化程度灵活调整。3. 设置动态优先队列，保存置信度高的样本，便于历时视觉原型推断。4. 采用多模态协同学习，保障视觉和文本原型的语义一致。5. 为更可靠评测，推出了新数据集C-Fashion，并清洗了MIT-States。

Result: 该方法在四个基准数据集（包含新提出和优化的）下，无论封闭世界还是开放世界设定，均超越了已有方法，达到最新最优水平。

Conclusion: 通过测试阶段的多模态原型动态更新和协同学习，有效减缓了因标签空间分布转变导致的性能损失，使CZSL模型在实际应用中更具鲁棒性。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .

</details>


### [61] [Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation](https://arxiv.org/abs/2602.23117)
*Xiaosen Wang,Zhijin Ge,Bohan Liu,Zheng Fang,Fengfan Zhou,Ruixuan Zhang,Shaokang Wang,Yuyang Luo*

Main category: cs.CV

TL;DR: 本文聚焦于对抗样本在不同模型间的迁移攻击，指出当前缺乏统一评测标准，系统梳理了迁移攻击方法并提出了综合评测框架。


<details>
  <summary>Details</summary>
Motivation: 迁移性对抗攻击可在无需目标模型信息的情况下威胁模型安全，但相关研究评估标准不统一，影响了方法的公平比较和发展。

Method: 系统回顾数百篇文献，将迁移攻击方法分为六类，并构建了统一的评测框架。此外，总结了提升对抗迁移性的常用策略，并梳理了导致不公平比较的主要问题。

Result: 提出了细致的攻击方法分类、统一的评测框架，以及迁移性提升策略，较为全面地归纳了现有成果，并指出了研究与评测中的共性问题。

Conclusion: 该工作规范了迁移性对抗攻击的评测体系，有助于推动领域研究的规范化和进步，对后续深入研究和应用有显著指导价值。

Abstract: Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.

</details>


### [62] [TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement](https://arxiv.org/abs/2602.23120)
*Arian Sabaghi,José Oramas*

Main category: cs.CV

TL;DR: TriLite是一种只用极少可训练参数的单阶段WSOL方法，通过冻结自监督预训练ViT主干和TriHead模块，在大幅提升性能的同时显著降低训练难度与计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管弱监督目标定位（WSOL）取得一定进展，现有方法多数依赖多阶段流水线或需要大规模backbone的完整微调，导致训练成本高。此外，普遍面临目标覆盖不全的问题。本文希望在不增加额外训练负担的前提下，提升WSOL对象覆盖能力。

Method: 提出TriLite框架：1）采用Dinov2自监督预训练的ViT主干并完全冻结参数，2）新增不到80万可训练参数用于分类和定位，3）设计TriHead模块将特征分解为前景、背景和模糊区域，优化对象覆盖，并将分类与定位目标解耦，提升泛化能力。

Result: 在CUB-200-2011、ImageNet-1K和OpenImages等数据集上，TriLite不仅在定位效果上刷新了当前最佳表现，而且参数更少，训练更简单高效。

Conclusion: TriLite为WSOL提供了一种高效、易训练且参数极为精简的新范式，有助于解决WSOL覆盖不全与训练开销高的问题。

Abstract: Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.

</details>


### [63] [From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification](https://arxiv.org/abs/2602.23133)
*Xin Yuan,Zhiyong Zhang,Xin Xu,Zheng Wang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为 CARE 的两阶段行人重识别方法，通过概率证据校准和传播，有效提升了含噪声标签和样本稀少场景下的鲁棒性，并在多个数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法在处理噪声标签和身份样本稀少的环境下表现不佳，主要原因是传统的 softmax 策略对噪声标签过度自信、以及样本筛选方法易误丢重要样本。本文动机是解决这些关键缺陷，提升模型鲁棒性。

Method: 提出两阶段的 CARE 框架，包括校准阶段和优化阶段。校准阶段利用概率证据校准(PEC)引入可学习参数破解 softmax 的平移不变性，并运用证据校准损失抑制对错标签的过度自信。优化阶段通过新设的复合角度边界(CAM)精确区分难学正样本与噪声样本，并利用基于置信度的球面加权(COSW)动态分配样本重要性，优先用干净样本更新模型。

Result: 在 Market1501、DukeMTMC-ReID、CUHK03 等主流数据集上，在随机及模式化噪声下，CARE 方法均获得了有竞争力的性能。

Conclusion: CARE 框架通过创新性的证据校准与传播机制，显著提升了面对噪声数据和难样本时的辨识鲁棒性，为实际复杂环境下的行人重识别提供了有效解决方案。

Abstract: With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.

</details>


### [64] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出了一种新的无监督在线视频防抖方法，无需配对数据集，通过多线程缓冲机制提升效率，并引入了新的无人机夜间遥感视频数据集，实验效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的视频防抖方法依赖配对稳定/不稳定数据集，且适用场景有限，硬件要求高，难以应用于如无人机夜间遥感等实际复杂领域，因此亟需更通用、高效的数据无关方法。

Method: 该方法采用经典防抖流程的三阶段架构，并结合多线程缓冲机制，无需配对数据集进行无监督训练。同时，构建并发布了针对多模态无人机航拍的新数据集（UAV-Test），支持夜间等复杂场景下的防抖评测。

Result: 实验结果显示，该方法在定量指标和视觉质量上均优于主流在线视频防抖器，且与离线方法性能相当，扩展了夜间遥感等新场景的适用性。

Conclusion: 所提无监督在线视频防抖框架不仅有效缓解了数据和硬件资源的限制，还提升了泛化能力和实际应用价值，在无人机夜间遥感等领域展现出广阔的应用前景。

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [65] [Efficient Encoder-Free Fourier-based 3D Large Multimodal Model](https://arxiv.org/abs/2602.23153)
*Guofeng Mei,Wei Lin,Luigi Riz,Yujiao Wu,Yiming Wang,Fabio Poiesi*

Main category: cs.CV

TL;DR: Fase3D是一种无需繁重编码器的高效3D多模态模型，利用傅里叶变换与序列化技术实现无需编码器的点云建模，性能与主流方案相当但更高效。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态模型依赖大型视觉编码器，计算与扩展成本高。最近2D领域逐步摆脱编码器，但3D数据的无序和点云大规模特性让无编码器方法落地极具挑战。主要问题是如何无需复杂编码器，高效将无序3D点云转为适合LMM处理的tokens。

Method: 提出Fase3D，引入三大技术创新：1）用结构化superpoints轻量表征大场景；2）采用填充曲线对点云序列化，再结合FFT高效建模全局上下文，同时通过图结构融合token；3）用基于傅里叶变换的LoRA适配器，在极小计算开销下引入频率感知交互。此方法整体去除了传统重量编码器，全流程更加高效。

Result: Fase3D在性能上达到与主流基于编码器的3D LMMs相当的水准，但计算和参数大幅度减少，效率显著提升。

Conclusion: Fase3D开创性实现了无需编码器的3D点云LMM，为3D视觉高效处理带来新范式，有望推动大型多模态模型在3D领域的可扩展性和实用性。

Abstract: Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.

</details>


### [66] [DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation](https://arxiv.org/abs/2602.23165)
*Yichen Peng,Jyun-Ting Song,Siyeol Jung,Ruofan Liu,Haiyang Liu,Xuangeng Chu,Ruicong Liu,Erwin Wu,Hideki Koike,Kris Kitani*

Main category: cs.CV

TL;DR: 提出了一种名为DyaDiT的多模态扩散Transformer模型，可根据对话音频生成更自然、符合社交情境的人体动作，并在主观和客观指标上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于音频的手势生成方法通常只关注单一说话人的动作，忽略了对话中的社交动态和互动情境，导致生成的动作缺乏自然性和互动性。

Method: 本方法提出DyaDiT，将双人（dyadic）音频信号与可选社交情境标记，结合动作词典和对方手势信息，利用多模态扩散Transformer生成更符合对话语境的人体动作。模型在Seamless Interaction Dataset上进行训练，并融合双方信息以捕捉互动动态，还可根据对话伙伴的手势生成更具响应性的动作。

Result: DyaDiT在标准动作生成评估指标和用户调查中均优于现有方法，用户更偏好DyaDiT生成的动作，显示出其更强的鲁棒性和社交适宜性。

Conclusion: DyaDiT能够生成更自然且符合社交语境的人体动作，提升虚拟人的对话交互质量，有望推动数字人社交自然性发展。

Abstract: Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.

</details>


### [67] [AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios](https://arxiv.org/abs/2602.23166)
*Zhaochen Su,Jincheng Gao,Hangyu Guo,Zhenhua Liu,Lueyang Zhang,Xinyu Geng,Shijue Huang,Peng Xia,Guanyu Jiang,Cheng Wang,Yue Zhang,Yi R. Fung,Junxian He*

Main category: cs.CV

TL;DR: 本文提出了一个新的多模态智能体基准AgentVista，涵盖25个子领域和7大类，测试现实世界中智能体基于视觉证据完成多步、跨模态任务的能力。评测显示当前模型在长时序、多模态工具使用上的表现仍然有限。


<details>
  <summary>Details</summary>
Motivation: 目前主流多模态基准大多只关注单步视觉推理或特定工具技能，无法真实反映实际多模态智能体在复杂视觉场景和长时序工具交互上的能力。因此需要更具真实性、细节和任务跨度的新基准。

Method: 作者构建了AgentVista基准，设计了涵盖25个子领域、7大类的现实多模态任务，要求智能体通过网页搜索、图片检索、页面导航、代码操作等多种工具，完成长时序、跨模态的实际流程问题。

Result: 主流多模态模型（如Gemini-3-Pro with tools）在AgentVista上的整体准确率仅为27.3%，解决较难任务时工具调用次数可超25次，暴露出现有模型在复杂多步推理及工具联用方面的明显短板。

Conclusion: AgentVista可以推动研究者开发更加智能、可靠、多才多艺的新一代多模态智能体，能够应对现实和极具挑战性的任务需求。

Abstract: Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.

</details>


### [68] [Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration](https://arxiv.org/abs/2602.23169)
*Xiaole Tang,Xiaoyi He,Jiayi Xu,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR是一种用于图像修复的表示学习框架，通过在Wasserstein重心空间对多源退化特征进行对齐，有效提升了模型对未知退化类型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前一体化图像修复方法在面对训练未见过的退化类型时泛化能力有限，严重影响其在真实场景下的应用。为提升模型对未知退化的处理能力，本文旨在从分布对齐和特征解耦视角出发构建更鲁棒的表征。

Method: 提出BaryIR框架，通过在Wasserstein重心空间对多种退化特征进行分布对齐，构建退化无关的联合表征。同时提出残差子空间，分别编码退化无关和退化相关的信息，两者通过正交约束解耦，保证内容信息的泛化和退化特征的自适应。

Result: 大量实验显示，BaryIR不仅在常规评价上与最新一体化方法表现相当，在面对未见过的退化类型（如不同类型和程度的退化）及复杂真实混合退化时，展现出卓越的泛化能力与鲁棒性。

Conclusion: BaryIR通过创新的分布对齐与表征解耦方法，显著提升了一体化图像修复模型对未知退化的泛化能力，为实际应用提供了更强的稳定性与适用性。

Abstract: Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.

</details>


### [69] [Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms](https://arxiv.org/abs/2602.23177)
*Bin Zeng,Johannes Künzel,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: 该论文提出了一种基于火车车载单摄像头的站台人群计数新方法，通过物理约束下的三维跟踪大幅提升了动态场景下的人群计数准确率。


<details>
  <summary>Details</summary>
Motivation: 准确的实时站台人群计数对于运输安全和容量管理至关重要，但现有方法多依赖静态摄像头且在动态复杂场景下效果不佳。

Method: 作者提出物理约束跟踪框架，将迁移学习的YOLOv11m目标检测器、EfficientNet-B0外观编码与DeepSORT结合，创新性地引入基于针孔几何的三维物理约束Kalman模型（Phys-3D），并通过虚拟计数区带提高遮挡下的计数稳定性。

Result: 在MOT-RPCH站台人群数据集上，该方法将计数误差降低至2.97%，展现了在摄像头移动、遮挡等极端条件下的强鲁棒性。

Conclusion: 结合物理几何先验显著提升了动态场景中人群计数的可靠性和准确度，有助于提升轨道交通排班和站台安全管理的有效性。

Abstract: Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.

</details>


### [70] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: 该论文提出了一个名为Uni-Animator的新型Diffusion Transformer（DiT）框架，实现了图像和视频素描上色的统一。通过多项创新方法，有效提升了颜色转移的精确度、物理细节的还原以及视频时序一致性，达到了与专门方法相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的素描上色方法很难同时适用于图像和视频任务，并在颜色转移精度、细节保留和视频时序一致性等方面表现不佳。作者希望提出一种统一框架，同时提升上述各方面表现。

Method: 1. 引入视觉参考增强模块，通过实例补丁嵌入提升颜色信息的精准对齐与融合；2. 设计了物理细节强化机制，利用物理特征有效捕捉并保留高频纹理信息；3. 提出基于素描的动态RoPE编码，自适应建模运动感知的时空依赖关系，从而改善运动场景下的时序一致性。

Result: 大量实验证明，Uni-Animator在图像和视频素描上色任务上均表现出色，无论是细节还原还是时序一致性都可与专用方法媲美，同时实现了跨域统一能力。

Conclusion: Uni-Animator有效解决了现有方法在素描上色领域的关键问题，实现了跨图像和视频的高质量、高一致性上色，为该领域的发展提供了有力的新工具。

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [71] [FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification](https://arxiv.org/abs/2602.23192)
*Thomas Woergaard,Raghavendra Selvan*

Main category: cs.CV

TL;DR: 本文提出了一种面向公平性的混合精度量化方法FairQuant，以在模型参数量化压缩过程中兼顾模型性能与算法公平性，主要应用于医学图像分类任务；新方法可在接近4-6bit平均精度下，实现与8bit量化相似的整体准确率，同时提升对弱势群体的表现。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络量化方法主要关注模型性能与计算效率间的权衡，忽视了参数量化对算法公平性的影响，尤其是在医疗图像等对公平性要求较高的场景下。为此，有必要在压缩神经网络时显式纳入公平性考量。

Method: 提出FairQuant框架，包含基于群体感知的重要性分析、预算约束下的混合精度分配、权重与bit分配的联合可学习Bit-Aware Quantization（BAQ）模式，并引入比特率与公平性正则；通过在不同类别的数据上进行分组分析，合理分配比特预算。

Result: 在Fitzpatrick17k和ISIC2019医学图像数据集，以及ResNet18/50、DeiT-Tiny、TinyViT模型上的实验表明，FairQuant在平均4-6bit精度下恢复接近Uniform 8bit模型的精度，并在预算相同条件下，相较于统一精度4bit和8bit基线模型，提升了对最弱群体的性能和公平性指标。

Conclusion: FairQuant能在不牺牲整体性能的前提下，通过合理分配量化精度，显著改善模型在不同群体上的公平性，有潜力应用于对公平性有要求的实际场景中。

Abstract: Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.

</details>


### [72] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的肠镜视频生成框架ColoDiff，实现了高质量、临床可控、动态一致的肠镜合成视频生成，缓解数据稀缺问题，并验证了其在各类下游任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 肠镜视频在肠道疾病诊断中极为重要，但高质量带标签的视频数据稀缺。现有方法难以满足数据的动态一致性和临床属性的精细控制，且肠道形态多变、病变表现复杂，成像方式多样，加大了合成视频生成的难度。因此迫切需要可控、兼具动态一致性的新生成方法。

Method: 提出ColoDiff扩散框架，含两大创新模块：1) TimeStream模块用于跨帧令牌化解构视频序列时的时序依赖，提升对复杂动态的建模能力；2) Content-Aware模块通过噪声嵌入和可学习原型实现对临床属性的精准建模与控制。此外，采用非马尔可夫采样机制，大幅提升采样效率，加快视频生成速度。

Result: 在三个公开数据集和一院内数据库上，ColoDiff于生成质量评测和多个下游任务（疾病诊断、模态区分、肠道准备评分、病变分割）均表现优异。实验证明本方法生成的视频具备平滑过渡与丰富动态特性。

Conclusion: ColoDiff首次实现了对肠镜视频的可控高质量合成，在临床和研究中显示出以合成视频补充真实数据、缓解样本稀缺问题的巨大潜力。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>


### [73] [EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents](https://arxiv.org/abs/2602.23205)
*Wenjia Wang,Liang Pan,Huaijin Pi,Yuke Lou,Xuqian Ren,Yifan Wu,Zhouyingcheng Liao,Lei Yang,Rishabh Dabral,Christian Theobalt,Taku Komura*

Main category: cs.CV

TL;DR: 本文提出一种便携且低成本的数据采集方案EmbodMocap，通过两部iPhone即可实现高质量人体与场景同步采集，无需昂贵设备和专门环境，推动大规模、多场景人体运动数据的获取。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动与场景数据采集主要依赖昂贵的设备、固定的场地和穿戴式传感器，难以实现大规模、日常环境下的数据采集，制约了具身智能研究的数据基础。

Method: 设计了利用两部运动中的iPhone的双目RGB-D采集方案，通过联合校准，重建统一坐标系下的人体与场景信息，提升了数据的尺度一致性与深度准确度。并与光学动捕等方法进行对比验证。

Result: 相比单iPhone或单目模型，双视角系统在消除深度歧义、对齐与重建精度上优于现有消费级方案。基于采集的数据，在单目重建、物理驱动角色动画以及机器人动作控制等AI任务上均取得良好效果。

Conclusion: EmbodMocap为非结构化、真实环境下的人体-场景协同采集提供了高性能、低门槛方案，并成功赋能了多项具身智能任务，推动相关AI研究的发展。

Abstract: Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.

</details>


### [74] [Through BrokenEyes: How Eye Disorders Impact Face Detection?](https://arxiv.org/abs/2602.23212)
*Prottay Kumar Adhikary*

Main category: cs.CV

TL;DR: 本文提出了一个使用BrokenEyes系统的计算框架，模拟五种常见视觉障碍，并分析这些疾病对深度学习模型中特征表征的影响。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍显著影响人类对视觉信息的感知与处理，但目前缺乏系统方法来量化这些疾病对神经类特征表征的具体影响。

Method: 开发了BrokenEyes系统，模拟与分析五种常见视觉障碍（黄斑变性、白内障、青光眼、屈光不正、糖尿病性视网膜病变）；运用人类与非人类数据集，分别在正常与疾病条件下训练模型，通过激活能量和余弦相似度等指标量化特征扰动。

Result: 结果显示，白内障和青光眼对特征表征产生了显著扰动，这与临床上已知的神经处理障碍相符。

Conclusion: 该框架可有效量化退化视觉输入对深度网络特征学习的影响，为理解视觉障碍的神经机制和辅助疾病相关AI模型研发提供了新思路。

Abstract: Vision disorders significantly impact millions of lives, altering how visual information is processed and perceived. In this work, a computational framework was developed using the BrokenEyes system to simulate five common eye disorders: Age-related macular degeneration, cataract, glaucoma, refractive errors, and diabetic retinopathy and analyze their effects on neural-like feature representations in deep learning models. Leveraging a combination of human and non-human datasets, models trained under normal and disorder-specific conditions revealed critical disruptions in feature maps, particularly for cataract and glaucoma, which align with known neural processing challenges in these conditions. Evaluation metrics such as activation energy and cosine similarity quantified the severity of these distortions, providing insights into the interplay between degraded visual inputs and learned representations.

</details>


### [75] [Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks](https://arxiv.org/abs/2602.23217)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.CV

TL;DR: 本文提出了一种基于广义爱因斯坦多层感知机（GE-MLPs）操作张量的统一多任务学习（MTL）数学框架，突破了传统矩阵方法的限制，能自然表达更复杂的视觉任务。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉任务建模依赖于矩阵权重和向量偏置，需对输入进行结构展开平坦化，导致任务表达受限且信息损失。作者希望克服这种局限，实现对多维任务的自然表达和统一建模。

Method: 作者提出使用广义爱因斯坦多层感知机（GE-MLPs）直接对张量进行操作，通过控制参数维度，实现对信息维度的精确保留或约分。通过数学推导，证明分类、分割和检测只是MTL中特定维度配置的特殊情形。

Result: 证明了其框架下任务空间比传统基于矩阵的方法表达能力更强，能够实现如时空预测、跨模态预测等复杂任务，这些任务在常规方法下需要信息损耗性地展开才能实现。

Conclusion: 该工作为理解、比较和设计计算机视觉任务提供了张量代数视角下的数学基础，可支持更丰富、更自然的任务表达。

Abstract: This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.

</details>


### [76] [MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction](https://arxiv.org/abs/2602.23228)
*Yizhi Li,Xiaohan Chen,Miao Jiang,Wentao Tang,Gaoang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、基于工具辅助的长视频（如电影、电视剧）自动摘要系统——MovieTeller，显著提升了摘要中的事实准确性、角色一致性与叙述连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在处理长视频自动摘要任务时，存在角色识别不一致、叙事断裂等问题，难以胜任电影、剧集等长视频的精确内容理解与生成。

Method: 提出MovieTeller框架：利用免训练“外部工具”——人脸识别模型获取关键角色身份与位置，作为事实依据嵌入VLM提示词，保证角色一致与事实正确；通过多阶段分层抽象，解决输入片段过长造成的信息丢失与上下文理解受限的问题；无需端到端重新微调，仅需调用现有模型。

Result: 在实验中，MovieTeller在事实准确率、角色一致性、故事连贯性等方面均优于现有主流端到端摘要模型。

Conclusion: MovieTeller通过工具辅助与分层摘要流程，为视频摘要等实际应用提供了一种高效、免微调、可解释性强的新范式，展现了长视频理解领域的突破性进展。

Abstract: With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external "tool" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.

</details>


### [77] [Large Multimodal Models as General In-Context Classifiers](https://arxiv.org/abs/2602.23229)
*Marco Garosi,Matteo Farina,Alessandro Conti,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TL;DR: 当涉及多模态分类任务时，尽管CLIP类的对比视觉-语言模型（VLMs）在零样本分类上表现突出，但本文发现大型多模态模型（LMMs）通过上下文学习能力，在少样本（in-context）下同样能达到甚至超越CLIP表现，并在开放世界分类中有更大潜力。为进一步提升LMM的表现，作者提出了无需训练的CIRCLE方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多数认为CLIP式的对比视觉-语言模型是多模态分类任务的首选，因其零样本分类性能优越。但LMM虽然在零样本下表现稍逊，却具备强大的上下文学习（in-context learning）能力。作者试图重新评估LMMs作为分类器的潜力，特别是在开放世界分类场景下的应用和改进可能。

Method: 对比主流LMMs与CLIP在不同多模态分类任务上的表现，关注其上下文学习能力。在开放世界分类情境下，提出CIRCLE方法：一种基于生成式LMM、无需新训练、通过伪标签迭代优化in-context样例的方法，从而提升分类性能。

Result: 实验表明，在有限的in-context示例下，LMMs的表现可以与CLIP并驾齐驱甚至超越CLIP+cache方法。CIRCLE方法在开放世界任务上，建立了坚实的基线，优于对比VLMs，并显著提升了LMM作为通用分类器的可行性。

Conclusion: LMMs不仅适用于复杂多模态推理任务，在给定少量上下文信息时，分类能力也极为强大。CIRCLE方法为开放世界分类带来新的思路，LMM有潜力成为比当前专用模型更灵活、统一的多模态分类框架。

Abstract: Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their "in-context" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.

</details>


### [78] [Skarimva: Skeleton-based Action Recognition is a Multi-view Application](https://arxiv.org/abs/2602.23231)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 本文指出使用多视角摄像头生成更高质量 3D 骨骼输入数据，能显著提升当前主流动作识别模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨骼的人体动作识别研究多专注于改进算法本身，而对输入骨骼数据的质量关注较少。本文关注数据质量是否限制了模型表现。

Method: 通过利用多摄像头视角进行三角测量生成更精确的 3D 骨骼数据，并将该数据输入主流动作识别模型，比较性能变化。

Result: 实验显示，多视角三角测量获得的高质量 3D 骨骼数据能显著提升动作识别模型的效果。

Conclusion: 输入数据质量是限制骨骼动作识别模型性能的关键因素。多摄像头方案在实际应用中的性价比高，未来研究应将多视角作为标准配置。

Abstract: Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.

</details>


### [79] [Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents](https://arxiv.org/abs/2602.23235)
*Zhou Xu,Bowen Zhou,Qi Wang,Shuwen Feng,Jingyu Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的高分辨率GUI导航压缩方法GUIPruner，大幅提升纯视觉GUI智能体的资源利用效率且几乎不损失性能。


<details>
  <summary>Details</summary>
Motivation: 当前纯视觉GUI智能体虽然具备通用交互能力，但由于高分辨率截图和历史轨迹的信息冗余，效率极低。此外，现有压缩方法存在时间与空间结构上的错配问题，造成信息丢失和性能下降。

Method: 作者提出GUIPruner框架，包含两个核心模块：一是时间自适应分辨率(TAR)，根据“记忆衰减”规律调整历史信息分辨率；二是分层结构感知裁剪(SSP)，优先保留互动前景和语义锚点，同时维护全局布局。该方法为无训练框架，适用于高分辨率场景。

Result: 在多个基准测试上，GUIPruner均实现了最先进的压缩效率和性能。对于Qwen2-VL-2B模型，FLOPs减少3.4倍，视觉编码速度提升3.3倍，性能保留94%以上，实现高精度实时导航。

Conclusion: GUIPruner能有效克服现有方法在高压缩场景下的崩溃问题，实现了高精度、低资源消耗的GUI视觉智能体导航，具备实用价值。

Abstract: Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's "fading memory" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.

</details>


### [80] [Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling](https://arxiv.org/abs/2602.23262)
*Jasmine Bayrooti,Weiwei Kong,Natalia Ponomareva,Carlos Esteves,Ameesh Makadia,Amanda Prorok*

Main category: cs.CV

TL;DR: 本文提出了一种基于光谱空间的差分隐私（DP）生成模型，专门针对敏感图像数据，在提升生成质量的同时确保隐私保护。通过只对低频成分应用DP微调，并借助公开模型进行高频细节重建，有效平衡了隐私与图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私图像生成方法（如DP-SGD）为了保障隐私，会在所有参数中添加噪声，导致生成图像质量下降（高频细节模糊）。本工作希望既保证隐私，又提升合成图像的保真度。

Method: 提出一种光谱差分隐私框架，基于低频分量更敏感、高频分量较公共的假设。具体做法是：（1）首先用DP技术对敏感图像的低分辨率小波系数进行图像分词模型的微调；（2）然后用公开的超分辨率模型对低分辨率图像进行高分辨率上采样。

Result: 实验证明，该方法在MS-COCO和MM-CelebA-HQ数据集上，生成的图像质量和风格捕获能力优于当前主流DP生成框架。

Conclusion: 利用分阶段DP策略，重点保护图像的整体结构，辅以后处理细节重建，实现了隐私和图像质量的较优权衡。此方法对敏感图像的私有生成任务具有实际价值。

Abstract: Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.

</details>


### [81] [LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction](https://arxiv.org/abs/2602.23290)
*Zhengyang Wei,Renzhi Jing,Yiyi He,Jenny Suckale*

Main category: cs.CV

TL;DR: 本论文提出了一种新的自动化道路提取框架LineGraph2Road，通过全局稀疏欧氏图和图变换器提升连接性预测，显著提升了道路提取的精度与细节表现。


<details>
  <summary>Details</summary>
Motivation: 现有从卫星图像中提取道路的方法（如关键点提取与连接预测）难以捕捉长距离依赖和复杂网络结构，难以满足自动化、高精度的需求。

Method: 构建稀疏欧氏图，将关键点作为节点，距离阈值范围内的节点对作为边，将道路连接预测转化为边的二分类任务。通过将欧氏图变换为线图，引入图变换器进行连接性学习，增加一对多层级交叉检测与耦合NMS以增强关键连接的保留。

Result: 在City-scale、SpaceNet和Global-scale三大基准测试中，LineGraph2Road在TOPO-F1和APLS两大指标上取得了最优表现，并能捕捉道路的精细视觉特征。

Conclusion: LineGraph2Road有效提升了卫星图像道路提取的全局结构理解与细节还原能力，具备实际应用价值，未来代码将公开。

Abstract: The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.

</details>


### [82] [PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning](https://arxiv.org/abs/2602.23292)
*Fuqiang Chen,Ranran Zhang,Wanming Hu,Deboch Eyob Abera,Yue Peng,Boyun Zheng,Yiwen Sun,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 本文提出了一种仅利用单一IHC训练数据实现虚拟多重IHC染色的引导框架（PGVMS），通过引入三项创新性技术，解决了现有方法在语义指导、染色一致性和空间对齐等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管IHC染色对蛋白表达的分子分型非常重要，现代病理学已有200余种基于抗体的检测，但受小体积活检组织材料有限的影响，难以做全面IHC分析；传统虚拟多重染色又存在语义指导不足、染色分布不一致和多模态空间不对齐等问题。

Method: 作者提出了一种新框架PGVMS，针对三大挑战分别设计：（1）自适应prompt引导，对语义指导不足采用病理视觉语言模型动态调整染色提示；（2）蛋白感知学习策略（PALS），通过定量并约束蛋白分布提升染色一致性；（3）原型一致性学习策略（PCLS），用于跨图像的语义交互修正空间错位。

Result: 该方法仅需uniplex（单一标记）训练数据，在虚拟多重IHC染色任务中，实现了更好的语义指导、染色一致性和空间对齐。

Conclusion: PGVMS框架有效解决了虚拟多重IHC染色的三大核心挑战，有望为病理数字多标记分析提供更高效且数据需求低的解决方案。

Abstract: Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).

</details>


### [83] [Towards Long-Form Spatio-Temporal Video Grounding](https://arxiv.org/abs/2602.23294)
*Xin Gu,Bing Fan,Jiali Yao,Zhipeng Zhang,Yan Huang,Cheng Han,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的自回归Transformer结构ART-STVG，专为解决长时长视频的时空定位问题而设计。该方法通过顺序处理视频流、引入记忆库和记忆选择策略，并采用级联的空间-时间解码器，实现了对长视频中目标的高效精准定位，显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的时空视频定位方法(STVG)主要集中在几十秒内的短视频，无法有效处理实际场景中的长时长视频。这限制了其在现实中的应用，需要一种适合长视频的解决方案。

Method: 提出了一种自回归Transformer架构ART-STVG，以流式方式顺序处理视频帧，避免一次处理全部帧带来的计算压力。通过空间和时间记忆库为解码器提供语境信息，并设计记忆选择机制筛选相关记忆，提升信息利用度。采用级联的空间-时间解码器，用空间信息辅助复杂的时间定位。

Result: 在新扩展的长时长视频数据库上，ART-STVG方法明显优于现有最先进STVG方法。在短视频场景下，也能取得有竞争力的表现。

Conclusion: ART-STVG有效解决了长视频下的时空视频定位难题，提高了效率和精度，有望扩展STVG在实际应用中的场景覆盖范围。

Abstract: In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.

</details>


### [84] [ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2602.23295)
*Ayush Roy,Wei-Yang Alex Lee,Rudrasis Chakraborty,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新型扩散模型数据蒸馏方法Manifold-Guided Distillation（ManifoldGD），能高效合成小型高质量数据集，显著减少存储和计算开销，并且在多项指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集不仅导致模型训练低效，还包含大量冗余信息。当前无训练的扩散模型蒸馏方法受到指导策略有限的瓶颈：简单或者无指导，容易丢失数据的多样性和语义信息，因此亟需更精细、有代表性的数据蒸馏方法。

Method: 提出ManifoldGD框架：通过VAE潜在空间的分层聚类，提取多尺度的实例原型中心（IPC）；在扩散去噪过程中，利用局部IPC邻域构建每步的潜在流形，并将对齐向量投影到该流形的切空间，确保生成过程始终保持在高语义一致性的流形内。该方法无需对生成模型进行重新训练。

Result: 在与现有无训练和需训练的数据蒸馏方法对比中，ManifoldGD在FID分数、合成与真实数据集的l2距离、分类准确率等多个评估标准上取得了持续且显著的提升。

Conclusion: ManifoldGD为数据蒸馏领域首个具有几何认知能力的无训练扩散模型蒸馏方法，有效提升了塑造小型高质量数据集的能力，对模型压缩与高效训练具有重要意义。

Abstract: In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.

</details>


### [85] [PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM](https://arxiv.org/abs/2602.23297)
*Yiqing Wang,Chunming He,Ming-Chen Lu,Mercy Pawar,Leslie Niziol,Maria Woodward,Sina Farsiu*

Main category: cs.CV

TL;DR: PRIMA提出了一种将医学图像和临床元数据深度融合的新框架，通过引入领域知识和多模态对齐，极大提升了疾病诊断的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常把元数据当作孤立标签，未能有效利用临床描述中的丰富语义和领域知识，限制了多模态诊断模型的性能。

Method: 首先通过RAG收集专家级别的风险-疾病关联语料，微调Clinical ModernBERT将诊断先验注入文本编码器。采用DINOv3（图像编码）和改进BERT（文本编码）双编码器框架，通过四种互补损失函数进行预训练，实现多粒度语义对齐，并用软标签解决临床相关性的不确定性。最后用Qwen-3将对齐的特征融入，实现精准分类。

Result: 在多个实验中，PRIMA显著优于现有多模态医学诊断方法，无需大规模数据和高昂计算资源，表现出更高的鲁棒性和准确率。

Conclusion: PRIMA框架高效融合了图像和临床知识，为医学诊断提供了更加全面、精准和高效的解决方案，具备良好实用前景。

Abstract: Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.

</details>


### [86] [ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306)
*Yiran Guan,Sifan Tu,Dingkang Liang,Linghao Zhu,Jianzhong Ju,Zhenbo Luo,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练和数据、提升多模态大语言模型推理能力的新框架ThinkOmni。


<details>
  <summary>Details</summary>
Motivation: 现有的omni-modal大语言模型（OLLM）虽然能处理多模态感知任务，但推理能力不足，而提升该能力通常需高质量数据和大量计算资源，难以推广。

Method: 提出ThinkOmni框架，通过两个关键组件：（1）利用成熟的大推理模型（LRM）引导OLLM解码；（2）逐步对比缩放机制，自适应平衡感知与推理信号，无需人工超参调整。

Result: 在六个多模态推理基准任务上，ThinkOmni均取得性能提升，主结果：MathVista得分70.2，MMAU得分75.5。

Conclusion: ThinkOmni为提升和通用多模态推理能力提供了灵活且通用的解决方案，并为推理能力的泛化与应用带来新视角。

Abstract: Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

</details>


### [87] [Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339)
*Tilemachos Aravanis,Vladan Stojnić,Bill Psomas,Nikos Komodakis,Giorgos Tolias*

Main category: cs.CV

TL;DR: 本文提出了一种结合少量标注样本和文本提示的新型开集词汇分割方法，大幅提升了像素级预测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（VLM）的开集词汇分割（OVS）在像素级预测方面落后于全监督方法，主要受限于粗粒度的训练监督和自然语言的语义歧义。

Method: 提出在测试时利用带像素标注的支持集样本，通过融合文本和视觉特征，利用检索增强的小型自适应分类器进行每一查询（图片）自适应融合学习，实现精细分割。与以往人工后期融合方式不同，采用端到端可学习的、多查询融合方法。

Result: 实验结果显示，新方法在缩小零样本分割与全监督分割之间的性能差距方面表现显著，同时保持了开放词汇分割能力，并支持个性化或细粒度的分割任务。

Conclusion: 所提出的方法显著提升了OVS在精度和灵活性上的表现，为视觉-语言模型在更复杂分割任务上的应用提供了新的思路。

Abstract: Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

</details>


### [88] [SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation](https://arxiv.org/abs/2602.23359)
*Vaibhav Agrawal,Rishubh Parihar,Pradhaan Bhat,Ravi Kiran Sarvadevabhatla,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 本文提出SeeThrough3D，一个能够在三维布局条件下显式建模物体遮挡关系的生成模型，并通过构建合成数据集实现对遮挡场景和新物体类别的有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有三维布局条件生成方法虽能生成与输入布局一致的场景，但未准确建模物体之间的遮挡关系，难以实现物体遮挡且几何深度一致的真实场景合成。为此，作者关注于在三维布局条件下，实现精准遮挡推理和场景生成。

Method: 提出了遮挡感知的三维场景表示OSCR，将物体表示为带有透明度的三维盒子，通过渲染实现不同视角下的遮挡可视化。模型将该三维渲染图像转换为视觉token，结合文本，通过条件化扩散模型进行生成。此外，采用掩码自注意力机制将物体描述与边界框精确绑定，避免属性混淆。模型训练使用合成的多物体、强遮挡三维场景数据集。

Result: SeeThrough3D能够精确建模物体间遮挡关系，在控制三维布局与相机视角时生成真实、深度一致的多物体场景，并能有效泛化到未见过的物体类别。

Conclusion: SeeThrough3D增强了三维布局条件生成的遮挡推理能力，实现了遮挡关系与相机视角的可控生成，对真实场景合成具有重要意义。

Abstract: We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [Human Label Variation in Implicit Discourse Relation Recognition](https://arxiv.org/abs/2602.22723)
*Frances Yung,Daniil Ignatev,Merel Scholman,Vera Demberg,Massimo Poesio*

Main category: cs.CL

TL;DR: 本研究比较了两种应对标注分歧的NLP建模方法，并在高歧义的隐式话语关系识别任务上评估其性能，发现基于标注分布的方法比基于特定标注者的方法表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 越来越多的NLP任务中，标注数据并没有唯一“正确答案”，标注者的判断体现了多样视角，因此如何更好地刻画这种多样性成为亟需研究的问题。

Method: 本研究在隐式话语关系识别（IDRR）这一高歧义NLP任务上，系统比较了基于标签分布建模和基于特定标注者（perspectivist）建模的方法，并通过实验和数据分析考察两者的有效性和局限。

Result: 实验发现，针对IDRR任务，现有的perspectivist模型如果不减少歧义，效果很差；而基于标签分布训练的模型能给出更稳定的预测结果。进一步分析显示，涉及更多认知负担的语料引发更大的人类判读不一致，影响了perspectivist模型的表现。

Conclusion: 在高认知复杂度和高歧义的NLP任务中，直接建模标签分布比依赖perspectivist方法更具鲁棒性。对于IDRR类任务，如何处理和建模认知不一致性是未来需要关注的方向。

Abstract: There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.

</details>


### [90] [Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks](https://arxiv.org/abs/2602.22730)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的捷克语餐饮领域方面情感分析数据集，支持多种复杂任务，并结合Transformer和大模型进行了系统实验，还提出LLM驱动的翻译与标签对齐方法，显著推动了低资源语言ABSA研究。


<details>
  <summary>Details</summary>
Motivation: 当前方面情感分析（ABSA）在低资源语言（如捷克语）领域缺乏高质量标注数据集，且现有模型在处理这类语言时表现受限。本文旨在为捷克语餐饮领域构建高质量ABSA数据集，并探索提升低资源语言ABSA表现的方法。

Method: 构建捷克语餐饮领域ABSA数据集并细致标注观点词，设置三种不同复杂度的ABSA任务。利用Transformer系模型（包括大语言模型）在单语、跨语和多语设定下进行实验，针对跨语场景提出基于LLM的翻译与标签对齐方法，并进行详细误差分析。

Result: 所建数据集成为新捷克ABSA基准。使用大模型和提出的方法，在单语、跨语、多语场景下取得优异效果。翻译-标签对齐方法在跨语实验中带来一致提升，相关实验展示了模型面对低资源语言的优势和挑战。误差分析发现情感表达细腻与观点词识别是主要瓶颈。

Conclusion: 新数据集极大促进了捷克及其他低资源语言ABSA研究，翻译-标签对齐方法为资源适配提供了可扩展路径。现有模型虽有效，但在复杂表达的检测上仍有挑战。

Abstract: This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.

</details>


### [91] [Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction](https://arxiv.org/abs/2602.22752)
*Nils Schwager,Simon Münker,Alistair Plum,Achim Rettinger*

Main category: cs.CL

TL;DR: 该论文提出并验证了“条件化评论预测”（CCP）任务，系统评估LLM在模拟社交媒体用户行为方面的能力，揭示了精调在提升输出表面结构但弱化语义基础的现象，并提出采用数字行为轨迹比描述性人格更适用于高保真模拟。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLM）日益被用作社会科学中的“硅基主体”，但缺乏对于其在用户行为模拟方面操作有效性的系统评估。作者旨在建立科学评测框架，推动该领域研究的准确性。

Method: 作者设计了CCP任务，让模型预测用户对某一刺激的评论，并与真实数字足迹进行比对。在英文、德文和卢森堡语环境下，系统评估了多种开源8B模型（Llama3.1, Qwen3, Ministral），比较了显式/隐式提示策略及有无监督微调（SFT）对表现的影响。

Result: SFT能提升模型输出的表层形式（如长度和句法），但会降低语义扎实度。显式条件输入如生成人物简介在进行SFT后变得多余，模型可以直接根据历史行为推断。作者发现，采用实际的数字行为足迹能够更好地模拟用户行为，优于手动设定人物特征。

Conclusion: 论文挑战了“天真提示”范式，建议研究者优先依据真实行为轨迹，而非通过设定人格描述，以实现高精度社交行为模拟，对未来LLM应用于社会科学研究给出关键指导。

Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active "silicon subjects" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current "naive prompting" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.

</details>


### [92] [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755)
*Abhay Sheshadri,Aidan Ewart,Kai Fronsdal,Isha Gupta,Samuel R. Bowman,Sara Price,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 本文介绍了AuditBench，这是一个用于人工智能对齐审计的测试基准，包含有意植入隐蔽行为的56个语言模型，用于评测和提升对齐审计工具的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可能隐藏危险或不良行为（如虚假服从、反对AI监管等），而且很难用现有工具揭露出来。需要系统的基准和工具来评估和改进AI模型的对齐审计能力。

Method: 作者开发了AuditBench基准，其中包含56个语言模型，每个模型都被植入特定的隐藏行为，并通过不同训练策略让模型不直接承认这些行为。作者还设计开发了能自动化使用不同审计工具的调查代理agent，并对这些工具结合agent的表现进行评估。论文同时比较了各种模型植入和训练场景下的行为可检测性。

Result: 发现独立评测表现良好的审计工具，结合agent时效果却大打折扣（即tool-to-agent gap）。黑箱工具和基于辅助模型生成多样化提示的scaffold策略比白盒工具效果更好。使用合成文档训练的模型更易于审计，基于示例和高水平对抗训练的模型更难被揭示隐藏行为。

Conclusion: AuditBench为AI对齐审计研究提供了系统测试平台。不同训练策略和工具对隐藏行为检测效果差异很大，未来工具开发需注意实际与理论性能差别。作者开放模型、agent与评测框架以促进领域发展。

Abstract: We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.

</details>


### [93] [Towards Better RL Training Data Utilization via Second-Order Rollout](https://arxiv.org/abs/2602.22765)
*Zhe Yang,Yudong Wang,Rang Li,Zhifang Sui*

Main category: cs.CL

TL;DR: 本文提出在大语言模型强化学习训练中，不仅关注回答生成，还引入了争议点评（critique）能力的训练，通过一阶和二阶rollout的联合框架，有效提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法主要关注生成能力，只利用一阶rollout（为问题生成多个回答），忽略了点评能力的训练，未能充分利用数据。鉴于生成和点评能力对提升推理和泛化均重要，作者希望通过补充点评训练提升整体效果。

Method: 作者提出“二阶rollout”，即针对生成的回答生成多个点评，并提出统一的联合训练框架，同时优化生成和点评能力。采用不同模型和数据集做实验，探索标签平衡、奖励噪声等训练细节。

Result: 实验表明，联合生成与点评训练能比传统RL方法更有效利用相同数据，带来更高性能。揭示了二阶rollout、标签平衡、奖励设计等对训练效果的重要影响。

Conclusion: 本文验证了动态数据增强与生成-点评联合训练的潜力，为RL训练方法的创新和大模型能力挖掘提供了新思路和实践依据。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training

</details>


### [94] [Imagination Helps Visual Reasoning, But Not Yet in Latent Space](https://arxiv.org/abs/2602.22766)
*You Li,Chi Chen,Yanghao Li,Fanhu Zeng,Kaiyu Huang,Jinan Xu,Maosong Sun*

Main category: cs.CL

TL;DR: 本论文分析了隐式视觉推理在多模态大模型中的实际作用，并提出其效用存疑，最终提出更直接有效的文字想象方法。


<details>
  <summary>Details</summary>
Motivation: 虽然隐式视觉推理被认为能更好地模拟人类想象过程，其有效性的真正来源仍不清楚，因此作者希望通过因果中介分析探究其工作机制。

Method: 作者将隐式推理建模为一个因果链条（输入→隐式token→输出），通过扰动输入和隐式token，并用因果中介分析考察各环节的关联和影响。同时，进行探查性分析检验隐式token的视觉信息承载能力和多样性。

Result: 结果显示：1）输入对隐式token几乎没有影响，2）隐式token对输出影响微弱，3）隐式token携带的视觉信息有限且高度相似。实验发现，提出的显式文字想象方法（CapImagine）在视觉基准测试中显著优于现有隐式推理方法。

Conclusion: 隐式视觉推理机制的实际因果贡献有限，未如预期发挥作用；相比之下，直接基于文字想象的新方法更为高效且优越，值得推广。

Abstract: Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.

</details>


### [95] [Probing for Knowledge Attribution in Large Language Models](https://arxiv.org/abs/2602.22787)
*Ivo Brink,Alexander Boer,Dennis Ulmer*

Main category: cs.CL

TL;DR: 本文提出了一种判定大语言模型（LLMs）输出内容主要依赖于外部上下文还是模型内部知识的方法。通过简单的线性探针，能够较为准确地识别每条输出的知识来源，有助于缓解模型幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs易生成流畅但错误（幻觉）的内容，包括“忠实性问题”和“事实性问题”。解决幻觉问题需先区分输出内容是受用户输入还是模型本身知识主导，因此有必要发展精确识别答案来源（知识归属）的有效方法。

Method: 提出“贡献归因”任务。通过训练线性探针（一个简单分类器）对模型隐层表示进行学习，预测每个输出主要依赖的知识来源。同时开发了AttriWiki自监督数据集，通过控制提示指令自动标注数据，用于训练探针。

Result: 在Llama-3.1-8B、Mistral-7B、Qwen-7B等主流LLM上，探针基于AttriWiki训练后，归因预测Macro-F1最高可达0.96，并能无须微调就稳定迁移到SQuAD、WebQuestions等新数据集（Macro-F1 0.94-0.99）。知识归因混乱时，错误率最高上升70%。

Conclusion: 通过分析模型隐层，简单探针可有效识别答案知识来源。知识归因精度与回答忠实性紧密相关，但归因正确不能完全避免错误回答，因此还需更广泛的检测手段以提升模型可靠性。

Abstract: Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.

</details>


### [96] [Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift](https://arxiv.org/abs/2602.22790)
*Hyunwoo Kim,Hanau Yi,Jaehee Bae,Yumin Kim*

Main category: cs.CL

TL;DR: 本文讨论了自然语言声明式提示（NLD-P）作为在大模型演化中实现治理和稳定控制的新方法，并提出了为非开发者友好的治理框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的快速迭代，prompt工程面临因模型行为变化（即GPT规模模型漂移）带来的管理和稳定性挑战。以往的表层格式或临时优化方法已经无法适应模型持续演变下的需求，因此需要新的治理思路。

Method: 作者将自然语言声明式提示（NLD-P）重新定义为一种声明式治理方法，而非固定模板，并将其形式化为一个模块化的控制抽象，分离了来源、约束逻辑、任务内容和后生成评估，全部用自然语言实现，无需外部编排代码。文中还定义了基本的合规标准，并分析了不同模型对该方案的接受度。部分草稿和编辑也是在NLD-P配置下由模型助手辅助完成，最终由人类作者审阅并批准。

Result: NLD-P被证明是一种适用于非开发者、能适配LLM持续演化的治理框架。对制度合规和模型适应性的最小标准有明确定义。实验表明，NLD-P可有效分离各治理模块，提升了模型控制的解释性和稳定性。

Conclusion: NLD-P为大模型生态系统中的声明式控制和治理提供了新的思路和实践方案，对未来模型持续进化中的治理提出了建议，并指出需要进一步的实证验证。

Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.

</details>


### [97] [TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models](https://arxiv.org/abs/2602.22827)
*Reihaneh Iranmanesh,Saeedeh Davoudi,Pasha Abrishamchian,Ophir Frieder,Nazli Goharian*

Main category: cs.CL

TL;DR: 本文提出了一个面向波斯语的大型语言模型（LLM）文化能力评估框架，通过更适合波斯语特点的短答题和先进匹配评分机制，系统性地优化了现有评测可靠性，并公开发布了标准化基准。


<details>
  <summary>Details</summary>
Motivation: 现有面向波斯语的文化基准多采用多项选择题和以英语为中心的指标，难以覆盖波斯语在形态和语义上的复杂性，导致评估对真实文化理解的捕捉不足。

Method: 提出波斯语专用短答题评估框架，结合基于规则的形态归一化与混合句法/语义相似性模块，实现了超越字面匹配的软匹配打分机制。

Result: 对15个现有主流开源和闭源模型评测，所提混合评估方法相比精确匹配基线评分一致性提升10%以上，能够捕捉到表层方法遗漏的意义。

Conclusion: 本框架为波斯语文化理解提供了首个标准化基准和可复现研究基础，有助于促进跨文化大型语言模型评估研究。

Abstract: This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.

</details>


### [98] [TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought](https://arxiv.org/abs/2602.22828)
*Jianmin Li,Ying Chang,Su-Kit Tang,Yujia Liu,Yanwen Wang,Shuyuan Lin,Binkai Ou*

Main category: cs.CL

TL;DR: 本文提出了一种专为中医领域设计的改进型RAG框架TCM-DiffRAG，通过结合知识图谱和思维链，显著提升了大语言模型在中医个性化诊疗中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法难以应对中医诊疗中复杂推理和个体化差异，导致大语言模型在该领域表现不佳。该研究希望突破RAG在中医领域应用的瓶颈。

Method: 提出TCM-DiffRAG框架，将知识图谱与思维链（CoT）结合，增强模型推理能力，并在三个中医测试数据集上进行性能评估。

Result: TCM-DiffRAG对比原生LLM、其他RAG方法以及SFT模型，均取得了较大性能提升。例如，qwen-plus模型的某项分数从0.927提升至0.952，改进更明显。

Conclusion: 将结构化知识图谱与思维链推理结合，能大幅提升中医个体化诊断任务的表现，为LLM在中医等复杂领域的应用提供了新思路。

Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.

</details>


### [99] [Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features](https://arxiv.org/abs/2602.22846)
*Mohammad Yeghaneh Abkenar,Weixing Wang,Manfred Stede,Davide Picca,Mark A. Finlayson,Panagiotis Ioannidis*

Main category: cs.CL

TL;DR: 本文提出利用上下文嵌入扩展情感词典，并结合神经网络提升立场分类准确率，实验覆盖多领域五个数据集，获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有立场分类研究大多依赖非论证性文本且未系统结合细粒度情感分析，方法和数据集通用性有限。鉴于争议性议题常涉及情感言辞，有效利用情感分析有望提升分类表现。

Method: 使用DistilBERT生成的上下文化嵌入扩展NRC情感词典（bias-corrected），得到新的eNRC词典，并作为输入特征强化神经网络立场分类模型。实验覆盖五个包含争议性话题的不同领域数据集。

Result: 所提出的eNRC词典对比原有NRC、LLM方案以及无词典基线，五个数据集F1有最高6.2个百分点提升，在四个数据集上优于原版NRC，几乎全部超越基于大模型方法。

Conclusion: 在跨领域争议型立场分类任务中，引入扩展的情感词典和神经网络能有效提升性能，所有数据与代码公开，有助于促进后续相关研究。

Abstract: Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.

</details>


### [100] [Effective QA-driven Annotation of Predicate-Argument Relations Across Languages](https://arxiv.org/abs/2602.22865)
*Jonathan Davidov,Aviv Slobodkin,Shmuel Tomi Klein,Reut Tsarfaty,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 该论文提出了一种将问答式语义角色标注（QA-SRL）迁移到多语言环境下的方法，无需大量人工标注，即可实现高质量的谓词-论元关系分析，并超越了多语言大模型。


<details>
  <summary>Details</summary>
Motivation: 现有的显式谓词-论元语义分析高度依赖人工标注，主要集中在英语，扩展到其他语言成本极高，限制了可解释语义分析在多语言环境的应用。

Method: 利用QA-SRL作为跨语言语义注释的基础，通过受限的翻译与词对齐流水线，将英语QA-SRL解析器投射到目标语言（如希伯来语、俄语和法语），自动生成与目标语言谓词对应的问题-答案标注，然后基于这些高质量训练数据微调目标语言的解析器。

Result: 该方法在希伯来语、俄语和法语上生成了高质量的训练数据和专用标注器，其性能优于现有主流多语言大模型（如GPT-4o、LLaMA-Maverick）。

Conclusion: QA-SRL作为可迁移的自然语言语义接口，可以高效并广泛地实现多语种的谓词-论元分析，降低了语义标注的门槛，提升了跨语言语义分析的可及性和实用性。

Abstract: Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.

</details>


### [101] [Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference](https://arxiv.org/abs/2602.22868)
*Yushi Ye,Feng Hong,Huangjie Zheng,Xu Chen,Zhiyong Chen,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 该文提出了一种名为ReMix的框架，通过引入连续混合状态，有效解决了DLLMs在并行解码时语义一致性差和质量-速度权衡的问题，可实现2-8倍的推理加速且无质量下降。


<details>
  <summary>Details</summary>
Motivation: DLLMs推理速度快但并行解码时容易出现“组合矛盾”，即多个token合成的语义不一致，影响模型输出质量。作者希望解决这一质量-速度矛盾。

Method: 提出ReMix框架：在离散解码流程中引入一个连续混合状态，允许token表示在连续空间中逐步优化，减少token间冲突；当某些表示不确定时，通过拒绝规则将其退回mask状态重新处理，保证稳定性。整个方法无需重新训练。

Result: 大量实验表明，ReMix可以在无需重新训练的情况下，实现2-8倍的推理速度提升，且输出质量无损。

Conclusion: ReMix显著缓解了并行解码时的组合矛盾，在不损失质量的情况下，大幅提升了DLLMs的推理效率，具有实际应用价值。

Abstract: Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.

</details>


### [102] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 本文提出一种基于链式推理的自洽框架，通过在不同推理轨迹间重组高分中间步骤，提高大语言模型在复杂问题上的推理准确率和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多链路推理时，通常只聚合最终答案，忽略了来自部分或“近似正确”推理中的有价值中间步骤，导致信息浪费和推理效率不高。

Method: 作者提出“Stitching Noisy Diffusion Thoughts”方法：首先利用带掩码的扩散语言模型对同一问题生成大量多样、低成本的推理轨迹；其次，使用既有的过程奖励模型对每一个中间步骤评分；然后，从所有推理路径里选取最高质量的步骤重组成为新推理链；最后，将重组链路交给自回归模型最终计算答案，分离推理探索与评估、解答过程。

Result: 在数学推理等六项任务中，该方法平均准确率提升最高达23.8%，推理延迟最高降低至原有系统的1.8倍，尤其在难题上效果显著。消融实验强调最终自回归求解器对于提升准确率的关键作用。

Conclusion: 通过复用和重组中间推理步骤，该无训练框架在提升模型推理准确率的同时大幅优化速度，对扩散类与统一大模型结构均有显著优势，表现出强大的通用性与实用价值。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [103] [Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models](https://arxiv.org/abs/2602.22918)
*Jonathan Steinberg,Oren Gal*

Main category: cs.CL

TL;DR: 本论文研究了视觉语言模型（VLMs）中的OCR（光学字符识别）信息在语言处理流程中的注入位置，并通过因果干预分析了多种架构下OCR信号传递机制。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs可以识别图片中的文字信息，但目前尚不清楚OCR信息在模型内部是如何与语言处理流程整合的。理解其注入和传递机制有助于优化模型表现，尤其是在涉及场景文本推理、数数等任务中。

Method: 作者对三类架构（Qwen3-VL、Phi-4、InternVL3.5）进行了因果干预实验，比较原始图片与文本局部涂抹后激活的变化，定位OCR信号进入模型的具体层级，并利用主成分分析（PCA）研究OCR信号的低维特性及不同数据集间的可迁移性。同时考察删除OCR模块对模型任务表现的影响。

Result: 结果显示：Qwen（DeepStack架构）模型对场景文本最敏感的层位于中层（约50%）；Phi-4和InternVL（单阶段投影架构）对OCR的敏感层集中于前部（6-25%），但具体层数视数据集略有变化。OCR信号低维，主成分PC1解释了72.9%方差，且PCA方向具备跨数据集迁移能力。在部分模块化强的模型（如Qwen3-VL-4B）中，移除OCR可提升数数任务表现最多6.9个百分点，暗示OCR信息在部分场景下可能干扰其它视觉推理流程。

Conclusion: 不同VLM架构对OCR信号的处理存在显著差异，部分模型OCR信号瓶颈集中且表现低维、可迁移。OCR组件在某些高度模块化架构中甚至可能对其它视觉任务产生负面影响，这提示模型设计时应权衡OCR与其它视觉功能的集成方式。

Abstract: Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.

</details>


### [104] [Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department](https://arxiv.org/abs/2602.23062)
*Gabriela Anna Kaczmarek,Pietro Ferrazzi,Lorenzo Porta,Vicky Rubini,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本文提出并公开一个来自意大利急诊科、用于CRF自动填写的临床笔记数据集，并用LLM开展实验，分析了其表现和局限性。


<details>
  <summary>Details</summary>
Motivation: 由于带注释的CRF数据稀缺，阻碍了LLM等自动化CRF填写技术的发展，因此急需公开相应数据和相关评测标准，以推动任务进展。

Method: 作者构建了一个包含来自意大利急诊的临床笔记，并参照预定义CRF（含134个条目）手工注释；定义了CRF自动填写任务及评估指标，并利用开源SOTA的LLM开展零-shot实验，分析模型表现。

Result: 实验表明，零-shot条件下LLM可一定程度上自动填写CRF，但其输出受到偏见影响（如更倾向于选择“未知”答案），需要后续方法矫正此问题。

Conclusion: 本工作为自动CRF填写领域提供了高价值公共数据，并证实LLM具备处理此类任务的潜力，但数据稀缺与模型偏见仍是重要挑战。

Abstract: Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours "unknown" answers), which need to be corrected.

</details>


### [105] [Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody](https://arxiv.org/abs/2602.23071)
*Yuqi Shi,Hao Yang,Xiyao Lu,Jinsong Zhang*

Main category: cs.CL

TL;DR: 本文探讨越南籍汉语学习者在掌握汉语句法-韵律接口时表现出的特点，发现虽然高级学习者在韵律边界数量上接近母语者，但在具体结构映射上仍有显著偏差，导致句法-韵律界面僵化和系统性错误。


<details>
  <summary>Details</summary>
Motivation: 尽管二语学习者能掌握目标语言的句法序列，但将正确句法映射到恰当的韵律结构上仍然困难。因此，研究汉语学习者句法-韵律接口的化石化与稳定性，有助于理解二语韵律习得的瓶颈和策略。

Method: 使用BLCU-SAIT语料库，对67名汉语母语者和67名越南汉语学习者进行了对比。采用C-ToBI韵律标注结合依存句法分析，考察了韵律边界的数量及其与句法关系的映射情况。

Result: 研究发现二语习得过程中韵律习得呈现非线性，即使高级学习者在大词组层级（B3）的韵律边界数量上接近母语者，但在结构映射上，高级学习者倾向于弱化主谓界面韵律边界，强化动宾界面韵律边界，导致韵律结构失衡，出现与母语者相反的结构模式。

Conclusion: 越南籍汉语学习者在句法-韵律接口上表现出稳定的非母语模式，表明句法-韵律映射在二语习得中容易出现僵化，并通过“牺牲结构准确性换取韵律输出流畅度”的策略，影响了整体的韵律层级结构。

Abstract: While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.

</details>


### [106] [CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery](https://arxiv.org/abs/2602.23075)
*Mengze Hong,Di Jiang,Chen Jason Zhang,Zichang Guo,Yawen Li,Jun Chen,Shaobo Cui,Zhiyang Su*

Main category: cs.CL

TL;DR: 本文提出了CiteLLM平台，通过将LLM集成到LaTeX编辑器，实现学术写作中高可信度、隐私保护的自动化参考文献发现与验证系统。


<details>
  <summary>Details</summary>
Motivation: 在学术活动中使用大语言模型（LLM）有提升效率的潜力，但存在内容可信度、学术诚信、知识产权和隐私保护等伦理挑战。

Method: CiteLLM将LLM功能嵌入本地LaTeX编辑器，无需外部数据传输，通过学科感知动态路由，仅从可信学术仓库检索文献。LLM仅用于生成搜索查询、相关性排序、语义验证与解释，同时集成聊天机器人辅助理解。

Result: 系统评估显示，该平台在返回有效且高可用性参考文献方面表现优异。

Conclusion: CiteLLM可以实现无幻觉、隐私保护、用户体验良好的学术参考发现，推动LLM在学术写作领域的可信使用。

Abstract: Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.

</details>


### [107] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种LLM代理，通过SALA方法结合风格计量学和大模型能力，用于评估和降低文本作者去匿名化风险，并证明该方法具高准确性与可解释性，同时提出重写减少作者可识别性的策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具备强大的作者归因能力，导致文本去匿名化风险增加，亟需有效方法检测和减少这种风险，保护作者隐私。

Method: 提出SALA（风格计量辅助LLM分析）方法，将量化的风格计量学特征与LLM推理相结合，并设计了数据库模块增强归因准确性，同时利用代理推理过程指导文本重写以降低作者可识别性。

Result: 在大规模新闻数据集上，SALA方法（特别是加入数据库模块后）在多种场景下实现了高准确率的作者归因，并展示了可解释性。通过推理溯源引导的重写策略，有效降低了作者可识别性，并保持文本含义不变。

Conclusion: LLM代理对作者去匿名化有很大潜力，需关注其隐私风险。SALA方法为理解和防御去匿名化攻击提供了透明且高效的解决方案，有助于保护作者隐私。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [108] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: 本文提出了MTRAG-UN基准，用以测试和分析多轮检索增强生成（multi-turn retrieval augmented generation, RAG）任务下，现有大模型面临的难题。


<details>
  <summary>Details</summary>
Motivation: 多轮检索增强生成是大语言模型的热门应用，但其在实际对话场景中，存在诸如无法回答、问题不明确、响应不清晰等难点，目前缺乏系统的基准来评估和推进该领域研究。

Method: 作者构建了包含666个任务、共2800多轮对话的新基准数据集，覆盖6个领域，并带有配套语料库。设计实验分析现有检索和生成模型在多种复杂情境下（如无法回答、信息不足的问题等）的表现。

Result: 实验结果表明，现有检索和生成模型对于无解（UNanswerable）、信息不足（UNderspecified）、非独立（NONstandalone）的问题以及不清晰（UNclear）的回答仍然表现很差。

Conclusion: 当前多轮RAG模型仍存在明显的短板，MTRAG-UN基准能够有效推动社区在该领域识别和解决实际对话中的关键挑战。

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


### [109] [Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?](https://arxiv.org/abs/2602.23225)
*Pengxiang Li,Dilxat Muhtar,Lu Yin,Tianlong Chen,Shiwei Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为NAP（Non-Autoregressive Parallel DLMs）的新方法，通过数据和监督方式的改进，使扩散语言模型（DLMs）能够实现非自回归的并行生成，并在数学推理任务上显著提升了并行解码效果。


<details>
  <summary>Details</summary>
Motivation: 尽管DLM常被认为可以实现并行生成，但实际上在实际应用中却倾向于自回归（左到右）式的解码方式，难以发挥出真正的并行优势。作者认为主要原因是在于DLM训练目标与主流训练数据（如标准预训练语料和长流程链式推理数据）高度顺序化的结构发生了不匹配，导致模型在推理过程中表现出类自回归行为。提升并行能力对于减小同步和通信开销、改善生成长度的延迟有重要意义。

Method: 作者提出NAP，这是一种以数据为中心的方法。核心做法是在数据层面，将监督数据整理为多个独立推理路径，每个路径相对独立，使其适合并行生成，同时配合一种强行并行解码策略，鼓励模型进行多token的并行更新。

Result: 在数学推理基准测试上，NAP在并行解码设置下优于用标准长CoT（Chain of Thought）数据训练的DLM，且随着并行度提升，效果增益也更明显。

Conclusion: 本文实验显示，重新考虑数据与监督设计能够有效缓解DLM的自回归倾向，是推进真正非自回归并行生成的有前景方向。

Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.

</details>


### [110] [Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems](https://arxiv.org/abs/2602.23266)
*Siyuan Liu,Jiahui Xu,Feng Jiang,Kuang Wang,Zefeng Zhao,Chu-Ren Huang,Jinghang Gu,Changqing Yin,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为DDTSR的新型语音对话系统框架，通过并行和流式协作机制显著降低响应延迟，同时保持对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有ASR-LLM-TTS级联对话系统由于严格顺序处理，导致响应延迟较高，不符合类人对话的即时响应需求，因此需研发更具低延迟能力的新架构。

Method: 1. 小-大模型协同机制：辅助小模型提前生成讨论连接词，大模型并行进行知识推理。2. 流式跨模态协作：ASR、LLM推理与TTS动态重叠，尽早推进可发声节点。3. 基于课程学习的连贯性增强：保证前后输出逻辑一致和连贯。

Result: 在两个对话基准集上验证，DDTSR能降低19%到51%的响应延迟，同时保持对话连贯性和质量。

Conclusion: DDTSR具备良好的可移植性和伸缩性，能与多种LLM兼容，在不同长度语音交互中表现稳定，适合实时应用。

Abstract: Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.

</details>


### [111] [SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables](https://arxiv.org/abs/2602.23286)
*Sungho Park,Jueun Kim,Wook-Shin Han*

Main category: cs.CL

TL;DR: SPARTA是一个自动化生成面向真实世界表格-文本问答的大规模基准构建框架，显著提升了问题复杂性和范围，并暴露了现有模型在深层跨模态推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有表格-文本问答基准规模小、人工标注费时、问题浅显，未能涵盖多跳推理及复杂操作，难以有效驱动通用且强大的表格-文本问答模型发展。

Method: SPARTA框架通过先自动构建带原子事实的表格-事实数据库，再合成针对不同跳数的嵌套自然语言查询问题，对每条SQL以基于溯源的改写和真实性结构约束保障其可执行性和语句流畅性，最后经轻量人工校验快速生成大规模高质量数据集。

Result: SPARTA能够生成数千个涵盖聚合、分组和深多跳推理的高质量问答样本。实验证明，在SPARTA基准上，现有最先进的模型性能大幅下滑（F1下降30分以上），展现其在复杂推理上的瓶颈。

Conclusion: SPARTA大幅提升了表格-文本QA基准的规模与复杂性，是更严苛的跨模态推理能力评测工具，有望推动相关模型研究进步。

Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.

</details>


### [112] [A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300)
*Soumya Dutta,Smruthi Balaji,Sriram Ganapathy*

Main category: cs.CL

TL;DR: 本文提出了一种新型情感识别模型MiSTER-E，通过多专家机制有效融合语音与文本信息，在多个基准数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 情感对话识别（ERC）面临如何建模多轮对话时序性以及多模态线索融合的双重挑战，已有方法在这两方面常常处理不足。

Method: 提出MiSTER-E模型，将语音与文本调优后的大语言模型生成表征，通过卷积-递归层建模上下文，利用多专家（语音专家、文本专家、跨模态专家）体系，通过门控机制整合输出，并引入监督对比损失和KL正则化促进模态一致。全程不依赖说话人身份特征。

Result: 在IEMOCAP、MELD和MOSI三个数据集上，MiSTER-E模型加权F1分别达到70.9%、69.5%和87.9%，优于多个现有的语音-文本情感识别系统。

Conclusion: MiSTER-E有效提升了多模态对话情感识别的性能，表明基于专家的解耦和综合策略在该领域具有良好应用潜力。

Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion](https://arxiv.org/abs/2602.22862)
*Enda Xiang,Haoxiang Ma,Xinzhu Ma,Zicheng Liu,Di Huang*

Main category: cs.RO

TL;DR: 本文通过引入抓取先验知识到扩散式策略学习框架中，提升了机器人抓取动作的精度与泛化能力，实验证明方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习抓取策略存在抓取动作不精确、空间与物体泛化性差等问题，亟需提升其在实际操作中的表现。

Method: 采用潜变量扩散策略，并结合抓取位姿先验指导策略生成；提出在扩散过程中加入自监督重建目标，通过手腕摄像头图像重建抓取性，从中间表示中嵌入抓取先验。

Result: 无论在仿真还是真实机器人实验中，该方法均显著优于基线，在动态抓取任务上表现出较强能力。

Conclusion: 将抓取先验引入扩散式策略学习框架能大幅提升机器人抓取任务的精度和泛化性，是提升仿生机器操作能力的有效方法。

Abstract: This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.

</details>


### [114] [Bayesian Preference Elicitation: Human-In-The-Loop Optimization of An Active Prosthesis](https://arxiv.org/abs/2602.22922)
*Sophia Taddei,Wouter Koppen,Eligia Alfio,Stefano Nuzzo,Louis Flynn,Maria Alejandra Diaz,Sebastian Rojas Gonzalez,Tom Dhaene,Kevin De Pauw,Ivo Couckuyt,Tom Verstraten*

Main category: cs.RO

TL;DR: 本论文提出了一种基于人体反馈的最优化方法，实现了假肢参数的个性化调节，提高了对用户需求的响应效率和效果。


<details>
  <summary>Details</summary>
Motivation: 目前主动假肢的参数调优依赖繁琐的程序和不完全反映用户真实需求的指标，实际应用中效率低下。研究者希望引入能直接反映用户偏好的调优新方法，以更好地满足使用者的个性化需求。

Method: 采用基于偏好的多目标贝叶斯优化算法，分别实现了离散版本（EUBO-LineCoSpar）和连续版本（BPE4Prost），并引入先进的采集函数以适应偏好学习。方法通过让用户直接表达偏好，快速调节假肢控制器的四个参数。

Result: 仿真基准测试和实际应用实验均显示，该方法可以高效收敛，精确识别用户偏好，并带来可观的生物力学改善。

Conclusion: 本研究证明了用户偏好驱动的人体反馈优化方法在假肢参数调优中的有效性和实用性，为个性化假肢控制提供了有前景的技术路线。

Abstract: Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently. Our method employs preference-based Multiobjective Bayesian Optimization that uses a state-or-the-art acquisition function especially designed for preference learning, and includes two algorithmic variants: a discrete version (\textit{EUBO-LineCoSpar}), and a continuous version (\textit{BPE4Prost}). Simulation results on benchmark functions and real-application trials demonstrate efficient convergence, robust preference elicitation, and measurable biomechanical improvements, illustrating the potential of preference-driven tuning for user-centered prosthesis control.

</details>


### [115] [Considering Perspectives for Automated Driving Ethics: Collective Risk in Vehicular Motion Planning](https://arxiv.org/abs/2602.22940)
*Leon Tolksdorf,Arturo Tejada,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 本文提出了一种将所有道路使用者的风险纳入自动驾驶车辆（AV）决策的新规划策略，从而实现更具伦理性的交通行为。


<details>
  <summary>Details</summary>
Motivation: 目前的自动驾驶汽车运动规划主要以自身风险最小化为目标，忽视了其他道路使用者的风险和伦理问题。作者质疑这种只关注自身视角的做法未必能降低整体交通风险，可能导致部分道路使用者风险反而增加，无法达到社会接受的伦理性驾驶标准。

Method: 作者提出了一种能够在所有道路使用者风险最小化策略之间切换的运动规划方法，并比较了从自动驾驶车辆和其他道路使用者视角评估风险的不同结果。他们还尝试以集体风险最小化为目标，平衡所有人风险，检验其在实际交通环境下的表现。

Result: 实验发现，从不同道路使用者的视角计算的风险确实大不相同。采用集体风险最小化策略时，自动驾驶车辆能够更好地降低整体交通风险，即使自身风险略有提升，也能带来整体效益。同时，该策略在面对其他交通参与者低风险评估时能够果断推进，提高通行效率，而在计划动作不易被理解时则采取更保守策略。

Conclusion: 结论认为，只有将所有道路使用者的风险纳入自动驾驶车辆的决策过程，才能实现符合伦理和社会期望的交通行为。集体风险视角不仅提升了交通整体安全，也使自动驾驶行为更易于被社会接受，体现了对自身行为的反思能力和道德责任。

Abstract: Recent automated vehicle (AV) motion planning strategies evolve around minimizing risk in road traffic. However, they exclusively consider risk from the AV's perspective and, as such, do not address the ethicality of its decisions for other road users. We argue that this does not reduce the risk of each road user, as risk may be different from the perspective of each road user. Indeed, minimizing the risk from the AV's perspective may not imply that the risk from the perspective of other road users is also being minimized; in fact, it may even increase. To test this hypothesis, we propose an AV motion planning strategy that supports switching risk minimization strategies between all road user perspectives. We find that the risk from the perspective of other road users can generally be considered different to the risk from the AV's perspective. Taking a collective risk perspective, i.e., balancing the risks of all road users, we observe an AV that minimizes overall traffic risk the best, while putting itself at slightly higher risk for the benefit of others, which is consistent with human driving behavior. In addition, adopting a collective risk minimization strategy can also be beneficial to the AV's travel efficiency by acting assertively when other road users maintain a low risk estimate of the AV. Yet, the AV drives conservatively when its planned actions are less predictable to other road users, i.e., associated with high risk. We argue that such behavior is a form of self-reflection and a natural prerequisite for socially acceptable AV behavior. We conclude that to facilitate ethicality in road traffic that includes AVs, the risk-perspective of each road user must be considered in the decision-making of AVs.

</details>


### [116] [Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy](https://arxiv.org/abs/2602.22952)
*Yuan Tang,Bruno V. Adorno,Brendan A. McGrath,Andrew Weightman*

Main category: cs.RO

TL;DR: 该论文提出了一种基于机器人和电磁导航的PDT穿刺自动化系统，并通过体模实验验证了其高精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 手动PDT穿刺因无导航辅助，误差大且易导致严重并发症（如大出血和后壁穿孔），亟需提升精度与安全性。

Method: 采用速度控制机器人操控穿刺针，通过两个电磁传感器（针尖与气管内部）动态引导，利用自适应约束控制器实时调整参数及规避碰撞。系统在模拟和体模上共完成400例穿刺实验。

Result: 实验中穿刺位置绝对中位误差为1.7 mm，角度偏差中位数为4.13度，性能远优于传统手工操作，并能确保避开重要体组织。

Conclusion: 机器人辅助PDT穿刺系统在高精度和安全性方面具备可行性，有望显著改善临床PDT操作质量并降低并发症风险。

Abstract: Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient's body and tissues near the target. Simulations were performed to validate the controller's implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.

</details>


### [117] [A Perspective on Open Challenges in Deformable Object Manipulation](https://arxiv.org/abs/2602.22998)
*Ryan Paul McKennaa,John Oyekan*

Main category: cs.RO

TL;DR: 本文综述了机器人柔性物体操作领域的最新进展，包括多模态感知、强化学习、可微仿真等方法，并提出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 柔性物体（如布料、食材等）的操作因其高维形变和复杂交互而难以实现，但具备广泛应用前景，因此需要系统性地梳理该领域的主要难题和主流解决方案。

Method: 通过文献调研，梳理了多摄像头、主动视觉、触觉等多模态感知系统如何提升环境适应性，总结了物理知识驱动的强化学习和可微仿真在效率与精度上的优势，并评述了专家演示与生成式神经网络对标准化任务和仿真到现实迁移的促进作用。

Result: 系统总结了主流柔性物体操作方法的关键突破点，包括更好地处理遮挡、更强的任务泛化能力以及实时性和可扩展性的提升。指出了图神经网络和完备数据集对未来研究的重要性。

Conclusion: 随着感知和建模方法的进步，柔性物体操作有望推动机器人实现更广泛、更复杂的实际应用，未来应重点发展高层决策和丰富数据基础。

Abstract: Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM's real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.

</details>


### [118] [DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks](https://arxiv.org/abs/2602.23017)
*Dean Zadok,Tom Naamani,Yuval Bar-Ratson,Elisha Barash,Oren Salzman,Alon Wolf,Alex M. Bronstein,Nili Krausz*

Main category: cs.RO

TL;DR: 该论文提出了一种低成本、轻量化、3D打印的新型假肢手，能显著提升使用者在与电子设备交互、键盘打字及钢琴演奏等动作时的灵巧性和精度。


<details>
  <summary>Details</summary>
Motivation: 当前假肢手主要局限于简单抓握，缺乏对精细手指动作和复杂设备操作的支持，难以实现诸如打字、弹钢琴等任务。因此，亟需开发可复制、易获取且功能更强大的假肢手。

Method: 作者设计了一种3D打印的假肢手，具备手指间距调节机制、支持尺/桡偏的二维手腕（优化打字动作），以及手指独立按压控制。通过实验，展示参与者能用该假肢手完成实时打字和钢琴演奏。

Result: 结果显示，所设计假肢手能实现更高效的打字和钢琴键按压操作，显著提升了假肢用户对于精巧手部动作的控制能力。

Conclusion: 所提出的假肢手在维持传统能力的基础上，实现了在电子设备等多场景下的高精度手部操作，有助于提升假肢手的功能和用户体验。

Abstract: Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.

</details>


### [119] [InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation](https://arxiv.org/abs/2602.23024)
*Jiahao Liu,Cui Wenbo,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种用于全身移动操作的新框架InCoM，采用意图驱动感知和结构化协同，有效提升了机器人在协调控制和动态感知下的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的全身移动操作方法面临两个主要挑战：1）底座和机械臂的动作耦合，优化复杂度高；2）由于视点动态变化，机器人在移动操作时的感知注意力分配不佳。作者希望解决这两个问题，提升机器人在复杂场景下的操作能力。

Method: 提出InCoM框架，通过推断潜在运动意图，动态重加权多尺度感知特征，实现阶段自适应的感知注意力分配；引入几何-语义结构化对齐机制，加强多模态感知；在控制上，设计解耦的协调流匹配动作解码器，专门建模底座与机械臂的协调动作生成，降低优化难度。

Result: 在无需特权感知信息的条件下，InCoM在ManiSkill-HAB三个测试场景的成功率分别提升了28.2%、26.1%和23.6%，明显优于最先进的基线方法。

Conclusion: InCoM框架有效提升了全身移动操作机器人的协调控制及多模态感知能力，解决了动作耦合优化和动态感知分配难题，在实际任务中表现优异。

Abstract: Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.

</details>


### [120] [An Empirical Analysis of Cooperative Perception for Occlusion Risk Mitigation](https://arxiv.org/abs/2602.23051)
*Aihong Wang,Tenghui Xie,Fuxi Wen,Jun Li*

Main category: cs.RO

TL;DR: 本文针对自动驾驶车辆因视野遮挡带来的风险评估难题，提出了新的遮挡跟踪损失风险（RTL）评估指标，并基于实车数据验证其有效性，同时提出一种新型非对称式V2X通讯策略以提升风险缓释效果。


<details>
  <summary>Details</summary>
Motivation: 目前面向遮挡等感知盲区的自动驾驶风险评估不足，主流风险指标难以全面反映风险的累积效应，亟需更准确的风险度量方式，指导V2X部署策略并提升道路安全。

Method: 1. 提出聚合遮挡期间瞬时风险的RTL指标，用于量化遮挡风险整体效应；2. 利用真实多样的大规模数据集进行统计分析验证；3. 评估不同V2X渗透率和部署策略下RTL指标表现；4. 设计并测试一种可向非联网车辆传递预警的非对称通信架构。

Result: （1）提出的RTL指标能够反映遮挡下风险的整体暴露水平；（2）完全V2X渗透可理论消除该风险，但需要75%-90%的高渗透率才能获得可观成效；（3）创新的非对称通信机制即使在25%的渗透率下也比传统对称模型在75%下表现更优，且50%即趋于饱和。

Conclusion: 本研究提出了覆盖风险累积效应的评价新指标RTL，并通过创新V2X通讯机制，显著提升了低渗透环境下的风险缓释能力，为V2X部署策略和智能交通安全提升提供了高性价比技术路径与理论依据。

Abstract: Occlusions present a significant challenge for connected and automated vehicles, as they can obscure critical road users from perception systems. Traditional risk metrics often fail to capture the cumulative nature of these threats over time adequately. In this paper, we propose a novel and universal risk assessment metric, the Risk of Tracking Loss (RTL), which aggregates instantaneous risk intensity throughout occluded periods. This provides a holistic risk profile that encompasses both high-intensity, short-term threats and prolonged exposure. Utilizing diverse and high-fidelity real-world datasets, a large-scale statistical analysis is conducted to characterize occlusion risk and validate the effectiveness of the proposed metric. The metric is applied to evaluate different vehicle-to-everything (V2X) deployment strategies. Our study shows that full V2X penetration theoretically eliminates this risk, the reduction is highly nonlinear; a substantial statistical benefit requires a high penetration threshold of 75-90%. To overcome this limitation, we propose a novel asymmetric communication framework that allows even non-connected vehicles to receive warnings. Experimental results demonstrate that this paradigm achieves better risk mitigation performance. We found that our approach at 25% penetration outperforms the traditional symmetric model at 75%, and benefits saturate at only 50% penetration. This work provides a crucial risk assessment metric and a cost-effective, strategic roadmap for accelerating the safety benefits of V2X deployment.

</details>


### [121] [Marinarium: a New Arena to Bring Maritime Robotics Closer to Shore](https://arxiv.org/abs/2602.23053)
*Ignacio Torroba,David Dorner,Victor Nan Fernandez-Ayala,Mart Kartasev,Joris Verhagen,Elias Krantz,Gregorio Marchesini,Carl Ljung,Pedro Roque,Chelsea Sidrane,Linda Van der Spaa,Nicola De Carli,Petter Ogren,Christer Fuglesang,Jana Tumova,Dimos V. Dimarogonas,Ivan Stenius*

Main category: cs.RO

TL;DR: 本文介绍了一种新型可扩展的水下研究平台 Marinarium，其提供了现实条件下的机器人试验场，对比已有设施，展示了其在海事与太空机器人领域的多种实验应用。


<details>
  <summary>Details</summary>
Motivation: 当前水下与太空机器人实验存在实验环境与真实环境差距大、实验成本高等问题，亟需一种资源高效、可扩展、逼真的试验平台。

Method: 设计构建了 Marinarium，包括可扩展的水下/空中双域操作空间、可开闭天窗以引入真实气候、SMaRCSim 数字孪生系统，并与太空机器人实验室深度集成。通过数据和实验展示其在动态学习、多机器人协同、数字孪生以及跨域导航验证等领域的适用性。

Result: (1) 利用高保真动态数据，验证了基于学习的水下动力学辨识方法的可行性；(2) 展示了跨水下、地表和空中异构机器人协同任务；(3) 通过数字孪生降低了仿真与现实的差距；(4) 用中性浮力 ROV 验证了水下替代体对太空导航的方法。

Conclusion: Marinarium 为机器人领域提供了从仿真、实验室到真实环境的实验桥梁，是打造资源高效、模块化、允许多领域协同实验的研究设施，为海事和空间机器人实验提供了新方向和参考范式。

Abstract: This paper presents the Marinarium, a modular and stand-alone underwater research facility designed to provide a realistic testbed for maritime and space-analog robotic experimentation in a resource-efficient manner. The Marinarium combines a fully instrumented underwater and aerial operational volume, extendable via a retractable roof for real-weather conditions, a digital twin in the SMaRCSim simulator and tight integration with a space robotics laboratory. All of these result from design choices aimed at bridging simulation, laboratory validation, and field conditions. We compare the Marinarium to similar existing infrastructures and illustrate how its design enables a set of experiments in four open research areas within field robotics. First, we exploit high-fidelity dynamics data from the tank to demonstrate the potential of learning-based system identification approaches applied to underwater vehicles. We further highlight the versatility of the multi-domain operating volume via a rendezvous mission with a heterogeneous fleet of robots across underwater, surface, and air. We then illustrate how the presented digital twin can be utilized to reduce the reality gap in underwater simulation. Finally, we demonstrate the potential of underwater surrogates for spacecraft navigation validation by executing spatiotemporally identical inspection tasks on a planar space-robot emulator and a neutrally buoyant \gls{rov}. In this work, by sharing the insights obtained and rationale behind the design and construction of the Marinarium, we hope to provide the field robotics research community with a blueprint for bridging the gap between controlled and real offshore and space robotics experimentation.

</details>


### [122] [Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios](https://arxiv.org/abs/2602.23109)
*Kai Chen,Yuyao Huang,Guang Chen*

Main category: cs.RO

TL;DR: 提出了一种基于主动推断的新方法，有效提升自动驾驶在处理遮挡行人场景下的安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 遮挡行人会导致自动驾驶系统面临高不确定性的极端安全挑战，现有方法难以应对这些长尾场景。

Method: 引入基于主动推断框架，使用Rao-Blackwellized粒子滤波器高效估计行人混合状态，并通过条件信念重置和假设注入手段，建模行人多种潜在意图。规划算法采用CEM增强的MPPI控制器，实现高效搜索与鲁棒性。

Result: 实验证明，该方法相比基于规则和强化学习等主流基线，显著降低了碰撞率，并展现出可解释且类人化的驾驶行为。

Conclusion: 基于主动推断的信念驱动机制为自动驾驶在极端不确定性场景下提供了更加安全和类人化的决策能力，具备良好可解释性，优于现有主流方法。

Abstract: The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.

</details>


### [123] [Grasp, Slide, Roll: Comparative Analysis of Contact Modes for Tactile-Based Shape Reconstruction](https://arxiv.org/abs/2602.23206)
*Chung Hee Kim,Shivani Kamtikar,Tye Brady,Taskin Padir,Joshua Migdal*

Main category: cs.RO

TL;DR: 本文探索了不同接触方式对机器人利用触觉重建物体形状的影响，实现了更高效的触觉感知和更快的形状重建。


<details>
  <summary>Details</summary>
Motivation: 传统的依赖视觉的物体识别与形状重建在遮挡或透明场景下存在局限，触觉能够弥补这些不足。然而，触觉采集虽然信息丰富，却因每次接触都需耗时且增加硬件磨损，如何在最少接触下快速重构物体形状成为关键挑战。

Method: 作者将三种触觉接触方式（抓取-释放、手指扫掠滑动、手掌滚动）与基于信息理论的探索框架相结合，由形状补全模型指导下次最佳接触点选择，从而实现高效采样，并通过UR5e机械臂和Dexterous Hand硬件平台在不同物体几何下验证方案。

Result: 实验证明，手指扫掠滑动和手掌滚动两种方式大幅提升了触觉采集效率，使形状重建接触次数减少34%、重建精度提升55%。

Conclusion: 合理结合高效的触觉接触策略与信息理论驱动的探索机制，可显著提升机器人对物体形状的重建速度和精度，为智能机器人的物理交互感知能力带来新进展。

Abstract: Tactile sensing allows robots to gather detailed geometric information about objects through physical interaction, complementing vision-based approaches. However, efficiently acquiring useful tactile data remains challenging due to the time-consuming nature of physical contact and the need to strategically choose contact locations that maximize information gain while minimizing physical interactions. This paper studies how different contact modes affect object shape reconstruction using a tactile-enabled dexterous gripper. We compare three contact interaction modes: grasp-releasing, sliding induced by finger-grazing, and palm-rolling. These contact modes are combined with an information-theoretic exploration framework that guides subsequent sampling locations using a shape completion model. Our results show that the improved tactile sensing efficiency of finger-grazing and palm-rolling translates into faster convergence in shape reconstruction, requiring 34% fewer physical interactions while improving reconstruction accuracy by 55%. We validate our approach using a UR5e robot arm equipped with an Inspire-Robots Dexterous Hand, showing robust performance across primitive object geometries.

</details>


### [124] [SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly](https://arxiv.org/abs/2602.23253)
*Yijie Guo,Iretiayo Akinola,Lars Johannsmeier,Hugo Hadfield,Abhishek Gupta,Yashraj Narang*

Main category: cs.RO

TL;DR: 本文提出了一种结合仿真和现实强化学习的混合方法，用于提升机器人装配任务在现实环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人装配需要高精度和复杂的接触操作，仿真学习虽能训练出鲁棒策略，但难以直接迁移到现实；现实RL则较依赖人工监督，且泛化性差。作者旨在解决“仿真到现实转移”（sim-to-real gap）和人工依赖的问题。

Method: 作者提出SPARR方法，首先在仿真中用低层次状态和密集奖励训练基础策略，再在现实环境中借助视觉观测和稀疏奖励训练残差策略，对仿真与现实的差异进行补偿。

Result: 在多种现实两件式装配任务中，SPARR取得了接近100%的成功率，相较于最优零样本迁移方法，成功率提升38.4%，装配周期缩短29.7%。

Conclusion: SPARR混合方法有效提升了机器人装配能力，实现高成功率且无需人工专业知识，大幅优于现有仿真和现实RL方法。

Abstract: Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.

</details>


### [125] [Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots](https://arxiv.org/abs/2602.23283)
*Mike Y. Michelis,Nana Obayashi,Josie Hughes,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本文提出了一种基于简化无状态流体动力学模型的腱驱动仿生鱼机器人仿真环境，能够高效模拟鱼类运动，并以开源方式发布，可促进软体机器人水下运动的学习与控制研究。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在水下优雅游动受到流体-结构耦合与控制难题制约，现有方法计算量大，难以支持复杂控制或深度强化学习。为解决这一瓶颈，需要有效且易于集成的物理建模办法。

Method: 作者基于MuJoCo平台，采用简化、无状态的流体动力学模型，结合腱驱动鱼机器人，仅用两段真实游动轨迹，拟合出五个流体参数，匹配实验行为，并能广泛适用于不同驱动频率。

Result: 提出的无状态流体模型表现出较强泛化能力，能适应未见过的驱动方式，并优于经典解析模型（如长体理论）。仿真速度超过实时，可直接用于强化学习算法（如目标追踪，成功率达93%）。

Conclusion: 简单高效的仿真模型，通过与实物数据匹配后，能作为软体水下机器人数字孪生体，有利于大规模自动化学习与控制，推动水下软体机器人研究发展。

Abstract: Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.

</details>


### [126] [Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning](https://arxiv.org/abs/2602.23287)
*Demiana R. Barsoum,Mahdieh Nejati Javaremi,Larisa Y. C. Loke,Brenna D. Argall*

Main category: cs.RO

TL;DR: 这篇论文提出了一种轨迹重建算法，能够利用任务、环境和接口约束，将受限控制接口下得到的机器人演示动作，转换到完整的机器人控制空间中，实现更优的运动效率。


<details>
  <summary>Details</summary>
Motivation: 辅助机器人能为严重运动障碍者提供帮助，但由于高自由度机器人常被低维度接口（如单维吸吹管或2D摇杆）控制，导致用户无法充分展现其意图，只能生成受限且低效的机器人动作。作者希望突破这种接口带来的运动局限，将用户真实意图还原到机器人可实现的全部自由度中。

Method: 作者提出了一种轨迹重建算法，该算法综合了任务要求、物理环境以及接口本身的约束，自动将低维控制演示“提升”到高维空间，在不失去用户操作偏好的情况下消除接口带来的次优轨迹。算法在两类7自由度机械臂、分别用2D摇杆与1D吸吹控制接口进行的真实世界日常任务示范数据上进行了评估。

Result: 重建后的轨迹与从原始接口直接录制的轨迹相比，表现出更快、更高效的动作执行，同时仍然保留用户的个性化运动偏好。重建的演示可用于生成更优的控制策略。

Conclusion: 该算法有效突破了低维度接口对机器人运动的限制，使辅助机器人更好地反映用户真实意图，对提升辅助机器人用户体验具有重要意义。

Abstract: Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.

</details>
