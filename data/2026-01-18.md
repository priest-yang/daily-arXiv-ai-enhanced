<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.CL](#cs.CL) [Total: 78]
- [cs.RO](#cs.RO) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 本文提出了一套端到端流程，实现对人脸识别系统的对抗性补丁生成、优化与评估，可广泛应用于司法分析和安全测试领域。通过多种方法提升补丁隐蔽性并考察对识别系统的规避效果，同时引入AI生成语义描述以及检测机制。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别系统在安全场景中的广泛应用，系统面临对抗性攻击风险。需要开发对抗性补丁生成和评估方法，探究其在实际安全与司法中的应用价值。

Method: 1）采用FGSM生成针对身份分类器的对抗性噪声。2）利用扩散模型+反扩散增强对抗补丁的隐蔽性（平滑&亮度自适应）。3）将优化补丁贴到人脸图片，测试其规避能力。4）利用ViT-GPT2模型对生成对抗图像进行身份语义描述，用于司法解释与记录。5）通过感知哈希和分割检测对抗补丁，分别评估认证性能变化、语义描述变化和对抗检测能力。

Result: 优化后的对抗补丁能有效逃避人脸识别系统，同时保持自然外观；ViT-GPT2语义描述有助于理解对抗图片下的身份变化；感知哈希与分割技术可实现高达0.95的SSIM进行检测和分析。

Conclusion: 本文提出的端到端流程可有效生成、优化与评估对抗补丁，对提升面部识别系统安全性及司法场景分析有重要参考价值，同时为对抗补丁的检测和解析提供了新方法。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 提出了一种新颖的多传感器融合框架LCF3D，结合RGB图像2D目标检测与激光雷达3D目标检测，从而提升自动驾驶中的3D目标检测性能，尤其对行人、自行车等挑战性目标有效。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶需要高精度的3D目标检测，单一传感器（如激光雷达或相机）的检测结果存在误差。融合多源传感器数据虽有潜力，但高效融合面临挑战，尤其在不同传感器组合或数据分布变化下，方法的泛化能力有限。本文旨在提出一种更有效且具备泛化能力的数据融合方法。

Method: LCF3D 框架通过多模态融合提升3D检测。一方面采用late fusion（后期融合），筛除激光雷达误报——即仅保留与RGB 2D检测对应的激光雷达3D检测结果。另一方面用cascade fusion（级联融合）方案，用未匹配的RGB检测反推出新的3D候选区域，从而弥补激光雷达漏检。整体流程结合2D与3D检测结果以优化最终检测输出。

Result: 在KITTI与nuScenes等数据集上，LCF3D相较于单一激光雷达方法在行人、自行车、摩托车等难检测类别上有明显性能提升。此外，该方法对于训练测试域传感器组合不一致时，仍表现出较好的泛化能力。

Conclusion: LCF3D通过创新的融合策略，有效提升了多类别3D目标检测，尤其适用于传感器配置不一致的现实场景。该框架为自动驾驶等实际应用提供了更鲁棒的3D检测方案。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 本文比较了DenseNet121和EfficientNet-B0两种先进CNN架构在自动化儿童肺炎检测中的性能，结果表明EfficientNet-B0表现更优，并结合可解释性技术提升了模型透明度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎依然是全球儿童发病和死亡的主要原因，现有诊断方法依赖医生经验，容易受主观影响，亟需准确高效的智能诊断辅助工具。深度学习在医学影像分析领域已展现出巨大潜力，故本研究旨在评估并比较主流CNN模型在儿童肺炎X射线影像自动诊断中的表现。

Method: 利用公开的5,863例儿童胸部X光影像数据，通过归一化、调整尺寸和数据增强进行预处理。分别选用DenseNet121和EfficientNet-B0，在ImageNet预训练权重基础上微调，应用统一训练参数。模型性能用准确率、F1分数、MCC和召回率评估。引入Grad-CAM和LIME进行模型可解释性分析，定位影响预测的影像区域。

Result: EfficientNet-B0取得了84.6%的准确率、0.8899的F1分数、0.6849的MCC，均优于DenseNet121的79.7%准确率、0.8597 F1分数和0.5852 MCC。两模型召回率均高于0.99，灵敏度强，可解释性可视化结果显示关注肺部关键区域，模型决策具有临床相关性。

Conclusion: EfficientNet-B0综合性能更优且计算资源需求较低，适合临床推广。引入可解释性方法增强了AI在儿童肺炎辅助诊断中的透明度和可信度，有助于实际应用。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 本文提出NanoSD，通过网络结构优化和特征层生成蒸馏，将Stable Diffusion 1.5精简到可在移动端实时运行，并广泛适用于多种图像恢复和生成任务。


<details>
  <summary>Details</summary>
Motivation: 主流的潜在扩散模型在图像恢复等任务上性能优异，但推理开销大，难以部署到移动和边缘设备，目前轻量化方法多聚焦于U-Net精简或扩散步数减少，可能破坏潜在流形，影响泛化和多任务能力。

Method: NanoSD采用网络手术、特征层生成蒸馏和结构化缩放的联合优化，不仅作用于U-Net，也调整VAE编码-解码器，兼顾模型大小、推理时延和准确率，在模型权重大幅压缩的同时保持生成能力和多任务适应性。

Result: NanoSD参数量为130M-315M，可在移动端NPU上实现20ms级别的实时推理，作为基础模型在超分、去模糊、人脸修复和深度估计等任务上均优于现有轻量化扩散模型，兼具感知质量和易部署性。

Conclusion: NanoSD成为适合移动和边缘设备实时视觉生成与恢复任务的通用扩散基础模型，为端侧AI视觉应用提供了新的高效方案。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: 提出了一种统一深度哈希检索方法UniHash，在已见和未见类别下均获得了优异检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希方法主要分为pointwise与pairwise两类，分别在已见和未见类别检索上有优势，难以兼顾，因此迫切需要一种能统一两者优点的方法。

Method: 提出了双分支结构UniHash，其中center-based分支采用pointwise范式，pairwise分支采用pairwise范式，并通过一种新的哈希码学习方法实现分支间的双向知识迁移。引入互学习损失和SM-MoH模块，促进哈希表示对齐与信息交换。

Result: 在CIFAR-10、MSCOCO与ImageNet数据集上，经大量实验验证，UniHash在已见与未见类别检索任务中均达到了当前最优或领先的表现。

Conclusion: UniHash有效统一了pointwise和pairwise范式的优点，显著提升了哈希检索在多场景下的通用性和检索效果。

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 本文提出ViSIL评分，用于衡量多模态视频摘要（如文本和关键帧）中遗漏的信息量，是一种创新的信息论评价指标。


<details>
  <summary>Details</summary>
Motivation: 传统的自动摘要指标（如BLEU或ROUGE）无法度量不同模态（视频关键帧与文字）之间的信息覆盖程度，限制了多模态视频摘要质量的有效评价。

Method: 提出ViSIL评分，利用视觉-语言模型（VLM）推断，基于信息论框架量化视频摘要未覆盖的信息损失，实现不同模态摘要之间的直接比较。

Result: 实验表明ViSIL评分与人类及VLM在视频问答（VQA）任务中的表现高度相关。通过优化信息损失与处理速度的权衡，ViSIL指导下选出的摘要，在不增加处理负担的情况下，VQA准确率比纯文本摘要提升7%。

Conclusion: ViSIL为多模态视频摘要提供了统一且有效的评估标准，能够优化摘要选择，提升自动化处理任务效果。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 提出了TuneCLIP，一种提升开放权重CLIP模型在多下游任务表现的自监督微调方法，无需重新大规模预训练，能够有效提升主流开源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前CLIP如需提升性能往往需从零开始用大量数据进行预训练，成本极高。作者希望探索能否仅用已有自监督数据，通过微调开放权重的CLIP模型整体提升其在多下游任务的泛化能力。

Method: 提出TuneCLIP方法，包括两部分：1）warm-up阶段恢复优化统计信息，减少冷启动带来的偏差；2）fine-tune阶段采用新对比损失，降低误判负样本带来的惩罚，有效提升微调表现。

Result: TuneCLIP在多种模型结构和规模下均显著提升性能。以SigLIP（ViT-B/16）为例，ImageNet及相关out-of-distribution基准提升最高可达2.5%，DataComp基准提升1.2%。

Conclusion: TuneCLIP为后训练阶段快速适应和提升CLIP开放权重模型性能提供了新方法，在无需大量额外标注数据和重新预训练的情况下，显著增强了模型下游泛化能力，树立了高效微调新基线。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: VibrantSR是一种利用Sentinel-2卫星影像实现0.5米分辨率冠层高度模型（CHM）估计的新方法，准确度优于现有多种卫星方法，可用于大尺度持续森林监测。


<details>
  <summary>Details</summary>
Motivation: 现有基于航空影像的方法时间覆盖有限且获取成本高，无法满足大范围、持续的监测需求。基于卫星的低分辨率方法虽覆盖广但精度有限，因此迫切需要结合高频、全球覆盖与高精度的方法。

Method: VibrantSR采用生成式超分辨率框架，使用全球可用的Sentinel-2卫星的季节性复合影像，以10米数据生成0.5米分辨率的冠层高度模型。

Result: 在美国西部22个生态区做空间独立的验证分割实验，对>=2米高的树冠平均绝对误差为4.39米，优于Meta、LANDFIRE、ETH等卫星基线方法。虽然基于航空的VibrantVS更精确（2.71米），但其成本和时间局限更大。

Conclusion: VibrantSR在保持高频、低成本的同时大幅提升了冠层高度的监测精度，使得大洲级森林监测和碳核算变为可能，有望替代依赖昂贵且偶发的航空影像的传统做法。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 本文提出MedVL-SAM2，一种统一的3D医学多模态模型，支持报告生成、VQA和多种分割任务，通过多阶段训练实现报告、视觉推理与3D分割的深度融合，在多个任务上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在图像级文本任务上取得显著进展，但缺乏细粒度的3D空间推理和视觉定位能力，且难以在同一框架下统一处理多种任务。因此，亟需一个能在3D医学场景下同时实现高层次推理与精确定位的多功能模型。

Method: MedVL-SAM2通过整合图像级推理与像素级感知，提出一套统一架构，并引入基于SAM2的体积分割模块以实现精细的空间推理。训练流程分为两阶段：先用大规模3D CT图文对进行预训练，将视觉特征与文本嵌入对齐，随后在3D CT分割数据集上以语言理解和分割为目标进行联合优化，实现灵活的交互。

Result: MedVL-SAM2在报告生成、VQA和多种3D分割任务上取得了当前最优（SOTA）表现。大规模实验展示了其3D视觉定位、可控分割和跨模态推理的优异能力。

Conclusion: 该工作证明了高层次语义推理与精确3D空间定位可在统一的3D医学视觉-语言模型中同时实现，推动了多模态医学AI的发展。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: 本文提出了一种名为Transition Matching Distillation (TMD)的新方法，用于将高质量视频扩散模型蒸馏为高效的少步生成器，实现了速度与质量的兼顾。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频扩散模型因多步采样过程效率低，难以应用于实时交互式场景。亟需方法将其高质量生成能力迁移到快速推理模型。

Method: TMD将多步扩散去噪过程简化为少步的概率转移，每步由轻量级条件流建模。原模型分解为主干和流头，主干负责特征提取，流头负责多次流动更新。通过流头整合与分布匹配蒸馏训练学生模型。

Result: 对Wan2.1 1.3B和14B文本生成视频模型的蒸馏实验表明，TMD能在推理速度和视觉质量之间实现更优权衡，在生成质量、忠实度等方面优于现有蒸馏方法。

Conclusion: TMD框架有效提升了视频扩散模型的推理效率，同时保持高视觉质量和提示适应性，在视频生成领域有广泛应用潜力。

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

</details>


### [11] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OT-Drive的新型多模态融合方法，以解决自动驾驶在非结构化环境下可通行区域分割在分布外（OOD）场景中的泛化能力差问题。该方法在实验中表现优越，大幅提升了分割准确率。


<details>
  <summary>Details</summary>
Motivation: 现有可通行区域分割方法在面对未见过的环境（分布外场景）时，性能往往会明显下降，影响实际自动驾驶任务中的规划与决策。因此，提升分割模型在OOD场景下的泛化能力成为亟需解决的问题。

Method: 作者提出了一种基于最优传输（Optimal Transport）的多模态融合框架OT-Drive，将RGB和表面法线数据融合视为分布间的传输问题。具体方法包括设计一个场景锚点生成器（SAG），将场景信息分解为天气、时间、路面类型等联合分布，以此生成可泛化的语义锚点。然后，提出基于最优传输的多模态融合模块（OT Fusion），将多模态特征映射到上述锚点流形，实现更强的分布外鲁棒性。

Result: 实验结果显示，该方法在ORFD数据集的OOD场景上取得了95.16%的mIoU，较已有方法提升6.35%。在跨数据集迁移任务中获得89.79% mIoU，超过基线方法13.99%。

Conclusion: OT-Drive能够在仅需有限训练数据的情况下，在分布外场景中实现强泛化性能，显著提升了自动驾驶系统在真实环境中的实用性和效率。

Abstract: Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [12] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

TL;DR: 当前视觉-语言模型（VLMs）在空间关系理解上存在不足。本论文通过采用替代训练目标和2D位置编码提升了模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多采用类似CLIP的训练方式，将图像压平成一维序列，导致空间结构信息丢失，这对需要空间感知的应用如机器人和具身智能构成瓶颈。

Method: 作者探索了采用替代训练目标的图像编码器，以及在模型中引入二维位置编码，来增强空间信息建模能力。

Result: 经实验验证，这些改进能在多个空间推理基准任务上显著提升VLM的空间推理表现。

Conclusion: 通过改进图像编码方式和引入2D位置信息，可有效提升视觉-语言模型的空间推理能力，补齐其在空间感知上的短板，有助于相关应用的发展。

Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [13] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言推理分割方法DR^2Seg，通过自我奖励机制和两阶段推理策略，提升了推理效率和分割精度，并无需额外的推理监督。实验显示在多种大模型和分割模型上均取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在复杂文本推理分割任务中，常因“过度思考”产生冗余推理链，反而干扰目标定位，影响分割效果。因此需要提升推理分割中的推理效率与分割准确率。

Method: 方法提出了DR^2Seg框架，包括两阶段策略：首先进行多模态推理，生成清晰描述目标物体的文本；接着将该描述替换原始复杂查询用于分割验证。同时引入两个自我奖励机制，提升目标导向的推理、抑制冗余思考。

Result: 在多种规模的多模态大语言模型和分割模型上进行广泛实验，DR^2Seg均提升了推理效率和分割性能，优于现有方法。

Conclusion: DR^2Seg能在不需额外监督的情况下，有效提升推理分割任务的效率与准确率，为视觉-语言分割领域带来新思路。

Abstract: Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [14] [DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2601.10001)
*Chengjia Liang,Zhenjiong Wang,Chao Chen,Ruizhi Zhang,Songxi Liang,Hai Xie,Haijun Lei,Zhongwei Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的动态图加权双图注意力网络（DW-DGAT）用于帕金森病和阿尔茨海默病的早期诊断，在多个大型真实数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 帕金森病和阿尔茨海默病是最常见且不可治愈的神经退行性疾病，早期诊断至关重要。然而，多指标异构数据的高维性、来自神经影像和表型等不同类型数据的异质性，以及类别不平衡问题，极大地增加了早期诊断的难度。

Method: 作者设计了DW-DGAT模型，创新点包括：（1）通用数据融合策略，实现不同结构多指标数据的合并；（2）基于脑区和样本关系的双图注意力架构，提取微观和宏观多层次特征；（3）类别权重生成机制与两类损失函数结合，以解决类别不平衡影响。

Result: 基于PPMI和ADNI两个大规模公开数据集开展实验，DW-DGAT在早期帕金森及阿尔兹海默诊断任务中均表现出最优的性能。

Conclusion: DW-DGAT模型有效应对了多指标异构数据的挑战和类别不平衡问题，在神经退行性疾病的早期诊断领域具有较高的应用价值和推广能力。

Abstract: Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.

</details>


### [15] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 本文提出了一个新的基准VERHallu，用于评估视频大模型在事件关系上的幻觉问题，并提出了相应的方法改进。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型主要关注视频中事件、物体和场景的存在性幻觉，忽视了事件之间关系的幻觉（如因果、时序等），这影响了模型对复杂视频内容的理解。

Method: 作者提出了VERHallu基准，涵盖了关系分类、问答和反事实问答三类任务，专门考察因果、时序以及子事件间的关系幻觉，并使用人工标注样本区分视觉-语言和纯文本偏见。同时，提出了关键帧传播（KFP）策略，通过在中间层重新分配帧级注意力，以加强模型的多事件理解能力。

Result: SOTA的视频大模型在事件关系推理方面表现不佳，尤其是面对密集事件和反直觉场景时，模型往往依赖于先验知识而非充分利用帧级信息。应用KFP策略后，模型在事件关系幻觉上显著改善且推理速度不受影响。

Conclusion: 目前视频大模型在事件关系层面存在理解不足的问题，提出的新基准和KFP方法有效缓解了这一挑战，为提升视频理解能力提供了新方向。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [16] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像的人体再识别方法DiCo，通过层次化、可解释的跨模态表示，有效提升了细粒度检索能力。


<details>
  <summary>Details</summary>
Motivation: TIReID任务面临视觉与文本之间巨大模态差距，以及细粒度特征对齐难题。传统方法难以充分建模区分细节（如颜色、纹理、服饰风格）且缺乏可解释性。

Method: 提出了DiCo框架，利用共享的slot结构作为跨模态各部位锚点，并将每个slot进一步分解为多个概念块，实现属性（如颜色、纹理、形状）的解耦和，并保持图文细节的一致对齐。

Result: 在CUHK-PEDES、ICFG-PEDES、RSTPReid等公开数据集上，DiCo取得了与最新技术水平相当的性能，同时在细粒度检索和结果可解释性上表现突出。

Conclusion: DiCo不仅提升了TIReID任务的准确率，还为图文检索提供了更高的可解释性，有利于实际应用中对检索结果的理解与信任。

Abstract: Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [17] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了第一个基于事件相机的水下光流合成数据集，用以推动水下事件视觉领域的发展。


<details>
  <summary>Details</summary>
Motivation: 水下成像受多种物理因素影响，造成运动真值难以获取，限制了事件相机在水下环境下的研究进展。缺乏真实水下光学特性的事件光流数据集，也是主要障碍。

Method: 作者基于物理渲染的RGBD序列，利用先进的视频转事件处理流程，合成了具备真实水下光学特性的事件流，配套稠密真值光流、深度和相机运动。同时基准测试了多种光流预测方法。

Result: 数据集真实还原了水下光传播影响，为事件视觉研究者提供了丰富数据和算法基线，评估了主流方法在水下环境下的表现。

Conclusion: 该数据集为事件相机水下感知算法的开发与评测设立了全新标准，对未来研究具有重要推动作用。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [18] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种将链式推理（Chain-of-Frame, CoF）引入文本到图像（T2I）生成的模型CoF-T2I，通过逐步视觉细化过程提升图像生成质量，并取得了优异实验结果。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成模型展现了逐帧推理能力并在多种视觉任务中表现出色，但该能力尚未被有效应用于T2I生成，主要原因在于缺乏明确的视觉推理起点和中间可解释状态。该工作旨在弥补这一空白，探索如何利用视频模型提升T2I生成过程的可解释性和质量。

Method: 作者提出CoF-T2I模型，将CoF推理机制融入T2I生成流程，通过设计逐步的视觉细化过程，使中间帧充当推理步骤，最终帧作为生成结果。同时，构建了CoF-Evol-Instruct数据集，模拟从语义到美学的生成轨迹，并针对每一帧采用独立编码以提升画质并减少运动伪影。

Result: 实验显示，所提CoF-T2I模型在多个评测基准上优于基础视频模型，并取得了GenEval 0.86和Imagine-Bench 7.468的优异成绩。

Conclusion: CoF-T2I充分证明了视频模型在提升高质量T2I生成任务中的潜力，通过明确的逐步生成过程和合理机制，有望推动T2I技术进一步发展。

Abstract: Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [19] [ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology](https://arxiv.org/abs/2601.10073)
*Hyun Do Jung,Jungwon Choi,Hwiyoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的多实例学习方法ReaMIL，用于全视野数字病理切片分析，能够在保证模型准确率的同时，显著减少判别所需的切片数。


<details>
  <summary>Details</summary>
Motivation: 在病理图像分析中，传统多实例学习方法往往依赖大量切片信息，既影响模型推理透明度，也增加了计算负担。如何提高模型对关键信息的关注度、提升证据利用效率，是当前亟需解决的问题。

Method: ReaMIL在强基线MIL框架上增加了一个轻量级选择头，通过软门控机制选择部分切片，并采用预算充足目标函数（budgeted-sufficiency objective）训练：在限定选中切片数的稀疏预算下，仅用所选证据迫使预测概率大于等于τ。这一方法无额外监督，直接与标准MIL流程兼容。

Result: 在TCGA-NSCLC（LUAD vs. LUSC）、TCGA-BRCA（IDC vs. Others）和PANDA多个数据集上，ReaMIL与或略优于基线AUC，同时所需切片数显著减少（例如NSCLC上，AUC为0.983，仅需约8.2个切片），并能输出证据利用效率等诊断指标。

Conclusion: ReaMIL无需附加标注，便捷集成现有MIL流程，不但保证识别准确率，还极大提升了模型证据利用效率，为WSI分析提供了有效、透明的新范式。

Abstract: We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.

</details>


### [20] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D高斯泼溅(3DGS)的流引导几何驱动框架，实现了更具表现力的后印象派艺术风格3D风格迁移，侧重于几何抽象而非传统纹理投射。通过从2D画作中提取流场，将其反向映射到3D空间，实现与场景拓扑一致的结构变形，并创新性地用AI美学判断进行风格评价。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移主要将几何作为静态载体，仅在表面投射纹理，无法实现后印象派将结构形变作为表达核心的艺术追求。作者希望打破这一限制，使3D风格迁移也能实现‘在本质上夸张’——即几何结构本身根据艺术流派发生变形。

Method: 1）提出基于投影的无网格(flow-guided)几何变形机制，将2D艺术画中的方向流场映射到3D高斯原语，实现结构对齐和艺术变形；2）引入亮度-结构解耦方法，分离几何变形与颜色优化，避免大幅结构变化带来的伪影；3）利用AI大模型（VLM）作为评价标准，从美学判断艺术真实性，替代传统像素指标。

Result: 该方法能生成更符合后印象派美学的3D风格模型，结构表现更有表现力且远离机械投射纹理，艺术风格迁移更具主观真实性。亮度-结构解耦和AI美学评测辅助了稳定高质量生成。

Conclusion: 本工作首次在无网格3D风格迁移中全面实现几何抽象与艺术运动驱动的结构变形，突破了仅渐变色彩的惯例。为3D艺术风格迁移及美学计算提供了创新框架，对提升数字艺术表达力具启发意义。

Abstract: In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [21] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种新的Open-vocabulary 3D场景图（3DSG）生成方法RAG-3DSG，通过多图像构建对象和关系的图模型，显著提高了识别准确率和处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Open-vocabulary 3DSG方法在对象识别的准确率和生成速度上存在不足，主要受限于视角、遮挡和冗余点云问题。因此，提升3DSG的表现对于机器人感知与决策至关重要。

Method: 本文提出RAG-3DSG方法，通过re-shot引导的不确定性估计抑制聚合噪声，利用低不确定性对象实现检索增强生成（RAG）；同时引入动态下采样映射策略，实现跨图像对象聚合的自适应粒度和提升效率。

Result: 在Replica数据集上的实验证明，RAG-3DSG在生成3DSG时提升了节点标注（captioning）准确率，同时与基础版本相比，将映射时间减少了三分之二。

Conclusion: RAG-3DSG方法在保持高准确率的同时，大幅提升了3DSG生成的速度，有效缓解了多视角、遮挡等带来的聚合噪声，对机器人场景理解具有重要应用价值。

Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [22] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种基于难度引导的采样（Difficulty-Guided Sampling, DGS）方法，改善了数据集蒸馏目标与下游任务间的偏差，提高了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络训练资源消耗大，虽然数据集蒸馏能生成紧凑高质量的数据集用于高效训练，但现有方法多忽略下游任务的特定需求，导致蒸馏结果与实际下游性能不符。作者认为，结合有益于下游任务的特征进入蒸馏过程，有助于缩小蒸馏目标和任务目标之间的差距。

Method: 作者提出DGS方法：基于下游任务（如图像分类）的难度分布，对由现有蒸馏方法生成的图像池进行后处理采样，获得更符合下游特性的子集；并引入Difficulty-Aware Guidance（DAG），在数据生成阶段引入难度信息以辅助数据质量提升。

Result: 通过在多项实验设置下的结果验证，DGS和DAG能显著提升数据集蒸馏后在各种下游任务中的表现。

Conclusion: 论文证明了难度分布的引入能缩小蒸馏与下游任务的目标偏差，不仅提升图像分类任务表现，也为其它任务中的数据集蒸馏提供了新的思路。

Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [23] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

TL;DR: V-Zero是一种利用无标注图像自我提升视觉-语言模型的新框架，仅依靠模型自身生成的问题与答案进行训练，无需人工标注数据，结果在多个任务上取得了自我提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型依赖大量人工标注数据，收集代价高、耗时，因此需要找到减少或摆脱人工标注的新方法。

Method: V-Zero提出一种后训练框架，通过设定模型中的“提问者”和“解答者”两个角色，利用无标注图像进行自我问答，并各自通过奖励机制和伪标签优化，采用群体相对策略优化算法（GRPO）实现二者的共同提升。

Result: 在不使用任何人工标注数据的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上，视觉数学推理提升了1.7分，通用视觉任务提升了2.6分。

Conclusion: V-Zero证明了多模态系统可以仅凭无标注数据实现自我改进，为减少人工标注依赖提供了新途径。

Abstract: Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [24] [See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection](https://arxiv.org/abs/2601.10707)
*Amir Mallak,Erfan Aasi,Shiva Sreeram,Tsun-Hsuan Wang,Daniela Rus,Alaa Maalouf*

Main category: cs.CV

TL;DR: 本文发现基于基础模型提取的patch特征在自动驾驶中存在高度冗余，会导致策略对伪相关性过拟合，影响泛化性，并提出随机patch选择(SPS)方法，有效增强了策略的鲁棒性和泛化性能，同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前端到端自动驾驶依靠基础模型提取的patch特征，特征间高度冗余，可能导致模型仅依赖局部或非本质的关联，因而影响在新环境下(OOD)的泛化能力，亟需一种增强鲁棒、降低冗余的方法。

Method: 提出Stochastic-Patch-Selection (SPS)方法：每帧随机掩掉部分patch特征输入，不改变剩余patch的空间位置，使每次输入为同一场景的不同patch子集，促进模型基于不依赖特定token的特征决策。通过PCA和token相关度分析冗余性，并在多种设置下做消融实验。

Result: SPS在所有OOD场景下，都超过了之前的SOTA方法，平均提升6.2%，闭环仿真中最高提升20.4%，同时推理加速2.4倍。9种系统配置下有8种超过了原有SOTA。

Conclusion: SPS方法能显著提升端到端自动驾驶策略面对分布外场景的鲁棒性，并具有良好的迁移能力（无需额外调优可直接用于真实车辆），为泛化性强化提供了新思路。

Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.

</details>


### [25] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本论文提出了InfoSculpt框架，通过信息论方法，有效解决了通用类别发现（GCD）任务中已知与新类别的判别与泛化难题。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法（如伪标签、两阶段聚类）难以分离类别关键信号与实例噪声，缺乏理论机制，难以应对开放世界下的复杂场景。

Method: 采用信息瓶颈（IB）原理进行信息论建模，提出InfoSculpt框架，分别在有标签数据上采用类别级条件互信息（CMI）目标，在全数据上用实例级CMI目标，联合优化，促使模型学到紧凑、判别性强的类别表示，并压缩实例特有噪声。

Result: 在8个基准数据集上进行了大量实验，InfoSculpt方法都显示出显著优于以往 GCD 方法的效果，说明所提信息论建模和特征空间雕刻策略有效。

Conclusion: InfoSculpt能在保持类别信息的同时，有效去除实例噪声，实现了鲁棒可泛化的表征，推动了GCD任务在实际开放世界应用的发展。

Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [26] [FlowAct-R1: Towards Interactive Humanoid Video Generation](https://arxiv.org/abs/2601.10103)
*Lizhen Wang,Yongming Zhu,Zhipeng Ge,Youwei Zheng,Longhao Zhang,Tianshu Hu,Shiyang Qin,Mingshuang Luo,Jiaxu Zhang,Xin Chen,Yulong Wang,Zerong Zheng,Jianwen Jiang,Chao Liang,Weifeng Chen,Xing Wang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: 本文提出了FlowAct-R1框架，实现了高真实感、可互动的人形视频实时生成，在性能和实时性的权衡上取得了突破。


<details>
  <summary>Details</summary>
Motivation: 现有视频合成技术难以兼顾高保真度和实时性，特别是在交互式人形视频生成领域，面临响应速度慢、时序一致性差等问题。

Method: 基于MMDiT架构，提出FlowAct-R1引入chunkwise diffusion forcing与自监督变体以减少错误积累，保证长时交互的一致性，并结合蒸馏和系统优化，实现流式高效视频合成。

Result: 在480p分辨率下达到25fps实时性能，首帧响应仅需1.5秒，表现出全身细致控制和动作自然过渡，支持多样风格角色。实验结果显示生成视频的行为生动且感知真实。

Conclusion: FlowAct-R1能够解决以往高保真和低延迟难以兼得的问题，为交互式人形视频生成提供了有效方案，在实际应用场景中展现出良好的性能和泛化能力。

Abstract: Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

</details>


### [27] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

TL;DR: 本文提出了MathDoc数据集，用于从真实高中数学试卷中自动提取结构化题目信息，并评估模型在视觉噪音下的性能。


<details>
  <summary>Details</summary>
Motivation: 目前自动化从数学试题纸中抽取结构化题目在智能教育中极其重要，但现实场景下因视觉噪声极大而有挑战性。现有基准更关注于干净文档或通用布局分析，忽视了数学问题结构的完整性以及模型拒绝不完整输入的能力。

Method: 作者构建了MathDoc，这是首个来自真实高中数学试卷的信息抽取基准，包含3609个精心整理的问题，同时显式包括难以辨识的样本，评估模型主动拒绝无效输入的能力。提出了涵盖题干准确性、视觉相似性和拒绝能力的多维度评测框架。

Result: 在主流多模态大模型（如Qwen3-VL和Gemini-2.5-Pro）上的实验表明，虽然这些端到端模型在信息提取上表现优异，但它们普遍不能对不可辨认的输入做出有效拒绝，反而自信地输出无效结果。

Conclusion: 当前多模态大模型在应对劣质文档可靠性方面存在重大缺陷，MathDoc为评估模型在复杂文档环境下的可靠性提供了新基准。

Abstract: The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [28] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视觉上下文学习（VICL）框架，突破了以往单一提示融合的限制，通过多分支、多组合的协同融合方式提升模型在多视觉任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的视觉上下文学习方法通常只选取最优提示或将前K个提示简单融合为单一表达，这样做忽略了提示间的多样性和丰富信息，限制了模型的推理能力。作者认为需要更具协同和多元性的融合方式。

Method: 作者提出了一种多组合协同融合框架，不是将多个提示合并为一个表达，而是利用多组高质量提示分别生成三条上下文表示分支。这些互补的引导信号被送入新设计的MULTI-VQGAN架构，实现对多源信息的联合解析和利用。

Result: 通过在前景分割、单目标检测、图像上色等任务上的大量实验证明，所提方法在跨任务泛化能力、上下文融合效果及预测的鲁棒性和准确率方面，相较于现有方法有显著提升。

Conclusion: 多组合协同融合和MULTI-VQGAN结构能有效发挥视觉提示的多样性，充分融合丰富信息，对提升视觉上下文学习的泛化和表现极具潜力。

Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [29] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉上下文学习（VICL）框架，通过整合多个提示的信息和引入特定排列的轻量级模块，提升了图像修补等视觉任务的适应能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有VICL方法通常只挑选最相似的提示，忽略了其他高质量提示中包含的互补信息；另外，这些方法没有充分利用不同提示排列中蕴含的结构化信息，导致模型适应性受限。

Method: 1. 设计自适应融合模块，将多个提示中的关键信息与注释综合，生成更精准的上下文提示。2. 引入针对排列特定的轻量级MLP，将布局先验与核心模型解耦，减少对整体模型的影响。3. 提出双向微调机制，交换查询和提示的角色，促使模型通过融合上下文重建原始提示，强化融合模块和修补模型间的协作。

Result: 在前景分割、单物体检测和图像着色等任务上的实验结果显示，该方法取得了更优的性能，并展现出较强的跨任务泛化能力。

Conclusion: 提出的端到端VICL框架能有效利用多个多样提示及其结构信息，相比以往方法表现更优，对多种视觉任务具有较好适应性和泛化性。

Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [30] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种替代dropout的新型特征扰动方法，用于提升半监督医学图像分割的效果，并通过大量实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统基于dropout的特征扰动方法在半监督医学图像分割中需要手动调整dropout率，这一超参数调优困难，且可能导致不理想的正则化效果。因此，需设计一种无需精细调节且扰动效果可控的新机制。

Method: 作者提出VQ-Seg，首次将向量量化（VQ）用于特征空间离散化，引入了新的可控扰动模块QPM，通过打乱代码本索引的空间位置来扰动离散特征。此外，采用双分支架构，共享量化后特征用于重建和分割以减小信息损失，并引入后量化特征适配器PFA，结合基础模型指导以补偿高层语义丢失。论文还构建了一个大规模肺癌CT数据集进行实证。

Result: 在自行收集的大型肺癌数据集及多个公开基准上，VQ-Seg的性能优于现有主流半监督分割方法，证明了其优势。

Conclusion: VQ-Seg有效克服了dropout相关超参数敏感和不易调优的问题，在半监督医学图像分割任务中具备更好的可控性和泛化性，有望在相关领域获得广泛应用。

Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [31] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: 该论文提出LaViT框架，通过对齐教师和学生模型的视觉注意力过程，提升多模态推理中视觉信息的利用效率，显著增强了模型的视觉基础能力，让小型模型也能超越更大规模竞品。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法主要依赖外部辅助监督，忽视了模型自身的视觉注意力动态，导致学生模型在知识蒸馏时仅模仿文本输出但视觉关注区域与教师模型偏离，未能真正实现视觉信息的有效迁移。

Method: 本文提出LaViT框架，通过对齐学生和教师在生成文本前的视觉语义和注意轨迹，让学生模型自回归地复现教师的视觉理解过程，且借助课程式感官门控机制防止捷径学习。

Result: 实验显示，LaViT在复杂推理任务上的视觉落地能力显著提升，最大取得16.9%的性能增益，小型3B模型甚至超越了更大规模的开源模型和GPT-4o等专有模型。

Conclusion: LaViT证明了对齐视觉思维（而非仅静态表征）的有效性，为多模态推理中视觉感知能力的提升提供了新路径。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [32] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了视频异常推理（VAR）新任务和大规模数据集，并开发出新的多模态大模型，显著提升了模型在视频异常分析中的分阶段推理与决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频异常检测与理解领域的推理能力有限，主要停留在异常定位或事后描述，缺乏系统的推理过程、风险感知与决策解释。因此，亟需推动视频异常分析从描述性理解升级到结构化、多阶段推理。

Method: 1）定义了视频异常推理（VAR）任务，要求模型对异常事件进行逐步推理（包括视觉感知、因果解释和风险决策）。2）构建了包含8,641段视频、逾50,000条标注的大型新数据集，标注基于“感知-认知-行动”链式思维。3）提出异常感知组相对策略优化方法，以弱监督提升推理可靠性。4）开发名为Vad-R1-Plus的新型MLLM推理模型。

Result: 实验结果显示：新任务和数据集可系统评估多阶段异常推理；Vad-R1-Plus模型在VAR任务中优于现有开源和专有基线模型，推理稳定性与风险决策能力提升明显。

Conclusion: 本文推动了视频异常理解从简单描述到结构化推理的新阶段，通过任务、数据集和方法的创新促进了多模态大模型在复杂视频推理领域的进步。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [33] [From Physical Degradation Models to Task-Aware All-in-One Image Restoration](https://arxiv.org/abs/2601.10192)
*Hu Gao,Xiaoning Lei,Xichen Xu,Xingjian Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: OPIR方法通过预测针对任务的逆降质算子，实现了一种高效、一体化的图像修复方案，且在多项任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有一体化图像修复方法为应对多种修复任务常需引入提示信息或大模型，导致系统复杂度升高，不利于实际应用和实时处理。作者希望设计一个结构简洁、效率高、适用多任务的修复框架。

Method: 方法以物理降质建模为基础，预测任务感知的逆降质算子用于初步修复，并生成难以重建区域的不确定性感知图。第二阶段依据不确定性图进一步精修修复结果。两个阶段均复用同一逆算子预测网络，通过后置任务相关参数适配不同修复任务，并加速逆算子的卷积操作，实现高效推理。

Result: 该OPIR架构在多项实验中，展示出优异的一体化修复性能，并且在单一任务上的表现也具高度竞争力。

Conclusion: OPIR方法结构紧凑，推理高效，能适应多种图像修复任务，且无需引入大量额外模块，兼顾了修复质量与系统简洁性。

Abstract: All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.

</details>


### [34] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了ELITE方法，通过结合3D和2D生成先验，实现了高效、拟真、可动画的人头头像建模并具备极强的泛化能力，显著提升了合成速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频生成三维头部头像的方法依赖3D或2D数据先验，各自存在泛化性差或计算消耗大且易出现身份幻觉等缺陷。本文旨在结合两者优点，解决现有技术的局限。

Method: 提出了Mesh2Gaussian先验模型（MGPM），实现高效头像初始化，并设计了测试时生成自适应阶段，融合真实与合成图像监督。通过渲染引导的单步扩散增强器，基于高斯头像渲染恢复视觉细节，规避了以往耗时且易幻觉的问题。

Result: ELITE在视觉质量上优于同类方法，尤其在复杂表情处理上表现出色，同时合成速度比2D生成先验方法快60倍。

Conclusion: ELITE证明了结合3D和2D生成先验的有效性，兼具高效、拟真、高泛化性，为现实场景下高质量头像合成提供了新方案。

Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [35] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DepthDirector的视频重渲染框架，实现了在精准摄像机控制下的视频内容一致生成。通过引入深度视频和Dual-Stream条件机制，显著提升了摄像机控制的精度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有条件视频生成方法难以在精确更改摄像机轨迹时保证原本视频内容的一致性。主流3D扭曲方法未充分利用视频扩散模型（VDM）的3D先验，且易陷入Inpainting Trap，导致主体失真、生成质量下降。

Method: DepthDirector利用显式三维表示生成的深度视频来引导摄像机运动。提出View-Content双流条件机制，将源视频与目标视角下渲染的深度序列共同输入预训练的视频扩散模型。同时，基于LoRA设计轻量适配器进行训练，保留模型知识。提出并构建了包含8K视频的大规模多摄像机同步数据集（MultiCam-WarpData）。

Result: DepthDirector在摄像机可控性和视觉质量方面显著优于现有方法。

Conclusion: 该方法通过深度引导和双流条件机制，有效实现了摄像机轨迹精确可控的视频生成，并能保持视频内容一致性，具有良好的应用前景。

Abstract: Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [36] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个集成多步推理和多阶段优化的系统，显著提升了MLLMs在复杂视频问答任务（如HD-EPIC VQA）上的表现，达到了41.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在处理如HD-EPIC VQA等复杂视频问答任务时表现不佳，主要是因为查询/选项含糊、难以进行长时序推理和输出格式不统一。

Method: 作者提出了端到端优化的框架，包括：1）对查询和选项的预处理；2）基于领域知识对Qwen2.5-VL进行微调；3）创新性地提出时序链式思维（T-CoT）提示进行多步推理；4）对输出进行鲁棒后处理。

Result: 该系统在HD-EPIC VQA基准上取得了41.6%的准确率，优于以往方法。

Conclusion: 证明了复杂视频理解任务需要端到端整体优化。作者提供了完整的代码和微调模型，有望推动视频问答技术发展。

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [37] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

TL;DR: 提出一种方法，根据演讲者的讲述内容自动识别并高亮幻灯片中最相关的区域，提高观众理解和吸收关键信息的效率。


<details>
  <summary>Details</summary>
Motivation: 现实中，观众在跟随演讲时，常因不能及时找到幻灯片上与讲述内容相关的区域而造成理解障碍，尤其在信息密集或节奏较快的会议报告中更为突出。这个问题影响了多媒体内容的无缝理解。

Method: 通过分析演讲者的语音内容，并将其与幻灯片上的文本或图形内容进行匹配，自动检测并高亮与讲述同步的重要幻灯片区域。文中还对不同的解决方案进行了探索，并分析了它们的成功与失败案例。

Result: 该方法能够提升观众对内容丰富的视频（如教育视频、会议报告）的理解力和吸收效率，显著减少了寻找关键信息及同步视觉与听觉内容的认知负担。相关代码和数据集已开放获取。

Conclusion: 自动高亮幻灯片相关区域的方法，有助于提升多媒体文档的理解和学习效率，对教育及学术领域具有实际应用价值。

Abstract: Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [38] [DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset](https://arxiv.org/abs/2601.10305)
*Hengyu Shen,Tiancheng Gu,Bin Qin,Lan Wu,Yuling Wu,Shuo Tan,Zelong Sun,Jun Wang,Nan Wu,Xiang An,Weidong Cai,Ziyong Feng,Kaicheng Yang*

Main category: cs.CV

TL;DR: 本文提出了一个高质量的中文跨模态图文数据集DanQing，总计1亿组图文对，以解决当前中文视觉-语言预训练领域数据稀缺的问题。DanQing通过严格筛选流程，数据质量优于现有数据集，且基于最新的网络数据更具实用性。实验表明，使用DanQing继续预训练后，模型在中文下游任务中表现更佳。该数据集将开源促进社区研究。


<details>
  <summary>Details</summary>
Motivation: 当前英文视觉-语言预训练模型在多个任务中表现优异，但中文领域由于缺乏大规模高质量图文数据集，发展缓慢。因此需要一个高质量的中文图文数据集来弥补这一空白。

Method: 作者构建了一条完整的中文图文数据收集与筛选流程，从Common Crawl中获取并严格筛选了1亿组2024-2025年间的图文对作为数据集，命名为DanQing。然后用SigLIP2模型在该数据集上进行继续预训练，与其他数据集进行了对比实验。

Result: 在零样本分类、跨模态检索和多模态大模型评测等多种中文下游任务中，基于DanQing继续训练的模型性能均优于现有数据集。

Conclusion: DanQing数据集在数据量和质量上均优于现有中文图文数据集，并能显著提升下游任务表现。数据集将以开源形式发布，推动中文视觉-语言预训练领域研究。

Abstract: Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.

</details>


### [39] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉-语言模型（VLP）的通用多模态对抗攻击框架——分层精炼攻击（HRA），针对现有对抗攻击方法样本依赖性强、计算开销大的问题，显著提升了攻击泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLP模型的对抗攻击大多数依赖于针对具体样本生成扰动，拓展到大规模数据集或新场景时计算负担重且适应性差。因而亟需能在多任务、多模型、多数据集上有效泛化且高效的通用对抗攻击方法。

Method: HRA框架分别在样本级和优化级对通用对抗扰动（UAP）进行细致优化。视觉模态方面，将对抗样本拆分为干净图像和扰动分别处理，并引入ScMix增强策略提升UAP的多样性和全局/局部有效性，同时在优化中利用历史与预估梯度的时间层级，改善收敛路径。文本模态方面，融合句内与句间重要性度量，识别全球性关键扰动词，实现通用文本扰动。

Result: 在多种下游任务、VLP模型及数据集上做了大量实验，验证了HRA通用多模态攻击方法的有效性与优势。实验结果显示其对比现有方法表现更优，攻击能力和效能显著提升。

Conclusion: 分层精炼攻击（HRA）不仅提升了对抗扰动的泛化能力和高效性，也为多模态通用对抗攻击领域提供了新框架和理论支持，具有良好的实用性和推广价值。

Abstract: Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [40] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

TL;DR: 本文提出ROMA，一种用于统一音频、视觉和文本的实时多模态大模型助手，能够流式处理和主动-被动交互。ROMA通过密集音频与离散视频同步并引入新型响应机制，在多个基准上实现了新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多模态大模型在支持流式音视频理解时能力分散，难以实现完整、统一且能主动监控的多模态输入。作者希望解决现有方法模态不全或缺乏主动交互的问题。

Method: ROMA统一处理连续音频和离散视频帧，通过同步不同模态来解决粒度不匹配问题。为保证在线决策的准确响应，设计轻量级speak head组件，将响应的触发和生成过程解耦。模型以特定流式数据集和两阶段课程学习进行训练，并重新组织现有基准任务为主动和被动设置下的统一评测套件。

Result: ROMA模型在12项主动与被动基准测试中，主动任务（如告警、解说）取得最优结果，在被动任务（如问答）也表现出很强竞争力，展现了统一实时多模态理解的稳健性。

Conclusion: ROMA有效实现了音频、视频、文本的流式统一理解和主动推理，推动了实时多模态大模型向更完整且智能助手方向发展，对端到端应用具有重要意义。

Abstract: Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [41] [SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition](https://arxiv.org/abs/2601.10324)
*Yiming Zhang,Weibo Qin,Yuntian Liu,Feng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SRAW的新型对抗攻击方法，通过优化的空间变形对SAR目标识别系统生成高隐蔽性的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络驱动的SAR自动目标识别虽然效果优良，但对抗鲁棒性差，容易受到对抗样本攻击，且现有攻击方法常需较大、可见的扰动，不利于实际隐蔽攻击场景，亟需在效果与隐蔽性中取得平衡的新方法。

Method: 提出空间再加权对抗形变（SRAW）方法，通过在SAR图像中针对前景和背景区域分配差异化的扰动预算，并利用空间优化的方式实现对抗攻击，从而生成既有效又难以被察觉的对抗样本。

Result: 大量实验证明，SRAW方法能显著降低先进SAR-ATR模型的识别性能，并在对抗样本不可察觉性和攻击可迁移性方面均优于现有方法。

Conclusion: SRAW方法为SAR图像目标识别安全性带来了新挑战，且在对抗攻击有效性和隐蔽性方面取得了更好平衡，显示其在实际防御和安全评估中的重要应用前景。

Abstract: Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.

</details>


### [42] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种新的文生图生成范式“think-then-generate（T2G）”，即先让大语言模型（LLM）对文本提示进行推理和重写，再据以生成图像，以提升生成内容的语义和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 目前的文生图扩散模型（T2I DMs）大多仅将大语言模型作为文本编码器使用，未充分发挥其推理能力，导致生成的图像常常仅仅是文字到像素的简单映射，缺乏深入理解和推理。

Method: 作者提出T2G范式，首先以轻量监督微调激活LLM的‘先思考后改写’能力, 让其对用户原始文本进行推理和重写，然后将重写后的文本作为扩散模型的条件继续生成图像。LLM编码器与扩散骨干采用Dual-GRPO协同优化，通过基于图像的奖励强化LLM推理世界知识，同时优化视觉语义一致性。

Result: 该方法在推理型图像生成和编辑基准上，事实一致性、语义对齐和视觉逼真度显著提升，在WISE指标上达到0.79，几乎与GPT-4持平。

Conclusion: 实验结果表明该方法有效地提升了具备推理和表现能力的统一文生图模型，向具备推理、表达和演示能力的下一代模型迈出了重要一步。

Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [43] [An analytic theory of convolutional neural network inverse problems solvers](https://arxiv.org/abs/2601.10334)
*Minh Hai Nguyen,Quoc Bao Do,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: 本文提出用局部等变MMSE（LE-MMSE）方法剖析和解释卷积神经网络（CNN）在成像逆问题中的表现，从理论层面揭示CNN的黑盒特性，并通过实验证明LE-MMSE能很好预测CNN的输出。


<details>
  <summary>Details</summary>
Motivation: 目前虽然CNN在成像逆问题中非常有效，但其为何有效的理论基础不明确，理解局限于经验层面。作者希望以理论化的视角量化和解释CNN的推断过程，减少其作为“黑盒”的弊端。

Method: 作者以MMSE为理论基础，融入CNN的两种重要先验：平移等变性和局部感知野，推导出适用于这类网络的解析公式，称为Local-Equivariant MMSE（LE-MMSE）。随后在多种逆问题、数据集、神经网络架构下对比实测NN输出与理论结果的一致性。

Result: 实验证明，LE-MMSE方法能准确预示已训练CNN在多种常用数据集和网络结构上的输出表现（PSNR大于25dB）；理论与实际模型表现高度吻合。并深度分析了在物理相关和无关的估计器、高密度区域、数据/patch大小等因素对性能的影响。

Conclusion: LE-MMSE为理解CNN在成像逆问题中的行为提供了清晰的理论工具，弥补了理论和实践之间的缺口。该工具有助于未来进一步优化和解释相关网络架构的工作。

Abstract: Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).

</details>


### [44] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了用于文本引导的人体姿态编辑评测的新基准HPE-Bench，以及基于多模态大模型的统一评估框架，有效提升了姿态编辑的检测与质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本引导的人体姿态编辑存在结构异常与伪影问题，且现有评估方法很难细致地反映姿态编辑中的一致性和真实性，为了解决这些问题，需要更专业的评测基准及多维度精准评价方式。

Method: 作者构建了HPE-Bench基准，采集自17种先进编辑模型的1700个样本，标注了真实性和多维质量分。提出基于分层选择的多模态大语言模型（MLLM），结合LoRA调优和新的层敏感机制（LSA），以确定评估姿态的最优特征层，并用于真实性检测与质量回归。

Result: 所提框架在真实性判别和多维质量回归中均优于现有方法，能更准确评估编辑后的姿态一致性和图像质量，填补了取证检测与质量评价之间的空白。

Conclusion: HPE-Bench基准和统一多模态大模型评测框架有效提升了姿态编辑的评估能力，为后续AIGC相关研究提供了标准数据集与评估流程。

Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [45] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

TL;DR: 文章提出了一种新的基于扩散模型的图像压缩方法DiffCR，显著提高了压缩效率和图像质量，同时大幅度提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像压缩方法虽然效果好，但存在采样解码慢和比特分配不佳的问题，影响实际应用。作者希望提升压缩率和加速解码过程。

Method: 提出了DiffCR压缩框架，核心包括FaSE模块（利用频率解耦注意力，结合预训练扩散模型优化预测先验）和轻量级一致性估计器，用于实现高效的两步解码。无需修改主扩散模型即可提升效果。

Result: DiffCR比现有最前沿扩散压缩方法节省27.2%（LPIPS）和65.1%（PSNR）的比特率，且解码速度提升超过10倍。

Conclusion: DiffCR在不改变主模型的前提下，大幅提升了基于扩散模型的图像压缩质量和效率，为实用化应用提供了有力支持。

Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [46] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉编码与文本输入的新型Transformer（VIST2），通过图像形式对文本进行全局上下文压缩，并有效减少推理过程中的计算和内存消耗，取得了显著加速和资源节省效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在端到端OCR和部分压缩上已有探索，但在逐步推断阶段无法节省资源，亟需一种在预填充和推断阶段皆可减少Token数的方法，从而提升效率。

Method: 提出VIST2模型，将文本分块并渲染为素描图像，与文本原文交错输入Transformer。模型主要依赖视觉Token预测下一个文本Token，通过分阶段训练，包括光学语言模型的课程式预训练和跨模态指令微调。

Result: 在0.6B到8B参数规模的VIST2模型上，通过4倍压缩比，在长文本生成任务上，相较于基线方法，实现了首Token生成速度提升3倍、显存消耗降低77%、FLOPS降低74%的优异表现。

Conclusion: VIST2通过全局上下文压缩和视觉编码的创新架构，显著提升了大模型在长文本生成方面的效率和资源利用率。代码和数据集开源，有望推动该领域进一步研究。

Abstract: Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [47] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 论文提出了一种缺失感知的多模态生存分析框架，能集成CT、全切片病理图像和临床变量，有效预测不可切除II-III期非小细胞肺癌（NSCLC）患者的整体生存期，并且能自然应对部分模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习在精确预后和生存预测中具有前景，但实际应用受限于小样本量和常见的模态缺失，导致数据利用低下。此问题在肺癌等异质性强的疾病中尤为突出，需有新方法充分利用多模态数据并增强模型的鲁棒性。

Method: 引入了一个缺失感知的多模态生存框架，采用基础模型对不同模态（CT影像、全切病理切片、结构化临床变量）进行特征提取，并设计了缺失感知的编码策略以支持模态缺失条件下的中间层融合。该策略使得模型能够在训练和预测时利用所有可用数据，无需因模态缺失而舍弃病例或强行填补数据。

Result: 实验证明该框架的中间层融合方法持续优于单一模态、早期融合和晚期融合，并且在病理图和临床变量融合时C-index最高（73.30）。进一步分析发现，模型能自适应地降低信息量较少的模态（如CT）的权重，减小其对最终预测的影响。

Conclusion: 所提框架具备处理多模态不完整数据的能力，显著提升了NSCLC患者生存期预测的准确性，有望推动多模态深度学习在临床预后中的实际应用。

Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [48] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

TL;DR: 论文提出了一个新的计算方法，将多张时序荧光显微图像整合为单张高质量图像，有效提升了生物内容的可视化和细胞计数精度。


<details>
  <summary>Details</summary>
Motivation: 荧光显微用于活体生物样本分析，但其图像受噪声、时间变化及信号不稳定影响，造成后续分析受限。因此需要改进时序图像的整合与显示方法，以提升信号的一致性和图像质量。

Method: 提出一个结合多种可解释计算机视觉技术的框架，将多个时序帧的信息融合，生成一张高质量、保留生物学内容的复合图像。通过111种不同配置和具挑战性的数据集（包括动态异质、复杂形态的心脏细胞单层）对方法进行了验证。

Result: 新框架生成的复合图像在信息保持和增强方面优于单帧显微图像，并且与现有方法相比，细胞计数平均提升了44%。

Conclusion: 该方法不仅提升了荧光显微图像的可用性，同时适用于其他需要多时序图像融合成高质量2D图像的领域，有利于后续的标注与分割任务。

Abstract: Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [49] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

TL;DR: 本论文提出了一种可以从月球数字高程模型（DEM）直接预测空间变化BRDF参数的新方法Lunar-G2R，无需多视角图像或专用硬件，即可有效还原地表真实反射特性，显著提升渲染和导航的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有月球渲染流程通常采用简化或空间均匀的BRDF模型，难以估计参数，也无法捕捉地表的局部反射特性，导致照片真实感受到限制。缺乏更真实、空间变化反射参数的估计方法，限制了高保真渲染和视觉导航的精度。

Method: 提出Lunar-G2R，一个基于U-Net和可微分渲染的几何到反射率学习框架。方法直接从DEM计算空间变化的BRDF参数，无需多视图、受控照明或反射采集硬件。在训练过程中，通过可微分渲染主导损失，最小化真实轨道图像和物理渲染结果之间的光度差异。

Result: 在Tycho陨石坑的地理隔离区域实验表明，该方法比最先进的基线方法将光度误差降低了38%，同时获得更高的PSNR、SSIM和感知相似度，能准确捕捉到空间均匀模型缺失的细粒度反射变化。

Conclusion: 这是首次可以仅从地形几何直接推断空间变化反射模型的方法，对高保真月球渲染和自主导航具有重要意义，也为其他行星表面的反射率自动估计提供了新途径。

Abstract: We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [50] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉-语言推理框架，能够从卫星图像中分割社会语义实体，并发布了相应的数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 目前的分割模型虽然对物理属性明确的实体（如建筑物、水体）表现良好，但对社会属性（如学校、公园）等社会语义类别分割效果不佳。因此，亟需有效方法针对社会语义实体进行分割，以满足实际应用需求。

Method: 1. 提出了Urban Socio-Semantic Segmentation数据集SocioSeg，包含卫星图像、数字地图及按层级组织的社会语义实体像素级标注。
2. 提出SocioReasoner视觉-语言推理框架，模拟人类识别和标注社会语义实体的过程，结合跨模态识别与多阶段推理。
3. 采用强化学习优化此类不可微分流程，促进视觉-语言模型的推理能力。

Result: 实验证明，该方法在社会语义分割任务上优于现有最新模型，并展现出很强的零样本泛化能力。

Conclusion: 本文提出的SocioReasoner框架和SocioSeg数据集为社会语义实体分割提供了有力工具，将推动相关领域的研究和应用发展。

Abstract: As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>


### [51] [mergetune: Continued fine-tuning of vision-language models](https://arxiv.org/abs/2601.10497)
*Wenqing Wang,Da Li,Xiatian Zhu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文提出了一种新范式“持续微调(CFT)”，在视觉-语言模型（如CLIP）微调后，恢复其预训练知识。提出了基于线性模式连接(LMC)的MERGETUNE策略，可后处理已微调模型，无需结构变更或大规模数据重放。实验表明MERGETUNE能有效提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微调视觉-语言模型（如CLIP）通常会导致灾难性遗忘，即模型在新任务适应的同时丢失原有的预训练知识，影响泛化。现有方法多关注于适应过程中的遗忘，但不能彻底避免遗忘。本文旨在提出能在模型微调后恢复预训练知识的新方案。

Method: 提出MERGETUNE方法，其核心是基于线性模式连接(LMC)。具体做法是在已微调模型基础上，继续微调其可训练参数（如soft prompts或线性头），通过优化使模型在loss空间中同时靠近原始（zero-shot）和微调（fine-tuned）解。为避免对原始任务数据的重放，采用二阶近似作为约束，减小成本。该方法对模型结构无要求，可后处理应用。

Result: MERGETUNE在基准测试中将CoOp的base-novel泛化能力调和平均提升了5.6%，同时未增加任何参数。在DTD和EuroSAT跨数据集迁移任务上表现首次优于CLIP。LMC合并模型推理成本低于集成方法，在鲁棒性评估中超过集成基线，与zero-shot模型集成时还可进一步获得SOTA性能。

Conclusion: MERGETUNE为视觉-语言模型微调后的预训练知识恢复提供了简洁有效的方案，无需结构更改或大量数据重放，在提升模型泛化和鲁棒性方面优于现有方法，具备广泛的实际应用前景。

Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.

</details>


### [52] [SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction](https://arxiv.org/abs/2601.10512)
*Kanak Mazumder,Fabian B. Flohr*

Main category: cs.CV

TL;DR: SatMap通过融合卫星图像和多视角摄像头观测，提出了一种有效的在线矢量化高清地图估算方法，在nuScenes数据集上大幅提升了地图构建精度，尤其在远距离和恶劣天气中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于车载摄像头的在线高清地图构建方法受限于深度感知和遮挡问题，精度下降，急需突破以提升自动驾驶系统的地图生成能力。

Method: SatMap方法整合了来自卫星图的鸟瞰视角语义与纹理信息，作为全局先验，结合多视角摄像头观测，直接输出用于下游预测和规划的矢量化高清地图。通过利用卫星图提供的全局信息，有效减少了深度歧义和遮挡的影响。

Result: 在nuScenes数据集实验中，SatMap比仅用摄像头的基线方法提升了34.8%的mAP，相较摄像头与LiDAR融合基线也提升了8.5%的mAP。同时，在远距离和恶劣天气条件下，SatMap也展现出显著优势。

Conclusion: SatMap证明了融合卫星地图先验和多视角视觉观测能够有效提升自动驾驶系统在线高清地图构建的精度和鲁棒性，为未来复杂环境下的高精地图生成提供了新思路。

Abstract: Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.

</details>


### [53] [BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition](https://arxiv.org/abs/2601.10521)
*Max A. Buettner,Kanak Mazumder,Luca Koecher,Mario Finkbeiner,Sebastian Niebler,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 本文提出了FUSE-Bike，这是首个以骑行者视角采集的开放感知平台，并基于此平台推出了BikeActions多模态数据集，用于提升对易受伤道路使用者（VRU）行为的理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于车辆视角下的行人过街行为，对于密集共享空间的VRU互动研究不足。作者希望通过更贴近VRU（骑行者）真实视角的数据和工具，提升自动驾驶与机器人对复杂交通场景下VRU意图的预测能力。

Method: 1. 设计并搭建了FUSE-Bike平台，集成双LiDAR、摄像头和GNSS，实现高精度、近距离的骑行者视角数据采集；2. 基于该平台采集并标注了BikeActions多模态数据集，包含5类动作、852个样本；3. 用图卷积网络和Transformer对数据集进行基准评测，建立首个公开性能基线；4. 公开发布数据集、设备设计和基线代码。

Result: 1. 成功推出了首个骑行者视角的开放感知平台及标注数据集；2. 基于BikeActions数据集，评估了主流方法在VRU动作理解任务上的表现，并建立公开基线。

Conclusion: 作者为骑行者及其他弱势道路使用者行为建模，提供了全新视角和公开资源，有助于推动自动驾驶系统在复杂交通场景下的安全性和智能性研究。

Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.

</details>


### [54] [SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery](https://arxiv.org/abs/2601.10535)
*Chong Liu,Luxuan Fu,Yang Jia,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: SVII-3D提出了一套统一框架，实现了利用稀疏影像在资产数字化和精准定位中的高识别精度和低定位误差，有效支持智能城市建设及设施全周期管理。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市和设施全生命周期管理中，自动化创建数字孪生和精准资产盘点至关重要。但受限于现有低成本稀疏图像方法的鲁棒性、定位准确度及细粒度状态识别能力，亟需有效解决方案。

Method: SVII-3D融合了经过LoRA微调的开集检测器与空间注意力匹配网络，实现对稀疏视角观测的强健关联。引入几何引导的精化机制，提升结构误差修正，实现分米级的三维精准定位。此外，集成基于多模态提示的视觉-语言模型Agent，实现资产细粒度操作状态的自动诊断。

Result: 实验显示，SVII-3D显著提升了识别准确度并降低了定位误差。

Conclusion: SVII-3D为大规模、高保真度基础设施数字化提供了可扩展且经济的解决方案，有效填补了稀疏感知与自动化智能运维之间的空白。

Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.

</details>


### [55] [Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning](https://arxiv.org/abs/2601.10537)
*Oscar H. Ramírez-Agudelo,Akshay N. Shewatkar,Edoardo Milana,Roland C. Aydin,Kai Franke*

Main category: cs.CV

TL;DR: 本文提出利用深度学习方法改善烟雾和雾霾环境中模拟仪表的图像可读性，从而提升应急场景下的自动仪表读数准确性。


<details>
  <summary>Details</summary>
Motivation: 烟雾和雾霾导致监控仪表图像清晰度降低，影响关键基础设施监控和应急响应。本研究旨在解决恶劣环境下模拟仪表自动识别难题。

Method: 作者利用Unreal Engine生成了包含14000多张合成模拟仪表图像的新数据集，分别用FFA-Net和AECR-Net两种深度学习结构，对图像进行去雾、去烟处理，并采用80%训练、10%验证、10%测试比例训练模型。性能评价指标采用SSIM和PSNR。

Result: 在合成雾霾数据集上，模型取得了SSIM为0.98、PSNR为43 dB的良好结果，表现媲美现有主流方法，AECR-Net优于FFA-Net。合成烟雾数据集上的结果虽不及去雾效果，但模型依然具备一定提升能力。

Conclusion: 深度学习模型可显著提升烟雾和雾霾场景下模拟仪表图像质量，增强后图像可用于自动仪表读数，有助于应急场景下的仪表自动化监控。

Abstract: Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\% train, 10\% validation, and 10\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges

</details>


### [56] [Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure](https://arxiv.org/abs/2601.10551)
*Luxuan Fu,Chong Liu,Bisheng Yang,Zhen Dong*

Main category: cs.CV

TL;DR: 提出了一套专为城市道路基础设施智能感知设计的VLM适应框架，通过知识增强和高效微调，提高了设施检测与属性识别的准确性，对智能基础设施监测具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有通用视觉语言模型（VLMs）在复杂设施状态的感知及工程标准合规方面表现不佳，限制了其在智慧城市管理中的应用，因此亟需面向该领域的专用感知模型。

Method: 提出了一个域适应框架，包括两个核心技术：1）在Grounding DINO上基于开放词表的小样本微调，实现多样化基础设施资产的高效定位；2）在Qwen-VL上基于LoRA的适应，提升复杂语义属性的推理能力。同时，引入了双模态的RAG检索增强生成模块，推理时动态查找权威行业规范和视觉范例，增强专业合规性与减少模型幻觉。

Result: 在新的城市道路基础设施数据集上，框架实现了58.9 mAP的检测性能和95.5%的属性识别准确率，展现出在现实应用场景下的强大能力。

Conclusion: 该方法有效提升了VLM在智能基础设施感知领域的适应性和专业性，为智慧城市中的设施监测提供了更为可靠和智能的解决方案。

Abstract: Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.

</details>


### [57] [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553)
*Jianhao Yuan,Xiaofeng Zhang,Felix Friedrich,Nicolas Beltran-Velez,Melissa Hall,Reyhane Askari-Hemmat,Xiaochuang Han,Nicolas Ballas,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，通过引入WMReward，在视频生成的推理阶段提升物理合理性，方法基于利用潜在世界模型作为奖励机制，显著提高了生成视频的物理合理性，并在权威挑战赛中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在视觉效果上已处于前沿水平，但经常违反物理规律，降低了其实用性。作者发现问题不仅源自物理知识学习不足，还与生成推理过程中的策略有关，因而有必要改进推理环节以增强物理合理性。

Method: 作者提出WMReward，将提高视频生成物理合理性的问题视为一次性对齐（inference-time alignment）问题。具体做法为，利用具有强物理先验的潜在世界模型（VJEPA-2）作为奖励函数，在推理去噪过程中引导和优化候选生成路径，提高生成样本的物理准确性。该机制可拓展至推理层面的算力规模。

Result: 该方法在图像条件、多帧条件和文本条件的视频生成任务上都实证性地显著提升了物理合理性。通过人为偏好实验验证了改进的有效性，在ICCV 2025 Perception Test PhysicsIQ挑战赛中获得62.64分，超越前一水平7.42个百分点，获得第一名。

Conclusion: 使用潜在世界模型优化视频生成推理阶段的物理合理性是一条有效途径，不局限于具体实现或参数配置。这为物理精准的视频生成打开了新思路和技术路线。

Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.

</details>


### [58] [DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery](https://arxiv.org/abs/2601.10554)
*Constantin Selzer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 本文提出了DeepUrban数据集，为自动驾驶系统的轨迹预测与规划领域提供了高密度交通场景，弥补现有基准中此类场景稀缺的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶基准很少包含密集城市交通场景，难以覆盖复杂的道路参与者交互，阻碍了模型能力的提升。该论文旨在通过新的数据集丰富预测与规划能力评测。

Method: 作者与工业伙伴DeepScenario合作，利用无人机在约100米高空拍摄高分辨率城市交叉口图像，构建了3D交通对象数据集DeepUrban，并补充地图与场景信息。随后用此数据集对主流预测与规划方法进行了评估和泛化实验。

Result: 实验证实，使用DeepUrban扩展nuScenes数据集后，车辆预测与规划准确率显著提升，在ADE/FDE指标上分别提升至44.1%/44.3%。

Conclusion: DeepUrban有效增强了自动驾驶系统在高密度城市交通预测与规划方面的表现，是未来复杂交通场景研究的重要资源。

Abstract: The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban

</details>


### [59] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: 本文针对图像分割评估中现有指标难以反映结构和拓扑一致性的问题，提出基于Jordan曲线定理的拓扑一致性衡量方法。该方法可以无监督评估分割掩码的结构合理性。


<details>
  <summary>Details</summary>
Motivation: 传统的像素、区域或边界评价指标无力衡量分割结果的整体结构和拓扑连续性，导致形状不完整的分割也能获得高分。在医学影像等场景中，保持结构和连通性至关重要，因此需要新的评估方法。

Method: 作者提出以Jordan曲线定理为基础，定义一种'Jordan-segmentatable mask'，结合数字拓扑与同调理论，通过Betti数来验证分割掩码结构的拓扑有效性。具体地，提取4-连通曲线候选、计算Betti数，当候选曲线满足β₀=β₁=1，或其补集仅分为两个8-连通分量时，视为合法的Jordan分割。

Result: 所提出方法可以无监督、严格地评估分割掩码的结构一致性，能有效辨别常规指标忽略的结构或连通性错误。通过实验展示该指标在要求拓扑正确的应用场景下具有独特价值。

Conclusion: 结合数字Jordan理论与同调不变量，本文方法为分割结构一致性评价提供了新思路，更好地满足了对拓扑正确性有硬性要求的应用场景。

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


### [60] [Adversarial Evasion Attacks on Computer Vision using SHAP Values](https://arxiv.org/abs/2601.10587)
*Frank Mollard,Marcus Becker,Florian Roehrbein*

Main category: cs.CV

TL;DR: 本文提出了一种基于SHAP值的白盒攻击方法，能在视觉上不可察觉的情况下削弱深度学习模型性能，且在某些情形下比传统攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习视觉模型容易受到对抗性攻击，现有攻击如FGSM在梯度隐藏情景下效果有限，亟需更鲁棒且难以被察觉的新型攻击方式。

Method: 作者提出利用SHAP（Shapley Additive Explanations）值，量化输入特征对模型输出的影响，从而针对性地构造能够误导模型判断的对抗性扰动，并与传统FGSM方法进行对比分析。

Result: 实验结果表明，基于SHAP值的攻击在梯度隐藏等复杂场景中，比FGSM等方法更容易导致模型误分类，表现出更高的鲁棒性和攻击效果。

Conclusion: SHAP值驱动的白盒攻击揭示了深度视觉模型的安全隐患，对抗性攻防研究亟需关注模型解释性和鲁棒性提升。

Abstract: The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.

</details>


### [61] [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592)
*Delong Chen,Tejaswi Kasarla,Yejin Bang,Mustafa Shukor,Willy Chung,Jade Yu,Allen Bolourchi,Theo Moutakanni,Pascale Fung*

Main category: cs.CV

TL;DR: Action100M是一个超大规模开源动作视频数据集，包含1.2百万个网络视频，约1亿个带有详细注释的动作片段，用于提升视频理解和世界建模能力。


<details>
  <summary>Details</summary>
Motivation: 推断物理动作是计算机智能的一大挑战，现有数据集在开放性、规模和领域广度上不够，为实现通用视频理解，需要更大且具开放词汇的视频动作数据集。

Method: 提出全自动数据集构建流程：1）利用V-JEPA 2进行分层时序分割；2）生成多层次帧和片段描述，以Caption树组织；3）结合GPT-OSS-120B推理模型，通过多轮Self-Refine过程整合证据，生成结构化注释（动作、参与者、简/详细描述）。

Result: 利用Action100M训练VL-JEPA模型，在不同动作识别基准上实现了持续的数据规模提升和强大的零样本识别能力。

Conclusion: Action100M树立了开放大规模视频理解与世界建模的新基石，为后续相关研究提供了强大支持。

Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.

</details>


### [62] [RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation](https://arxiv.org/abs/2601.10606)
*Peng Chen,Xiaobao Wei,Yi Yang,Naiming Yao,Hui Chen,Feng Tian*

Main category: cs.CV

TL;DR: 本文提出了RSATalker框架，首次结合3D Gaussian Splatting（3DGS）和社会关系感知，实现了高真实感、低计算量的多轮对话虚拟人头像生成。


<details>
  <summary>Details</summary>
Motivation: 传统3D网格方法虽可支持双人对话但贴图不真实，2D大模型尽管生成自然却计算开销大。3DGS方法高效写实，但仅限于单说话人且忽略社交关系。现有技术已无法满足VR社交对话中对真实性和社交互动的双重需求。

Method: RSATalker先用语音驱动网格3D人脸动作，再将3D高斯绑定到网格，实现高保真的2D头像渲染。引入社会关系感知模块，采用可学习查询机制将血缘/非血缘、对等/非对等等社会关系编码为高层次embedding。采用三阶段训练，并自建有社会关系标注的数据集RSATalker。

Result: 实验结果表明，RSATalker在真实性和社会感知性能上均超过当前最先进方法，并支持多轮对话场景。

Conclusion: RSATalker为面向多轮社交对话的虚拟现实场景提供了高效、写实且具备社交敏感度的头像生成方法，有望推进虚拟人VR交互的发展。代码和数据集公开可得。

Abstract: Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.

</details>


### [63] [Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding](https://arxiv.org/abs/2601.10611)
*Christopher Clark,Jieyu Zhang,Zixian Ma,Jae Sung Park,Mohammadreza Salehi,Rohun Tripathi,Sangho Lee,Zhongzheng Ren,Chris Dongjoo Kim,Yinuo Yang,Vincent Shao,Yue Yang,Weikai Huang,Ziqi Gao,Taira Anderson,Jianrui Zhang,Jitesh Jain,George Stoica,Winson Han,Ali Farhadi,Ranjay Krishna*

Main category: cs.CV

TL;DR: Molmo2是目前最强的开源视频语言模型（VLMs）之一，特别在指向式视频定位任务上表现出色，数据集和训练方法完全自主研发，不依赖封闭模型。


<details>
  <summary>Details</summary>
Motivation: 目前最强的视频语言模型多为专有模型，开源VLMs要么在数据或训练流程上依赖专有模型，要么未公开数据和方法，并且现有模型在指向或跟踪像素级别的定位能力上严重不足，甚至专有模型也无此能力。为推动开源技术发展，亟需完全开源、定位能力强的VLMs基础与方法。

Method: 作者自主采集了7个新视频数据集和2个多图像数据集，用于预训练和微调，包括详细的视频字幕、自由形式视频问答、复杂查询物体跟踪和新的视频指点任务等。训练采用高效的编码和打包技术，并在视觉token上引入双向注意力与新颖的token加权策略。

Result: Molmo2（8B参数规模）在短视频、计数和描述领域超越所有已开源可用模型，在长视频任务也极具竞争力。在视频定位任务上其准确度大幅优于公开模型(Qwen3-VL等)和部分专有模型（如Gemini 3 Pro），实现新的开源SOTA。

Conclusion: Molmo2填补了开源领域高性能视频语言模型及像素级定位能力的空白，新数据集、方法和模型为研究社群提供了可复用和可扩展的坚实基础，对推进下游多样化、复杂化视频理解任务带来重要意义。

Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).

</details>


### [64] [CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos](https://arxiv.org/abs/2601.10632)
*Chengfeng Zhao,Jiazhi Shu,Yubo Zhao,Tianyu Huang,Jiahao Lu,Zekai Gu,Chengwei Ren,Zhiyang Dou,Qing Shuai,Yuan Liu*

Main category: cs.CV

TL;DR: 该论文提出CoMoVi框架，通过共生成方法让3D人体动作和2D视频同步生成，显著提升了动作和视频生成的自然性及一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体动作和2D视频生成任务虽然密切相关，但大多数方法是分开处理的，导致生成结果难以保证结构一致性和语义契合。研究人员希望将两者耦合生成，充分发挥各自模型的优势。

Method: 提出了CoMoVi框架，使用两个视频扩散模型并在同一去噪循环内实现3D人体动作与2D视频的同步生成。为此设计了高效的2D人体动作表征，利用预训练视频模型优势，并构建了双分支扩散模型，通过特征交互和3D-2D交叉注意力实现联合建模。此外，作者还构建了包含文本与动作注释的大规模人体视频数据集CoMoVi Dataset。

Result: 实验结果表明，提出的方法在3D人体动作生成和2D视频生成任务上均取得了较好效果，生成的视频和动作更加自然且一致性高。

Conclusion: 3D人体动作与2D视频本质耦合，将其同步生成能提升结果质量。CoMoVi框架为此提供了有效方法，并为多模态人类运动和视频生成任务带来了新思路。

Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.

</details>


### [65] [CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning](https://arxiv.org/abs/2601.10649)
*Darshan Singh,Arsha Nagrani,Kawshik Manikantan,Harman Singh,Dinesh Tewari,Tobias Weyand,Cordelia Schmid,Anelia Angelova,Shachi Dave*

Main category: cs.CV

TL;DR: 提出了CURVE基准数据集，专注于多文化、多语言的视频推理任务，包含18个地区的本地化人工标注。通过CURVE评估发现现有视频模型在文化感知上有明显短板。


<details>
  <summary>Details</summary>
Motivation: 目前主流视频理解基准以西方和英语为主，存在评测偏见，不能真实反映全球多元文化和多语言视频场景下的模型表现。需要一个公平、全面的新基准推动领域发展。

Method: 构建了CURVE数据集，涵盖18个地区多文化本地视频，由本地人员以母语手工生成复杂问题和多步推理链。还利用推理链建立证据图，并提出基于证据图的细致错误分析方法。

Result: 测试中，当前最先进的视频大模型（Video-LLMs）在CURVE上表现远低于人类，主要错误来源于对文化视觉要素的感知和理解不足。

Conclusion: CURVE揭示了视频模型在全球多文化感知和推理上的局限，为此领域研究和模型改进提供了方向和高质量基准。数据集及方法具有实用价值。

Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural

</details>


### [66] [A continental-scale dataset of ground beetles with high-resolution images and validated morphological trait measurements](https://arxiv.org/abs/2601.10687)
*S M Rayeed,Mridul Khurana,Alyson East,Isadora E. Fluck,Elizabeth G. Campolongo,Samuel Stevens,Iuliia Zarubiieva,Scott C. Lowe,Michael W. Denslow,Evan D. Donoso,Jiaman Wu,Michelle Ramirez,Benjamin Baiser,Charles V. Stewart,Paula Mabee,Tanya Berger-Wolf,Anuj Karpatne,Hilmar Lapp,Robert P. Guralnick,Graham W. Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本论文针对无脊椎动物性状数据的缺失问题，构建了一个超13000只步甲虫的高分辨率数字化数据库，涵盖美国本土及夏威夷30个采样点，并实现了亚毫米级别的性状自动提取精度。


<details>
  <summary>Details</summary>
Motivation: 尽管无脊椎动物在生态系统中具有重要意义，但全球性状数据库主要集中在脊椎动物和植物，无脊椎动物尤其是多样性极高的步甲类缺乏系统性状数据，阻碍了生态学综合分析与生物多样性研究的发展。

Method: 将NEON项目采集到的步甲标本进行高分辨率数字化拍摄，从美国大陆及夏威夷30个采样点获得超1.3万份标本，并利用人工智能进行鞘翅长度和宽度等数字性状的自动提取，并与人工测量结果进行了精度校验。

Result: 通过高分辨率图像和自动化AI提取，建立了涵盖物种性状测量的数据库，实现了亚毫米级别的测量精度，并通过人工方式验证了其可靠性。

Conclusion: 该数据库填补了无脊椎动物在全球性状数据库中的空白，为AI自动化物种识别和基于性状的生态研究提供了坚实的数据基础，有力促进了生物多样性监测与保护工作的开展。

Abstract: Despite the ecological significance of invertebrates, global trait databases remain heavily biased toward vertebrates and plants, limiting comprehensive ecological analyses of high-diversity groups like ground beetles. Ground beetles (Coleoptera: Carabidae) serve as critical bioindicators of ecosystem health, providing valuable insights into biodiversity shifts driven by environmental changes. While the National Ecological Observatory Network (NEON) maintains an extensive collection of carabid specimens from across the United States, these primarily exist as physical collections, restricting widespread research access and large-scale analysis. To address these gaps, we present a multimodal dataset digitizing over 13,200 NEON carabids from 30 sites spanning the continental US and Hawaii through high-resolution imaging, enabling broader access and computational analysis. The dataset includes digitally measured elytra length and width of each specimen, establishing a foundation for automated trait extraction using AI. Validated against manual measurements, our digital trait extraction achieves sub-millimeter precision, ensuring reliability for ecological and computational studies. By addressing invertebrate under-representation in trait databases, this work supports AI-driven tools for automated species identification and trait-based research, fostering advancements in biodiversity monitoring and conservation.

</details>


### [67] [From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion](https://arxiv.org/abs/2601.10710)
*Cheng Chen,Yuyu Guo,Pengpeng Zeng,Jingkuan Song,Peng Di,Hang Yu,Lianli Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Cross-Layer Injection (CLI)的新框架，有效改善了视觉-语言模型（VLM）融合信息有限的问题，从而提升了多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型仅将视觉编码器的输出单向传递给大语言模型，导致视觉特征利用受限，难以实现细粒度和全局语义的有效融合，影响多模态推理能力。

Method: 作者提出CLI框架，包括自适应多投影模块（AMP）整合不同视觉层的特征，以及自适应门控融合机制（AGF），根据LLM的解码上下文动态选择最相关视觉信息并注入。该方法高效且参数量小。

Result: CLI被集成到LLaVA-OneVision和LLaVA-1.5模型，并在18个多样化基准上进行了大量实验，显著提升了性能。

Conclusion: CLI为LLM提供了按需获取完整视觉层次信息的能力，极大增强了多模态理解，是一种可扩展的多模态融合新范式。

Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

</details>


### [68] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: 提出了Alterbute方法，能在编辑物体固有属性（颜色、材质、形状等）时保持其身份和场景一致性，性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有物体属性编辑方法在保持物体身份或生成多样性方面各有不足：无监督方法易丢失身份信息，有监督方法又限制了属性变化。该工作旨在解决如何在不损失身份前提下编辑固有属性的问题。

Method: 采用基于扩散模型的方法，在训练时允许同时编辑固有与外在属性但以身份参考图和文本提示等进行调控；推理时锁定外在属性，仅改变固有属性。引入精细的视觉命名实体(VNE)标签，并利用视觉-语言模型自动获取大规模身份标签，实现可扩展的监督。

Result: Alterbute方法在身份保持型的属性编辑任务上优于现有方法，能够实现更高质量以及更符合要求的物体固有属性修改。

Conclusion: Alterbute为图像中物体固有属性编辑带来突破，可有效扩展应用场景，实现高质量且身份一致的多样属性编辑。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


### [69] [WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments](https://arxiv.org/abs/2601.10716)
*Xuweiyi Chen,Wentao Zhou,Zezhou Cheng*

Main category: cs.CV

TL;DR: WildRayZer是一种自监督的动态环境下新视角合成方法，通过分析静态渲染残差定位动态区域，并以此改进监督过程，有效提升动态场景的新视角图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法主要针对静态环境，面对动态内容（如相机和物体同时运动）时，容易出现鬼影、虚假几何和姿态估算不稳等问题，亟需解决动态场景的新视角合成难题。

Method: WildRayZer利用静态渲染分析动态场景，仅解释刚性结构，将渲染残差作为动态（瞬态）区的线索。据此构建伪运动掩码，训练运动估计器，通过掩码聚焦背景的跨视角恢复。在损失反向传播时用掩码门控，使监督集中在静态部分。为支持大规模训练，作者还构建了两个真实动态场景数据集。

Result: 在作者自建的真实动态数据集上，WildRayZer在动态区域去除和全帧新视角合成方面，相比优化式和前馈模型基线均表现更优，且只需单次前向计算。

Conclusion: WildRayZer突破了动态场景中新视角合成的主要瓶颈，提升了动态图像合成质量，并为大规模动态场景NVS研究提供了基准数据和方法。

Abstract: We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue](https://arxiv.org/abs/2601.09713)
*Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent*

Main category: cs.CL

TL;DR: 文章提出了ProUtt方法，通过构建意图树和明确建模用户意图推理过程，生成用于预测用户下一个发言的数据，提升了人机对话系统对下文预测的能力。实验显示优于现有方法，并已公开代码和数据。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统API存在隐私问题，本地部署通用大模型代价高昂，因此有必要开发高效且具有任务针对性的小型语言模型用于主动预测用户下文。同时，现有的数据合成方法不能显式建模用户的意图推理过程，制约了性能提升。

Method: 提出ProUtt方法，将对话历史转换为“意图树”，明确建模意图推理轨迹，包括利用与探索两个角度来预测下一步意图路径。通过在意图树的不同未来回合扰动或修正路径，合成偏好和非偏好的推理过程，生成高质量训练数据。

Result: 在四个基准数据集上，ProUtt通过LLM与人工评测均显著优于其它数据合成方法、用户模拟器和商业API解决方案。

Conclusion: ProUtt有效提升了主动预测对话下文的能力，为相关任务提供了更优的数据合成方式。成果的公开有助于推动领域发展。

Abstract: Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.

</details>


### [71] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型（LLM）通过多步推理流程（agentic workflows）生成更具创造性和可行性科研方案的潜力，并对多种推理架构进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科研中的广泛应用，其生成结果的创造力与原创性成为焦点，尤其是“智能抄袭”问题。作者希望通过多步推理流程提升生成内容的新颖性，验证是否能超越单步生成的局限。

Method: 作者对比了五种主流推理构架（包括反思迭代、进化算法、多智能体、递归分解及多模态长上下文管道），对每种方法生成的30份科研提案在新颖性、可行性、影响力三方面进行评估。

Result: 递归分解与长上下文型流程的新颖性评分（4.17/5）显著高于反思型方法（2.33/5）；多步智能体流程在不同领域均表现出色，兼顾了创造性与可行性。

Conclusion: 精心设计的多阶段智能体工作流，可显著提升AI辅助科研构思的创造性和可行性，推动AI在科研创新领域的作用提升。

Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [72] [Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents](https://arxiv.org/abs/2601.09715)
*Adam Bradley,John Hastings,Khandaker Mamun Ahmed*

Main category: cs.CL

TL;DR: 本论文介绍了Axlerod，一个专为保险行业独立代理人设计的AI对话系统，能够高效处理保单推荐和理赔分流等任务。系统结合NLP、RAG以及领域知识，实验证明可大幅提高搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展，保险行业希望通过自动化和智能化服务提升运营效率。然而，大部分研究集中在面向终端消费者的机器人，忽略了对保险代理人等内部用户的赋能。本研究意在填补此空白。

Method: 作者设计并实现了Axlerod系统，结合自然语言处理（NLP）、检索增强生成（RAG）和保险领域知识库。系统能解析用户意图，访问结构化保单数据库，实时做出上下文相关回复。

Result: 实验显示，Axlerod 在保单检索任务中的准确率为93.18%，平均检索时间减少2.42秒，显示出强大的实际应用价值。

Conclusion: Axlerod系统显著提升了保险代理人的工作效率和服务质量，为企业级AI保险科技应用提供了有效实践依据，推动了聚焦代理人辅助系统的研究方向。

Abstract: The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.

</details>


### [73] [Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research](https://arxiv.org/abs/2601.09716)
*Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal*

Main category: cs.CL

TL;DR: 本文全面回顾了塞内加尔六种官方民族语言（Wolof、Pulaar、Sereer、Joola、Mandingue、Soninke）在自然语言处理（NLP）领域的进展与挑战，强调了数据、工具与基准的缺口，并提出了促进学科协作与资源共享的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP正在全球范围内深刻影响各学科研究，但非洲语言（尤其是塞内加尔官方语言）在这一技术革新中还处于边缘地位。因此，作者希望系统梳理这些语言在NLP领域的现状和所面临的挑战，从而促进相关技术的发展和应用。

Method: 作者综述了该地区六种官方语言的语言学、社会技术和基础设施现状，评估其数字化准备度，以及现有数据、工具和基准的不足。同时，调研和分析了文本规范化、机器翻译、语音处理等具体NLP应用的现有努力。还建立了一个GitHub仓库集中公开相关NLP资源，促进合作和实验可复现。

Result: 本文梳理了相关语种在NLP任务方面存在的主要问题及已有进展，总结了文本、语音处理和机器翻译等领域的代表性成果，并提供了整合型开源资源库支持进一步研究。 NLP在社会科学领域的应用得以突出讨论，显示其在多语种田野研究中的重要作用。

Conclusion: 文章最后提出了一个以社区为中心、可持续化的塞内加尔语言NLP生态系统发展路线图，强调了道德数据治理、开放资源和跨学科协作对这些少数语种NLP发展的关键作用。

Abstract: Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.

</details>


### [74] [SALP-CG: Standard-Aligned LLM Pipeline for Classifying and Grading Large Volumes of Online Conversational Health Data](https://arxiv.org/abs/2601.09717)
*Yiwei Yan,Hao Li,Hua He,Gong Kai,Zhengyi Yang,Guanfeng Liu*

Main category: cs.CL

TL;DR: 本研究提出了SALP-CG，一种基于大语言模型的自动化隐私风险分类与分级方法，用于在线医疗对话数据，显著提升了分类准确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 在线医疗对话数据中包含大量敏感的受保护健康信息，现有分类与分级方法缺乏统一标准且自动化程度有限，难以满足合规与治理需求。

Method: 基于GB/T 39725-2020标准，设计了一套利用大语言模型的抽取流程SALP-CG，结合少样本学习、JSON Schema约束解码与确定性高风险分级规则，无需特定后端即可实现高效的数据分类和敏感度分级。

Result: 在MedDialog-CN 基准上，所提模型表现出实体识别数量稳定、Schema 合规率高、敏感分级准确性强，其中最佳模型在最高级别预测上达到了micro-F1=0.900。结果展示，不同敏感度分级下类别分布合理，具较高实用价值。

Conclusion: SALP-CG方案在多种大语言模型下均表现出可靠的健康数据分类和敏感度分级能力，为在线医疗对话健康数据的治理与合规提供了可行且实用的方法。

Abstract: Online medical consultations generate large volumes of conversational health data that often embed protected health information, requiring robust methods to classify data categories and assign risk levels in line with policies and practice. However, existing approaches lack unified standards and reliable automated methods to fulfill sensitivity classification for such conversational health data. This study presents a large language model-based extraction pipeline, SALP-CG, for classifying and grading privacy risks in online conversational health data. We concluded health-data classification and grading rules in accordance with GB/T 39725-2020. Combining few-shot guidance, JSON Schema constrained decoding, and deterministic high-risk rules, the backend-agnostic extraction pipeline achieves strong category compliance and reliable sensitivity across diverse LLMs. On the MedDialog-CN benchmark, models yields robust entity counts, high schema compliance, and accurate sensitivity grading, while the strongest model attains micro-F1=0.900 for maximum-level prediction. The category landscape stratified by sensitivity shows that Level 2-3 items dominate, enabling re-identification when combined; Level 4-5 items are less frequent but carry outsize harm. SALP-CG reliably helps classify categories and grading sensitivity in online conversational health data across LLMs, offering a practical method for health data governance. Code is available at https://github.com/dommii1218/SALP-CG.

</details>


### [75] [StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model](https://arxiv.org/abs/2601.09718)
*Jing-Yi Zeng,Guan-Hua Huang*

Main category: cs.CL

TL;DR: 本文提出基于LLaMA-3.2-3B，系统性地探索统计学领域大语言模型（LLM）的高效定制方案，比较不同多阶段微调流程，实现了在数学推理、常识推理和统计专长上的强劲平衡表现。


<details>
  <summary>Details</summary>
Motivation: 统计学领域对专业语言模型的需求日益增长，但如何基于资源高效的小参数LLM（如LLaMA-3.2-3B）实现鲁棒的领域定制尚无定论。本文试图找到最有效的微调路线，以推动资源敏感领域的模型实用化。

Method: 作者设计并比较三种多阶段训练流程：一是从无指令微调能力的基础模型出发，二是对基础模型后置指令微调（instruction tuning），三是基于已具备通用推理能力的指令微调模型，在每种流程下分别进行持续预训练、SFT、RLHF优选以及下游任务微调，并深入分析其表现差异。

Result: 仅以基础模型为起点，即便经过多轮微调和对齐，模型在统计推理方面依然表现不佳；而以指令微调模型（LLaMA-3.2-3B-Instruct）为起点则能够高效地实现统计领域专化。此外发现RLHF直接偏好优化方法带来更加稳定和有效的对齐表现；通过实验证明下游微调需极为低强度，以避免严重遗忘。最终模型StatLLaMA在各类推理和统计评测中取得优异及均衡表现。

Conclusion: 统计专业LLM的高效开发应从通用能力强的指令微调模型出发，采用多阶段微调和极谨慎的下游微调，在权衡领域专长与通用推理能力方面取得最佳实践。本文为资源受限环境下的专业LLM开发提供了系统性方法参考。

Abstract: This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.

</details>


### [76] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: 本文提出了一种新的归一化方法BHyT，用于替代大模型中的Pre-LN，同时提升训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: Pre-LN虽然是大语言模型（LLMs）中的主流方案，但其存在效率低和深度扩展训练不稳定两大问题，包括重复统计和随层数加深激活值和方差的爆炸。

Method: BHyT结合了tanh非线性和数据驱动的输入范围约束，使激活保持在非饱和区间，并理论上保证稳定性。它每个block仅计算一次精确统计，第二步用轻量级的方差近似，减少冗余操作。

Result: BHyT相比RMSNorm，在预训练期间获得了更高的稳定性和效率，平均训练速度提升15.8%，token生成吞吐提升4.2%，并在推理性能和鲁棒性上保持或超越。

Conclusion: BHyT是一种有效且高效的Pre-LN替代方案，能同时保证训练稳定和速度，适合深层大模型使用，具有良好的理论和实践表现。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


### [77] [Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering](https://arxiv.org/abs/2601.09720)
*Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti*

Main category: cs.CL

TL;DR: 本文提出了一种具备不确定性感知功能的动态知识图谱（KG）用于问答系统，以提升在不完整或有噪音的证据条件下的可靠性，并展示在医疗健康领域的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有基于KG的问答系统只能处理静态、确定性的事实，难以反映信息随时间变化及推理过程中的不确定性，这在高风险领域如医疗健康尤为致命。因此亟需一种方法能动态更新知识图谱并显式表达不确定性。

Method: 提出的框架包括：（1）动态构建随信息变化而演化的知识图谱；（2）为KG三元组打分并在信息检索时考虑不确定性；（3）开发交互界面，使用户可以探索和比较不同答案（包含信心分 vs. 传统答案）及其推理路径。

Result: 系统被应用于医疗领域，通过从电子健康记录生成个性化KG，动态可视化患者访视中的不确定性，并在预测患者死亡风险任务上进行了评估，证明其有效性和数据解释优势。

Conclusion: 不确定性感知的动态KG能够提升问答系统在高风险领域（如医疗）的可靠性和可解释性，对更多重要应用场景具有推广价值。

Abstract: Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.

</details>


### [78] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型在儿科医疗咨询中应对焦虑家长（具有对抗性压力）时的安全性，发现规模更大的模型未必更安全，且模型在某些紧急情况（如癫痫）上表现差，提出了公开基准和加强安全性的建议。


<details>
  <summary>Details</summary>
Motivation: 现有对医疗大语言模型安全性的评估大多基于中性问询，未充分考虑实际咨询中由于用户焦虑带来的对抗性压力，因此需要研究模型在更真实压力情境下的表现。

Method: 基于PediatricAnxietyBench数据集（150真实、150对抗性问题，涵盖10主题），分别调用Llama-3.3-70B、Llama-3.1-8B（Groq）和Mistral-7B（HuggingFace）三种模型API生成900个回答。安全性按0-15分考察克制、转介、回避、急诊识别和非处方性，采用配对t检验与自助法置信区间分析结果。

Result: Llama-3.1-8B模型得分高于Llama-3.3-70B（差值0.66, p=0.0001）, Mistral-7B表现最强（得分10.39）。对抗性压力下模型得分普遍提升，且安全性具有平台泛化。Llama-3.3-70B有8%回答不合格，癫痫相关提问错误率高达33%。逃避型回答比例能预测整体安全性。

Conclusion: 模型安全性更多取决于对齐能力和架构，而非规模。更小模型有时更安全，且模型在公开迭代中逐步改善。部分极端情景下的缺陷显示其不适合用于急诊分诊。应强化对抗性测试与针对性训练，相关基准有助于AI医疗安全的进一步提升。

Abstract: Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [79] [ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language](https://arxiv.org/abs/2601.09722)
*Franciszek Górski,Andrzej Czyżewski*

Main category: cs.CL

TL;DR: 本文提出了一套利用多语种大型语言模型自动标注文献的框架，极大提升了波兰语医学文本标注效率，并训练了高效小型分类器。


<details>
  <summary>Details</summary>
Motivation: 医学文本多分类任务常受限于标注资源稀缺，尤其是小语种（如波兰语），难以训练高性能分类模型。

Method: 首先利用多语种LLM（Llama3.1）自动标签大规模波兰语医学文本，仅有限人工校验构建测试集，然后用这些数据训练和验证三种BERT架构小模型（DistilBERT, BioBERT, HerBERT）。

Result: DistilBERT在各临床类别F1分数均超过0.80，且有三个类别F1超过0.93。同时该模型比大型语言模型小近500倍，显存占用和推理速度也有极大优势。

Conclusion: 利用LLM自动标注结合小模型微调，可以在资源有限环境下获得高效、准确的医学文本分类器，是LLM推理的实际可用替代方案。

Abstract: In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.

</details>


### [80] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于完整小说的长上下文基准数据集SagaScale，具备现实性、可扩展性和高质量，支持中英文，并公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准在任务真实性、数据规模和质量上存在局限，难以充分评估LLM理解长文档的能力。

Method: 作者提出了SagaScale基准，通过自动化流程和外部资源（如Wikipedia）生成高质量问答对，涵盖中英文小说，并设置严格评测流程；同时，比较了不同长上下文处理方法（Naïve RAG、Agentic RAG、Long Context）。

Result: 在12个主流LLM与三种长上下文方法上评测，发现直接使用全部上下文效果最优；多数LLM处理超长文本仍有困难，但Gemini-2.5-Pro表现突出；Agentic RAG能有效缓解Naïve RAG的检索瓶颈。

Conclusion: SagaScale为LLM长文本理解能力的研究提供了高质量基准和工具，推动相关领域的进一步发展。

Abstract: Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


### [81] [Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions](https://arxiv.org/abs/2601.09724)
*Katherine Elkins,Jon Chun*

Main category: cs.CL

TL;DR: 本文提出了 Syntactic Framing Fragility (SFF) 框架，通过逻辑极性归一化（LPN）来检测大型语言模型（LLMs）在语法结构变化下的伦理判断一致性。研究发现，多数模型在语法极性变化时表现出明显的不一致，特别是在开放源代码模型中更为严重。链式思维推理可有效缓解该问题。该框架建议成为 LLM 安全评估标准流程。


<details>
  <summary>Details</summary>
Motivation: LLMs 在重要的自动决策情景中被广泛使用，其伦理判断的稳健性关系重大。当前，模型对语法上等价但表达不同（如否定与条件句结构）的提示的反应一致性未被充分研究，存在潜在风险。为保障实际应用中的伦理可靠性，需要系统评估 LLM 对语法变化的鲁棒性。

Method: 引入了 Syntactic Framing Fragility (SFF) 评价框架，通过逻辑极性归一化（LPN）控制提示的正负语法差异，在14种伦理场景下用4类受控表达，审计23种主流中美大模型及小型开源模型，总计近4万次决策，比较其正、负表达下伦理判断的一致性。

Result: 主流 LLMs 广泛存在因语法极性变化导致伦理判断反转现象。开源模型的脆弱性是商用模型两倍以上。部分模型面对否定提示时，80-97%情况下仍作出相反伦理判断。引导模型采用链式思维推理能大幅降低语法影响。同时，金融和商务等场景比医疗场景更易出现一致性风险。

Conclusion: 语法一致性是 LLM 伦理鲁棒性中独立且重要的维度。SFF 风格的语法脆弱性审计应成为 LLM 安全评估的标准流程，提升模型实际部署的伦理安全保障。

Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with "should not." We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.

</details>


### [82] [Assessing and Improving Punctuation Robustness in English-Marathi Machine Translation](https://arxiv.org/abs/2601.09725)
*Kaustubh Shivshankar Shejole,Sourabh Deoghare,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了Virām，这是首个用于评估英译马拉地语机器翻译中标点符号鲁棒性的诊断基准，并比较了不同增强策略。


<details>
  <summary>Details</summary>
Motivation: 标点符号对解决书面语言中的语义和结构歧义至关重要，但在许多机器翻译系统（尤其是低资源语言）中，这一问题尚未充分研究。以马拉地语为例，现有研究缺少专门针对标点鲁棒性的评测手段。

Method: 作者构建了Virām基准，包括54个人工筛选的、标点存在歧义的英译马拉地语实例。研究比较了两种主要增强策略：先恢复标点再翻译的流水线方法，以及直接在含有多样标点数据上微调模型。

Result: 实验表明，专门微调的模型和流水线系统在Virām基准上的翻译质量显著优于标准基线方法。定性分析显示，原始模型邦易产生错误翻译，导致理解偏差，而微调模型能显著提升鲁棒性。同时，当面临标点歧义文本时，当前大语言模型表现不如这些特定任务模型。

Conclusion: 针对标点歧义问题，专门设计的微调模型和流水线系统能有效提升英译马拉地语翻译的鲁棒性。现有大语言模型对此类任务仍有不足，需要更多研究以提高其在保留原文意义方面的能力。

Abstract: Punctuation plays a critical role in resolving semantic and structural ambiguity in written language. Machine Translation (MT) systems are now widely applied across diverse domains and languages, including many low-resource settings. In this work, we focus on Marathi, a low- to middle-resource language. We introduce Virām, the first diagnostic benchmark for assessing punctuation robustness in English-to-Marathi machine translation, consisting of 54 manually curated, punctuation-ambiguous instances. We evaluate two primary strategies for enhancing reliability: a pipeline-based restore-then-translate approach and direct fine-tuned on punctuation-varied data. Our results demonstrate that specialized fine-tuned models and pipeline systems significantly improve translation quality over standard baselines on the Virām benchmark. Qualitative analysis reveals that the original model may result in wrong translations leading to wrong interpretations, while fine-tuned models significantly improve overall reliability. Furthermore, we find that current Large Language Models (LLMs) lag behind these task-specific approaches in preserving meaning for punctuation-ambiguous text, thus necessitating further research in this area.

</details>


### [83] [Forgetting as a Feature: Cognitive Alignment of Large Language Models](https://arxiv.org/abs/2601.09726)
*Hien Tran,Quinten Steenhuis,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本文提出将大语言模型（LLM）的遗忘现象视为一种类人适应机制，并开发基准来对比模型与人类记忆动态，同时提出概率记忆提示方法以提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 以往评测LLM时多以完美贝叶斯推断为标准，然而实际却发现LLM常常遗忘部分上下文信息。作者旨在解释这种遗忘现象的认知意义，并借鉴人类记忆机制，将遗忘视作适应性优势，而非模型缺陷。

Method: 作者借鉴人类记忆的指数衰减过程，提出用概率记忆过程来建模LLM的推理。设计了一套基准，测试模型在时间推理、概念漂移适应与联想回忆上的能力，并与人类认知特性进行比较。此外，提出概率记忆提示（probabilistic memory prompting）策略，以模拟人类记忆衰减，增强证据整合能力。

Result: 实验证明LLM的遗忘率与人类记忆在稳定性与适应性权衡上的效率相当。概率记忆提示策略能提升LLM在长时推理任务上的表现。

Conclusion: 遗忘现象可被理解为模型自适应智能的重要机制，而不是一种失败。适当引导模型利用类人记忆机制，有助于提升其推理与适应能力。

Abstract: Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.

</details>


### [84] [SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis](https://arxiv.org/abs/2601.09727)
*Sauhard Dubey*

Main category: cs.CL

TL;DR: 本文提出了SciNets方法，通过构建概念图，实现对科学文献中机制性关联的多跳推理，从而支持跨学科的科学综合。作者系统比较了不同图约束推理方法，并提出新的行为评估框架，揭示了推理深度、多样性与基础稳定性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 跨学科的科学综合需要在分散的文献中连接机制性解释，这对于检索系统和通用语言模型来说仍然具有挑战性。现有方法缺乏对推理深度和结构化支撑的有效控制，因此迫切需要新的方法来实现更可控且有结构的机制综合。

Method: 作者将机制性综合建模为一个基于图约束的多跳推理问题，首先根据科学查询和相关文献构建有向概念图，然后以多种策略（最短路径、多样性k最短路径、随机游走、检索增强LLM作为基线）实现概念之间的多跳推理，并提出新的行为评估体系。

Result: 实验覆盖机器学习、生物学和气候科学等领域，显示显式的图结构约束可实现可控的多跳推理，而且推理越深、越多样则基础稳定性降低，而最短路径推理则最稳定但结构上较保守。

Conclusion: 通过系统行为性评估，本文揭示了当前图结构与大模型集成进行科学综合的能力与局限性，对后续科学机制推理方法提供了重要参考。

Abstract: Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.

</details>


### [85] [Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens](https://arxiv.org/abs/2601.09728)
*Meicong Zhang,Tiancheng su,Guoxiu He*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，直接在大语言模型中参数化写作引导流程，用于高质量生成学术引言，显著优于传统基于代理链的工作流。


<details>
  <summary>Details</summary>
Motivation: 传统使用代理工作流引导大语言模型进行文献分类和综述已取得一定进展，但学术引言写作更具挑战，需要逻辑严密、结构连贯、抽象总结。现有工作流推理链长，易累积错误，文本连贯性减弱。为克服这些局限，论文提出新方法。

Method: 提出“引言生成阶段标记（STIG）”，将原有多阶段写作流程转为显式阶段信号，直接引导大模型按逻辑顺序生成文本。通过指令微调，模型学习各阶段标记与写作功能的映射及其序列规律，内化工作流于参数中，一步输出完整引言。

Result: 实验结果表明，STIG方法无需显式工作流调用，能一次性生成多阶段引言文本，在语义相似度和句子级结构合理性上超越传统代理工作流和其他基线方法。

Conclusion: 新方法消除了外部工作流的繁琐，实现高质量、逻辑连贯的学术引言生成，为学术写作自动化提供了更有效的解决方案。

Abstract: In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.

</details>


### [86] [Enhancing Business Analytics through Hybrid Summarization of Financial Reports](https://arxiv.org/abs/2601.09729)
*Tohida Rehman*

Main category: cs.CL

TL;DR: 该论文提出了一种结合抽取式和生成式方法的混合摘要框架，用于高效且准确地总结财报电话会议文本。


<details>
  <summary>Details</summary>
Motivation: 财报和业绩沟通内容信息量大，人工分析低效且容易产生偏差。需要自动化工具帮助高效、客观地提取关键信息。

Method: 设计了两阶段管道，首先用LexRank抽取重要句子，再用微调版BART和PEGASUS生成摘要；同时微调LED模型直接处理长文本。并引入多种通用和金融领域自动评价指标，以及实体级的事实准确性度量。

Result: 长文本模型（如LED）整体表现最佳，混合框架在保持高事实一致性的同时，在资源受限下也取得了具有竞争力的结果。

Conclusion: 混合和长期上下文模型可用于高效、可靠地总结财务文本，为建立实用金融摘要系统提供了支持。

Abstract: Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.
  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.

</details>


### [87] [SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability](https://arxiv.org/abs/2601.10455)
*Ruochen Li,Kun Yuan,Yufei Xia,Yue Zhou,Qingyu Lu,Weihang Li,Youxiang Zhu,Nassir Navab*

Main category: cs.CL

TL;DR: 本文提出了一项用于评估外科手术规划中视觉-语言模型(VLMs)的多中心元评测基准，并发现现有的序列相似度指标在判定规划质量时存在系统性偏差，推荐采用基于规则的目标可满足性指标。


<details>
  <summary>Details</summary>
Motivation: 当前外科手术规划需要模型具备视觉感知、长期推理和程序性知识，但如何可靠地评测VLMs在高安全要求环境下的能力尚不明确。作者以外科手术规划为目标，提出新颖的正确性定义标准，以应对评测方法的局限。

Method: 研究者提出了一套基于专家外科规则的手术阶段-目标可满足性标准来定义规划正确性，开发了一个包含有效和无效(顺序和内容错误)手术方案的多中心元评估基准。利用该基准，系统性地比较了序列相似度评价方法与规则可满足性方法的优缺点。

Result: 结果表明，序列相似度度量无法准确反映手术规划的有效性，常常惩罚有效方案且未能识别无效方案。基于规则的目标可满足性指标则具有更高的评价精度。同时，结构性知识始终能提升模型表现，而纯粹语义引导仅在结合结构约束和更大模型规模时有效。

Conclusion: 在安全关键的手术规划场景下，VLMs的评测应更注重结构和规则约束。建议采用高精度的基于目标可满足性的评价方式，以更公正、更实用地衡量VLMs的失误与优势。

Abstract: Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.

</details>


### [88] [Clinical Document Metadata Extraction: A Scoping Review](https://arxiv.org/abs/2601.09730)
*Kurt Miller,Qiuhao Lu,William Hersh,Kirk Roberts,Steven Bedrick,Andrew Wen,Hongfang Liu*

Main category: cs.CL

TL;DR: 本文回顾了临床文档元数据自动化提取的研究现状，分析了方法演变和应用趋势，并指出当前存在的不足。


<details>
  <summary>Details</summary>
Motivation: 临床文档元数据（如类型、结构、作者、科室等）对于精确解读临床信息至关重要，但文档内容高度异质且随时间变化，给元数据标准化带来巨大挑战。因此，有必要综述和梳理自动化提取方法的研究进展、趋势和应用价值。

Method: 本综述采用PRISMA-ScR指南，对2011年1月至2025年8月公开的相关文献进行系统筛查和分析。共筛选出266篇文献，最后纳入67篇，归类为方法学研究、下游应用与元数据组成分析三大类，并总结了其研究内容与方法。

Result: 方法学研究占比最多，应用方向多样。除结构化数据外，标注数据资源仍非常有限。元数据提取方法经历了从基于规则、传统机器学习（依赖大量特征工程）到如今的Transformer为代表的深度学习方法，实现了更通用且对特征依赖更小的模型。大语言模型的出现显著提升了模型泛化能力。

Conclusion: 预计未来相关研究将聚焦于更丰富和复杂的文档元数据表现形式，并进一步与临床实际应用和工作流程深度融合。该领域仍存在公开数据缺乏、边界不断拓展等挑战。

Abstract: Clinical document metadata, such as document type, structure, author role, medical specialty, and encounter setting, is essential for accurate interpretation of information captured in clinical documents. However, vast documentation heterogeneity and drift over time challenge harmonization of document metadata. Automated extraction methods have emerged to coalesce metadata from disparate practices into target schema. This scoping review aims to catalog research on clinical document metadata extraction, identify methodological trends and applications, and highlight gaps. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines to identify articles that perform clinical document metadata extraction. We initially found and screened 266 articles published between January 2011 and August 2025, then comprehensively reviewed 67 we deemed relevant to our study. Among the articles included, 45 were methodological, 17 used document metadata as features in a downstream application, and 5 analyzed document metadata composition. We observe myriad purposes for methodological study and application types. Available labelled public data remains sparse except for structural section datasets. Methods for extracting document metadata have progressed from largely rule-based and traditional machine learning with ample feature engineering to transformer-based architectures with minimal feature engineering. The emergence of large language models has enabled broader exploration of generalizability across tasks and datasets, allowing the possibility of advanced clinical text processing systems. We anticipate that research will continue to expand into richer document metadata representations and integrate further into clinical applications and workflows.

</details>


### [89] [Geometric Patterns of Meaning: A PHATE Manifold Analysis of Multi-lingual Embeddings](https://arxiv.org/abs/2601.09731)
*Wen G Gong*

Main category: cs.CL

TL;DR: 该论文提出了一个多层次的语义几何分析框架，并开发了Semanscope工具，利用PHATE流形学习分析多语言嵌入表示。通过对不同语言层级和数据类型的可视化分析，揭示了嵌入模型中的系统性几何模式与局限。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型和词嵌入的广泛应用，理解多语言嵌入空间中的语义结构及其几何特性变得尤为重要。然而，当前缺乏可以跨多个语言层次进行系统分析和可视化的方法，且不少嵌入模型未能充分捕捉语义与结构的区分。

Method: 作者提出了一个包含四个语言层级的分析框架，并实现了Semanscope可视化工具。该工具借助PHATE流形学习算法，将不同语言和特定语义集合（如汉字部件、字母表、词汇语义域、数字等）映射到低维空间，并对其几何结构进行可视化和分析。

Result: 分析表明：在字符内部层面，如汉字部件，嵌入模型未能区分语义和结构成分，表现为“几何坍缩”；在字符和词汇层面，不同语言和语义域出现了各具特色的聚类和分支模式；在数字范畴，数字分布为螺旋结构，违反了常规分布式语义假设。

Conclusion: PHATE可作为研究多语言嵌入空间语义几何结构的有力工具，有助于诊断和改进当前语义嵌入模型的有效性，检验其对语义关系的表达能力。

Abstract: We introduce a multi-level analysis framework for examining semantic geometry in multilingual embeddings, implemented through Semanscope (a visualization tool that applies PHATE manifold learning across four linguistic levels). Analysis of diverse datasets spanning sub-character components, alphabetic systems, semantic domains, and numerical concepts reveals systematic geometric patterns and critical limitations in current embedding models. At the sub-character level, purely structural elements (Chinese radicals) exhibit geometric collapse, highlighting model failures to distinguish semantic from structural components. At the character level, different writing systems show distinct geometric signatures. At the word level, content words form clustering-branching patterns across 20 semantic domains in English, Chinese, and German. Arabic numbers organize through spiral trajectories rather than clustering, violating standard distributional semantics assumptions. These findings establish PHATE manifold learning as an essential analytic tool not only for studying geometric structure of meaning in embedding space, but also for validating the effectiveness of embedding models in capturing semantic relationships.

</details>


### [90] [Benchmarking Cross-Lingual Semantic Alignment in Multilingual Embeddings](https://arxiv.org/abs/2601.09732)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估多语言嵌入模型跨语言语义对齐的新指标——Semantic Affinity（SA），并通过实验系统比较了13种主流多语言嵌入模型的语义对齐能力，揭示了只有经过翻译对齐监督训练的BERT模型才能实现优异的跨语言语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前市面上有上百种多语言嵌入模型，但缺少对其跨语言语义对齐能力的科学评测。因此使用者在选择多语言模型时，没有明确的标准，现有数据驱动测试（如MTEB）无法揭示模型的根本对齐能力。

Method: 作者提出了Semantic Affinity（SA）指标，通过余弦距离衡量跨语言与同语言语义的距离比值（0到1之间），并结合PHATE可视化，形成Semanscope分析框架。对13个流行多语言嵌入模型在4个数据集上（共52组实验）进行SA评测和可视化对比。

Result: 实验发现，受翻译对齐监督训练的BERT模型（如LaBSE、USE、S-BERT）SA值最高，可达0.68-0.70。其次是LLM大型语言模型，但SA仅为0.55-0.61，并且参数量从6亿到80亿无明显提升。单纯多语言训练的BERT（如mBERT、XLM-R）SA不足0.5，无法有效对齐。训练目标优于模型规模或架构。

Conclusion: 本研究为多语言嵌入模型的实际选择提供了语义对齐分析工具，强调只有明确的翻译监督才能实现高质量的跨语言语义对齐，仅靠大模型或多语料训练不够。同时也揭示目前模型学到的是语料相关模式而非认知层面的原始语义。

Abstract: With hundreds of multilingual embedding models available, practitioners lack clear guidance on which provide genuine cross-lingual semantic alignment versus task performance through language-specific patterns. Task-driven benchmarks (MTEB) may mask fundamental alignment shortcomings. We introduce Semantic Affinity (SA), a bounded (between 0 and 1) metric measuring inter-lingual to intra-lingual spread ratio using cosine distance, combined with PHATE visualization in our Semanscope framework. Benchmarking 13 models across 4 datasets (52 experiments) reveals a three-tier structure: (1) Top BERT models (LaBSE SA = 0.70, USE SA = 0.68, S-BERT SA = 0.68) achieve strong alignment via translation-pair supervision; (2) LLM embeddings plateau at SA between 0.55 and 0.61 regardless of 0.6 B to 8 B scale; (3) MLM-only BERT models (mBERT, XLM-R, SA < 0.50) fail despite more than 100 language training. Training objective, not architecture or scale, determines alignment. Oracle Bone primitives (1200 BCE) expose semantic drift-models learn corpus patterns rather than cognitive primitives. This work provides semantic benchmarking to help practitioners select quality multilingual embeddings from hundreds of available models, showing cross-lingual alignment requires explicit translation supervision, not merely model scale or multilingual data.

</details>


### [91] [Closing the Data Loop: Using OpenDataArena to Engineer Superior Training Datasets](https://arxiv.org/abs/2601.09733)
*Xin Gao,Xiaoyang Wang,Yun Zhu,Mengzhang Cai,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本论文提出了一种通过OpenDataArena (ODA) 实现闭环数据集工程的方法，用于优化大型语言模型（LLMs）监督微调阶段的数据集构建，显著提升了模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）数据集的构建通常依赖经验性方法，缺乏对每个样本对模型性能贡献的系统理解，容易导致效率低下和表现不佳。

Method: 作者提出使用OpenDataArena（ODA）的闭环数据集工程流程，结合以价值为锚点的排序和多维分析，采用“Anchor-and-Patch”策略和难度感知流程来构建和优化数据集。具体应用在数学推理和多领域指令数据集的构建上。

Result: 提出的ODA-Math-460k数据集在数学推理基准（AIME、HMMT）上取得SOTA成绩，ODA-Mixture（100k与500k规模）在多领域任务上优于更大规模的开源数据集，对比实验显示ODA方法明显提高了推理能力和数据利用效率。

Conclusion: ODA方法能够高效透明地评估和指导高质量训练数据的工程，实现从经验驱动到数据主动优化的转变，推动数据中心化AI发展。

Abstract: The construction of Supervised Fine-Tuning (SFT) datasets is a critical yet under-theorized stage in the post-training of Large Language Models (LLMs), as prevalent practices often rely on heuristic aggregation without a systematic understanding of how individual samples contribute to model performance. In this report, we propose a paradigm shift from ad-hoc curation to a closed-loop dataset engineering framework using OpenDataArena (ODA), which leverages value-anchored rankings and multi-dimensional analysis to transform value benchmarking into feedback signals guiding dataset construction. We instantiate this methodology through two new datasets: \textbf{ODA-Math-460k}, a specialized mathematics reasoning dataset that utilizes a novel two-stage difficulty-aware pipeline to achieve State-of-the-Art (SOTA) results on benchmarks such as AIME and HMMT, and \textbf{ODA-Mixture (100k \& 500k)}, a series of multi-domain instruction datasets built via an ``Anchor-and-Patch'' strategy that outperforms significantly larger open-source baselines. Our empirical results demonstrate that ODA-driven datasets significantly improve both domain-specific reasoning and general utility while achieving superior data efficiency, validating a transition toward data-centric AI where transparent evaluation serves as the primary engine for engineering high-quality training data.

</details>


### [92] [From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis](https://arxiv.org/abs/2601.09734)
*Yanyi Liu,Qingwen Yang,Tiezheng Guo,Feiyu Qu,Jun Liu,Yingyou Wen*

Main category: cs.CL

TL;DR: 本文提出了一种从“检测”到“诊断”大模型幻觉的新研究范式，不仅检测幻觉，还能定位、解释原因并修正内容，提升生成式AI的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有大模型幻觉研究多以二元检测为主，虽能发现幻觉但无法为模型改进提供可解释、可操作的反馈，限制了实际应用价值。

Method: 作者提出“幻觉诊断任务”，要求模型检测幻觉、定位错误、解释因果并修正文案。为此构建了HDG自动管道，通过多维增广（如受控事实伪造、推理链扰动）生成高质量样本及诊断元数据，并用GRPO算法训练4B参数的诊断模型HDM-4B-RL。

Result: 在HaluEval基准上，HDM-4B-RL在幻觉检测任务上优于SOTA模型，在综合诊断任务上与更大模型表现相当。

Conclusion: 本研究验证了幻觉诊断的可行性和价值，为构建更可信、可靠的生成式AI系统提供了有效方法论。

Abstract: Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary "detection" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from "detection" to "diagnosis". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance to advanced general-purpose models. In comprehensive diagnosis tasks, HDM-4B-RL matches the capabilities of larger general models while maintaining a smaller size. This work validates the feasibility and value of hallucination diagnosis, providing an effective methodology for building more trustworthy and reliable generative AI systems.

</details>


### [93] [Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations](https://arxiv.org/abs/2601.09833)
*Xiaoxu Ma,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CL

TL;DR: 本文提出了一种名为PVNI的新方法，通过分析大语言模型的内部激活向量，对其人格特质进行更稳定、可解释的评估，优于现有基于问卷的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于问卷的方法在评估LLM人格特质时稳定性差，容易因提示细节变化而导致结果不一致，缺乏可解释性，因此需要更可靠的新方法。

Method: 引入了Persona-Vector Neutrality Interpolation (PVNI)方法，从模型内部的激活中，通过对比型prompt提取与目标人格特质相关的向量，并通过在该向量方向上的插值估算中性分数，实现中性与人格表现方向的可解释比较。

Result: 理论分析和在多种LLM上的实验表明，PVNI方法无论在问卷还是角色扮演等多种设置下，都能显著提升人格特质评估的稳定性。

Conclusion: PVNI为LLM人格特质评估提供了更稳定且可解释的方法，有助于模型的分析、比较及其负责任地部署。

Abstract: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.

</details>


### [94] [Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences](https://arxiv.org/abs/2601.09852)
*Sriram Padmanabhan,Siyuan Song,Kanishka Misra*

Main category: cs.CL

TL;DR: 该论文探讨了视觉语言模型（Vision Language Models，VLMs）在归纳推理任务中，是否与人类儿童在处理不同泛指表达（如全称、泛指、不定项）时表现出相似的行为和认知表现。通过复现实验，发现VLMs和人类儿童的行为趋同。


<details>
  <summary>Details</summary>
Motivation: 先前发展心理学研究发现，儿童能够区分语言表达中的泛指句、全称和不定项句，并据此推断事物属性。该研究的动机在于，探索大型统计学习模型（如视觉语言模型）是否也能自然区分这些细微的语言表达差异，并据此进行归纳推理，从而了解这类模型的“认知”能力与人类的异同。

Method: 复现Gelman等人（2002）的实验，首先通过预备测试确保模型能够识别图片中的类别及对全称和不定项表达的敏感性，然后让模型参与归纳属性扩展任务。随后，对模型的内部表示进行分析，比较其归纳方式是否基于抽象语义约束而非表层语言差异。

Result: 实验结果显示，VLMs在归纳任务中的行为与儿童在类似实验中的行为一致，对不同语言表达作出区分并依此进行推理。分析表明，这种区分源自模型对归纳约束的理解，而不仅仅是对表层语言形式的敏感。

Conclusion: 视觉语言模型在归纳推理中能区分全称、泛指和不定项表达，其表现与人类儿童趋同。这种能力来源于模型对语言归纳约束的理解，提示大型多模态模型具备高级层次的语义推理潜力。

Abstract: Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements ("Bears are daxable"), universally quantified NPs ("all bears are daxable") and indefinite plural NPs ("some bears are daxable") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.

</details>


### [95] [MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication](https://arxiv.org/abs/2601.09853)
*Sraavya Sambara,Yuan Pu,Ayman Ali,Vishala Mishra,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文提出并分析了大模型（LLMs）在处理真实健康咨询中判断并纠正患者错误前提的能力。结果显示，LLMs在此方面表现不佳，存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 患者在健康提问时常常包含错误的假设。现实中，医生会先纠正这些错误前提再作答。而大模型应用越来越广，但其在医疗交流这一关键能力上的表现未知。

Method: 作者开发了半自动流程，构建了含1100+需重定向医疗问题的数据集MedRedFlag，对比分析了LLMs与临床医生的回答表现。

Result: 分析表明，LLMs即使识别出有问题的前提，也常常未能进行有效重定向，甚至给出可能危害患者决策的回答。

Conclusion: 当前LLMs在真实健康交流场景下，对带有错误前提的提问安全性不足，存在重大潜在风险，对面向患者的医疗AI系统提出了警示。

Abstract: Real-world health questions from patients often unintentionally embed false assumptions or premises. In such cases, safe medical communication typically involves redirection: addressing the implicit misconception and then responding to the underlying patient context, rather than the original question. While large language models (LLMs) are increasingly being used by lay users for medical advice, they have not yet been tested for this crucial competency. Therefore, in this work, we investigate how LLMs react to false premises embedded within real-world health questions. We develop a semi-automated pipeline to curate MedRedFlag, a dataset of 1100+ questions sourced from Reddit that require redirection. We then systematically compare responses from state-of-the-art LLMs to those from clinicians. Our analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, and provide answers that could lead to suboptimal medical decision making. Our benchmark and results reveal a novel and substantial gap in how LLMs perform under the conditions of real-world health communication, highlighting critical safety concerns for patient-facing medical AI systems. Code and dataset are available at https://github.com/srsambara-1/MedRedFlag.

</details>


### [96] [OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing](https://arxiv.org/abs/2601.09858)
*Yilin Bao,Ziyao He,Zayden Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的强化学习框架，使大模型在生成科学论文时具备更好的全局结构和事实一致性，并展示了显著的效果提升。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型虽然在局部语言流畅性上表现强，但在生成科学论文时常出现结构混乱、输入内容利用不足、引用不一致等问题。论文动机在于解决这一科学写作生成领域的重要痛点。

Method: 将科学论文的大纲构建视作一个长期规划问题，通过分层文档结构上的序列编辑动作，自底向上逐步生成论文。提出两阶段优化流程：首先通过逆向回溯已生成部分进行结构一致性约束，然后通过前向基于价值的强化学习，结合科学性、连贯性和引用准确性的奖励信号优化。

Result: 实验中在新提出的科学论文生成基准之上，该方法在结构连贯性和引用可靠性等方面，均优于强大的神经网络与主流大模型。

Conclusion: 提出的框架能够明显提升论文生成的结构合理性和事实一致性，对科学写作自动化有积极推动作用。

Abstract: Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

</details>


### [97] [Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL](https://arxiv.org/abs/2601.09876)
*Yifei Shen,Yilun Zhao,Justice Ou,Tinglin Huang,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了CLINSQL基准，用以评测模型在现实临床EHR文本转SQL任务中的表现，并对多种最新模型进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 现有的文本转SQL技术在实际临床应用中表现有限，难以处理异构EHR表、多步骤查询和临床语境下的复杂需求。因此，需要开发和评测更能满足临床需求的基准任务和解决方案。

Method: 作者构建了包含633个由专家标注的任务的CLINSQL基准，涉及MIMIC-IV v3.1数据库，覆盖多表连接、临床相关过滤和复杂可执行SQL任务。采用链式思维自我优化(CoT self-refinement)及基于执行与临床需求的多项评测打分方式，对22个业界和开源模型进行了系统性评测。

Result: GPT-5-mini在测试集上最高执行得分74.7%；开源模型DeepSeek-R1得分69.2%；Gemini-2.5-Pro在简单任务得分85.5%，但在难任务上下降到67.2%。所有模型距离临床可靠性需求仍有较大差距。

Conclusion: CLINSQL推动了临床可用的文本转SQL发展，但当前模型无法完全满足高可靠性的实际需求，仍需持续提升。

Abstract: Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.

</details>


### [98] [Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal](https://arxiv.org/abs/2601.09886)
*Sathvik Nair,Byung-Doh Oh*

Main category: cs.CL

TL;DR: 本文对比了基于人类完形填空任务(cloze task)与语言模型（LM）概率的词可预测性指标，发现LM概率能更好预测语言加工负担，并讨论了这种优势的三个原因。


<details>
  <summary>Details</summary>
Motivation: 传统上，语言中词的可预测性常通过cloze任务（让人类补全缺失词）获取概率数据，近年来又开始用语言模型直接输出的概率。已有研究发现，LM 概率对处理负担有更强的预测力，但还需确认其优势的具体原因及其科学解释影响。

Method: 作者提出并验证了三种有关LM概率优势的假设：1）LM概率不会像cloze一样存在分辨率低问题；2）LM概率能区分语义上相似的词；3）LM概率对低频词概率分配更精确。

Result: 实验验证了上述三点，发现语言模型概率的数据细度和区分力均优于cloze概率，特别是在处理语义、分辨同类词和低频词上表现出色。

Conclusion: LM概率在表征词预测性时具有天然优势，建议未来除了提升cloze实验分辨率外，也应进一步探究人类预测机制是否同样敏锐于LM能区分的精细差异。

Abstract: How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.

</details>


### [99] [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953)
*Christabel Acquaye,Yi Ting Huang,Marine Carpuat,Rachel Rudinger*

Main category: cs.CL

TL;DR: 本文探讨利用开源大语言模型（LLMs）来预测多项选择数学题的真实难度，发现在特定模拟条件下，LLM的预测与真实学生难度高度相关，部分情况下弱模型预测效果更优。


<details>
  <summary>Details</summary>
Motivation: 传统标准化数学测评依赖昂贵的人类预测试来评估题目难度，因此希望寻求自动化、低成本的评估方式，尤其是用LLM来辅助判断题目难度。

Method: 作者提出让LLM以不同水平的4年级、8年级和12年级学生身份模拟答题，形成“虚拟教室”，再用模拟结果拟合项目反应理论（IRT）模型，并与NAEP真实数据进行对比，实验比较不同教室规模、角色命名方式等对结果的影响。

Result: 模拟出的IRT难度参数与真实学生数据的相关性分别达到0.75（四年级）、0.76（八年级）和0.82（十二年级）；虚拟学生使用具体名字效果优于ID，再按性别和种族分配名字则进一步提升了预测效果。另外，数学能力较弱的Gemma模型在难度预测上反而优于更强的Llama和Qwen。

Conclusion: 开源LLMs，特别是对数学不那么擅长的模型，可以在适当模拟设计下有效预测数学题难度，有望为标准化数学测评大规模自动评估提供低成本新工具。

Abstract: Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a "classroom" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different "classroom sizes," showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.

</details>


### [100] [Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG](https://arxiv.org/abs/2601.09982)
*David Samuel Setiawan,Raphaël Merx,Jey Han Lau*

Main category: cs.CL

TL;DR: 论文针对低资源语言NMT在领域迁移下性能大幅下降的问题，提出结合NMT和RAG方法，有效恢复翻译质量。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏大规模数字资源，NMT模型在遇到领域外（如圣经不同卷本）文本时表现急剧下降，需要有效缓解领域迁移带来的性能损失。

Method: 提出混合框架：先用已微调的NMT模型生成初译稿，再借助检索增强生成（RAG）的LLM进一步润色。检索出的相关例句数量成为关键因素。

Result: 新系统在未见过的领域实现chrF++上大幅提升（从27.11提升到35.21），基本恢复到原领域内的翻译水平。

Conclusion: LLM结合检索增强方法可成为领域外NMT的重要补充，显著提升低资源语言跨领域翻译的鲁棒性和质量。

Abstract: Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust "safety net," repairing severe failures in zero-shot domains.

</details>


### [101] [SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction](https://arxiv.org/abs/2601.10003)
*Sanghyeok Choi,Woosang Jeon,Kyuseok Yang,Taehyeong Kim*

Main category: cs.CL

TL;DR: 提出了一种通过引入问答对中介表示的新方法 SocraticKG，有效在三元组抽取前展开文档语义，解决了知识图谱构建中的信息覆盖与关联碎片化权衡难题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的知识图谱构建方法在追求事实覆盖时容易导致关系碎片化，而过早的融合则会造成信息丢失，亟需新的方法在二者之间取得平衡。

Method: 提出 SocraticKG 方法，使用以 5W1H（谁、何时、何地、什么、为何、如何）为导向的问题-答案扩展，将文档语义通过结构化问答对系统性展开，作为三元组抽取前的中间语义支架，强化上下文依赖和隐式关系。

Result: 在 MINE 基准上验证了 SocraticKG，有效提升了事实保留能力，并在知识量增加时依然保持较高的结构连贯性。

Conclusion: 通过问答中介的语义支架，在知识图谱抽取前更好地结构化语义信息，实现了更连贯、可靠的知识图谱构建。

Abstract: Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.

</details>


### [102] [EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records](https://arxiv.org/abs/2601.10020)
*Lingfei Qian,Mauro Giuffre,Yan Wang,Huan He,Qianqian Xie,Xuguang Ai,Xeuqing Peng,Fan Ma,Ruey-Ling Weng,Donald Wright,Adan Wang,Qingyu Chen,Vipina K. Keloth,Hua Xu*

Main category: cs.CL

TL;DR: 本文提出了EHRNavigator，一个多智能体框架，可在真实电子健康档案(EHR)环境中高效完成患者级问题回答，并在实际医院条件下展示出强大的泛化能力和高准确率。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的自然语言问答系统仅在标准化数据集上评估，忽略了真实临床环境中的多样化数据结构和复杂需求，导致实际应用效果有限。因此，需要开发能适应真实EHR多模态、异构数据且贴合临床场景的问答系统。

Method: 研究团队设计了EHRNavigator，这是一种多智能体架构，借助AI代理在异构和多模态EHR数据中完成高效患者级问答。该系统在公共基准数据集以及医院内部真实数据集上进行评估，并加入了多样化的数据结构、时间推理和多模态证据整合的场景设置。

Result: 在定量评估和临床医生复查病例的联合评价下，EHRNavigator在真实世界案例中准确率达86%，同时保持了临床可接受的响应速度，显示出良好的泛化能力和实际部署潜力。

Conclusion: EHRNavigator有效地弥合了标准基准评价与临床实际部署之间的差距，展现出鲁棒、自适应和高效的EHR问答解决方案，为实际医疗场景中的辅助决策提供了有力工具。

Abstract: Clinical decision-making increasingly relies on timely and context-aware access to patient information within Electronic Health Records (EHRs), yet most existing natural language question-answering (QA) systems are evaluated solely on benchmark datasets, limiting their practical relevance. To overcome this limitation, we introduce EHRNavigator, a multi-agent framework that harnesses AI agents to perform patient-level question answering across heterogeneous and multimodal EHR data. We assessed its performance using both public benchmark and institutional datasets under realistic hospital conditions characterized by diverse schemas, temporal reasoning demands, and multimodal evidence integration. Through quantitative evaluation and clinician-validated chart review, EHRNavigator demonstrated strong generalization, achieving 86% accuracy on real-world cases while maintaining clinically acceptable response times. Overall, these findings confirm that EHRNavigator effectively bridges the gap between benchmark evaluation and clinical deployment, offering a robust, adaptive, and efficient solution for real-world EHR question answering.

</details>


### [103] [EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels](https://arxiv.org/abs/2601.10033)
*Wan Jou She,Lis Kanashiro Pereira,Fei Cheng,Sakiko Yahata,Panote Siriaraya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 该论文介绍了EmplifAI，一个面向日本慢性病患者情感对话的数据集，旨在提升情感对齐和共情对话能力。通过精细划分情感类别与实际医疗情境，对多款大模型进行了评估和微调，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 慢性病患者在疾病管理过程中会经历复杂多变的情绪，现有情感对话数据集多缺乏针对具体医疗情景和细致情感分类的支持，因此需要针对日本语境下，能够捕捉细腻复杂情感的对话数据集帮助改进对话系统的共情能力。

Method: 构建了基于28类细分情感标签（来自GoEmotions拓展）的情境对话数据集，包括280个医疗情景和4125个双轮对话，数据通过众包与专家校审获得。用BERTScore对多款大模型在情感对齐指标上进行评估，并对日本基线大模型进行EmplifAI数据集微调。还比较了AI评判与人工评分的一致性以验证评价流程。

Result: 微调后的日语大模型在对话流畅度、一般共情性及特定情感共情性方面均有明显提升。在情感对齐任务上达到0.83的F1分数。LLM和人类判分者的评分高度相关，保证了评价流程的可信度。

Conclusion: EmplifAI弥补了日语医疗共情对话语料的空白且细致的情感颗粒度有助于复杂情绪的建模，对提升大模型在医疗情感场景下的共情生成有重要促进作用。同时验证了AI判分在此类任务中的适用性及当前方法的有效性。

Abstract: This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.

</details>


### [104] [Long-Chain Reasoning Distillation via Adaptive Prefix Alignment](https://arxiv.org/abs/2601.10064)
*Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识蒸馏方法P-ALIGN，通过自适应地截取教师模型生成的推理轨迹前缀，有效提升了小规模学生模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型的复杂推理轨迹虽然能增强学生模型推理能力，但其冗长和复杂结构导致学生模型学习困难，监督信号与学生模型学习能力不匹配。

Method: P-ALIGN框架会根据教师推理轨迹的结构，自适应地截取有用而简洁的前缀部分，如果剩余的后缀部分冗余或对指导无帮助，则舍弃。学生模型仅以这些高质量前缀进行学习并对齐，实现有效的知识蒸馏。

Result: 在多个数学推理基准测试上，P-ALIGN方法的准确率比所有对比基线高3%以上。分析表明，所构造的前缀为学生模型提供了更有效的监督信号，避免了冗余和不确定成分的负面影响。

Conclusion: P-ALIGN通过前缀对齐方法，提高了学生模型学习效率及推理能力，是解决长推理轨迹蒸馏难题的有效方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.

</details>


### [105] [Deriving Character Logic from Storyline as Codified Decision Trees](https://arxiv.org/abs/2601.10080)
*Letian Peng,Kun Zhou,Longfei Yun,Yupeng Hou,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出了一种新的可执行和可解释的行为配置文件结构（CDT），用于提升角色扮演智能体在多样叙事环境中的一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 已有的角色扮演智能体行为配置文件结构松散、不可执行且验证不足，导致角色行为脆弱、不可靠。因此作者希望构建一种结构化、可执行且可以验证的新行为配置框架，以提升智能体表现的可靠性。

Method: 作者提出Codified Decision Trees（CDT）框架，通过从大规模叙事数据中学习，自动归纳条件行为规则，并用树状结构组织，将情景判断和具体行为陈述结合。通过数据驱动迭代归纳、验证和逐层细化，得到可透明检查和可更新的行为配置文件。

Result: 在16个不同作品的85个人物上的多项基准测试中，CDT框架在角色行为表现上显著优于人工编写的配置文件和以往的方法。

Conclusion: CDT能有效生成更可靠、可验证的行为配置文件，提升角色扮演智能体的真实感和行为连贯性，推动相关技术发展。

Abstract: Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.

</details>


### [106] [Is MT Ready for the Next Crisis or Pandemic?](https://arxiv.org/abs/2601.10082)
*Vipasha Bansal,Elizabeth Brown,Chelsea Kendrick,Benjamin Pong,William D. Lewis*

Main category: cs.CL

TL;DR: 本研究评估了四个主流商业机器翻译系统在医疗和危机环境下，针对低资源语言的表现。使用TICO-19语料库，对疫情相关语言的翻译效果进行分析，并基于输出结果可用性，讨论了机器翻译系统在未来疫情的“就绪”程度。


<details>
  <summary>Details</summary>
Motivation: 危机时期，比如疫情，政府、医疗机构和援助者常与受助人有交流障碍，特别是面对低资源语言，沟通问题更加突出。本研究旨在检验现有商业机器翻译系统在处理低资源语言、特别在危机和医疗语境下的实际效果，为未来类似危机提供参考。

Method: 本文选用TICO-19数据集，该数据集包含了疫情相关、来自多种高优先级语言（多为低资源语言）的文本。研究者评估了四种主流商业机器翻译系统，将这些语句进行翻译，并针对译文的可用性进行对比分析。

Result: 结果展示了不同商业MT系统在处理低资源语言疫情相关文本上存在的优缺点。部分系统在某些特定低资源语言上表现较好，但整体上各系统都有提升空间。目前的翻译系统尚未能完全满足未来疫情期间对低资源语言信息传播的需求。

Conclusion: 现有商业机器翻译系统对低资源语言的应急翻译能力有限，危机场景下不能完全依赖。建议在未来加强对低资源语言机器翻译系统的优化和数据资源积累，提高全球公共卫生危机下的沟通效率。

Abstract: Communication in times of crisis is essential. However, there is often a mismatch between the language of governments, aid providers, doctors, and those to whom they are providing aid. Commercial MT systems are reasonable tools to turn to in these scenarios. But how effective are these tools for translating to and from low resource languages, particularly in the crisis or medical domain? In this study, we evaluate four commercial MT systems using the TICO-19 dataset, which is composed of pandemic-related sentences from a large set of high priority languages spoken by communities most likely to be affected adversely in the next pandemic. We then assess the current degree of ``readiness'' for another pandemic (or epidemic) based on the usability of the output translations.

</details>


### [107] [CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking](https://arxiv.org/abs/2601.10085)
*Viet Cuong Nguyen,Nhi Yen Nguyen,Kristin A. Candan,Mary Conlon,Vanessa Rumie,Kristen Risola,Srijan Kumar,Munmun De Choudhury*

Main category: cs.CL

TL;DR: 本文提出了CALM-IT框架，专注于模拟和评估长篇动机访谈（MI）对话，通过建模双向会话动态显著提升对话连贯性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在心理健康相关对话中难以维持长时间目标导向的对话，容易出现连贯性丧失和策略漂移。亟需一种更能模拟现实治疗会话动态的方法，以提高人工智能生成对话的质量和有效性。

Method: 提出CALM-IT，将治疗师与来访者的互动建模为双向状态空间过程，两个“智能体”根据推断的对齐、心理状态和短期目标同步调整会话策略和发言内容。通过大规模评测，与多个强基线方法比较对话效果和一致性。

Result: CALM-IT在对话有效性和目标一致性方面均优于对比方法，且在会话变长时表现出更稳定的性能。同时，虽然CALM-IT减少了治疗师的主动引导，但来访者接受率最高（64.3%），显示了更精准的干预时机。

Conclusion: 通过建模不断变化的会话状态，是生成高质量长篇合成对话的关键。CALM-IT为心理健康相关应用带来了更真实、稳健的长对话生成能力。

Abstract: Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.

</details>


### [108] [SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature](https://arxiv.org/abs/2601.10108)
*Yiming Ren,Junjie Wang,Yuxin Meng,Yihang Shi,Zhiqiang Lin,Ruihang Chu,Yiran Xu,Ziming Li,Yunfei Zhao,Zihan Wang,Yu Qiao,Ruiming Tang,Minghao Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出新的评估方法FITO及相应数据集和基准，用于检测多模态大模型在科学文献理解中的证据链能力，发现当前主流模型在证据溯源上存在显著短板。


<details>
  <summary>Details</summary>
Motivation: 传统评测方法往往只考查模型答案的正确性，但忽视了答案背后是否有充分的证据支撑，无法判定模型是否真正理解科学文献内容，因此亟需更加严格、能关联文本和图表证据链的评测方案。

Method: 提出Fish-in-the-Ocean（FITO）评测范式，要求模型在原生科学文献中构建跨模态证据链；同时搭建了SIN-Data（原生文献文本和图片交错语料）、SIN-Bench（涵盖证据发现、假说验证、扎根问答和基于证据的综合）四套任务，并首创“No Evidence, No Score”打分机制，只对依据可验证证据的答案打分并诊断证据质量。

Result: 在8个主流多模态大模型（MLLMs）上测试，发现证据溯源能力是主要瓶颈：Gemini-3-pro整体得分最高（0.573），GPT-5在问答准确率最高（0.767），但在证据支持得分较低，揭示出正确率和可追溯证据之间存在显著差距。

Conclusion: 当前多模态大模型虽然能给出正确答案，但缺乏跨模态证据支撑，不能有效追溯答案来源。FITO范式和SIN-Bench为科学文献理解提供了更有针对性的评测工具，有助于推动模型的证据关联与溯源能力提升。

Abstract: Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic "Needle-In-A-Haystack" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the "Fish-in-the-Ocean" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce "No Evidence, No Score", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.

</details>


### [109] [Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation](https://arxiv.org/abs/2601.10109)
*Lechen Zhang,Yunxiang Zhang,Wei Hu,Lu Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种以技能为中心的知识蒸馏方法，只需少量数据即可将大型推理模型的能力高效传递到小模型上。


<details>
  <summary>Details</summary>
Motivation: 传统大模型知识蒸馏需大量有标签数据，效率较低，作者希望探索更数据高效的蒸馏方式。

Method: 方法有两个关键点：1）根据学生模型薄弱技能挑选训练样本；2）微调过程中鼓励显式技能分解。只用1000条数据对学生模型进行训练。

Result: 在五个数学推理基准测试上，Qwen3-4B和Qwen3-8B小模型的性能分别比随机SFT提升1.6%和1.4%。改进主要集中于训练时强调的技能。

Conclusion: 以技能为中心的数据筛选和训练能在低数据量下显著提升学生模型在推理任务上的表现，是一种高效的知识迁移方式。

Abstract: Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.

</details>


### [110] [Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends](https://arxiv.org/abs/2601.10122)
*Ye Wang,Jiaxing Chen,Hongjiang Xiao*

Main category: cs.CL

TL;DR: 本文系统回顾了近年来角色扮演语言智能体（RPLA）的技术发展、关键方法、数据构建、评估体系及未来展望，总结了从规则模板到认知模拟的技术演进与研究现状。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的飞速发展，RPLA成为自然语言处理与人机交互领域的重要研究热点，亟需对其技术路径、数据与评价体系进行系统梳理，指导后续研究。

Method: 通过文献调研，本文梳理了RPLA的技术发展历程，归纳高质量角色扮演的关键技术（如心理量表驱动建模、记忆增强提示、动机-情境决策等），分析了角色语料库构建的方法和难点，并对多维度评估框架与基准进行了总结，对不同评价手段的利弊进行了讨论。

Result: 总结出RPLA技术发展的阶段划分、支撑路径、数据处理要点和主流评估方法，并归纳其各自优势与不足，系统性提供了该领域研究的现状图谱。

Conclusion: 文章展望了RPLA的未来方向，包括个性演变、多智能体协作、多模态互动及与认知神经科学结合，为后续研究提供系统视角与方法论参考。

Abstract: In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.

</details>


### [111] [ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156)
*Yutao Mou,Zhangchi Xue,Lijun Li,Peiyang Liu,Shikun Zhang,Wei Ye,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一套针对大语言模型(LLM)代理工具调用安全性的检测与防护方法，并有效降低了有害调用行为。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理能够使用外部工具执行任务，其安全风险随之上升。当前对LLM代理工具调用步骤的实时监控和预防干预研究较少，亟需有效的安全机制。

Method: 作者提出了TS-Bench基准用于评测LLM代理逐步工具调用的安全性，并开发了基于多任务强化学习的安全护栏模型TS-Guard，该模型通过推理整个交互历史，提前检测不安全操作。此外，设计了TS-Flow反馈驱动推理框架辅助代理安全决策。

Result: TS-Guard能够在执行前发现工具调用中的不安全操作，并提供可解释且具备泛化能力的安全判定与反馈。部署TS-Flow后，ReAct风格代理有害调用平均减少65%，在提示注入攻击场景下良性任务完成率提升约10%。

Conclusion: 本文方法能有效防控LLM代理工具调用风险，提升代理系统的整体安全性与可靠性，为LLM代理安全落地提供了新思路和工具。

Abstract: While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.

</details>


### [112] [What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models](https://arxiv.org/abs/2601.10159)
*Guimin Hu,Meng Li,Qiwei Peng,Lijie Hu,Boyan Xu,Ruichu Cai*

Main category: cs.CL

TL;DR: 本文分析了Mixture-of-Experts（MoE）大语言模型中专家激活机制，首次区分了领域专家和驱动专家，提出新方法以增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 目前关于大语言模型的可解释性研究多聚焦于Transformer的层或神经元层面，对于MoE模型中的专家级行为关注较少。受到人脑功能分区启发，作者希望通过研究专家激活，揭示模型内部机制。

Method: 作者在三个公开领域数据集上，提出了基于熵和因果效应的新指标，分析哪些专家被激活、激活模式是否一致，并研究不同专家（领域和驱动专家）的激活与输出贡献，以及单个token如何影响专家激活。

Result: 分析发现：(1) 一些激活专家存在明确领域偏好，另一些则对模型表现有显著因果影响；(2) 句子前部token更易激活驱动专家；(3) 调整领域和驱动专家权重可显著提升模型在各领域的表现。

Conclusion: 工作揭示了MoE模型中专家的内部机制，区分并量化了领域专家与驱动专家的作用，显著提升了MoE模型的可解释性。

Abstract: Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.

</details>


### [113] [Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment](https://arxiv.org/abs/2601.10160)
*Cameron Tice,Puria Radmard,Samuel Ratnam,Andy Kim,David Africa,Kyle O'Brien*

Main category: cs.CL

TL;DR: 本文通过控制实验探究了预训练数据中关于AI系统对齐/未对齐话语对下游大模型行为对齐性的因果影响。结果证明，训练语料中未对齐的讨论会加剧模型未对齐行为；而强调对齐的语料有助于显著降低未对齐表现。


<details>
  <summary>Details</summary>
Motivation: 过去关于大模型的对齐训练主要关注后训练（post-training）过程，对预训练阶段语料内容对行为对齐的影响研究较少。本文动机在于检验：如果预训练语料中AI相关的描述以负面表述（如未对齐行为）为主，模型是否会“内化”这种偏见，并实际表现出更多未对齐行为。

Method: 作者预训练了6.9B规模语言模型，控制性地调整语料库中对AI系统的对齐与未对齐讨论比例。随后，评估模型在下游对齐表现时，比较不同比重下生成结果的对齐性差异。

Result: 实验发现：提升语料中AI未对齐讨论比例，会显著提高模型未对齐行为的发生率；反之，提高对齐议题语料比例，未对齐得分有显著降低（从45%降至9%）。该影响即便在经过post-training阶段后仍然存在，只是有所减弱。

Conclusion: 论文证实了预训练语料对模型对齐倾向有显著影响，引入“alignment pretraining”的概念，建议在预训练阶段就关注与对齐相关的文本分布，以补充后训练中的alignment技术。

Abstract: Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai

</details>


### [114] [AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers](https://arxiv.org/abs/2601.10161)
*Prachuryya Kaushik,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出了AWED-FiNER，一个面向36种全球语言的细粒度命名实体识别（FgNER）开源生态系统，涵盖66亿人口，支持低资源语言和专业细粒度NLP任务。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在通用NLP任务表现优良，但在低资源和细粒度命名实体识别任务中存在困难，特别是在一些弱势语言上几乎没有高质量工具。作者旨在填补这一空白，为多语言细粒度实体识别提供高效、友好、可离线部署的解决方案。

Method: AWED-FiNER提供了一套包括多语言专家模型、自动分发路由工具、Web应用等工具体系；专家模型为极小型SOTA模型，便于本地或边缘设备部署；工具可根据文本自动选择匹配的多语言专家模型并返回实体注释。平台支持普通用户即点即用，无须编程经验。

Result: 实现了36种语言（覆盖66亿人，包括诸多弱势语言）FgNER支持，发布了49个极小型专家模型。工具链在通用和弱势语言上的FgNER任务表现优良，能快速、高效、离线提供服务。所有平台和模型均已开源。

Conclusion: AWED-FiNER显著提升了全球多语言环境的细粒度实体识别能力，打破了低资源和弱势语言在该领域的技术壁垒；对实际应用和研究均有重要推动作用，成果完全开源、易于扩展，为相关领域基准设立了新标准。

Abstract: We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).

</details>


### [115] [Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection](https://arxiv.org/abs/2601.10167)
*Nhung Nguyen Thi Hong,Cuong Nguyen Dang,Tri Le Ngoc*

Main category: cs.CL

TL;DR: 本文提出了一个专为越南语债务催收场景设计的大模型Credit C-GPT，显著提升了对话理解等任务的效果，有望为企业呼叫中心提供实时智能支持和数据分析。


<details>
  <summary>Details</summary>
Motivation: 在BFSI行业，债务催收需大量人工对话，涉及非正式表达、情绪波动和复杂推理，传统NLP方法难以满足实际需求。

Method: 提出并开发了一个7亿参数的越南语专用大语言模型Credit C-GPT，融合对话理解、情感识别、意图检测、通话阶段分类及结构化信息抽取等多项对话智能任务，采用统一推理框架进行联合学习，通过自有标注数据集完成训练和评估。

Result: 实验证明，Credit C-GPT在多个对话任务上的表现均优于传统流水线方式，展现了其在实际催收场景的强大适应性和有效性。

Conclusion: 专用大语言模型能够为企业呼叫中心在债务催收等场景下，提供可扩展、隐私安全的实时对话辅助和呼叫后分析解决方案，具有广泛应用前景。

Abstract: Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.

</details>


### [116] [HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning](https://arxiv.org/abs/2601.10187)
*Ziang Cui,Mengran Yu,Tianjiao Li,Chenyu Shi,Yingxuan Shi,Lusheng Zhang,Hongwei Lin*

Main category: cs.CL

TL;DR: 论文指出当前大语言模型（LLM）在多语言翻译中存在输出冗长的问题，尤其影响字幕配音等时间受限场景。作者提出了新的评测基准和优化方法，有效解决了语义与时长约束的平衡问题。


<details>
  <summary>Details</summary>
Motivation: LLM在翻译场景下，常出现译文冗长，导致无法满足如字幕、配音等严格时长要求的应用需求。现有方法难以同时保证内容忠实和时长控制。

Method: 1. 新建了Sand-Glass基准，用于评测带音节级时长约束的翻译任务；2. 提出HOMURA强化学习框架，综合语义保持与时长约束进行联合优化，引入KL正则项和动态音节比奖励，直接“驯服”译文输出长度。

Result: 实验显示HOMURA方法对输出长度有显著控制能力，能兼顾语言密度、时长约束和语义充分性，表现优于现有LLM基线。

Conclusion: 通过提出的新基准和HOMURA框架，论文有效解决了多语言翻译中语义与时长的矛盾，为实际中的字幕、配音等场景赋能，提升了LLM在受时长约束应用中的实用价值。

Abstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively "tames" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.

</details>


### [117] [HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns](https://arxiv.org/abs/2601.10198)
*Xintao Wang,Jian Yang,Weiyuan Li,Rui Xie,Jen-tse Huang,Jun Gao,Shuai Huang,Yueping Kang,Liyuan Gou,Hongwei Feng,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了HUMANLLM框架，通过将心理模式视为相互作用的因果力量，提升大型语言模型对人类行为模式的真实模拟能力，并在多模式动力学模拟和人类一致性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在角色扮演和人格模拟方面取得进展，但在与人类认知和行为模式的真实对齐上仍存在不足，因此需要更复杂且更贴合人类心理规律的建模方式。

Method: 作者从约1.2万篇学术论文中总结了244种心理模式，并合成了11359个模式间相互强化、冲突或调节的多场景，通过多轮对话展现内心想法、行动和对话行为，建立双层检查机制同时评估单一模式和多模式交互的准确性。

Result: 新的HUMANLLM框架在多模式动力学上的表现超过了参数量4倍的Qwen3-32B，在与人类行为模式对齐上也取得了很高的相关性（r=0.91）。

Conclusion: 实现逼真的人性化语言模型需进行心理认知过程建模，仅仅模拟人怎么做还不够，还需模拟驱动其行为的心理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning and generation, serving as the foundation for advanced persona simulation and Role-Playing Language Agents (RPLAs). However, achieving authentic alignment with human cognitive and behavioral patterns remains a critical challenge for these agents. We present HUMANLLM, a framework treating psychological patterns as interacting causal forces. We construct 244 patterns from ~12,000 academic papers and synthesize 11,359 scenarios where 2-5 patterns reinforce, conflict, or modulate each other, with multi-turn conversations expressing inner thoughts, actions, and dialogue. Our dual-level checklists evaluate both individual pattern fidelity and emergent multi-pattern dynamics, achieving strong human alignment (r=0.91) while revealing that holistic metrics conflate simulation accuracy with social desirability. HUMANLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4x fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling--simulating not just what humans do, but the psychological processes generating those behaviors.

</details>


### [118] [One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?](https://arxiv.org/abs/2601.10205)
*Arya Shah,Himanshu beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 本文推出了一个覆盖12种印度语言的评测基准，用于衡量多语言助手在个性化指令兼容性上的表现，并评估了八个多语言嵌入模型，给出了多项任务的基线结果。


<details>
  <summary>Details</summary>
Motivation: 印度拥有十多亿人口和多种书写体系，现有多语言助手很难准确对齐用户的文化背景及个性化偏好，而且以往的测试方法多混合生成与检索，难以单独评价模型的嵌入兼容能力。因此，迫切需要更细致多语言、多任务的检索型指标。

Method: 作者开发了涵盖12种印度语言的统一评测基准，设计了个性化指令的单语/跨语检索、反向检索和二元兼容性分类四种任务，评测了8个冻结编码器的多语嵌入模型，使用轻量级逻辑回归头做兼容分类，并在标准指标如Recall@1、AUROC等上进行比较。

Result: E5-Large-Instruct模型在单语检索Recall@1达27.4%，跨语检索达20.7%；BGE-M3模型在反向检索Recall@1为32.1%；LaBSE模型在分类任务AUROC为75.3%，并具备良好校准性。

Conclusion: 本文建立了多语言助手个性化指令兼容性的系统评测基准，并给出了多模型的对比分数，为未来印度多语言检索和助手设计提供基线与参考，实现可复现的实验流程。

Abstract: Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4\% on monolingual retrieval and 20.7\% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1\% Recall@1. For classification, LaBSE attains 75.3\% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.

</details>


### [119] [GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients](https://arxiv.org/abs/2601.10229)
*Kentaro Kazama,Daiki Shirafuji,Tatsuhiko Saito*

Main category: cs.CL

TL;DR: 提出了GeoSteer框架，通过对大模型的推理过程在潜在空间中进行引导，提高了多步推理中间步骤的质量，实现更准确和可靠的推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多步推理场景下常依赖CoT（思维链）方法，但中间推理步骤常出现逻辑不一致的问题，降低了推理过程的可靠性。因此，作者希望提升中间推理步骤的质量。

Method: 1）构建带有片段级评分的CoT数据集；2）训练VAE模型和质量评估模型，学习高质量CoT推理轨迹的低维流形；3）以自然梯度的形式，在隐空间中引导目标LLM的隐藏状态，向高质量区域靠拢，实现几何一致性调整。

Result: 在GSM8k数据集和Qwen3大模型系列上测试，GeoSteer方法将精确匹配准确率提升了最多2.6个百分点，成对胜率提升5.3个百分点。

Conclusion: GeoSteer是一种高效可控的方法，能够系统性提升大模型多步推理过程中间步骤的质量，提高整体推理性能。

Abstract: Recent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.

</details>


### [120] [Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?](https://arxiv.org/abs/2601.10242)
*Guanxu Chen,Dongrui Liu,Jing Shao*

Main category: cs.CL

TL;DR: 本文探讨了循环Transformer（LTs）是否能够通过多次迭代共享层实现更强的模型自省，从而缩小大语言模型内在知识与外部文本输出之间的差距。结果显示，循环次数的增加虽然使差距缩小，但部分原因是内部知识表达的退化。此外，当前循环Transformer在多轮迭代中对表征的感知能力并无提升，仅在最后一轮迭代中体现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内部知识与生成文本之间存在差距，理解和缩小这一差距对模型表现和可信度有重要意义。作者提出循环Transformer，希望利用迭代机制提高模型自省能力，探讨其在桥接表征与自然语言之间的潜力。

Method: 作者通过多组实证实验，系统评估循环Transformer在不同迭代次数下，模型内在表征与外部输出之间的一致性变化，并进一步分析了模型在各轮迭代中对表征感知的能力。

Result: 增加循环迭代次数后，表征-输出差距有所缩小，但内部知识表征的质量同时下降。模型对表征的感知能力并没有随着循环增加而提升，仅在最后一轮有效。

Conclusion: 循环Transformer为增强模型计算深度提供了新方向，但目前还未实现真正意义上的自省能力，模型尚不能有效将内部表征空间与自然语言链接起来。

Abstract: Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.

</details>


### [121] [coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts](https://arxiv.org/abs/2601.10246)
*Prottay Kumar Adhikary,Reena Rawat,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了coTherapist系统，一种能够通过小型语言模型模拟治疗师核心能力的统一框架，在专业评测中表现优异，为数字心理健康工具的发展提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 受心理健康服务需求上升和从业者短缺的双重压力，亟需智能系统辅助专家提高服务可及性和质量。

Method: 提出coTherapist框架，通过针对心理健康领域的小型语言模型，结合领域微调、检索增强和智能推理等技术，模拟治疗师的专业能力；采用T-BARS评估量表和心理测量分析评估模型表现，并由领域专家进行人工评测。

Result: coTherapist在临床咨询问题上的回答比现有同类系统更相关、更具临床依据，在同理心和治疗师人格特质等方面得分较高，并被专家评定为准确、可信与安全。该系统已被专家实际部署和测试。

Conclusion: 经过专门设计的小型语言模型可以实现接近专家的行为表现，为数字心理健康工具的规模化应用提供了切实可行的路径。

Abstract: Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.

</details>


### [122] [Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs](https://arxiv.org/abs/2601.10257)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.CL

TL;DR: 本论文研究大语言模型（LLM）在面对不同语言的道德困境时判断是否存在差异，并提出了一种新方法来区分输入语言和推理语言的影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究在评估LLM道德判断时，往往混淆了道德困境的语言和模型推理时用的语言，无法清晰区分各自影响，从而影响多语言背景下LLM的公正性和适用性。

Method: 作者设计了一种能分别操控输入（道德困境）语言和模型推理语言的方法，包括匹配和不匹配的情况，并利用道德基础理论（Moral Foundations Theory）解释模型的道德判断，同时分析权威维度的拆分。研究对象涵盖13个LLM，分析中英两种语言组合。

Result: 1）框架发现推理语言对道德判断的影响是输入语言的两倍；2）在近半数模型中检测到环境依赖性，这是标准评估方法未能发现的；3）开发出一套诊断分类体系，可指导模型部署。

Conclusion: 论文验证了分离输入语言和推理语言分析的重要性，为跨文化、多语言LLM道德判断研究提供了新方法，并通过模型表现为实际应用提供了部署建议。代码和数据集已公开。

Abstract: When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.

</details>


### [123] [Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel](https://arxiv.org/abs/2601.10266)
*Hiroaki Yamagiwa,Yusuke Takase,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 本文提出利用Projection Kernel（PK）度量注意力头权重矩阵张成子空间间的关系，展示了更清晰的头之间结构与功能对比，并揭示了GPT2-small中重要的头。


<details>
  <summary>Details</summary>
Motivation: 理解注意力头之间关系有助于解释Transformer内部结构，但现有度量不够精准，有待改进。

Method: 采用PK（基于主角度的子空间相似性度量）定量分析注意力头权重矩阵张成的子空间关系，与以往指标做对比。进一步，提出比较PK分布与随机正交子空间参考分布的方法，量化其信息性，并用PK构建有向图分析头的作用。

Result: 在IOI任务上，PK比此前度量（如Composition Score）更清晰地反映头与头之间的已知交互关系。通过PK分布分析及有向图，发现GPT2-small中L4H7作为枢纽头扮演着恒等头的角色。

Conclusion: PK作为新的子空间相似性度量，在解释Transformer注意力头的关系与功能方面优于现有方法，有助于深入理解模型内部机制。

Abstract: Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as the Composition Score. We further introduce a framework to quantify the informativeness of PK distributions by comparing them with a reference distribution derived from random orthogonal subspaces. As an application, we analyze a directed graph constructed from PK and show that, in GPT2-small, L4H7 acts as a hub by functioning as an identity head.

</details>


### [124] [MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts](https://arxiv.org/abs/2601.10272)
*Yuxuan Lou,Kai Yang,Yang You*

Main category: cs.CL

TL;DR: MoST提出了一种新颖的多模态大模型，通过专为语音和文本设计的专家混合结构，实现更高效的语音-文本处理，并在多个任务中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型通常未能区分不同模态（如语音、文本）本身的表征差异，导致性能受限。为更好地优化各自模态能力、增强跨模态理解，有必要引入能区分不同数据类型的模型机制。

Method: 提出了Modality-Aware Mixture of Experts (MAMoE)架构，将输入根据模态（语音、文本）路由到不同的专家组，既包含模态专属专家加强本身学习，又有共享专家增强跨模态信息流通。模型基于MoE语言模型，通过ASR、TTS数据集后训练，再用精心挑选的语音文本指令数据集微调，且全程使用开源数据。

Result: MoST在ASR、TTS、音频语言建模、口语问答等任务多个基准上，优于同规模主流模型。消融实验证明，模态特定路由与专家共享机制明显促进性能提升。

Conclusion: MoST是首个基于专家混合架构、端到端、全开源的语音-文本多模态大模型，架构设计切实提升了多模态处理效率和效果，其相关代码、模型和数据已全部开源。

Abstract: We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST

</details>


### [125] [The Straight and Narrow: Do LLMs Possess an Internal Moral Path?](https://arxiv.org/abs/2601.10307)
*Luoming Hu,Jingjie Zeng,Liang Yang,Hongfei Lin*

Main category: cs.CL

TL;DR: 作者提出了一种基于道德基础理论（MFT）的新方法，用于细致地映射和调整大语言模型（LLM）的内在道德表征，以提升其道德对齐性和安全性。该方法通过跨语言线性探测、道德向量操控、以及自适应道德融合，实现更有效的安全性与有用性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）道德对齐方法多为表面性措施，未能实质性改变模型内在的道德表征，容易导致模型拒绝良性请求或被“越狱”。因此，急需更深层次和可控的对齐技术，确保模型在多语言环境中的道德一致性与安全性。

Method: 1）采用道德基础理论（MFT）构建道德维度；2）通过跨语言（英文与中文）线性探针，分析模型中层的道德表征共享性及异同；3）提取及实验验证可操控的道德向量，并在模型内部与行为层面评估其效果；4）提出自适应道德融合（AMF）方法，在推理阶段动态检测并注入道德向量，实现更精细的安全性-有用性调整。

Result: 实验证明：① 英文与中文模型中存在既共享又各异的道德子空间；② 提取的可控道德向量有效调整了模型的道德输出方向，内外部效果均被验证；③ AMF方法能显著减少模型对良性请求的错误拒绝，并能更有效防止“越狱”攻击，优于现有基线。

Conclusion: 本研究不仅揭示了LLM多语言间内在道德表征的细致结构，还证明了基于MFT的可控道德调控和AMF方法在提升安全性、降低误拒与越狱成功率方面的有效性，为打造更安全且有用的AI奠定了基础。

Abstract: Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.

</details>


### [126] [Multilinguality as Sense Adaptation](https://arxiv.org/abs/2601.10310)
*Jan Christian Blaise Cruz,David Ifeoluwa Adelani,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文提出了一种新的多语言对齐方法SENSIA，通过对“意义层次”进行对齐，而不是仅仅依赖参数共享和模型规模，提高了多语言模型的表现。实验表明该方法优于现有方法，并能在较少目标语言数据下取得接近单语训练基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有多语言模型主要依赖参数共享和大规模数据，但在跨语言语义对齐和低资源语言建模方面存在局限。该研究动机在于寻找更精细的意义层级对齐方法，从而提升模型多语言迁移能力和数据利用效率。

Method: 提出SENSIA方法，将一种Backpack语言模型从一种语言适配到另一种语言。具体做法是在双语并行语料上对语义成分和上下文表示进行显式对齐，同时联合目标语言的语言模型损失训练以保证流畅性。

Result: 在四种类型多样的语言基准测试中，SENSIA方法总体超过了同类多语言对齐方法，并且在只用2至4倍更少的目标语言数据的情况下，能获得与单语从零训练基线相当的准确率。

Conclusion: SENSIA能更好地对齐跨语言语义结构，表现出对设计和规模的鲁棒性，并在减少目标语数据的背景下保持高准确性，为多语言NLP任务提供新的有效方法。

Abstract: We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.

</details>


### [127] [ADVOSYNTH: A Synthetic Multi-Advocate Dataset for Speaker Identification in Courtroom Scenarios](https://arxiv.org/abs/2601.10315)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 该论文提出了Advosynth-500数据集，旨在研究合成语音在特定环境下的区分能力，并用Speech Llama Omni模型模拟法庭辩论，验证当前系统的说话人识别能力。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语音合成模型生成的语音质量不断提升，如何在结构化环境中区分不同合成声音变得极为重要，特别是在法律等重视身份识别的场景。

Method: 作者构建了Advosynth-500数据集，包括100段合成语音，代表10位虚拟律师身份，并通过Speech Llama Omni模型模拟了五对律师在法庭上的辩论场景，定义每位律师的声音特质，并设立说话人识别挑战任务。

Result: 通过在说话人识别任务上的实验，评估了现代系统对这些合成语音归属身份的判别能力。数据集已开源，便于后续研究。

Conclusion: Advosynth-500为研究合成语音区分与识别提供了重要资源，有助于提升系统对高保真合成语音的辨识和安全检验能力。

Abstract: As large-scale speech-to-speech models achieve high fidelity, the distinction between synthetic voices in structured environments becomes a vital area of study. This paper introduces Advosynth-500, a specialized dataset comprising 100 synthetic speech files featuring 10 unique advocate identities. Using the Speech Llama Omni model, we simulate five distinct advocate pairs engaged in courtroom arguments. We define specific vocal characteristics for each advocate and present a speaker identification challenge to evaluate the ability of modern systems to map audio files to their respective synthetic origins.
  Dataset is available at this link-https: //github.com/naturenurtureelite/ADVOSYNTH-500.

</details>


### [128] [Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis](https://arxiv.org/abs/2601.10318)
*Songsong Tian,Kongsheng Zhuo,Zhendong Wang,Rong Shen,Shengtao Zhang,Yong Wu*

Main category: cs.CL

TL;DR: 本文提出了BAR-SQL，一种具备边界感知和高可靠性的NL2SQL统一训练框架，通过创新的数据合成与多阶段训练机制，显著提升了SQL生成质量和对边界问题的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL方法在面对多步复杂分析、语义歧义和数据库边界（如业务规则、模式限制）时，往往缺乏可靠性和边界识别能力。此外，缺乏权威测评集来系统衡量生成质量和边界适应性。该论文旨在提升NL2SQL系统的可靠性、可解释性，并补全相关测评。

Method: 1）提出Seed Mutation数据合成范式，构建含多步骤分析、歧义与模式限制等边界案例的企业级语料；2）采用知识溯源链式推理，生成可解释的中间推理轨迹，锚定于业务规则与表结构元信息；3）两阶段训练，包括监督微调（SFT）和基于分组相对策略优化的强化学习；4）设计任务条件混合奖励机制，综合优化SQL执行及边界回避准确率。

Result: 构建了新基准Ent-SQL-Bench，综合评估SQL执行与边界回避能力。实验证明BAR-SQL在此基准上取得91.48%的平均准确率，并在SQL生成与边界感知回避两项指标上超越Claude 4.5 Sonnet和GPT-5等主流对手。

Conclusion: BAR-SQL显著提升了复杂NL2SQL任务在企业场景下的可靠性和边界感知能力，具备更强的解释性和通用性。相关代码及基准对外公开，有望推动NL2SQL领域应用及标准建设。

Abstract: In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.

</details>


### [129] [An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit](https://arxiv.org/abs/2601.10321)
*Warren Jouanneau,Emma Jouffroy,Marc Palyart*

Main category: cs.CL

TL;DR: 本文提出了一种新的重排序模型，能高效处理长且多语种简历与职位描述，实现更准确的人岗匹配，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际招聘场景中，简历和职位描述通常很长、结构复杂甚至多语种，如何实时找到最合适的候选人存在显著挑战。传统方法面临效率和匹配准确率的双重瓶颈，因此亟需更高效且效果更好的模型。

Method: 作者设计了一种基于新一代晚期交叉注意力架构的重排序模型，将长文本的简历与项目描述进行分解，以减少计算负担。同时，引入生成式大语言模型（LLM）作为“教师”为学生模型提供语义丰富、更细粒度的监督信号，并通过增强的蒸馏损失进行知识迁移。

Result: 实验结果显示，该模型在相关性、排序及校准等指标上均优于现有最优基线方法。模型输出的技能匹配分数具备较好的一致性和可解释性。

Conclusion: 作者的方法在复杂、长文本和多语种招聘数据集中表现出色，显著提升了人岗匹配的效果，为实际人力资源推荐场景提供了更优解决方案。

Abstract: Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.

</details>


### [130] [OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding](https://arxiv.org/abs/2601.10343)
*Deming Ding,Shichun Liu,Enhui Yang,Jiahang Lin,Ziying Chen,Shihan Dou,Honglin Guo,Weiyu Cheng,Pengyu Zhao,Chengjun Xiao,Qunhong Zeng,Qi Zhang,Xuanjing Huang,Qidi Xu,Tao Gui*

Main category: cs.CL

TL;DR: 本文提出了OctoBench基准，通过细致设计的环境和任务，衡量大语言模型在遵循异构和持续性scaffold指令时的表现。实验展现模型在任务完成与合规性之间存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 当前主流将大语言模型（LLM）作为软件代理使用，依赖各种Coding Scaffold实现更复杂的功能，但模型能否准确遵守这些复杂、异构并持续性的指令，仍未被充分研究。

Method: 提出OctoBench基准，涵盖34种环境、217个编程任务（三种scaffold类型），涉及7,098个检查项，并配套开发自动化观测与评分工具，能捕获完整的模型决策过程、实现细粒度评定。

Result: 在八个代表性模型上的测试显示，虽然模型“解决任务”的能力较强，但在严格遵循scaffold规则上表现较弱，揭示了任务解决与指令遵从间的系统性鸿沟。

Conclusion: 当前模型合规性不足，开发与评测中需重视scaffold多样性和指令依从性。作者公开基准，推动更合规的自动编码代理研发。

Abstract: Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.

</details>


### [131] [Training-Trajectory-Aware Token Selection](https://arxiv.org/abs/2601.10348)
*Zhanming Shen,Jiaqi Hu,Zeyu Qin,Hao Chen,Wentao Ye,Zenan Huang,Yihong Zhuang,Guoshan Lu,Junlin Zhou,Junbo Zhao*

Main category: cs.CL

TL;DR: 本文分析了在高性能模型蒸馏时，模型性能突然下降的原因，并提出了基于训练轨迹的Token选择方法（T3S），有效提升了学生模型的推理能力和整体表现。


<details>
  <summary>Details</summary>
Motivation: 虽然知识蒸馏是提升模型效率和部署能力的重要方法，但在学生模型已经具备较强推理能力时，传统的持续蒸馏往往效果不佳，有时还会导致性能下降。作者希望找出这一现象产生的根本原因，并提出更加有效的蒸馏策略。

Method: 研究首先分析了蒸馏过程中模型性能突然下降的现象，并通过token层面的机制揭示了“模仿锚点token”和“待学习token”之间不能共存的根本问题。针对这一问题，作者提出了基于训练轨迹感知的Token选择（T3S）方法，通过在token层面重构训练目标，优化待学习token的学习路径，缓解性能瓶颈。

Result: T3S方法在AR和dLLM两种设定下都带来了显著提升。具体地，Qwen3-8B模型在有数百样本时即在推理基准上超越DeepSeek-R1，Qwen3-32B性能接近Qwen3-235B，并且T3蒸馏得到的LLaDA-2.0-Mini超过了自身AR基线，在16B规模的无思考模型中取得了最新最好成绩。

Conclusion: 研究表明，token级的训练目标重构有效地解决了传统蒸馏方法在高性能模型上出现的性能瓶颈问题，为大模型推理能力的高效迁移提供了新思路。

Abstract: Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.

</details>


### [132] [Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text](https://arxiv.org/abs/2601.10355)
*Zhihao Xu,Rumei Li,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xunliang Cai,Xiting Wang*

Main category: cs.CL

TL;DR: 本文提出了一种利用文本语料自动合成多轮工具使用数据的方法，并开发了高效的轨迹生成模型GEM，提升了大模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有多轮工具使用数据难以获取，限制了自主智能体的发展。需要一种高效、大规模获得真实多轮工具使用数据的方法。

Method: 提出以文本为基础的数据自动合成范式，设计GEM数据合成流程（相关性筛选、工作流与工具提取、轨迹落地、复杂度优化），并训练一个高效的轨迹生成器（Trajectory Synthesizer），实现端到端自动生成多轮工具使用轨迹。

Result: GEM-32B模型在BFCL V3多轮基准测试上提升了16.5%，在跨领域实验中部分超越了依赖人工标注的τ-bench模型。Trajectory Synthesizer模型在大幅降低推理延迟和成本的同时，生成质量与完整合成管道相当。

Conclusion: 基于自然语言语料的多轮工具使用数据自动合成方法有效提升了大模型在工具使用场景的泛化能力，并为打造高效自主智能体提供了可行、低成本的解决方案。

Abstract: Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.

</details>


### [133] [The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models](https://arxiv.org/abs/2601.10387)
*Christina Lu,Jack Gallagher,Jonathan Michala,Kyle Fish,Jack Lindsey*

Main category: cs.CL

TL;DR: 本文研究大语言模型中角色人格（persona）的结构，发现模型存在一个主要的“助手轴”决定其作为助手的倾向，并提出沿该轴控制激活值可稳定模型的行为。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽然能模拟多种人格，但大多默认是“助手”型人格，容易出现角色漂移（比如表现出奇怪或有害的人格）。作者希望理解和控制这种人格空间的结构，减少危害和漂移。

Method: 作者通过分析不同模型的激活方向，识别出与多种人格原型相关的方向，重点发现了主导的 “助手轴”。他们测试了在该方向正负向移动对模型人格表现和行为的影响，同时在预训练和微调后模型中比对实验。

Result: 沿助手轴正向引导模型会更友好、无害，反方向则更易展现非助手甚至神秘、戏剧化的风格。该“助手轴”也内嵌于预训练模型中。对助手轴偏离程度的度量可用于预测人格漂移风险。在限制该轴范围后，模型在易出错场景（如自我反省或应对脆弱用户）下变得更稳定。

Conclusion: 后训练让模型更偏向助手人格，但这种约束是松散的。控制助手轴可提高稳定性。未来应加强模型在训练和引导阶段的人格锚定，从源头防止漂移和风险。

Abstract: Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an "Assistant Axis," which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts "persona drift," a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.

</details>


### [134] [INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects](https://arxiv.org/abs/2601.10388)
*Tarun Sharma,Manikandan Ravikiran,Sourava Kumar Behera,Pramit Bhattacharya,Arnab Bhattacharya,Rohit Saluja*

Main category: cs.CL

TL;DR: 本文提出了INDIC-DIALECT，一个涵盖印地语和奥迪亚语11种方言的人类标注双语语料库，并构建了三个NLP基准任务，显著提升方言分类、翻译等低资源印度方言的模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管印地语和奥迪亚语有大量使用者，其下属方言却因资源匮乏和缺乏网络数据而长期被忽视。面对印度如此庞大的多方言语言环境，建立相关数据集对于推动低资源方言的NLP发展具有重大意义。

Method: 作者建立了INDIC-DIALECT，含13,000个句对，覆盖11种方言和两种主要语言。基于该语料，设置了方言分类、选择题（MCQ）解答和机器翻译三项任务，并在多种模型上评测，尤其比较了主流LLM和针对印度语言微调的Transformer模型。

Result: GPT-4o和Gemini 2.5等LLM在分类任务上表现不佳，而经过印度语言数据微调的Transformer模型将F1分数从19.6%提升至89.8%。在方言到标准语翻译任务中，混合AI模型BLEU得分61.32（基线23.36）；标准语到方言翻译，'规则+AI'方法BLEU得分48.44（基线27.59）。

Conclusion: INDIC-DIALECT为印度语系低资源方言NLP研究提供了首次大规模基准，有力推动了方言理解、分类和翻译任务。作者计划开源，以支持更多相关研究。

Abstract: Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\% to 89.8\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.

</details>


### [135] [TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction](https://arxiv.org/abs/2601.10410)
*Mihai Dan Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: 本文提出了一个完整的罗马尼亚语语言建模流水线TF3-RO，实现了从分词器设计、模型训练到大规模合成数据生成的闭环，解决了形态丰富、资源稀缺语言模型训练难题。


<details>
  <summary>Details</summary>
Motivation: 针对罗马尼亚语等形态复杂且计算资源稀缺的语言，目前缺乏公开、可复现的端到端语言模型训练与合成数据生成方案，尤其难以兼顾分词器适配、表征效率及高质量数据扩展。

Method: 基于高质量英语和罗马尼亚语寓言数据集（TF1 和 TF2），设计构建了面向罗马尼亚语的BPE和Unigram分词器，并从零开始训练了51.65M参数规模的Transformer（LLaMA风格）；后续通过量化、剪枝和logit蒸馏，得到精简的26.45M参数学生模型，并用该模型结合组合式提示生成三百万条高质量罗马尼亚语寓言；全流程融入系统评测指标，包括语法、实体一致性和LLM自动评价。

Result: 成功训练出专为罗马尼亚语定制的紧凑型语言模型与分词器，大规模生成了三百万条具有语言本地化特征的合成寓言语料，模型在各项评测中表现良好，具备实际部署能力。

Conclusion: TF3-RO证明了缜密的语言学方法和高效流水线能够极大提升资源稀缺语言的模型训练和合成数据能力，为小语种NLP研发提供了可复现参考和标准方案。

Abstract: Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.

</details>


### [136] [Are Language Models Models?](https://arxiv.org/abs/2601.10421)
*Philip Resnik*

Main category: cs.CL

TL;DR: 该论文质疑语言模型（LMs）作为认知模型系统的合理性，认为这种说法过度夸大了LMs的作用。


<details>
  <summary>Details</summary>
Motivation: 当前有观点评价语言模型可以作为认知科学中人类语言处理的模型系统，作者试图系统性检验这一观点的合理性。

Method: 作者依据Marr提出的三个层次（实现、算法和表征、计算理论），逐层分析LMs是否适合做认知模型系统。

Result: 在实现层面，作者认为LMs根本不能担此重任；在算法和表征层面，这一观点动力不足；而在计算理论层面，这也存在显著问题。

Conclusion: 作者认为将语言模型称为认知模型夸大了其实际价值，这样的表述助长了LLM不应有的炒作，把它们作为工具使用更为合理。

Abstract: Futrell and Mahowald claim LMs "serve as model systems", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.

</details>


### [137] [Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models](https://arxiv.org/abs/2601.10460)
*Abhinaba Basu,Pavan Chakraborty*

Main category: cs.CL

TL;DR: 现有去除刻板印象的模型在不同应用场景下表现不稳定，提出了一个新基准和分析方法，用于系统性评估偏见的情境敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前模型在标准测试（如消除刻板印象）中表现良好，但在真实世界应用中可能仍产生刻板印象。现有评测常忽略不同情境对偏见表现的影响，导致模型评估不可靠。

Method: 提出Contextual StereoSet基准，控制刻板印象内容不变，系统性变更情境（如时间、地点、受众等），并在13个模型及不同协议下测试。设计了Context Sensitivity Fingerprints（CSF）分析技术，对多情境下模型偏见表现离散度和对比进行量化。

Result: 实验证明，不同情境（如以1990年或2030年为背景、八卦视角、外群体旁观者等）对模型产生刻板印象的概率有显著影响。所有模型在某些情境下偏见加重，有些变化高达13个百分点。结果跨招聘、放贷、求助等不同类型场景均可复现。

Conclusion: 模型在固定测试下测得的偏见不能可靠推广到实际应用中，单一情境下的评测不足以反映实际偏见。提出的新评测方法CSF强制评测者考虑“在何种情境下偏见会出现”, 提高了偏见评估的鲁棒性，并公开了基准、代码与结果。

Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.
  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.
  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.
  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, "Under what conditions does bias appear?" rather than "Is this model biased?" We release our benchmark, code, and results.

</details>


### [138] [DR-Arena: an Automated Evaluation Framework for Deep Research Agents](https://arxiv.org/abs/2601.10504)
*Yiwen Gao,Ruochen Zhao,Yang Deng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文介绍了DR-Arena，一个用于动态、实时评估大语言模型（LLMs）作为深度研究智能体任务性能的全自动框架，实现了高效且与人类评价高度一致的模型评测。


<details>
  <summary>Details</summary>
Motivation: 当前主流的LLM评估数据集依赖静态数据，存在适用范围有限、与现实时间不匹配及数据污染等问题，因此亟需更动态且能及时反映模型最新能力的自动化评测方法。

Method: DR-Arena通过构建基于最新网络趋势的实时信息树，针对LLM的深度推理与广泛覆盖能力生成结构化评测任务。同时采用自适应演化闭环机制，根据模型表现随时提升任务难度，持续逼近模型能力上限，无需人工参与。

Result: 在对六种先进DR智能体的实验中，DR-Arena与LMSYS Search Arena排行榜的人类评判结果达到了0.94的Spearman相关性，实现了自动评价与人类表现高度一致。

Conclusion: DR-Arena证明了自己可以作为无需人工、低成本且可靠的智能体能力评估方案，为动态、大规模评测任务提供了有效工具，极大提升了模型评测的效率和实用价值。

Abstract: As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.

</details>


### [139] [AEQ-Bench: Measuring Empathy of Omni-Modal Large Models](https://arxiv.org/abs/2601.10513)
*Xuan Luo,Lewei Yao,Libo Zhao,Lanqing Hong,Kai Chen,Dehua Tao,Daxin Tan,Ruifeng Xu,Jing Li*

Main category: cs.CL

TL;DR: 本论文提出了AEQ-Bench，这是一个用于系统评估全模态大模型（OLMs）同理心能力的新型基准，特别关注音频和文本的多模态输入与输出。


<details>
  <summary>Details</summary>
Motivation: 随着大模型应用扩展到多模态领域，自动评测其同理心等情感理解能力变得关键。然而，同理心的情感属性使其评估极具挑战性。为此，论文致力于提供一个系统方法，专门针对模型在多模态情境下感知和表达同理心的能力进行量化分析。

Method: 作者提出了AEQ-Bench基准，设计了涵盖音频与文本输入的设置，分别考查（1）OLMs通过多模态线索生成同理心回复的能力；（2）不依赖文本转录，仅通过音频判断回应的同理心水准。基准还创新性地引入了不同语境和语音语调的考查，结合语言和副语言（paralinguistic）指标进行综合评测。

Result: 实验结果表明，具备音频输出功能的多模态大模型在同理心表达方面普遍优于仅有文本输出的模型；但在细粒度的副语言表达能力评估上，与人类判断相比，当前模型表现尚不稳定。

Conclusion: AEQ-Bench为同理心多模态评测提供了首个系统平台，推动了模型在实际交流中同理心的深入衡量，但也暴露出现有大模型在细粒度情感表达方面的不足，未来需持续优化模型音频理解与表现能力。

Abstract: While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.

</details>


### [140] [PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://arxiv.org/abs/2601.10532)
*Chengbing Wang,Wuqiang Zheng,Yang Zhang,Fengbin Zhu,Junyi Cheng,Yi Xie,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 提出了一种结合心理学理论的新型双向情感奖励模型（PERM），显著提升了大语言模型的共情能力，且用户更偏好其生成的回复。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习提升LLM共情能力的方法大多只从单一视角评估共情，忽视了共情是支持者和寻求者之间的双向交互。本研究试图解决这种单向评估带来的局限。

Method: 作者构建了心理学理论指导的PERM奖励模型，将共情评估分为支持者和寻求者两个视角，并引入旁观者视角监控整体互动质量。在两个数据集上进行实验，比较了与当前主流方法的性能。

Result: PERM在情感智能基准和工业对话数据集上的表现均优于先进方法，提升超过10%。用户盲测中，70%的用户更偏好PERM生成的回复。

Conclusion: 基于双向视角和心理学理论的PERM模型能够大幅提升大语言模型的共情表现，具有实际应用和研究意义。

Abstract: Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\%. Furthermore, a blinded user study reveals a 70\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.

</details>


### [141] [Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure](https://arxiv.org/abs/2601.10566)
*Syed Naveed Mahmood,Md. Rezaur Rahman Bhuiyan,Tasfia Zaman,Jareen Tasneem Khondaker,Md. Sameer Sakib,Nazia Tasnim,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出了KIF框架，可真正实现大模型中的知识擦除，突破了以往擦除与模型稳定性之间的权衡困境。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的知识忘却（unlearning）方法多是表面行为压制，实际上潜在能力仍然存在，无法满足GDPR合规和模型安全需求。急需能实现真正有效知识擦除的新方法。

Method: 提出了KIF（Knowledge Immunization Framework）框架，通过识别和调整内部激活（activation signatures），结合动态压制和高效参数适应，实现耐久的无重训练知识擦除。还提出了结合表面泄露和潜在痕迹的双指标评测协议。

Result: KIF在Llama、Mistral、Qwen、DeepSeek等不同结构和参数规模（3B-14B）的大模型上实现了接近最优的知识擦除（FQ≈0.99），且保持高水平效用（MU=0.62），并首次揭示了不同模型架构在遗忘机制上的根本差异。

Conclusion: KIF不仅突破了知识擦除与效用稳定性之间的权衡，还提供了系统化评估大模型遗忘行为的新范式，是推动模型安全与合规的关键进展。

Abstract: Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.

</details>


### [142] [Form and Meaning in Intrinsic Multilingual Evaluations](https://arxiv.org/abs/2601.10580)
*Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 该论文分析了条件语言模型（CLM）在多语言环境下内在评测指标（如困惑度、每字符比特数）的适用性，并发现这些指标跨语言不可通用比较。


<details>
  <summary>Details</summary>
Motivation: 研究动机是目前常用的如困惑度等内在评测指标，在多语言环境下广泛使用，但这些指标是否能够有效、可比较地评价不同语言模型尚不明确。

Method: 作者分析了CLM评测指标背后的基本假设，明确了在多语言并行语料下这些假设的合理性与局限，并在两个多语言平行语料上，结合六种评测指标，对单语模型与多语模型分别进行了实验。

Result: 研究结果显示，现有的内在评测指标当跨语言比较时，不具备通用的可比性，不同语言间指标的直接比较容易导致误解。

Conclusion: 作者通过分析形式与意义（form-meaning）之争，解释了评测指标不具通用可比性的根本原因，并建议在多语言模型评测中需谨慎使用和理解这些指标。

Abstract: Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.

</details>


### [143] [Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs](https://arxiv.org/abs/2601.10645)
*Yuxi Xia,Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: 提出了TracVC方法追踪大模型自信表达的来源，发现模型自信表达常借鉴与事实内容不相关的语言，提醒信心表达的可靠性有待提升。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs表述自信可提升用户信任，但其表达常常与实际正确性不对应，滥用过度自信，研究动因在于理解其自信表达的来源及其是否靠谱。

Method: 提出TracVC方法，结合信息检索和影响估计，回溯大模型输出自信措辞时所参照的训练数据，并建立content groundness指标，分析模型的信心表达究竟扎根于内容相关的实例还是泛泛的自信表达上。

Result: 实验证明OLMo2-13B等模型在表达自信时，常受到与问题内容无关但表达自信的样本影响，表现为表面模仿自信措辞而非依据内容依据做判断。

Conclusion: 当前训练方式下，LLMs学会了如何显得自信，却未必能判断可信度是否合理。该分析为提升大模型表达可靠自信以及增强用户信任提供了基础。

Abstract: Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\textbf{Trac}ing \textbf{V}erbalized \textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.

</details>


### [144] [Detecting Winning Arguments with Large Language Models and Persuasion Strategies](https://arxiv.org/abs/2601.10660)
*Tiziano Labruna,Arkadiusz Modzelewski,Giorgio Satta,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 本文提出通过多策略说服评分法结合大语言模型（LLMs），提升论证文本说服性检测的准确性，并公开了带主题注释的数据集以促进未来相关研究。


<details>
  <summary>Details</summary>
Motivation: 说服性的自动检测对于理解人类沟通机制具有重要意义，然而如何准确衡量文本的说服性及其影响因素仍具挑战。现有方法往往忽略了具体说服策略（如攻击名誉、转移注意力、操控性措辞等）在其中的作用，因此需要更细致、更结构化的分析方法。

Method: 作者在三个有注释的论证数据集（包括Change My View子板块的“Winning Arguments”）上，采用基于大语言模型的多策略说服评分法，对六种说服策略进行推理引导，从而提升说服性预测。同时将Winning Argument数据集按主题划分，分析各主题下模型表现并公开此版本数据集。

Result: 实验证明，结合说服策略引导推理的方法，显著提升了说服性文本的判别准确率。并且对比不同主题显示，方法在多主题下具有较好的泛化性和鲁棒性。

Conclusion: 结构化、多策略感知的提示与推理方法不仅提升了说服性检测任务的效果，还增强了对论证质量评估的可解释性和稳健性，为后续相关研究提供了更好的数据和方法基础。

Abstract: Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.

</details>


### [145] [LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals](https://arxiv.org/abs/2601.10700)
*Gilat Toker,Nitay Calderon,Ohad Amosy,Roi Reichart*

Main category: cs.CL

TL;DR: 本文提出了LIBERTy框架，用于创建包含结构性反事实数据对的数据集，以真实评估基于概念的解释方法的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估概念解释可信度的基准依赖昂贵且可能不完美的人类撰写反事实文本，严重限制了此类研究的规模和准确性。

Method: 作者提出了LIBERTy框架，基于显式定义的结构因果模型（SCM）来生成结构性反事实数据对，并利用大语言模型（LLM）生成反事实文本。框架包含了三个新数据集，以及评估指标order-faithfulness，用于系统性测试概念解释方法。

Result: 实验涵盖了五种模型和多种方法，分析显示现有基于概念的解释方法在评估中的表现还有进一步提升空间。此外，分析也发现一些专有LLM在面对人口统计学干预时敏感性降低，可能与后期训练的干预策略有关。

Conclusion: LIBERTy为基于概念的解释性方法提供了新的高质量基准，推动更加忠实且系统的可解释性研究和方法改进。

Abstract: Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.

</details>


### [146] [Grounding Agent Memory in Contextual Intent](https://arxiv.org/abs/2601.10702)
*Ruozhen Yang,Yucheng Jiang,Yueqi Jiang,Priyanka Kargupta,Yunyi Zhang,Jiawei Han*

Main category: cs.CL

TL;DR: 本文提出了一种名为STITCH的新型语义记忆系统，通过结构化意图索引提升大语言模型在长时序、多目标交互中的历史检索表现，并在CAME-Bench和LongMemEval基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 长对话或任务中类似实体和事实重复出现，会导致大模型检索时混淆上下文，现有记忆机制容易因错配产生干扰，影响模型推理能力。因此需要强化根据具体意图精准检索相关历史。

Method: 提出STITCH系统，对每一步轨迹使用结构化检索线索（当前隐含目标、动作类型、关键实体类型等）进行意图索引，利用意图匹配对历史记忆片段过滤和优先级排序，有效区分多次出现的类似内容，抑制上下文不符的干扰。

Result: 在新提出的CAME-Bench与LongMemEval长程推理任务中，STITCH相比现有最强基线提升了35.6%，优势随着轨迹长度增加更加显著。

Conclusion: 基于意图的结构化索引能极大提高长时序交互中历史检索的精度，显著增强了大模型的长程推理与任务完成能力。

Abstract: Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.

</details>


### [147] [MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching](https://arxiv.org/abs/2601.10712)
*Changle Qu,Sunhao Dai,Hengyi Cai,Jun Xu,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 提出了MatchTIR，通过更细粒度的奖励机制改进工具集成推理（TIR）中的奖励分配，有效提升了大语言模型调用外部工具解决复杂任务的能力，尤其是在多步推理和多回合任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有TIR领域的强化学习方法通常采用粗粒度的奖励（对整条轨迹或结果），导致难以分辨模型推理过程中的有效工具调用和多余、错误的调用，尤其在长流程多回合场景下，这种奖励机制不足以提供有效指导。

Method: 作者提出MatchTIR方法，将奖励分配建模为二分图匹配问题，实现预测轨迹与标准轨迹的逐回合对齐，采用两种分配策略获取稠密的逐步奖励，并设计了结合逐步和全程信号的双层优势估算机制，实现对每次交互赋予区分度更高的奖励。

Result: 在三个基准任务上，MatchTIR性能优于现有方法。其4B参数量级模型在多个长流程及多回合任务上超越了大部分8B规模的对手。

Conclusion: MatchTIR通过更精准的奖励分配和优势估算，解决了TIR领域奖励分配粗糙的问题，有效增强了大语言模型复杂任务的推理能力与工具调用效率。

Abstract: Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [148] [Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates](https://arxiv.org/abs/2601.09740)
*Oumaima Barhoumi,Mohamed H Zaki,Sofiène Tahar*

Main category: cs.RO

TL;DR: 本文提出了一种将Barrier Certificates（障碍证书）与可解释的交通冲突度量（如时距碰撞TTC）结合的正式安全框架，并利用SMT求解器进行安全条件验证，实现自动驾驶车辆在实际场景下的实时安全控制。实验证明显著减少了不安全交互，提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶的AI组件虽能感知和决策，但多为黑箱模型，缺乏可解释性和严格的安全保障。而混合交通环境中的人车交互带来更多不确定性和安全挑战，因此亟需既可解释又有正式验证的安全框架。

Method: 结合Barrier Certificates与TTC等可解释交通安全指标，通过SMT求解器对系统的安全性进行验证，并引入自适应控制机制，确保车辆实时满足安全约束。

Result: 在真实高速公路数据集上评估，结果显示不安全交互显著减少：TTC低于3秒的事件减少40%，部分车道冲突事件完全消除。

Conclusion: 该方法兼具可解释性和可验证性，为自动驾驶提供了切实可行、可扩展的安全保障途径。

Abstract: Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.

</details>


### [149] [Interprofessional and Agile Development of Mobirobot: A Socially Assistive Robot for Pediatric Therapy Across Clinical and Therapeutic Settings](https://arxiv.org/abs/2601.09838)
*Leonie Dyck,Aiko Galetzka,Maximilian Noller,Anna-Lena Rinke,Jutta Bormann,Jekaterina Miller,Michelle Hochbaum,Julia Siemann,Jördis Alboth,Andre Berwinkel,Johanna Luz,Britta Kley-Zobel,Marcine Cyrys,Nora Flöttmann,Ariane Vogeler,Mariia Melnikova,Ira-Katharina Petras,Michael Siniatchkin,Winfried Barthlen,Anna-Lisa Vollmer*

Main category: cs.RO

TL;DR: 本文介绍了一款名为Mobirobot的社会辅助机器人，用于帮助儿科患者康复，通过个性化锻炼程序提升他们的治疗参与度，并探讨了多方协作的开发和初步临床应用情况。


<details>
  <summary>Details</summary>
Motivation: 社会辅助机器人在儿科临床中能够提升患者的康复积极性，但实际应用中不仅要技术过硬，还需考虑特定环境和多方需求，共同设计适配的解决方案。

Method: 采用敏捷、人本导向的开发流程，广泛吸纳了多学科医疗团队和终端用户的意见，持续优化机器人的交互设计、动作能力和技术架构，并将其部署到真实的外科及精神科儿童病房中，收集问卷、观察和访谈等多来源数据。

Result: 在医院实际部署中，发现并改进了设计需求和可用性限制，通过相关方反馈不断调整机器人设计。当前正开展可行性研究，评估接受度、可用性和治疗效益。

Conclusion: Mobirobot展示了多专业联合和多方主导开发能有效促进社会辅助机器人在住院环境中的适应和应用。初步结果强调了情境融入、系统稳健性和低干扰性的关键作用，尽管仍面临传感器局限、招募难题等挑战，但为后续研究与临床转化奠定了良好基础。

Abstract: Introduction: Socially assistive robots hold promise for enhancing therapeutic engagement in paediatric clinical settings. However, their successful implementation requires not only technical robustness but also context-sensitive, co-designed solutions. This paper presents Mobirobot, a socially assistive robot developed to support mobilisation in children recovering from trauma, fractures, or depressive disorders through personalised exercise programmes.
  Methods: An agile, human-centred development approach guided the iterative design of Mobirobot. Multidisciplinary clinical teams and end users were involved throughout the co-development process, which focused on early integration into real-world paediatric surgical and psychiatric settings. The robot, based on the NAO platform, features a simple setup, adaptable exercise routines with interactive guidance, motivational dialogue, and a graphical user interface (GUI) for monitoring and no-code system feedback.
  Results: Deployment in hospital environments enabled the identification of key design requirements and usability constraints. Stakeholder feedback led to refinements in interaction design, movement capabilities, and technical configuration. A feasibility study is currently underway to assess acceptance, usability, and perceived therapeutic benefit, with data collection including questionnaires, behavioural observations, and staff-patient interviews.
  Discussion: Mobirobot demonstrates how multiprofessional, stakeholder-led development can yield a socially assistive system suited for dynamic inpatient settings. Early-stage findings underscore the importance of contextual integration, robustness, and minimal-intrusion design. While challenges such as sensor limitations and patient recruitment remain, the platform offers a promising foundation for further research and clinical application.

</details>


### [150] [How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces](https://arxiv.org/abs/2601.09856)
*Andrew Stratton,Phani Teja Singamaneni,Pranav Goyal,Rachid Alami,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: 本论文系统研究了人类运动预测质量对机器人导航表现以及人类效率和体验的影响，发现当前主流评价指标并不适用于实际复杂动态场景，且机器人与人类在有限空间下协作存在误区。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在仓库、医院、制造工厂和家庭等环境中的应用，人机协作越来越紧密，安全、舒适和高效成为重要目标。但人类行为的不确定性、个体化偏好及数据匮乏使得对人类运动的准确预测极具挑战，因此迫切需要研究这种预测对机器人导航的实际影响。

Method: 作者设计了在人类和机器人共处有限空间中的导航场景，招募80位用户，使用两种不同的机器人平台，在全球两地开展对比实证研究，并重点考察不同人类运动预测方法对机器人导航、人类效率和体验的影响。

Result: 主要发现包括：（1）常用的平均位移误差指标无法准确预测机器人导航表现和用户感受；（2）人类往往不会像假设中那样主动配合机器人，特别是在狭窄空间中，机器人合作策略反而因人类的不配合导致表现变差；（3）机器人越高效，其实可能牺牲了人类的效率和舒适感。

Conclusion: 当前关于人类行为预测及机器人导航性能的评价指标和假设在真实动态、受限场景下效果有限，未来应发展更贴合实际的评价和导航策略，以提升人机共处效率与体验。

Abstract: Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.

</details>


### [151] [SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping](https://arxiv.org/abs/2601.09920)
*Ruopeng Huang,Boyu Yang,Wenlong Gui,Jeremy Morgan,Erdem Biyik,Jiachen Li*

Main category: cs.RO

TL;DR: SyncTwin是一种数字孪生框架，提升了机器人在动态及遮挡环境下的抓取准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，机器人在动态变化和视觉遮挡条件下实现高精度、安全的抓取操作非常困难，传统方法在重建、同步和规避碰撞上存在局限。

Method: 提出了SyncTwin框架，包括离线阶段利用VGGT从RGB图像快速重建3D物体资产，形成几何库；在线时，通过点云分割和有色ICP配准不断同步现实和虚拟场景，实现数字孪生更新；基于实时更新的孪生体，运动规划器可在虚拟环境中生成无碰撞、动态可行的轨迹，并通过闭环机制将其转化为现实机器人安全执行。

Result: 实验证明，在面对动态和遮挡环境时，SyncTwin提升了抓取准确率和运动安全性。

Conclusion: 数字孪生同步显著增强了机器人在真实复杂环境下的抓取能力和操作安全性，SyncTwin为实际机器人执行提供了有效解决方案。

Abstract: Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.

</details>


### [152] [In-the-Wild Compliant Manipulation with UMI-FT](https://arxiv.org/abs/2601.09988)
*Hojung Choi,Yifan Hou,Chuer Pan,Seongheon Hong,Austin Patel,Xiaomeng Xu,Mark R. Cutkosky,Shuran Song*

Main category: cs.RO

TL;DR: 该论文提出了一种创新的数据采集平台UMI-FT，实现了手持状态下的指尖级六轴力/力矩感知，并通过多模态数据学习自适应合规操控策略，在多个任务中超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前商用力/力矩传感器价格高昂、体积庞大且易损，限制了大规模以力为核心的机器人操控策略学习。因此需要一种便携、可扩展的力感知方案以支持丰富、真实场景中的动作演示和策略训练。

Method: 提出UMI-FT平台，将六轴力/力矩传感器集成到每个指尖，结合RGB、深度和位姿数据采集多模态信息。基于采集数据训练合规控制策略，实现位置目标、抓取力和刚度的联合预测，最终在标准合规控制器上执行。

Result: 在擦白板、穿刺西葫芦和安装灯泡三种接触强、力敏感任务中，利用UMI-FT训练的策略能更可靠地调节外部接触力和内部抓持力，显著优于无合规或无力感知的基线方法。

Conclusion: UMI-FT为从真实环境示范学习合规操控策略提供了可扩展方案，显著提升了任务表现，并通过开源硬件与软件推动了社区更广泛的应用和发展。

Abstract: Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.

</details>


### [153] [CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments](https://arxiv.org/abs/2601.10116)
*Xintong Zhang,Junfeng Chen,Yuxiao Zhu,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人协作的新框架CoCoPlan，优化任务分配与团队间的间歇性通信，显著提升任务完成率并降低通信开销，支持大规模动态环境。


<details>
  <summary>Details</summary>
Motivation: 实际多机器人系统因通信受限，难以实现高效协作；现有方案无法适应动态任务和时空变化，导致系统性能不佳。作者希望突破通信与任务协同的难题。

Method: 提出CoCoPlan，结合分支限界架构编码任务分配与通信事件；设计自适应目标函数权衡任务效率与通信延迟；引入通信事件优化模块，智能决定何时、何地以及如何重建全局通信。

Result: 相比最新方法，CoCoPlan任务完成率提升22.4%，通信开销降低58.6%；可支持100台机器人在动态环境运行。在2D办公场地和3D灾难救援等实物实验中验证了有效性。

Conclusion: CoCoPlan为多机器人系统在受限通信下协同任务规划与通信恢复提供了统一且高效的解决方案，极大提升了系统的性能与扩展性。

Abstract: Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.

</details>


### [154] [Terrain-Adaptive Mobile 3D Printing with Hierarchical Control](https://arxiv.org/abs/2601.10208)
*Shuangshan Nors Li,J. Nathan Kutz*

Main category: cs.RO

TL;DR: 提出了一个将AI与传感器融合、层级控制系统相结合的移动三维打印系统，实现了在非结构化地形上的高精度打印。


<details>
  <summary>Details</summary>
Motivation: 当前移动三维打印在复杂地形上难以兼顾平台移动性和打印精度，现有门式系统虽精度高却缺乏机动性，移动平台则难以确保打印质量。

Method: 构建了一个AI驱动的干扰预测框架，结合多模态传感器（IMU、视觉与深度），并通过三层控制结构（路径规划、车体-机械协作、精密执行）实现感知-学习-执行的闭环控制系统。AI模块通过传感器数据学习地形对平台扰动的影响，实现主动补偿。

Result: 系统在户外不规则、有坡度地形实验证明：平台保持完全移动性情况下，依然实现了亚厘米级打印精度。

Conclusion: AI与硬件的紧密集成为非结构化环境中的自主建造提供了切实可行的基础。

Abstract: Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.

</details>


### [155] [A Unified Framework for Kinematic Simulation of Rigid Foldable Structures](https://arxiv.org/abs/2601.10225)
*Dongwook Kwak,Geonhee Cho,Jiook Chung,Jinkyu Yang*

Main category: cs.RO

TL;DR: 本文提出了一种自动化方法，用于任意刚性可折叠结构（RFS）的运动学约束分析，通过生成Pfaffian约束矩阵，实现对多种折纸结构运动的统一建模和可视化。


<details>
  <summary>Details</summary>
Motivation: 随着折纸结构（如厚折纸、切纸、和多片结构）不断发展，现有的运动学分析方法难以统一处理不同类型结构的环路约束，导致分析繁琐且易出错，因此亟需一种通用自动化的约束整合方法。

Method: 作者提出了一套自动化流程：首先从扩展的结构数据自动构建面-铰链图（facet-hinge graph），然后提取捕获全部约束的最小循环基集，接着基于螺旋理论（screw theory）组装能够同时表示旋转与平移耦合的速度层约束矩阵，从而系统化地表示和求解所有环路约束。

Result: 该工具能够自动计算并可视化多种刚性可折叠结构的展开与折叠动作，极大地简化了约束计算过程，减少了人为错误，并提升了分析效率。

Conclusion: 提出的自动化框架为多样的折纸刚性结构统一提供了高效、可靠的运动学分析工具，有助于设计与理解复杂折纸结构的运动行为。

Abstract: Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential. Yet a general method that consolidates their loop constraints has been lacking. We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS). From a minimally extended data schema, the tool constructs the facet-hinge graph, extracts a minimum cycle basis that captures all constraints, and assembles a velocity-level constraint matrix via screw theory that encodes coupled rotation and translation loop closure. The framework computes and visualizes deploy and fold motions across diverse RFS while eliminating tedious and error-prone constraint calculations.

</details>


### [156] [Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control](https://arxiv.org/abs/2601.10233)
*Yifan Xue,Ze Zhang,Knut Åkesson,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种安全高效的移动机器人导航方法，在复杂动态环境中能应对凹形动态障碍物，结合了神经网络预测与Barrier函数，有效提升了避障能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于CBF的安全控制器仅依赖于障碍物当前状态，难以避免未来碰撞，特别是在面对复杂多模态障碍物运动时，安全性和效率有待提升。

Method: 采用能量基学习训练的神经网络，对障碍物进行多模态运动预测；利用高斯过程在线学习Barrier函数；将学到的Barrier函数输入带调制的CBF（MCBF）框架，并提出自动参数调整算法以适应Barrier的变形，最终通过二次规划实现导航决策。

Result: 框架在仿真和实际环境下均进行了测试，对比基线方法表现出更好的安全性和效率，能在拥挤动态环境下有效避障。

Conclusion: 所提方法能有效解决移动机器人在动态复杂环境中的避障与导航问题，提升安全性与实用性，为实际应用奠定基础。

Abstract: This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.

</details>


### [157] [The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation](https://arxiv.org/abs/2601.10268)
*Eszter Birtalan,Miklós Koller*

Main category: cs.RO

TL;DR: 本文通过仿真评估了六种不同密度和布局的触觉传感器配置对强化学习中机械手抓取稳定性的影响，确定了一种表现最优的传感器布置。


<details>
  <summary>Details</summary>
Motivation: 当前大多数机械手（包括假肢）在触觉传感器数量和分布上缺乏统一标准，感测设计多样且效果有待系统评估，因此需要找到更优布置以提升抓握稳定性。

Method: 作者利用仿真平台，设计了六种不同的触觉传感器配置，包括不同的密度和分布方式，并评估其在两套不同实验系统下对基于强化学习算法的机械手表现的影响。该研究不依赖于特定的物理模拟器、手型和算法，从而确保结果的普适性。

Result: 实验发现，某一特定传感器布置在两套系统下均表现最好，其它布局和密度在不同系统中的影响也被详尽记录，总体得到了布局和密度设计对性能的具体影响规律。

Conclusion: 工作为触觉传感器在机械手（含假肢）设计中的布局和密度选择提供了有力参考，有助于未来相关领域的优化与研究。

Abstract: Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.

</details>


### [158] [CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing](https://arxiv.org/abs/2601.10340)
*David Morilla-Cabello,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文提出了一个集成语义感知的多异构机器人协调框架，通过结合视觉模型构建度量-语义地图，并将其用于路径规划和任务分配，提高在复杂环境下的任务效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在复杂且未知的环境中，使用不同类型机器人协作巡检能提升任务完成效果，但现有方法多忽略机器人异构性带来的机会及场景理解对路径规划的作用。

Method: 作者提出了一个从空中侦查开始，利用开放词汇视觉模型构建度量-语义地图的方案，用于识别需要详细检查的区域，并结合各机器人能力规划相应路径。随后，通过多异构车辆路径规划算法，联合分配检查任务和生成轨迹。

Result: 在仿真和实际三平台机器人巡检测试中，该方法能更好地根据机器人能力与场景情境，规划出更安全高效的巡检路线。

Conclusion: 集成语义感知与能力感知，实现异构机器人团队更适应复杂环境的协作巡检，且系统已开源，促进实际应用和验证。

Abstract: Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot's strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform's navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.

</details>


### [159] [FastStair: Learning to Run Up Stairs with Humanoid Robots](https://arxiv.org/abs/2601.10365)
*Yan Liu,Tao Yu,Haolin Song,Hongbo Zhu,Nianzong Hu,Yuzhi Hao,Xiuyong Yao,Xizhe Zang,Hua Chen,Jie Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为FastStair的多阶段学习框架，结合基于模型的步点规划与无模型强化学习，实现了人形机器人快速且稳定地上楼梯。该方法通过将步点规划引入RL训练，平衡了动作的敏捷性与安全性，在Oli机器人上实现了高达1.65 m/s的上楼速度，并成功完成33级螺旋楼梯的高效攀登。


<details>
  <summary>Details</summary>
Motivation: 虽然人类轻松上楼梯，但人形机器人在这一任务上因需高机动性与严苛稳定性难以兼顾而表现欠佳。无模型强化学习虽能生成动态步态，但容易因奖励设计不佳而出现不安全行为；基于模型的步点规划虽保证了稳定，但动作保守，速度受限。如何兼顾速度与安全，是该工作的核心动机。

Method: 本方法提出FastStair框架，将基于模型的步点规划器并行嵌入到RL训练流程中，指导智能体以安全、可行的步点进行探索，并通过规划器预训练一个聚焦安全的基础策略。后续再针对速度进行微调，得到多个速度专家策略，最后采用低秩适应（LoRA）技术融合，实现全速度流畅控制。

Result: 所提出的控制器在Oli人形机器人上进行了实机部署，达到了高达1.65 m/s的命令速度，成功攀登了33级、每级17cm高的螺旋楼梯，全程仅耗时12秒，展现出高速度下的鲁棒性和稳定性。该方案还在广州塔机器人竞速赛中获得冠军。

Conclusion: FastStair框架有效结合了模型驱动的安全性和RL的敏捷性，实现了人形机器人在楼梯等复杂场景下的快速、稳定上楼。方法具备实际部署能力，并展现出优异性能与竞赛实绩，为机器人高难度动态步行提供了重要参考。

Abstract: Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.

</details>


### [160] [Online identification of nonlinear time-varying systems with uncertain information](https://arxiv.org/abs/2601.10379)
*He Ren,Gaowei Yan,Hang Liu,Lifeng Cao,Zhijun Zhao,Gang Dang*

Main category: cs.RO

TL;DR: 本文提出了一个新的贝叶斯回归符号学习（BRSL）框架，实现了数字孪生系统实时、可解释、具备不确定性量化和在线学习能力的模型。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生模型难以同时满足高预测精度、强可解释性和在线自适应能力。贝叶斯方法虽然能处理不确定性，但缺乏可解释性；而符号方法则难以实时更新。该论文旨在弥补这些方法间的语义和计算鸿沟。

Method: 提出BRSL框架，将在线符号发现表述为统一的概率状态空间模型。通过稀疏先验将模型选择转化为贝叶斯推断，并推导了带遗忘因子的递归算法。还提出了递归条件，既保证后验分布的良定性，又能实时监控数据有效性，并进行了参数估计的收敛性分析。

Result: 通过案例研究，验证了所提方法在实现可解释、概率预测和在线学习方面的有效性。同时保证了模型识别和不确定性量化的同步。

Conclusion: BRSL框架能在数字孪生领域兼顾实时性、可解释性和预测的可靠性，具有良好的工程应用前景和理论价值。

Abstract: Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.

</details>
