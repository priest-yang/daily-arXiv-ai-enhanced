<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]
- [cs.CL](#cs.CL) [Total: 109]
- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: 本研究提出了一种名为SoC-DT的数字孪生框架，能融合传统肿瘤反应扩散模型、常规治疗操作与个体化信息，提高肿瘤治疗预测准确性，并提出了稳定、高效的新数值解法。


<details>
  <summary>Details</summary>
Motivation: 目前，肿瘤在标准治疗下的轨迹预测准确性不足，主要由于传统模型难以处理实际复杂的治疗方式、多样的患者基因组与人口差异。因此，需要新的计算方法更真实地模拟临床干预与个体异质性，以优化肿瘤治疗方案。

Method: 研究提出了标准治疗数字孪生（SoC-DT）框架，将反应扩散模型与手术、化疗、放疗等常规治疗操作进行差分结合，并引入患者的基因与人口学个体化参数。此外，创新性地采用IMEX-SoC隐-显指数时间差分算法，确保模型对于多种治疗的数值稳定性和可扩展性。

Result: 在合成数据和真实的脑胶质瘤数据中，SoC-DT模型在肿瘤动态轨迹预测上显著优于传统PDE模型与纯数据驱动的神经网络模型。

Conclusion: SoC-DT框架成功实现了机制可解释性与现代可微分求解器的结合，为肿瘤学中个体化数字孪生提供了科学基础，并能生物学一致地估算肿瘤动态，为精准医疗奠定重要支持。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 本论文提出了一套混合框架，用于高效分析视频中人物（如名人）的动态，并以可视化方式展示多维度信息。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容成为主流媒介，理解其结构和动态对于内容分析和娱乐研究变得十分重要。

Method: 采用分布式多GPU推理系统和交互式可视化平台。推理框架利用优化的ONNX模型、异构批量推理和高吞吐并行，实现大规模视频数据的高效处理，生成带有时间戳的名人出现记录，并转化为多种可视化图表。

Result: 系统能生成多维度可视化如出现频率图、时长分析、饼图、共现矩阵、网络图、堆叠面积图、季节对比与热力图等，揭示名人曝光、屏幕时间分布、时序动态与共现关系。

Conclusion: 该系统通过融合分布式识别与结构化可视化分析，为娱乐分析、内容策略制定和观众研究开辟了新途径，有助于用户更深入地探索和理解视频内容。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 该论文对比了不同深度学习模型在水下塑料垃圾跨域检测任务中的表现，发现轻量级的MobileNetV2模型在跨域场景下表现最佳，也讨论了大规模预训练模型如CLIP和Gemini的优缺点。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染严重，亟需自动化的水下垃圾检测技术。然而，当前视觉系统常因域转移导致性能下降，因此需要评估各模型在跨域场景下的鲁棒性。

Method: 作者训练多种卷积神经网络（MobileNetV2, ResNet-18, EfficientNet-B0）和视觉Transformer（DeiT-Tiny, ViT-B16）在标注水下数据集上，并在另一个不同来源的跨域平衡测试集上评测。还测试了两种零样本预训练模型（CLIP ViT-L14和Gemini 2.0 Flash），比较了有监督和预训练模型的性能。

Result: MobileNetV2在跨域测试集上F1分数高达0.97，优于体积更大的模型。所有有监督模型精度均接近99%，但召回率有所不同。CLIP具备较高的召回率（约80%）但精准率较低（56%），Gemini相反（精准率高达99%，召回率81%）。误差分析说明模型常被珊瑚纹理、悬浮颗粒和高光干扰。

Conclusion: 轻量级CNN通过有监督训练能有效泛化于水下跨域垃圾检测任务；大规模预训练视觉-语言模型表现出互补特性，可以根据具体应用需求选用。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: 本文提出了VLCAP，一种结合CLIP视觉标签检索和多模态文本生成的阿拉伯语图像描述系统，通过多个多语种编码器获取视觉标签，并与主流视觉-语言模型结合，实现了可解释且语境相关的阿拉伯语图像描述。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语图像描述相关研究稀缺，且面临语义解释性不足、标签覆盖面有限等挑战。现有端到端方法难以确保描述的可解释性及文化相关性。因此，作者希望通过构建集成视觉标签和多语种编码器的系统，提升阿拉伯语图像描述的准确性与可解释性。

Method: 系统分两阶段：首先利用mCLIP、AraCLIP、Jina V4三种多语种编码器分别检索图片相关的阿拉伯语视觉标签，并将训练描述与视觉基因组翻译标签（约21000个）融合为混合词表；之后将检索到的前k个标签转换为流畅的阿拉伯语提示，与原始图片一起输入Qwen-VL和Gemini Pro Vision生成阿拉伯语描述，共形成六种编码器-生成器配置，分别评估。

Result: 实验表明mCLIP + Gemini Pro Vision配置获得最佳的BLEU-1分数（5.34%）和余弦相似度（60.01%）；AraCLIP + Qwen-VL配置获得最高的LLM-judge分数（36.33%）。这些结果优于传统端到端方法，且实现了更好的解释性。

Conclusion: VLCAP展示了结合多语种视觉编码器和主流视觉-语言大模型，通过中间标签驱动的方式可有效提升阿拉伯语图像描述的相关性、可解释性和文化契合度，并为低资源语言的视觉描述提供了可借鉴的方案。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 本论文比较了EfficientNet-B0和ViT-Base在SpaceNet数据集下表现，包括自然不均衡与平衡的类别分布。结果发现，类别均衡后，两者精度都很高，但EfficientNet-B0在效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 近年来ViT与CNN在视觉任务中的效果不断被研究，但两者在遥感领域（如SpaceNet数据集）下的性能和部署效率对比仍不清楚。作者希望通过严格对照实验探究两种主流架构在不同标签分布下的表现差异。

Method: 在空间卫星影像数据集SpaceNet上，对EfficientNet-B0（CNN）和ViT-Base分别在不均衡五分类和人工平衡五分类两种标签分布下进行了比较，统一了预处理、数据增强、模型训练资源与轮数，并评估多种分类和部署指标（精度、宏F1、延时、模型大小等）。

Result: 在不均衡分布下，EfficientNet-B0和ViT-Base都取得了93%的测试精度，但前者参数更少、速度更快；在均衡分布下，两者精度都很高（EfficientNet-B0达99%），但CNN依然在效率上有优势。

Conclusion: 标签分布对CNN和ViT分类效果差异有明显影响。类别均衡缩小了ViT与高效CNN间的差距，但在资源受限环境下，CNN（如EfficientNet-B0）依然在部署效率方面占优。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 保障弱势道路使用者（如行人和骑车人）安全依然严峻，尽管AI视觉感知技术进步显著，现有综述多只关注检测，忽视了全面的视觉任务。本文综述了近五年基于摄像头的AI感知系统在检测、跟踪与再识别、轨迹预测、意图识别与预测四项核心任务的最新进展，并讨论了未来面临的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基础设施措施在动态城市环境下常常无效，弱势道路使用者的安全面临挑战。AI视觉和推理的进步为主动、场景感知的保护带来新机遇，但目前相关综述覆盖不全，亟需系统性回顾各类视觉任务以指导未来研究。

Method: 系统梳理了近五年基于摄像头的AI感知系统在四大任务（检测与分类、跟踪与再识别、轨迹预测、意图识别与预测）方面的研究进展；同时，以数据、模型和部署为视角，归纳当前尚未解决的主要挑战。

Result: 详细回顾了每项视觉任务的最新进展和趋势，总结了各自面临的技术和应用难点，并指出多项亟待解决的开放问题，为未来研究提供了方向。

Conclusion: AI视觉感知支撑的多任务能力有望为弱势道路使用者提供更全面、更主动的保护。未来需突破数据质量、模型性能和实际部署等难题，推进下一代智能交通安全系统的发展。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 该论文分析了地球观测基础模型（EOFMs）在处理多样传感器数据时表现出的“表征空间”高度依赖于传感器架构，指出目前仅单一模态训练的EOFMs面临局限。


<details>
  <summary>Details</summary>
Motivation: 随着遥感和地球观测数据体量激增，EOFMs成为关键工具，学术界希望用预训练模型输出的embedding进行通用检索和查询。然而，大多数模型仅以单一数据模态训练，极少探讨不同传感器结构对模型内部表征的影响。理解这种影响对提升EOFMs泛化和适用性非常重要。

Method: 作者系统分析了多种EOFMs在不同传感器架构下生成的内部表征（eg.embeddings），比较了匹配与非匹配模态的数据表现，揭示当前模型表征受传感器差异影响极大。

Result: 研究发现，EOFMs的embedding空间对各类传感器架构高度敏感，不同传感器生成的数据难以在同一表征空间内无缝互用。此现象会导致模型在跨传感器、跨模态任务中的泛化能力受限。

Conclusion: 当前EOFMs设计存在重要缺陷，模型开发者与用户需重视不同传感器对表征空间的影响，推动更具稳健性和通用性的新一代地球观测基础模型。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 本论文提出了一种基于修复和扰动的解释方法，为生态视觉模型的预测结果提供更直观、可信的解释，有助于提升AI应用于生态监测的信任度和实用性。


<details>
  <summary>Details</summary>
Motivation: 由于生态监测自动化中视觉模型的预测过程不透明，限制了AI在实际生态领域的应用，因此亟需开发出能够提供可解释性、直观解释的方法，帮助专家信任并采用这些AI系统。

Method: 提出了一种结合图像修复（inpainting）与扰动的解释技术，通过Segment-Anything-Model分割出对象遮罩，进行有上下文的局部编辑，包括（i）将动物对象移除或替换为背景或其他元素；（ii）将动物对象叠加到新的背景上。编辑后的图像用于重新评估模型输出变化（图像翻转率、置信度降低），并由专家评审其生态学合理性和可解释性。

Result: 该方法能精确定位模型判定的关键形态结构，避免了传统扰动方法带来的删除伪影，生成的解释图像受专家认可，更易于人类专家理解模型预测依据。

Conclusion: 所提方法能产生高保真、具生态相关性的模型解释，提升了专家对AI预测的信任度，有助于推动AI在生态监测领域的实际部署。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 本文综述了医学图像分割（MIS）领域的方法，包括传统图像处理方法和当前主流的深度学习方法，涵盖最新趋势和实际挑战。


<details>
  <summary>Details</summary>
Motivation: MIS对于医学诊断、治疗规划和疾病监测具有极高的重要性。随着深度学习的快速发展，MIS方法亦不断演进，亟需系统梳理现有技术，指导未来研究。

Method: 作者系统回顾了MIS领域的各种分割方法，包括传统的阈值法、边缘检测、区域分割、聚类和模型驱动方法，同时重点评述了深度学习相关架构（CNN、FCN、U-Net及其变体），探讨了注意力机制、半监督学习、GAN和Transformer模型。同时，文章还关注了混合架构、跨模态学习、联邦及分布式学习、主动学习等新趋势，并以腰椎分割为案例研究，剖析其特殊难题和新进展。

Result: 论文总结了MIS领域的重要研究进展和创新方法，揭示了各方法的优势与不足，并通过实际案例（腰椎分割）展示了目前方法在特定部位上的研究现状及成效。

Conclusion: 尽管MIS研究取得显著进展，但依旧面临如数据集偏差、领域适应、模型可解释性、与临床工作流程结合等关键挑战，未来工作需重点突破这些瓶颈。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: 本文提出了DECOR，这是一种具有方向鲁棒性的深度聚类框架，能在存在多种复杂缺陷和数据不完美的情况下，有效对半导体晶圆缺陷进行聚类，并在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在半导体制造中，及早发现晶圆缺陷对于优化产品良率至关重要。然而，真实晶圆测试数据复杂、无标签、不平衡且常含有多种缺陷，这对现有聚类方法带来极大挑战，亟需能够处理此类数据的鲁棒聚类方法。

Method: 作者提出DECOR框架，通过深度学习方法实现聚类，同时引入方向鲁棒性，无论晶圆图案如何旋转或对齐，都能将空间相似的缺陷聚为一类。该方法在公开的MixedWM38数据集上进行评估。

Result: 实验结果显示，DECOR无需手动调参即可发现有效聚类，并且在MixedWM38数据集上的聚类表现超过了现有的聚类基线方法。

Conclusion: DECOR为自动化视觉检测系统提供了一种高效、可靠、可扩展的晶圆缺陷聚类解决方案，能更好地适应复杂且多缺陷的现实数据场景。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 本文提出了一种针对人脸表情分类中的类别不平衡和多分类任务的LSTM神经网络与注意力机制结合的方法，并验证了通过排除类别后再纠错的有效性，对小类别的识别表现有提升。


<details>
  <summary>Details</summary>
Motivation: 人脸表情分类在实际中常出现类别严重不平衡问题，尤其在罕见情绪识别等应用（如反欺诈系统）中，小类别难以稳定被正确分类。作者希望设计出可以提升这类少数类别识别效果的方法。

Method: 作者采用基于LSTM和注意力机制的神经网络模型，聚焦于为表情识别贡献最大的面部区域。实验方案为：训练时每次去掉1个类别，用其余6类训练模型，之后利用纠错机制尝试恢复被排除类别的判别能力，并比较不同类别的恢复效果。

Result: 纠错方法对全部类别都有一定修复能力，但表现差异较大。对于部分小类别，纠错后在测试集上的关键指标有所提升，显示方法在处理类别不平衡时有效，提升了小类别的表现。

Conclusion: 该方法能有效缓解人脸表情识别任务中的类别不平衡问题，对提升罕见类别识别有明显作用，可推广到安防、反欺诈等需要稳定分类罕见事件的实际系统中。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 该论文提出了第一个动态图表生成（DCG）评测基准DCG-Bench，并发布了高质量数据集DCG-8K，同时训练了新的多模态大模型Qwen2.5-VL-DCG-3B，有效提升了动态图表生成任务表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型（MLLM）在静态图表生成与理解上取得明显进展，对于动态图表生成与理解的能力却鲜有研究。因此，作者希望弥补这个研究空白，推动MLLM在动态图表领域的能力提升。

Method: 作者提出了DCG-Bench基准，从简单文本到图表、详细文本到图表、视频到图表三方面评测MLLM的动态图表生成能力，并构建了包含指令-代码-视频三元组和QA注释的8K数据集。基于数据集，采用了两阶段训练方案和联合代码-视觉奖励以优化模型，并训练了Qwen2.5-VL-DCG-3B模型。

Result: 作者在三个任务上对现有主流MLLM进行对比，发现其在视觉转图表任务上存在明显缺陷。新模型Qwen2.5-VL-DCG-3B在所有任务的平均性能提升8.31%，并以仅3B参数量与闭源大模型表现相当。

Conclusion: 作者证明了提出的DCG-Bench和训练策略有效提升了MLLM执行动态图表生成任务的能力。公开的数据和模型将促进该领域后续研究。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: 本论文提出了一种基于Transformer的端到端单目视觉里程计方法VoT，无需手工特征、相机标定等复杂流程，能高效预测相机运动，较传统方法更快更准。


<details>
  <summary>Details</summary>
Motivation: 现有单目视觉里程计方法依赖预训练的深度学习组件与优化模块，流程复杂，强依赖相机标定和参数调优，在实际陌生场景中表现不佳。虽然大规模3D模型有所改善，但难以处理长视频并精确获得每帧估计，影响VO应用。

Method: 作者提出VoT（Visual odometry Transformer），利用Transformer提取图像序列特征，通过空间与时间注意力机制建模全局关系，直接端到端预测相机运动，无需特征匹配、稠密重建、束调等传统手工流程，仅用相机位姿作为监督信号，并可灵活集成多种预训练编码器。

Result: 实验表明，VoT在更大数据集和更强预训练基础上能有效扩展，能适应多种相机运动与参数设置，相比传统方法不仅精度提升，还能快3倍以上。

Conclusion: VoT方法简化了视觉里程计流程，可获得更高效率和更强的通用泛化能力，为VO领域带来新的端到端解决方案。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 作者提出了一种新的扩散模型采样算法，通过利用附加侧信息，显著提升了图像重建质量，尤其对于高度病态的逆问题表现突出。该方法优于传统梯度引导法，实验结果在多种逆问题任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决逆问题时，往往忽视了可用的多样侧信息，而这些信息本可用于提升重建效果，尤其是在信息丢失严重的情况下。作者希望弥补这一研究空白，提高图像重建的准确性和可靠性。

Method: 作者提出了一种新的推理时搜索算法。该算法在扩散模型采样过程中，动态结合了侧信息，以平衡探索性与利用性，避免了传统基于梯度引导方法中出现的‘奖励劫持’伪影。此方法可无缝嵌入现有的扩散式图像重建流程。

Result: 在盒子修补、超分辨率、运动模糊、高斯模糊、非线性模糊、盲去模糊等多种逆问题图像重建任务中，新方法均展现出更高的定性与定量重建性能，并且在各项指标上优于现有梯度引导等基线方法。

Conclusion: 通过利用侧信息、新算法显著提升了扩散模型在逆问题中的重建效果，优于已有主流方法，并具有较强的通用性和可移植性。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 本文综述了当前声纳图像数据集的现状，汇总并分析了多种公开可用的水下声纳数据资源，并通过对比总结为总表和时间线，以支持未来该领域研究。


<details>
  <summary>Details</summary>
Motivation: 推进水下探测、自动导航和生态监测需依赖高质量声纳图像数据集，但受限于数据稀缺和注释不足，制约了机器学习模型的发展，因此有必要系统梳理现有数据集资源，发现不足，对未来研究者提供指导。

Method: 作者系统搜集了当前公开可用的各种声纳数据集，涵盖侧扫声纳、前视声纳、合成孔径声纳、多波束回声测深仪和双频识别声纳等多种声纳类型，并对这些数据集在分类、检测、分割和三维重建等应用方向上进行了整理和分析。

Result: 作者总结和对比了各类型声纳数据集的信息，包括特征、规模、注释方式，并制作了主表和时间线，对现有数据集的特点一目了然。同时还梳理了新近发布的数据集和前沿进展。

Conclusion: 该综述为水下声学数据分析研究者提供了重要参考工具，有助于新入门和深耕者快速了解数据现状、发展方向及亟待补足的空白，促进后续数据集构建与技术创新。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 本论文提出了结合无透镜相机与隐式神经表示算法的新方法，实现了无需专用硬件即可高效获取显示器多视角特性，简化了显示校准流程。


<details>
  <summary>Details</summary>
Motivation: 当前显示器校准通常需昂贵的专用设备和黑暗环境，令大部分用户难以操作。为了降低校准门槛，需要创新性地采用普通设备进行显示特性测量。

Method: 本研究联合设计了无透镜相机与基于隐式神经表示的算法。从不同视角采集显示器发出的光场，实现了在46.6°×37.6°视锥范围内的高效重建。

Result: 提出的方法能在较大视角范围内，准确捕获和重建显示屏的光场信息，无需传统的专业硬件和复杂设置。

Conclusion: 该方案首次通过软硬件协同，极大简化了显示校准与表征过程，为未来便捷的显示设备自校准奠定了基础。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 本文提出了provenance networks，一种将可解释性嵌入模型架构、通过链接预测与训练样本实现端到端解释的新型神经网络。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型普遍缺乏可解释性，传统post-hoc方法很难明确模型如何利用训练数据做出预测，难以满足对透明度与信任的需求。作者为此设计新模型结构，以增强神经网络的可解释性和可靠性。

Method: provenance networks通过架构设计，使每一个模型预测都可以与支持该预测的训练实例直接关联，类似“学习版KNN”，每个输出都由加权相关训练样本决定，并在训练时联合优化主任务和可解释性目标。

Result: 新架构支持系统性研究记忆与泛化的权衡、检测异常和错标数据、验证输入是否在训练集、应对输入扰动，提高了模型的透明性、鲁棒性和信任度，但计算开销增加，仅适用于中等规模数据集。

Conclusion: provenance networks为端到端解释性和神经网络可信性提供新路径，部分弥补传统可解释性方法和深度模型透明度的不足，对提升现代深度学习模型在实际应用中的可用性和信任度具有重要意义。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出针对单模态（如RGB图像）和多模态（如RGB-3D、RGB-文本）场景的统一无监督异常检测（UAD）框架，发展了一种统一代价过滤（UCF）方法，可以通用于多种UAD模型后处理，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前无监督异常检测（UAD）主要面对真实异常样本稀缺的问题，且现有方法多分为重建类和嵌入类，但它们普遍忽视了匹配噪声的问题，影响了检测能力。同时，单模态与多模态UAD研究领域各自为政，缺少统一分析与方法。

Method: 提出统一代价过滤（Unified Cost Filtering, UCF）框架，作为任意UAD模型的通用后处理方案：首先通过常规匹配方法构建异常代价体（cost volume），再设计可学习的多层注意力引导过滤模块，根据测试样本自适应抑制噪声、突出异常特征。

Result: 在22个数据集上体系性评测，与多种既有UAD方法结合后，无论单模态还是多模态任务，均带来显著性能提升，并刷新多项SOTA纪录。

Conclusion: UCF能够作为插件式模块，有效提升各类UAD模型在单模态和多模态下的异常检测能力，具有良好的通用性和推广价值。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 本论文提出利用视觉语言模型（VLMs）对工业流程图中的目标检测结果进行自动评估和优化。


<details>
  <summary>Details</summary>
Motivation: 目前工业流程图如P&ID的数字化过程中，目标检测的准确性评估环节仍缺乏自动化方法。这影响了数字孪生和智能自动化的建设。

Method: 作者提出用视觉语言模型（VLM）结合视觉和文本信息，对检测结果进行多模态分析，识别遗漏或不一致的检测，进而自动评价并指导检测结果的改进。

Result: 该方法能自动识别复杂工业流程图中目标检测的错误，提高了检测结果的质量和可靠性。

Conclusion: 利用VLM进行多模态质量评估，不仅填补了自动质量评估的空白，还有效提升了复杂工业图纸目标检测的整体水平。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT 通过融合空间特征（如深度图、三维坐标、边缘图），提升视觉-语言模型（VLMs）在三维场景与复杂物体配置下的空间推理能力，取得了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉-语言模型在多模态推理上取得进展，但在三维场景和复杂物体关系的空间推理方面仍存在显著不足。该研究动机在于增强 AI 对空间信息（如方向、位置、距离）的理解，为更复杂环境下的多模态任务提供支持。

Method: 论文提出了 SpatialViLT 框架，将深度图、三维坐标、边缘图等空间特征引入多任务学习流程中，极大丰富了模型的空间表征能力。作者还提出了两种变体：SpatialViLT（处理完整区域）和 MaskedSpatialViLT（对物体区域进行掩码），并通过 SpatialEnsemble 融合两者，提升整体表现。

Result: 在具挑战性的 Visual Spatial Reasoning (VSR) 数据集上，SpatialViLT 及其集成方法在方向、拓扑、邻近等空间推理类别上实现了新的最优结果。

Conclusion: 该工作显著提升了 AI 多模态模型的空间理解能力，对于推动复杂场景中的智能推理和实际应用具有重要意义。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出利用深度学习方法，通过合成训练数据对两相光学切片结构照明显微成像（OS-SI）的残留伪影进行抑制，显著提升图像清晰度。


<details>
  <summary>Details</summary>
Motivation: 两相光学切片结构照明显微（OS-SI）方法加快成像采集速度时容易产生抑制难度较高的伪影，而现有的传统去噪方法难以有效处理这些伪影，且缺少高质量的无伪影真实性能评估数据集限制了有监督深度学习去噪方法的发展。

Method: 作者提出使用编码器-解码器结构的深度神经网络（包括非对称去噪自编码器DAE和U-Net）处理伪影减少问题。网络的训练数据对采用了将实测伪影场应用于合成图像的方法，生成含有已知伪影的合成训练集，实现对真实数据的模拟和监督学习。

Result: 经过合成训练数据集训练后，DAE和U-Net网络在实际OS-SI成像中均能有效提升图像清晰度，不同网络对于各类伪影有不同程度的处理优势。

Conclusion: 本文证明了通过合成训练数据可实现对OS-SI成像伪影的有监督深度学习去噪，为编码器-解码器网络在相关成像重建流程中的应用提供了新思路和可能。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: 该论文提出了PEaRL框架，将转录组数据通过通路激活分数表征，并结合Transformer和对比学习方法，实现转录组与组织病理图像的多模态融合，提升了模型的可解读性和表现能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多模态算法仅使用极少数高变异基因，无法全面反映组织表型背后的协调生物学过程，因此亟需发展能够从更高生物学层面（如通路激活）编码信息的方法。

Method: 提出PEaRL框架，采用ssGSEA算法计算基因通路激活分数，用Transformer对其进行编码，并通过对比学习与病理图像特征对齐，实现跨模态表征学习和降维。

Result: 在三个癌症空间转录组数据集（乳腺、皮肤和淋巴结）上，PEaRL在基因和通路层面表达预测上均超越现有SOTA方法，Pearson相关系数提升达58.9%和20.4%。

Conclusion: 以通路为中心的表征不仅提升了多模态模型的表现，还增强了解释性，为计算病理学带来更生物学忠实的分析方式。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS 提出了一种新颖的视觉-语言深度学习框架，通过分层语义提示和双提示机制，实现多模态医学影像的高效、可扩展分析，显著优于现有专项及通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有的医学影像深度学习模型通常只能解决特定任务，通用模型又存在条件单一、医学语义理解差的问题，限制了实际应用和推广。

Method: DuPLUS 采用分层语义提示（hierarchical semantic prompts）和独特的双提示机制（dual-prompt），结合视觉和语言信息，实现任务的细粒度调控；架构支持外部医学数据（如EHR）的灵活集成，并通过高效参数微调适应不同任务和中心。

Result: DuPLUS 可在三种影像模态、十个解剖多样数据集（覆盖30+器官/肿瘤类型）间泛化，8/10数据集上优于现有专项和通用方法。在头颈肿瘤预后预测中，DuPLUS 的一致性指数（CI）达0.69，且能够高效适配新任务和模态。

Conclusion: DuPLUS 是一种具有广泛适用性、可扩展、参数高效和临床相关性的医学影像分析深度学习框架，能促进医学AI通用化和实用化。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 本文提出了一个移动端优化的双阶段深度学习框架，将YOLOv10检测与MobileSAM分割并行运行，通过线程机制显著提升了野生动物实时检测与分割的速度和精度。以优先保护物种Houbara Bustard为例，实现了高精度与低延迟，适合在自然环境下非侵入式监测动物。公开了相关数据集和代码，以促进生态保护领域应用。


<details>
  <summary>Details</summary>
Motivation: 野生动物保护领域需对动物进行非侵入式监测，但受限于自然环境中设备计算资源以及动物易混淆的外观（如保护物种Houbara Bustard），实时自动检测与分割效率和准确度难以兼顾，亟需既高效又便携的解决方案。

Method: 提出了一种移动端优化的双阶段深度学习框架：利用多线程并行YOLOv10作为检测模块和MobileSAM作为轻量级分割模块，提升在有限计算资源下的实时性能。模型并行处理，减少延迟，并针对Houbara Bustard建立了4万张多样化环境下的标注图像数据集。

Result: 在保护物种Houbara Bustard测试中，模型达到mAP50为0.9627，mAP75为0.7731，mAP95为0.7178，以及MobileSAM分割的mIoU为0.7421。YOLOv10检测速度为43.7毫秒/帧，完全满足实时检测需求。

Conclusion: 提出的并行YOLOv10+MobileSAM双阶段检测-分割框架，兼顾移动端计算效率与准确性，能够实现野生动物的实时自动检测与分割，特别适合资源受限场景下的自然环境监测。相关代码和大规模高质量数据集已公开，便于研究与实际应用推广。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer通过利用几何对称性，实现了在不增加计算成本的情况下，提高Transformer的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer缺乏对科学和计算机视觉中常见的几何对称性归纳偏置。而目前的等变方法通常需更复杂和耗时的设计，导致牺牲Transformer的效率和灵活性。因此，作者希望提出一种兼顾等变性和高效性的方案。

Method: 提出了Platonic Transformer，通过将注意力机制定义为相对于源自Platonic正多面体对称群的参考系，诱导出有原理的权重共享方式。这使得模型能同时具有对连续平移和Platonic对称性的等变性，并能保持标准Transformer的结构与计算成本不变。作者还证明这种注意力机制形式上等价于动态群卷积，可使模型学习自适应几何滤波器，还可实现线性时间的卷积变体。

Result: 在CIFAR-10（计算机视觉）、ScanObjectNN（3D点云）、QM9及OMol25（分子性质预测）等多种基准数据集上，Platonic Transformer通过利用几何约束，在不增加计算成本的前提下，取得了有竞争力的性能。

Conclusion: Platonic Transformer兼顾了Transformer的高效性和对几何对称性的等变性，能够在多种任务中以零附加计算成本提升模型表现，有望在需要考虑几何对称性的领域得到广泛应用。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本文综述了领域泛化（DG）在语义分割任务上的最新进展，聚焦于基础模型在域泛化中的作用，并比较了主流方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络已取得极大进展，但在未知领域的泛化能力依然不足。传统的领域自适应依赖目标域信息，而领域泛化更具挑战性且实际价值更高。因此，作者希望系统梳理该领域，尤其是在语义分割这一应用场景的研究脉络和发展。

Method: 采用调研综述的方法，对现有领域泛化的语义分割方法进行归类，总结这些方法的发展变化，重点关注基础模型范式的兴起及其影响，并详细对比各方法在公开基准任务上的表现。

Result: 文章对比分析了多类DG方法的性能，并突显了基础模型引入后领域泛化性能的显著提升。基础模型成为目前该领域最具影响力的方向。

Conclusion: 本文推动了领域泛化领域的研究，明确了基础模型的巨大潜力，为后续研究者指明了新的研究方向和思路。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 本论文提出了一种新型自动化报告生成模型，利用transformer架构的视觉编码器与文本解码器，旨在简化消化道内镜检查如胃镜和结肠镜的文档编写流程。


<details>
  <summary>Details</summary>
Motivation: 内镜检查在消化道疾病诊断和管理中至关重要，但繁重的报告书写工作加剧了医生压力，导致效率低下和职业倦怠。因此，有必要开发自动化工具以减轻医生负担。

Method: 提出了一种两阶段训练框架：第一阶段使用图像/文本描述对对视觉编码器和文本解码器进行预训练，以捕捉通用视觉-语言特征；第二阶段在图像/报告对上微调模型，从而实现具有临床意义的自动报告生成。

Result: 所提出的方法显著简化了内镜检查报告的生成流程，初步结果显示模型可以自动生成高质量、具有临床意义的检查报告。

Conclusion: 该模型有助于减轻消化科医生的文档负担、提高工作效率，并有潜力改善患者护理质量。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一个基于扩散模型的无人机导航规划方法，可以从2D手绘草图推理出3D飞行路径，实现零样本从仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 人类在与无人机等机器人交互时，直观表达导航目标很困难，手绘草图是一种自然的交互方式，但将草图正确转化为安全有效的3D路径存在挑战。

Method: 提出SketchPlan，包括：1）SketchAdapter模块，学习将人工草图映射为2D投影路径；2）DiffPath扩散模型，结合2D路径投影和第一视角深度图推理3D飞行轨迹。训练时，合成32000条3D飞行路径，并用真实和自动标签联合训练两个模块。

Result: 模型在模拟和真实环境下测试，有效推理3D路径。真实无人机测试中，在低/中等障碍环境下达到100%任务完成率，在未见过的高障碍场景也有40%成功率，显著优于消融版本。

Conclusion: SketchPlan能够准确解释人类意图，实现从手绘草图到无人机三维路径的高效转化，并具备很强的sim-to-real泛化能力。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: AI虚拟人头视讯通过发送紧凑姿态-表情潜变量来节省带宽，但该潜变量可被攻击者利用进行'傀儡操控'，现有深度伪造检测方法普遍失效。本文提出仅基于潜变量的生物特征泄露检测方法，有效防御身份劫持。


<details>
  <summary>Details</summary>
Motivation: 随着AI仿真人头视讯系统的发展，通过仅传输姿态-表情潜变量进行实时合成，极大压缩带宽，但也产生了被攻击者实时操控身份的安全风险，且现有伪造检测手段无效，亟需新型安全防护。

Method: 提出一种基于姿态条件的大边界对比编码器，能从传输的潜变量中分离出持久的身份信息，并抵消姿态与表情的瞬时性影响。通过简单的余弦相似度检验嵌入向量，在渲染时实时检测非法身份交换。

Result: 在多种AI说话人头生成模型上验证，该方法在实时性、不同行为/场景（分布外泛化）以及防御效果上显著优于以往傀儡操控防护方案。

Conclusion: 本文方法实现了无需还原RGB视频也能可靠检测身份劫持，提升了AI通信应用的安全性，具备良好的实用性和推广潜力。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了REVEL任务和DragStream方法，实现了对自回归视频扩散模型生成视频的任意时刻、任意对象的流式细粒度可控编辑。该方法在无需额外训练的前提下，显著提升了拖动编辑的自然性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型在生成过程中难以实现流畅、细粒度的交互式控制，经常导致用户难以按预期方式灵活编辑视频内容。主要挑战包括拖动操作在潜空间中的扰动会累积导致分布漂移，以及上下文帧信息干扰拖动结果。

Method: 本文提出了REVEL任务，允许用户通过拖动实现对生成视频任意部分的即时编辑，包括平移、变形、旋转等多种操作。为解决潜在问题，提出了DragStream方法：一是自适应分布自整策略，利用邻近帧统计特性约束潜空间漂移；二是空间-频率选择性优化机制，有效利用但抑制上下文帧干扰，实现更自然的过渡。该方法无需重新训练模型，可直接融入现有自回归视频扩散结构。

Result: 通过丰富的实验验证，DragStream方法能够有效缓解潜空间漂移和上下文干扰问题，大幅提升拖动编辑的视频自然性和用户交互体验。

Conclusion: DragStream为细粒度、流式视频编辑提供了有效解决方案，为自回归视频扩散模型的实际应用和交互式编辑能力带来了新突破。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: 本文提出了GAS-MIL方法，可以高效整合多种基础模型（FMs）的特征，无需复杂微调并保持模型间互补性，在多个癌症分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型虽强大，但针对特定病理诊断任务的定制和评测需耗费大量时间和计算资源，特别是面对多样的模型选择时，手动特征挑选和精细调优非常繁琐。

Method: 提出了一种名为Group-Aggregative Selection Multi-Instance Learning（GAS-MIL）的集成方法，将多个基础模型的特征无缝结合，通过群组聚合和选择机制，无需手动筛选特征或大量微调，直接用于特定任务。

Result: 在前列腺癌（PANDA）、卵巢癌（UBC-OCEAN）和乳腺癌（TCGA-BrCa）三个数据集的分类任务中，GAS-MIL方法与单一FM和主流MIL方法相比表现更优或相当，体现出方法的稳健性与泛化性。

Conclusion: GAS-MIL有效整合不同基础模型，降低定制和部署复杂度，为多模态和精准肿瘤学任务提供了可扩展的模型基础，对计算病理学实际应用具有重要意义。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 本论文提出并验证了利用无人机快速投递纳洛酮用于应对阿片类药物过量紧急事件的新方法，并通过新建数据集和深度学习框架提升旁观者的情境感知，从而提高救援效率。


<details>
  <summary>Details</summary>
Motivation: 随着阿片类药物过量事件频发，紧急医疗服务往往不能及时到达，尤其依赖非专业旁观者的救援能力。传统无人机投递方案未关注旁观者的情境感知对救援效果的影响，急需新的评估和提升方案。

Method: 构建了Drone-Assisted Naloxone Delivery Simulation Dataset（DANDSD），记录医学无经验的受试者（大学生）如何在无人机配送纳洛酮情境下表现。基于该数据集，提出结合图嵌入与transformer模型的视频实时情境感知评估框架，整合视觉、几何、动力学和交互等特征。

Result: 所提模型在旁观者情境感知预测上表现优异，相较于FINCH基线模型，Mean over Frames（MoF）提升9%，IoU提升5%。

Conclusion: 该方法可助力开发具备自适应引导能力的无人机系统，更好地协助非专业旁观者施救，提升急救响应成效，有望挽救更多生命。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 本文评估了四种开源OCR系统（Tesseract、EasyOCR、PaddleOCR和TrOCR）在实际食品包装图像上的表现，比较其在提取配料表和营养成分表时的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 食品包装信息的自动提取对于法规合规与营养监测非常重要，但由于多语种、排版复杂、字体多样、光斑和曲面等难点，导致OCR困难。当前缺乏针对真实世界包装图像的系统性评估。

Method: 研究者建立了一个包含231种产品、1628张图片的数据集，并另外标注了60种产品共113张图片作为准确性评估的真值集，使用速度、覆盖度、CER、WER、BLEU、ROUGE-L、F1等指标对四种OCR工具进行评测。

Result: Tesseract在准确性（CER最低、BLEU最高）上表现最好。EasyOCR多语言支持与准确性兼顾。PaddleOCR覆盖度极高，但只支持CPU速度较慢。TrOCR即使用GPU加速，表现反而最弱。

Conclusion: 本文为食品包装OCR提供了基准数据和基础评测，突显了布局感知与文字定位的重要性，也为后续模型改进指明了方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: 该论文提出了FrameOracle模块，能智能选取视频中最相关的帧，兼顾高效与准度，显著提升视觉-语言模型（VLMs）的视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs因输入帧数受限，采用固定或均匀采样策略，往往难以适应视频信息密度和任务复杂度的变化，导致效率低下及信息丢失。亟需一种自适应采帧方法提升效率和准确率。

Method: 提出FrameOracle模块，可预测与任务相关的帧及需求帧数。采用四阶段课程学习，前三阶段用弱监督信号如跨模态相似性，最后一阶段用新建立的带关键帧注释的大型FrameOracle-41K数据集进行强监督训练。

Result: 在五种VLMs和六个基准测试上，FrameOracle可将16帧输入优化到平均10.4帧而不损失准确性；从64帧降到平均13.9帧且准确率提升1.4%。

Conclusion: FrameOracle显著提升了视频理解任务中效率与准确率的平衡表现，是扩展VLMs处理更大规模视频的有效通用工具。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 本文提出了一种混合协同微调（Co-FineTuning, CFT）方法，用于提高电子游戏视觉Bug识别效率，兼顾有标签和无标签数据，显著减少对大量标注样本的依赖，且在多种环境下超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的电子游戏视觉Bug手动识别需大量资源和专业知识，现有监督方法虽有前景，但需大量标注数据，而实际Bug发生稀少，标注困难。因此，如何在标注数据有限情况下高效检测Bug是亟需解决的问题。

Method: 作者提出CFT框架，结合目标游戏和多种游戏领域（co-domain）的有标签样本，再利用无标签数据来增强特征表示，最大化利用所有可得数据资源，以减少仅靠目标游戏标注样本的需求。

Result: 实验结果表明，CFT方法在多种游戏环境下检测游戏视觉Bug具有更强的鲁棒性与性能优势，即使仅用目标游戏50%标注数据也能保持优异效果。

Conclusion: CFT方法提升了视觉Bug检测的可扩展性和适应性，在标注数据有限情况下仍具备竞争力，对游戏领域实际应用具有重要价值。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: 本文探讨了带有Transformer风格模块的分层推理模型（HRM）在图像分类任务中的有效性，发现其在小型自然图像数据集上表现不及传统CNN。


<details>
  <summary>Details</summary>
Motivation: 近年来Transformer结构在自然语言处理和视觉领域表现优异，作者希望探究结合Transformer模块和分层推理思想的HRM在标准图像分类任务上的适用性。

Method: 采用HRM架构，集成了两个Transformer式模块（f_L, f_H）、一步DEQ训练、深度监督、旋转位置编码和RMSNorm。评估在MNIST、CIFAR-10和CIFAR-100上，不进行数据增强，优化器统一设置，进行对比实验。

Result: HRM在MNIST数据集上表现良好（测试准确率98%），但在CIFAR-10和CIFAR-100等自然图像上表现一般且训练速度慢，严重过拟合，泛化能力弱，相比下简单的CNN架构更具优势。

Conclusion: 当前设计下，HRM对于小分辨率图像分类任务（无数据增强）不如简单CNN架构。但作者认为模型结构调整后仍有潜力提升。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 本文综述了DINOv2等自监督学习方法在视觉特征学习的最新进展，分析其核心思想与技术优势，比较其与其他方法在下游任务中的表现，并讨论未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来自监督学习极大提升了通用视觉特征的学习能力，尤其是DINOv2在多个基准上表现出比弱监督方法（如OpenCLIP）更优的性能。理解这些方法背后的机制及其发展脉络，有助于推动该领域进一步发展。

Method: 系统性梳理了DINOv2的两个核心技术：多视角增强（multi-crop view augmentation）和均值教师自蒸馏（self-distillation with a mean teacher），并结合前人工作对这些方法进行了追溯和分析。

Result: 对比实验显示，DINO及DINOv2在多项下游任务上超越了其他自监督和弱监督方法。此外，其基于transformer骨干的特征学习表现出一些显著的涌现性质。

Conclusion: DINOv2在视觉自监督学习领域具有里程碑意义，但依然存在一定局限性。文章总结了其影响及未来研究值得关注的方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 本论文针对少样本增量学习（FSCIL）提出了一种创新框架Diffusion-Classifier Synergy（DCS），通过扩散模型与分类器的协同进化，大幅提升模型对新类别的学习能力及旧知识的保持能力，达到了FSCIL领域的最优表现。


<details>
  <summary>Details</summary>
Motivation: FSCIL任务要求模型在样本极少、逐步引入新类的过程中，避免遗忘已学知识，但现有方法受限于数据稀缺和泛化性差。扩散模型虽然能增强数据，但直接应用容易带来语义偏差或难以指导生成，亟需新的方法突破这些限制。

Method: 提出了Diffusion-Classifier Synergy（DCS）框架：通过设计依靠分类器状态动态调整的多元奖励函数，指导扩散模型生成质量更高的新样本。该奖励系统包括特征层次（通过原型锚定的最大均值差异与逐维方差匹配，提升语义一致性与多样性）与logits层次（通过置信度重校与跨会话混淆控制，促进探测性生成和类间可分性）。生成的数据优化分类器，分类器反馈更优引导奖励，实现相互促进。

Result: DCS在FSCIL公开基准上取得了领先于以往方法的性能，在新类别学习与旧知识保持上都获得了显著提升。

Conclusion: 扩散模型和分类器的协同进化，是少样本增量学习领域突破泛化和记忆难题的有效路径。DCS框架证明了基于动态奖励引导的图像生成能切实提升FSCIL任务的整体表现。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: 本文提出了MonitorVLM系统，通过视频流自动识别矿业现场违规行为，极大提升了安全监控的智能化和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 高风险行业如矿业的工业事故大多源于工人不安全行为。现有人工巡查方式费力、易出错，难以适应大规模、动态的作业环境，因此亟需智能化、自动化的安全监控方式。

Method: 作者构建了一个涵盖9000个VQA样本、包含40种高频矿业规范的领域专用违规数据集，并提出：1）CF条款过滤模块，动态选取最相关规范，降低延迟；2）BM行为放大模块，增强工人区域以识别细粒度动作。此外，整合了轻量级Web端界面以实现实际部署。

Result: MonitorVLM在实验中大幅超越现有视觉-语言模型，在精度、召回率和F1分数上分别提升了22.01%、34.22%、28.37%，同时CF模块降低推理延迟13.56%，BM模块提升精度3.45%、召回率8.62%。

Conclusion: MonitorVLM有效提升了矿业安全合规监测的自动化和精度，展示了多模态大模型在工业安全领域的巨大潜力，具备实际部署和推广价值。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 本文提出了一种融合扩散模型与智能交通系统（ITS）的事故检测新方法，通过结合扩散技术与分类模型，实现对交通事故的高效识别，达到97.32%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有智能交通中的事故检测方法在处理复杂数据分布时效果有限，而扩散模型具备更强的数据理解能力，因此作者希望通过引入扩散模型提升事故检测的准确性和可靠性。

Method: 提出了一种结合指导分类与扩散模型的混合架构，利用微调后的ExceptionNet输出作为扩散模型的输入，并以图像张量作为条件。模型包含多个条件模块，通过时间嵌入和图像变量嵌入来调节输入的线性投影，使网络在扩散过程中动态自适应。此外，为解决扩散模型计算量大的问题，采用云端实现，提升可扩展性和效率。

Result: 通过在公开数据集上的全面对比实验，所提扩散模型在基于图像的事故检测任务中取得了97.32%的准确率，超过了基线模型。同时，通过消融实验探讨了时序调度器、时序编码技巧、步数及架构设计等对性能的影响。

Conclusion: 所提方法有效提升了智能交通中的事故检测精度，展示了扩散模型在复杂数据分布下的优越能力，并为后续相关研究提供了可行的技术路径。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: 本文提出了SAMSOD模型，通过单模态监督和梯度冲突消解技术提升RGB-T显著性目标检测任务的表现，实验结果表明方法具有优异的性能和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T显著性目标检测方法多忽视了两模态收敛不均衡和激活值差异带来的梯度冲突问题，导致模型性能有提升空间。

Method: 提出SAMSOD模型，采用单模态监督机制增强弱势模态学习，并利用梯度冲突消解降低互相矛盾的梯度对模型收敛的负面影响。通过设计两个独立的适配器，分别对高激活与低激活神经元进行mask处理，提升前景物体检测，强化背景学习。

Result: 在RGB-T SOD标准数据集、弱监督RGB-T SOD、全监督RGB-D SOD及铁路表面缺陷检测任务上，所提方法均取得了优异和令人信服的实验效果。

Conclusion: SAMSOD有效解决了多模态融合过程中监督不均和梯度冲突带来的问题，提升了RGB-T显著性目标检测的效果，同时在相关任务中的泛化能力也得到了验证。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文针对自然语言表达下的小目标定位难题，提出了新的数据集（SOREC）和方法（PIZA），显著提升了小目标定位的准确率。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言结合技术在基于描述的目标定位任务中取得了进展，但对于极小目标的定位效果依旧不佳，而这一能力在自动驾驶等应用中非常重要。

Method: 作者首先提出了包含10万组配对数据的小目标定位数据集（SOREC），每组数据由驾驶场景中的小目标及其描述组成。其次，提出progressive-iterative zooming adapter（PIZA）模块，通过参数高效的微调方式，使模型能逐步放大并准确定位小目标。

Result: 将PIZA模块应用于主流 GroundingDINO 模型后，在SOREC数据集上获得了显著的定位准确率提升。

Conclusion: 该工作通过数据集和算法两方面推动了小目标描述性定位技术的发展，对相关实际应用具有很高的价值。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的WNet深度学习模型（Attention-WNet），用于视网膜血管动静脉分割，并在公开数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 准确分割视网膜血管中的动脉和静脉对于分析眼底健康和预测全身血管系统问题（如中风、心肌梗死等）具有重要意义。

Method: 提出将注意力机制集成到WNet深度学习架构中，形成新的Attention-WNet模型，用于视网膜动静脉自动分割。模型在公开HRF和DRIVE数据集上进行了测试。

Result: Attention-WNet模型在HRF和DRIVE等数据集上分割效果超过了当前文献中其他模型。

Conclusion: Attention-WNet模型提升了视网膜动静脉分割的准确性，对相关疾病的诊断和风险预测具有应用前景。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文通过为大规模视觉-语言数据集（如LAION-400M）添加细致的人物相关标注，揭示了数据集内部存在的人口统计学偏差，并首次实证性地将数据成分与下游模型偏见联系起来。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型在大规模多模态数据集上表现出较强的人口统计学偏见，但训练数据在产生这些偏见中的具体作用尚不清楚，主要障碍在于缺乏大规模数据集的人口统计学标注。

Method: 作者利用自动化标注流程（结合目标检测、多模态描述生成与微调分类器）为LAION-400M全量数据集创建了超2.76亿人像标注，包括性别、种族/族裔标签及自动生成的描述。

Result: 新建立的标注揭示了数据集中人口成分的不均衡与有害关联，如将男性及被感知为黑人或中东裔人群不成比例地与犯罪及负面内容相关联。此外，约60-70%的性别偏见可通过数据中直接共现线性解释。

Conclusion: 该工作提供了首个大规模实证资源，明确了数据集构成与模型下游偏见之间的关系，为理解和缓解多模态模型的偏见提供了重要基础和工具。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 本文比较了两种预训练神经网络在检测里约热内卢非正规住区时的表现：一种是基于大规模通用数据集的网络，另一种是基于卫星影像预训练的专用网络。研究旨在探讨任务专属性和数据量哪个对性能提升更有效。


<details>
  <summary>Details</summary>
Motivation: 现有检测非正规住区的深度学习方法尚未充分利用最新的预训练神经网络。本研究旨在填补这一空白，通过比较不同预训练方式的网络表现，寻求性能最优策略。

Method: 采用两种预训练神经网络进行对比实验：1）在大型通用图像数据集上训练的通用网络；2）在卫星影像数据集上训练的专用网络，并将它们应用于里约热内卢非正规住区检测任务。

Result: 两种网络在检测非正规住区任务中的表现进行了对比，重点分析了任务专属性和训练数据量对性能的影响。

Conclusion: 研究结论有助于理解选择神经网络预训练策略时，是该优先考虑任务相关性还是数据规模，从而为城市非正规住区检测的方法改进提供参考。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 该论文提出利用LoRA patching方法能够轻松绕过当前主流嵌入对抗扰动的Deepfake防御手段，并给予可视化警告作为新补救方案，指出现有防御方法存在严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有对抗Deepfake的主动防御手段（如在人脸图像中嵌入对抗扰动）实用性和可靠性不足，仍无法有效阻挡新型攻击，需要寻找并验证更健壮的防御策略，并揭示当前方案的薄弱之处。

Method: 作者提出一种新方法，称为Low-Rank Adaptation (LoRA) patching，通过在Deepfake生成器中插入LoRA patch以突破最新主动防御机制。该方案包括可学习门控机制以动态调整patch影响、防止梯度爆炸，以及多模态特征对齐损失（MMFA loss），强化输出的语义一致性。同时，还介绍带有可视警告的防御性LoRA patching作为补充对策。

Result: 在实验中，利用1,000张人脸图像，仅需一次微调训练，LoRA patching即可成功绕过多种主流主动防御机制，表现出极高的攻击能力，暴露了当前方案的安全缺陷。

Conclusion: 当前深度伪造防御思路存在关键漏洞，LoRA patching能轻松突破，提醒研究社区需研发更坚固的Deepfake防护手段。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 本文提出了一种在测试时对参考图像集进行简单微调（RSF）的方法，有效提升了视觉定位（VPR）任务中在领域差异大的测试集上的识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管使用大规模、多样化数据集训练的视觉基础模型在部分VPR基准数据集上表现优秀，但在测试环境与训练集差异较大时，表现仍不理想。作者试图通过新的信息源提升模型在这类难题测试环境下的表现。

Method: 作者提出对测试时会用到的参考图像集（即“地图”）进行简单微调（Reference-Set-Finetuning，RSF）。该过程在正式接收查询之前进行，允许模型利用目标领域的图像和位置信息来调整参数。

Result: 在具有挑战性的VPR数据集上，采用RSF后的模型使Recall@1平均提升约2.3个百分点。微调后模型依然具有良好的泛化能力，对不同测试集均有效。

Conclusion: RSF是一种简单却有效的方法，可作为提升现有SOTA VPR模型在领域间差异测试集上表现的重要补充。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种加速Sharpness-Aware Minimization (SAM) 的方法，称为ARSAM，在显著提升计算效率的同时保持模型泛化能力，与原始SAM效果相当。


<details>
  <summary>Details</summary>
Motivation: SAM 能有效提升模型泛化能力，但由于每一步需要两次梯度计算，导致计算开销翻倍，阻碍了其实际应用。因此，研究如何降低SAM的计算成本且不牺牲性能成为重要问题。

Method: 作者发现SAM的梯度可以分解为SGD的梯度和投影的二阶梯度（PSF）。基于对梯度动态变化的观察，ARSAM采用自适应采样、复用与混合分解梯度的方法来更新PSF，减少不必要的重复计算。

Result: ARSAM在CIFAR-10/100等数据集上，在准确率上与SAM持平，但训练加速约40%。在人类姿态估计、模型量化等任务上也验证了其加速效果，且性能无明显损失。

Conclusion: ARSAM显著降低了SAM的计算成本，同时保持了优异的泛化能力，具有较强的实用价值和广泛的适用性。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种面向医学诊断的深度学习透明化新方法CoPA，通过多层概念提取和提示，有效提升了模型对概念和疾病的预测性能，并优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在临床诊断中面临“黑箱”问题，决策过程不透明。传统基于概念的方法虽然提供了一定解释性，但往往只利用模型最后一层特征，无法捕捉多尺度和浅层细粒度的信息，且缺乏明确的概念编码指导，影响临床应用。

Method: 提出CoPA框架，利用概念感知嵌入生成器（CEG）从视觉编码器各层提取概念表示，并将这些表示作为Concept Prompt Tuning（CPT）的提示，引导模型重点关注关键概念。各层视觉表示最终聚合以匹配对应文本概念，增强概念信息提取和利用。

Result: 大量实验表明，CoPA在三个公开医学影像数据集上，概念识别和疾病预测均优于现有最先进方法。

Conclusion: CoPA可以有效捕捉并利用图片中的多粒度概念信息，提升医学图像诊断的性能和可解释性，有较强的实用前景。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: 本研究评估了ZFP压缩技术在3D医学影像数据集上的应用，验证其不会影响自动脑血管分割性能。结果表明，ZFP可大幅降低数据量同时保持高分割准确率。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像数据集体积庞大，数据共享与协作受限。研究动机在于探索高效压缩技术以降低数据管理难度，同时不影响关键任务（如脑动脉瘤检测中第一步的血管分割）。

Method: 将ZFP压缩技术，分别在误差容忍和定速两种模式下，应用于最新、大规模带有真实分割标注的3D医学影像数据集，并评估压缩后用于自动脑血管分割的准确性（与未压缩基线对比，基线Dice≈0.8774）。

Result: ZFP压缩在误差容忍模式下可实现最高22.89:1的数据压缩比，同时分割准确性几乎未损失（平均Dice为0.87656）。

Conclusion: ZFP压缩技术可在不影响分割准确性的前提下大幅减少3D医学影像数据集体积，推动医学影像大数据的高效合作与共享，具有实际应用价值。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN、Transformer与Mamba注意力机制的新型三分支医学图像分割网络，在多个基准数据集上超越现有方法，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型在不同模态和解剖区域间表现不一，多为任务特定，如何兼顾模型复杂性与实际性能，特别是在临床对效率与精度兼顾的前提下，亟需创新。

Method: 提出一种包含三分支编码器的新架构，将CNN（局部特征）、Transformer（全局特征）、Mamba-based Attention Fusion机制（长距离依赖）进行融合。解码器使用多尺度注意力CNN，保持细粒度与上下文一致性，同时引入协同注意门改善跨尺度特征互动和选择。

Result: 在多个医学图像分割基准数据集上，所提方法在准确性与泛化能力上均超过了最新主流方法，并维持了类似的计算复杂度。

Conclusion: 该架构在分割性能与效率间取得良好平衡，具有实用性和可扩展性，对多样化医学影像任务有广泛推广意义，源码和模型将在论文接收后公开，促进研究复现。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv9与多边形标注的深度学习方法，实现了自动化的道路损坏与井盖检测，显著提升了标注精度和监测效率。


<details>
  <summary>Details</summary>
Motivation: 城市道路安全和基础设施维护对于智慧城市至关重要。传统人工巡检方法效率低、成本高、易出错，因此亟需自动化、智能化的检测方案。

Method: 作者构建了一个含千余张图像、拍摄自孟加拉国达卡市的新型数据集，并采用多边形标注方式，训练YOLOv9模型识别‘破损’、‘未破损’和‘井盖’三类目标，提升了定位精度。

Result: YOLOv9整体图像级准确率为78.1%；破损与未破损类别的F1分数较高（分别为86.7%和89.2%），但由于类别不平衡，井盖检测F1分数较低（18.2%）。

Conclusion: 本研究证明了基于多边形标注与YOLOv9的自动化道路检测方法在发展中国家场景下具备高效、可扩展性强的优势，有助于城市基础设施智能化管理和维护。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 本文提出了一种结合对比学习与扩散模型的新方法（Contrastive-SDE），用于无配对的图像翻译任务，实现了高效和高质量的结果，无需标签监督或分类器训练，且收敛速度更快。


<details>
  <summary>Details</summary>
Motivation: 无配对图像翻译常需在缺乏配对样本的情况下，实现不同域间的效果迁移。传统方法难以同时做到高保真度、多样性和高效率。论文旨在解决如何在无监督场景下有效学习语义一致的图像映射。

Method: 方法结合了基于SDE的score-based扩散生成模型与对比学习（SimCLR），提出时间相关的对比框架。模型通过对比学习将原图和其域不变特征作为正样本对进行训练，增强域共享特征保留，抑制域特定特征。对比模型最终指导预训练的SDE进行图像翻译推断。

Result: 与多个基线模型对比，所提Contrastive-SDE在三个主流无配对I2I任务和四项评测指标上达到与SOTA可比的结果，并且模型收敛更快，不需要标签或额外分类器训练。

Conclusion: 提出的方法在保持翻译质量的同时，实现了更快收敛和更高效率，减少了监督需求，是无配对图像翻译的有效替代方案。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: 该论文指出当前流行的VLA模型基准LIBERO存在评估方式不合理、结果虚高等问题，并提出了改进基准LIBERO-PRO，发现现有模型泛化能力极差。


<details>
  <summary>Details</summary>
Motivation: 现有LIBERO基准存在设置问题，导致模型容易仅靠死记硬背动作序列和环境布局取得虚高的成绩，不体现模型真正的理解和泛化能力。作者意在解决该基准误导评估的问题。

Method: 作者设计了LIBERO-PRO，该基准通过在对象、初始状态、任务指令和环境四个维度进行扰动，让模型在多种合理变化下接受评测，全面检验其泛化能力和对任务的实际理解。

Result: 现有模型在标准LIBERO下准确率超90%，但在LIBERO-PRO下准确率降为0%，并且模型在面对扰动（如目标物体更换、指令损坏）时行为不变，暴露出模型对死记硬背的依赖而非真实理解。

Conclusion: 作者呼吁社区摒弃有缺陷的官方评测方式，采用能真实反映模型泛化和理解能力的评测。提出的LIBERO-PRO为更加可靠的基准，促进行业内模型评估的健康发展。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 该论文提出了Mirage数据集，专注于收集当前AI检测器难以识别但人类能区分的AI生成图片，并评估大规模视觉语言模型（LVLMs）在AI图片检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型进步，AI生成图片越来越难被常规检测器区分，但人类仍可辨别。当前缺乏能够凸显此差距的数据集，亟需更好的检测和解释方法。

Method: 作者构建了Mirage数据集，涵盖多样的AI生成图片及其可见伪影，并使用现有检测方法和LVLMs在该数据集及其他基准集上进行实验，考察其在检测带有或不带可见伪影图片时的效果。

Result: 实验发现，LVLMs在检测有明显伪影的AI生成图片时表现良好，但若图片没有明显伪影，其检测能力显著下降。

Conclusion: 现有方法和LVLMs均在检测某些AI生成图片时存在局限。需要开发新的、更具泛化能力且更具解释性的检测方法。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: 本文提出了一种统一视觉指向范式UGround，将视觉 grounding 转化为通过在展开的 transformer 层中动态选择中间特征层作为提示，实现更有效的 mask 生成。


<details>
  <summary>Details</summary>
Motivation: 以往方法通常只利用 transformer 的最后一层作提示，导致错误累积且提示缺少空间信息，限制了视觉 grounding 的精度和泛化能力。

Method: 核心创新是 Policy-Prompted Masking，包括（1）随机跳跃连接机制（SSC），通过强化学习策略动态决定 <SEG> token 连接的 transformer 层，提升信息流灵活性；（2）Mask as Prompt（MasP），利用 <SEG> token 与图像 token 的相似度图作为软掩码，引入空间显式提示来驱动分割模型（如SAM）生成分割结果。

Result: UGround 将视觉定位任务统一到同一框架下，覆盖了描述性分割、推理分割、单目标到多目标、正反查询等场景，在这些多样任务上验证了优势。

Conclusion: UGround 在提升视觉 grounding 的准确性和适应多样任务方面表现出强大能力，有望为视觉分割领域提供统一、强化的解决方案。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4提出了一种优化的4D高斯溅射场景压缩方法，能大幅缩小游戏的高保真动态场景建模存储需求，同时保持可视质量。


<details>
  <summary>Details</summary>
Motivation: 现有4D Gaussian Splatting方法尽管能实现高质量动态场景实时渲染，却需要大量高斯原语，导致存储开销巨大。此前针对这一问题的压缩方法在压缩率或视效质量上均存在一定局限。因此，开发一种能大幅度压缩存储但保持高质量重建的新方法成为迫切需求。

Method: OMG4框架分三步精简高斯原语：（1）高斯采样，找出对重建最关键的原语；（2）高斯剪枝，去除冗余；（3）高斯合并，融合属性相近的原语。同时引入隐式外观压缩，并把子向量量化（SVQ）方法扩展到4D模型，进一步降低存储。

Result: 在标准数据集上，OMG4相比最新主流方法，在维持重建质量同时，将模型大小减少60%以上，且压缩后的视觉效果优异。

Conclusion: OMG4极大提升了4D场景表示的紧凑性，为动态场景的多应用场景带来可能性，是4D Gaussian Splatting存储与表现质量兼具的重大进步。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种新的结构化领域对齐框架，将地面视角图像的开放词汇检测能力迁移到遥感图像，显著提升了遥感场景下的零样本目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法只能识别固定类别，添加新类别成本高。面对遥感图像领域与地面视角的显著差异，现有开放词汇方法直接迁移效果有限，因此需要新的适应策略，实现高效扩展与泛化。

Method: 本论文提出结构化领域对齐框架，包括对比式图像到图像对齐增强遥感和地面视角的嵌入相似性，并采用多实例词汇关联对齐遥感图像和文本嵌入。

Result: 在xView、DOTAv2、VisDrone、DIOR和HRRSD数据集上进行了实验。与微调的封闭词汇模型相比，在零样本条件下，提出方法在DOTAv2、VisDrone和HRRSD数据集上mAP分别提升6.32、4.16和3.46。

Conclusion: 本文方法显著提升了遥感图像中的开放词汇目标检测能力，为更灵活、可扩展的遥感对象检测系统奠定基础。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 本文综述了深度学习在皮肤癌自动诊断方面的最新进展，重点分析了现存挑战及对应创新解决方案，并探讨了其在临床应用中的前景。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率高且致死率大，早期检测对提高患者生存率至关重要。然而，现有自动诊断方法面临准确率和效率不足等问题，亟需借助深度学习改善临床诊断效果。

Method: 文章回顾了采用深度学习进行皮肤癌诊断的研究，聚焦于处理复杂特征、图像噪声、类别内差异和类别间相似性以及数据不平衡等挑战，讨论了如数据增强、混合模型、特征融合等创新技术，并按照PRISMA标准系统梳理文献。

Result: 综述发现，各类创新方法在一定程度上提升了诊断结果的准确性和实用性，部分研究已将深度学习模型集成至临床流程，展现出促进临床决策的潜力。

Conclusion: 深度学习有望彻底改变皮肤疾病的诊断方式，但仍需不断技术进步和多方努力，才能充分释放其在皮肤病学护理领域的变革潜力。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 本文提出了一种新的GAN知识蒸馏方法SDAKD，通过引入学生判别器来解决学生生成器与教师判别器之间的能力不匹配问题，有效提升了超分辨率任务中轻量化GAN的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GAN模型在生成任务上表现优异，但其高计算需求限制了在资源受限设备上的应用。知识蒸馏作为模型压缩手段在GAN领域存在生成器与判别器能力失配导致蒸馏效果有限的问题。为此，研究者希望通过改进蒸馏过程，实现高效轻量化GAN。

Method: 提出Student Discriminator Assisted Knowledge Distillation（SDAKD）方法。SDAKD在蒸馏过程中引入了专门为学生网络设计的学生判别器，并采用三阶段训练流程，后两个阶段结合特征图蒸馏，缓解学生生成器和教师判别器间的能力差异。

Result: 在GCFSR和Real-ESRGAN两个超分辨率生成器上进行了实验，结果显示SDAKD方法优于基线模型和现有最先进的GAN知识蒸馏方法，在多个评价指标上取得了持续改进。

Conclusion: SDAKD通过引入学生判别器和分阶段训练，有效克服了传统GAN蒸馏中的能力不匹配难题，为轻量化、高性能的生成模型部署提供了新途径。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [61] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 该论文提出了PoseGaze-AHP，这是一个首次同时包含头部姿态与眼动信息、用于眼源性异常头位（AHP）诊断的3D公开数据集，旨在推动AI辅助的AHP诊断工具开发。


<details>
  <summary>Details</summary>
Motivation: 以往的AHP数据集往往只关注头位或眼动某一方面，导致不能支撑综合性诊断方法的发展，制约了AI在AHP分析中更广泛的应用。该研究旨在填补这一空白，推动AI辅助AHP临床分析。

Method: 研究团队结合LLM（Claude 3.5 Sonnet模型）和分步、分层、复杂提示策略，从已有医学文献中提取结构化临床数据，并用Neural Head Avatar（NHA）框架将其转化为3D头部与视线运动样本，最终生成7,920张涵盖多种眼病的合成图像。

Result: 数据抽取方法的总体准确率达到91.92%，表明该方法具有较高的可靠性。形成的PoseGaze-AHP数据集是首个能满足AI驱动AHP诊断需求、同时兼顾隐私合规性的公有数据集。

Conclusion: PoseGaze-AHP为AI辅助诊断AHP提供了强有力的数据基础，有助于未来发展更加准确、安全的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [62] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该论文针对动态4D人形网格模型的质量评价问题，提出了一个大型数据集和新的多模态大模型评价方法，显著提升了评价准确性。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描与重建技术的发展，基于4D网格的动态数字人广泛应用于游戏、动画和远程通信等领域。但在数据采集、压缩和传输过程中，4D数字人网格容易受到各种噪声影响，进而影响用户体验。因此，如何有效评价动态4D数字人的质量成为亟需解决的问题。

Method: 论文首先构建了一个大规模高质量动态数字人数据集DHQA-4D，包含真实扫描的4D人形网格序列与多种失真版本、主观评价分数。随后，提出多模态大模型DynaMesh-Rater，融合投影2D视频视觉特征、截取视频片段运动特征及4D网格几何特征，通过LMM整合多维信息，并采用LoRA方式微调模型以预测质量分数。

Result: 在DHQA-4D数据集上的大量实验表明，DynaMesh-Rater方法在质量评价准确性方面显著优于现有方法。

Conclusion: 提出的DHQA-4D数据集和DynaMesh-Rater方法为动态4D数字人质量评价领域带来了新的基准和有效手段，推动了该领域的发展。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [63] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 现有视觉语言模型（VLM）在实际物理世界中的应用受限，主要由于终端动作模块泛化能力差。本文提出通过稀疏3D轨迹作为中介，将VLM的高层规划与泛化动作专家结合，有效提升任务泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLM虽然在规划和推理上表现优异，但其实际用于机器人等物理环境时，受到动作模块泛化能力和数据稀缺的限制。此外，现有双系统方法解耦“思考”和“执行”时，动作模块语义模糊、合作机制不清，需要环境迁移时频繁微调，严重影响实际应用。

Method: 作者提出一种全新框架：VLM负责高层决策，仅输出稀疏3D路径点；再由泛化动作专家根据实测点云逐步生成稠密、可直接执行的动作序列。同时引入“动作预训练—点云微调”新范式以提升泛化和训练效率。

Result: 实验表明，该框架可有效提升在新场景、新任务下的泛化能力，无需每次都对动作模块重新微调，实现更智能、灵活的实际操作。

Conclusion: 本文首次以泛化动作专家为核心，通过3D中介表示和创新训练流程，有效打通视觉语言理解与低层动作系统，为VLM类机器人在物理环境中的广泛应用提供了新思路。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [64] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于ASFF（自适应空间特征融合）的改进型ResNet-50模型，在ISIC 2020皮肤癌图像分类任务中显著提升了准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类任务由于类别间相似性高、类别内差异大，以及图像噪声问题，导致分类准确率提升困难。现有方法对多尺度特征融合和噪声鲁棒性有限，迫切需要更有效的深度学习方案。

Method: 本文在ResNet-50基础上引入ASFF模块，采用双分支结构自适应融合高层语义特征与中层细节特征，通过全局平均池化和全连接层生成权重，实现多尺度加权特征融合，提升特征表达和抗噪声能力。

Result: 在ISIC 2020子集（3297张良恶性皮肤图像）上，所提ASFF-ResNet-50模型准确率达93.18%，各项性能指标（精度、召回率、特异性、F1分数）优于五种经典CNN。AUC在P-R和ROC曲线上分别达到0.9670和0.9717，经Grad-CAM可视化验证模型更关注病灶区而抑制背景噪声。

Conclusion: 改进后的ASFF-ResNet-50模型能更有效学习皮肤病变相关特征，提升分类性能，验证了其在计算机辅助皮肤癌诊断中的应用潜力。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [65] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: 本文介绍了OpenFLAME，一个面向增强现实应用的开放式联邦视觉定位系统（VPS）后端，支持不同组织独立维护和控制各自空间的3D扫描和定位服务，从而克服现有集中式VPS在隐私、可维护性和覆盖范围上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的VPS系统多为谷歌、Niantic等大型机构集中构建和维护，受限于隐私、法规及维护成本，难以覆盖到私人室内空间，阻碍了增强现实应用的普及和发展。

Method: 作者提出了OpenFLAME，一个基于联邦机制的VPS框架，允许各组织独立3D扫描各自空间，分别部署和维护VPS服务。文中还提出了联邦图像定位概念，并就多地图之间数据管理与融合（在不泄露隐私数据的前提下）提供了参考解决方案。

Result: OpenFLAME实现了空间定位服务的分布式部署和维护，为维护数据隐私、提升空间覆盖范围以及服务质量控制等问题提供了解决思路和具体工具。还解决了跨空间的定位一致性、服务质量把控、服务选择等实际挑战。

Conclusion: 作者证明了联邦VPS系统在保护隐私、提升系统可维护性和扩展性方面的有效性，为未来面向世界级AR应用的场景提供了更灵活和分布式的技术基础。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [66] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态深度学习框架，将口腔鳞状细胞癌（OSCC）的临床、影像学和组织病理学图像整合，通过加权集成DenseNet-121模型以提升早期检测能力。集成模型在验证集上总体准确率达84.58%。


<details>
  <summary>Details</summary>
Motivation: OSCC晚期诊断率高，全球五年生存率低于50%，主要由于早期发现困难和影像解读主观性强。亟需开发高效、客观、非侵入性的AI方法，辅助临床早发现高风险病灶，降低漏诊、延误。

Method: 研究回顾性分析三个公开医学图像集（临床照、影像、组织病理切片），采用DenseNet-121卷积神经网络进行迁移学习训练，并用数据增强与特定预处理提升鲁棒性。各模态模型预测结果通过验证集加权集成融合，采用准确率、精度、召回率和F1-score评估表现。

Result: 模型在影像（100%）、组织病理学（95.12%）均表现卓越，临床图片因异质性高表现较低（63.10%）。多模态加权集成模型在55例验证集上总准确率84.58%，诊断鲁棒性高于单模态。

Conclusion: 本多模态AI集成框架作为无创辅助决策工具，有效弥合当前诊断流程不足，提升高危癌变早识别能力，减少误诊、延误风险，助力改善OSCC患者预后，符合全球肿瘤诊疗规范发展趋势。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [67] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 该论文提出了一种针对端到端自动驾驶的增强学习方法RAP（Rasterization Augmented Planning），通过轻量级的三维光栅化数据增强与特征空间对齐，提升了无人车在闭环驾驶中的鲁棒性和泛化能力，同时替代了成本高昂的真实感渲染方法。


<details>
  <summary>Details</summary>
Motivation: 传统的仿真增广方法依赖于高保真的神经渲染或游戏引擎，计算和资源消耗大，难以应用于大规模训练，同时端到端模仿学习缺乏出错恢复的数据，导致模型在小错误下迅速崩溃。该论文追求具有语义忠实度和高可扩展性的增广方式以提升泛化效果。

Method: 提出用3D光栅化方法对标注几何体进行高效渲染，生成结构化、可编辑的多样合成视角和反事实恢复轨迹。为缩小仿真与现实的特征空间差异，提出Raster-to-Real特征对齐方法，并整合为RAP数据增强流水线，提升训练数据多样性和真实性。

Result: RAP在四大主流端到端自动驾驶基准（NAVSIM v1/v2，Waymo E2E driving，Bench2Drive）上取得了闭环鲁棒性和长尾泛化的SOTA表现，排名第一，验证了方法的高效性和实用性。

Conclusion: 端到端驾驶模型训练无需过度依赖高成本的真实感视觉合成，采用高效的光栅增广与特征对齐可以实现更大规模、可泛化的无人驾驶学习，是一种务实的替代方案。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [68] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文关注于可解释图像质量评价（IQA），提出了一种高效的数据筛选方法，显著减少数据量和计算成本的同时还能提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大规模指令微调数据集来增强多模态大模型对图像质量的感知能力，但过多的数据导致高昂的计算成本与数据冗余，甚至对模型性能造成负面影响。作者因此探索精简高质量数据的优化策略。

Method: 作者在强大的预训练多模态大模型（MLLM）基础上，首先实验了不同比例子集微调的效果，发现适当比例随机采样的子集优于全量数据。进一步，提出基于聚类的数据选择框架，分为三阶段：特征提取、聚类配额分配、与聚类采样策略，并提出了具体的高效方法IQA-Select。

Result: IQA-Select方法只需10%的精选数据，在Q-Bench及AesBench两个评测集上，性能分别达到全量微调的102.1%和103.7%，效果优于常规方法且极大节省了计算资源。

Conclusion: 挑选高质量、无冗余的数据子集用于指令微调，不仅能够显著降低算力开销，还可以提升可解释图像质量评价的性能。提出的IQA-Select方法在实验中取得了卓越表现，具备广泛应用前景。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [69] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 本文提出了一种将零样本细粒度图像分类任务转化为视觉问答框架的新方法，利用大规模视觉-语言模型（LVLM）的理解能力，并通过注意力干预技术提升性能，在多个公开基准上取得了优于当前SOTA的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLM在视觉-语言推理任务上表现出色，但其在零样本细粒度图像分类这一挑战性任务中的潜力尚未被充分挖掘，因此作者希望探索LVLM在该领域的应用。

Method: 将细粒度图像分类任务转化为视觉问答任务，使LVLM能够通过回答与类别特征相关的问题进行判别，而不是直接输出类别名称。同时引入了新的注意力干预技术以提升模型表现，并建立了更精细的类别描述基准数据集。

Result: 在多个细粒度图像分类数据集上进行了大量实验，所提方法在所有基准上均超越当前的SOTA方法。

Conclusion: 所提出的方法不仅证明了自身的有效性，还显示出LVLM在零样本细粒度图像分类任务中的广阔应用前景。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [70] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 该论文系统性评测了多种去雾方法（包括经典滤波、现代神经网络、组合链式方法和视觉语言模型）对自动驾驶感知系统的提升效果，发现提升图像质量不一定带来下游检测和分割性能提升，并建立了透明、面向任务的去雾评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法虽能提升图像质量，但并非总能有效提升自动驾驶下游任务（如目标检测和分割）的表现，且过去评测多基于合成数据，缺乏真实场景有效性验证。

Method: 作者基于Foggy Cityscapes数据集，系统分析了四类去雾策略（经典滤波、现代网络、链式组合、视觉语言模型编辑），评测了图像质量和下游检测、分割任务表现，并通过VLM判别打分与任务指标相关性分析。

Result: 实验揭示了不同去雾方案在提升或降低下游任务表现时的适用情境，发现VLM编辑方法与专用方法相比具有一定优势，VLM打分与主流检测分数（mAP）相关性高。

Conclusion: 本文建立了系统且透明的去雾方法评测基准，为择优选择实际自动驾驶去雾前处理方法提供参考，并强调只有在特定条件下前处理才真正有助于提升恶劣天气下的感知能力。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [71] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: 本文提出了CAMEO，一种级联式通用人类运动视频生成框架，能够更好地将文本到动作（T2M）模型与条件视频扩散模型（VDM）结合，在多基准和用例下展现优越表现。


<details>
  <summary>Details</summary>
Motivation: 目前的视频扩散模型在人类视频生成领域应用有限，大多数工作局限于图像到视频任务或诸如舞蹈等窄领域。如何实现更加通用的人物视频生成，并有效整合文本描述和条件视频内容，是亟需解决的问题。

Method: 作者提出CAMEO框架，将T2M模型与条件VDM级联，通过文本提示和视觉条件预处理、以及摄像机感知条件模块，有效训练VDM并自动选择与输入文本匹配的视角，实现运动描述、条件信号和生成视频之间的强一致性。

Result: CAMEO在MovieGen基准和新引入的适用于T2M-VDM结合的基准上进行了实验证明，展现了其出色的效果及广泛适用性。

Conclusion: CAMEO框架实现了文本与动作到视频生成流程的高效衔接，减少了人工干预，提升了一致性和表现，多场景下均展示了其通用性和优势。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [72] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合深度学习与大语言模型的新方法，实现自动网球击球动作分析，并将生物力学特征转化为易于理解的可操作性反馈。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习和生物力学特征已提升了网球击球分析的准确性，但现有系统缺乏将生物力学数据转化为选手和教练易于理解与应用的语言反馈能力。该研究旨在填补这一差距。

Method: 利用基于CNN-LSTM的模型从运动数据中提取关键生物力学特征（如关节角度、肢体速度和动力链模式），分析其与击球效果和受伤风险的关系，之后结合大语言模型生成技术准确、具生物力学依据、且可实际应用的反馈。

Result: 实验在THETIS数据集上验证了所提框架能提升击球动作分类准确率，并能够为用户提供更具解释性和实用性的技术反馈。

Conclusion: 该框架成功将可解释的人工智能方法与运动生物力学结合，提升了自动化击球分析的反馈效果，对运动训练和受伤预防具有重要应用价值。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [73] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出了一种提升视频大型语言模型（Video-LLMs）时间理解能力的新方法TimeWarp，通过生成合成的时间数据集，显著提升了模型在时间细粒度任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型在视频描述等任务上取得不错成绩，但在需要精细时间感知的任务上表现较差，原因在于现有微调数据集缺乏视觉复杂性和时间细节，导致模型更依赖语言推理而不是真正理解视频动态，亟需解决此问题。

Method: 提出了TimeWarp方法，系统性地生成面向时间动态的合成数据集，用于微调模型，使其更关注输入视频的视觉和时间信息，并构建了大规模的偏好数据集，涵盖复杂的时间动态，帮助模型进行更准确的时序理解。

Result: 通过在现有模型上应用TimeWarp方法，模型在7个时序理解基准测试中表现出了明显提升，证明了数据集和方法的有效性。

Conclusion: TimeWarp合成时序数据集能极大提升Video-LLMs对时间动态的理解能力，为提升视频大语言模型的细粒度时序任务表现提供了新思路和工具。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [74] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文提出通过扩展视觉-语言模型（VLM）中的文本编码器上下文长度以处理生物医学长文本图注，显著提升了检索和分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM主要只支持较短文本（<77 tokens），但生物医学文献的真实图注常常很长，导致大量信息被截断，影响模型性能。

Method: 作者调研真实生物医学图注长度，提出扩展VLM文本编码器窗口至512 tokens，并构建包含长图注的BIOMEDICA-LongCAP数据集，训练得到BMC-LongCLIP模型。

Result: BMC-LongCLIP模型有效利用了更长的文本信息，检索任务Recall@1提升最高30%，分类任务平均提升2%，模型收敛更快，同时减少了无效token截断。

Conclusion: 长上下文建模能大幅提升生物医学VLM性能，是未来改进该领域模型的有效方向。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [75] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 本文针对半监督学习中未标记数据分布未知的问题，提出了CPG框架，可控地筛选高可信伪标签并扩充标注集，从而提升模型表现，在多个数据集上显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法假设未标注数据服从预定义分布，现实中未标注数据真实分布往往未知，直接影响伪标签利用和模型泛化能力，需要一种对未标注数据分布无依赖的方法。

Method: 提出Controllable Pseudo-label Generation（CPG）框架，核心为动态可控伪标签筛选与强化自优化循环，将可靠伪标签逐步融入标注集，使标注集保持已知分布并提升训练效果。包括动态控制筛选机制、基于logit调整的Bayes最优分类器，以及辅助支路和类感知自适应增强模块。

Result: 理论上证明该优化循环可在某些条件下大幅降低泛化误差。实验中在多个基准数据集显著提升准确率，最多超越现有方法15.97%。

Conclusion: CPG方法对未标注数据分布鲁棒，全面提升半监督学习性能，有助于实际应用中长尾分布问题的解决。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [76] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 本文提出了一种针对PaddleOCRv5细调的方法，有效提升了对古越南汉喃文献字符的识别精度，尤其是在劣化图像条件下。


<details>
  <summary>Details</summary>
Motivation: 古越南汉喃文献在数字化与跨语言语义研究中十分重要，但现有OCR系统面对古文献图像劣化、字符不规范及手写变体时表现较差。

Method: 作者针对PaddleOCRv5的文本识别模块，选用经人工整理的古越南汉喃手稿子集进行再训练，并建立了包含预处理、LMDB转换、评估与可视化的完整训练流程。

Result: 实验数据显示，细调后模型在噪声图像条件下的准确率由37.5%提升至50%。

Conclusion: 细调优化显著提升了模型在古汉喃文献自动识别的能力，可助力汉越语义对齐、机器翻译及历史语言学等下游应用，相关演示已上线展示。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [77] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于医学图像分割的隐式神经表示（INR）元学习框架MetaSeg，能以更少参数实现与主流模型相媲美的分割性能。


<details>
  <summary>Details</summary>
Motivation: 虽然INR在信号紧凑表达方面表现优秀，但其在如分割此类需要学习信号语义结构的预测任务上存在天然限制。亟需一种兼顾准确率与资源效率的医学影像分割方法。

Method: MetaSeg结合INR与元学习，通过元训练获得INR参数初始值，使其在见到新图像时，仅需微调即可输出像素级强度和类别。同时，对比评估了MetaSeg在脑MRI分割（2D/3D）上的表现。

Result: MetaSeg在2D和3D脑MRI分割任务中，Dice分数与常用的U-Net模型相当，但参数量减少90%。

Conclusion: MetaSeg为医学图像分割提供了一个更轻量、可扩展的新选择，克服了传统重型架构如U-Net和ViT的劣势。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [78] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: 本文提出了Video-in-the-Loop（ViTL），一种面向长视频问题回答（long-video QA）的高效双阶段模型，结合事件区间定位与内容细致分析，并发布了新的span-grounded多选QA数据集。结果表明，在有限Token预算下，该方法能明显提升效率与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视频QA模型在处理长视频时难以兼顾效率和准确性，特别是在Token/算力受限的情况下，模型对关键时间段的利用率不高，难以提供便于解释的推理链。作者希望解决长视频内容分布稀疏、推理复杂、算力受限等挑战。

Method: 提出ViTL框架，分为两阶段：第一阶段以低帧率快速浏览视频并定位潜在相关区间；第二阶段根据问题把Token（也即帧或片段）细致分配到相关区间，提升局部特征分辨率，然后并联输出Span（视频区间）及最终答案。此外，介绍了将事件图转换为带时间区间标签的多选视频QA任务（数据集）。训练时，采用新的互相联系的目标函数，使得定位与QA表现相互促进。

Result: 在固定Token预算下，ViTL模型在Charades-STA、ActivityNet-Captions等公开长视频QA与时序定位任务上，比传统均匀采样等方法以更少帧输入带来高达8.6%的准确率提升。消融实验验证了Span-aware采样明显优于统一采样框架。

Conclusion: ViTL方法兼顾了可解释性和计算高效性，能在有限资源下应对大规模长视频QA任务。配套新数据集有助于推动领域发展。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [79] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据增强框架AgentAug，通过模拟造假流程自动生成多样化的假新闻短视频，有效提升假新闻检测的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的兴起，假新闻传播问题日益严重。现有检测方法依赖有限的数据和单一特征，难以应对现实中视频与造假事件之间复杂的多对多关系，导致检测性能受限。

Method: 提出AgentAug数据增强框架，模拟假新闻制作过程，利用多条由大语言模型驱动的造假生成管线，以及基于不确定性采样的主动学习方法，从增强样本中筛选有效数据，涵盖四类典型假新闻制作方式。

Result: 在两个主流基准数据集上的实验表明，AgentAug可以持续提升短视频假新闻检测模型的性能，相较于传统方法有明显改进。

Conclusion: 多样化假新闻样本的生成可增强检测模型泛化能力，AgentAug为假新闻视频检测任务提供了新的有效数据增强思路。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [80] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 本文针对基于深度学习的图像编辑，提出了提高文本驱动控制精度和一致性的方法，并开发了新的框架。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑借助了稳定扩散模型和Cross-attention以便文本驱动控制，极大简化了流程，但带来图像一致性差等新问题，如头发颜色编辑时不一致。为提升编辑精度和可靠性，亟需针对方法和超参数进一步优化。

Method: 1) 对“word swap”方法做了全面实验；2) 提出“attention re-weight method”以增强模型适应性；3) 提出新框架“CL P2P”以解决周期不一致问题，并系统分析了超参数和神经网络架构（尤其attention机制）之间的关系。

Result: 实验表明提出的方法能提升prompt-to-prompt图像编辑的精度、一致性和最终效果，特别是在复杂指令和高一致性要求场景表现更佳。

Conclusion: 本研究加深了对超参数设置与attention机制模型架构在生成图像质量中的作用理解，并为后续精细化、可靠的DL图像编辑工具开发提供了理论与方法支持。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [81] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: 本文提出了GUI-Spotlight模型，通过动态调用多种专用工具提升视图定位准确性，在GUI系统中实现更精确的操作。该方法在仅使用少量训练样本的情况下取得了高于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型推动了GUI系统向现实世界复杂环境的应用，但视觉定位的不可靠性限制了其实用性，特别是在精细的指针级操作（如点击与拖动）方面。因此，需要提升系统对屏幕元素的精确定位能力。

Method: 提出GUI-Spotlight模型，通过训练使其具备图像为基础的推理能力。该模型可以动态调用多种专门工具，逐步收缩其关注区域，进而锁定屏幕中的相关区域。这一做法大幅提高了视觉定位（visual grounding）的准确性。

Result: 在ScreenSpot-Pro基准测试中，GUI-Spotlight仅用1.85万训练样本就达到了52.8%的准确率，超过了V2P-7B（用960万样本，达50.6%准确率）和GTA-1-7B（用156万样本，达50.1%准确率）。

Conclusion: GUI-Spotlight通过更高效地利用训练样本，提高了视图定位准确性，为多模态大模型在实际GUI系统中的应用带来了更大可靠性和实用性。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [82] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 本文针对神经网络模型的后训练量化提出了一种新的范围估计方法，通过优化每一层的量化误差来提升低比特量化下的精度。该方法在主流分类模型上显著提升了低比特（如4位、6位、8位）量化的准确率。


<details>
  <summary>Details</summary>
Motivation: 如何在不牺牲模型精度的前提下，通过低比特量化技术大幅减少神经网络模型存储成本，是模型落地部署的重要挑战。现有后训练量化方法在低比特时面临明显的性能损失。

Method: 将每一层参数的量化范围估计建模为量化误差的最小化优化问题，并证明其为局部凸问题。提出高效的搜索算法寻找最优解，并将其应用于变换后的权重空间以进一步提升量化效果。

Result: 实验表明，该方法在ResNet和Inception-v3等主流分类模型上的8位、6位量化几乎无精度损失，4位量化的精度也大幅优于现有方法，整体性能达到甚至超过state-of-the-art。

Conclusion: 提出的范围估计与搜索方法极大改善了低比特后训练量化方案的性能，为神经网络高效落地提供了新方案。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [83] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: 本文提出了MetaFind，一个面向元宇宙场景生成的三模态（文本、图片、3D）组合式检索框架，通过从大规模3D资产库中检索符合场景需求的资产来提升生成效果。该方法在空间、语义与风格一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的3D资产检索多数依赖通用3D形状表示模型，缺乏针对3D场景生成中特定空间、语义与风格约束的检索范式，导致检索结果与场景不协调。

Method: MetaFind支持文本、图片和3D三种模态任意组合的检索方式，通过联合建模对象级特征（如外观）和场景布局结构，增强空间推理与风格一致性。其核心方法ESSGNN布局编码器可捕捉空间关系与对象外观，无惧坐标变换。此外，系统支持迭代式检索，实时适应场景变化。

Result: 实验证明MetaFind在3D资产检索任务中的空间一致性与风格一致性显著优于基线方法。

Conclusion: MetaFind为3D场景生成带来了灵活且精确的资产检索方式，提升了元宇宙等虚拟场景的构建效率与生成质量。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [84] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 本文提出了一种改进的损失函数，融合了太阳耀斑预测中类别间的序关系，以减少二分类模型在阈值附近的误判。


<details>
  <summary>Details</summary>
Motivation: 传统将太阳耀斑预测视作二分类任务，但忽视了分类内部子类别间的序关系，导致模型最常见的错误出现在阈值附近。作者希望通过增强模型对序关系的感知，提升预测准确率。

Method: 作者提出在传统的二元交叉熵损失（BCE）基础上，增加对类别子级间序关系的加权惩罚项，使模型对靠近分类界限（阈值）的预测错误受到更大惩罚，从而引导模型更好地区分强度接近的事件。

Result: 序权重损失函数在实验中有效减少了阈值附近的误判，提高了模型整体表现。

Conclusion: 通过在损失函数中引入序关系权重，能够更合理地利用数据的本身特性，缓解近阈值误判问题，增强模型性能。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [85] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为QuantDemoire的去摩尔纹图像模型量化框架，有效减少模型体量和计算消耗，同时保持图像质量，并显著优于现有量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习去摩尔纹方法效果好但资源消耗大，不适合边缘设备。而直接应用现有量化方法会显著降低去摩尔纹性能，主要由于分布异常值和光滑区域表征能力减弱。

Method: 提出QuantDemoire后训练量化框架。核心是：1）引入异常值感知量化器，通过采样估算范围减少激活异常值，并将少部分极端权重用FP16表示以降低误差；2）设计频率感知校准策略，在微调时重点强化低、中频分量，减轻低比特量化下的条带伪影。

Result: 大量实验证明，QuantDemoire在参数量和计算量大幅下降的同时，输出图像质量几乎不变，并在W4A4量化配置下比现有量化方法高出4dB。

Conclusion: QuantDemoire针对去摩尔纹模型量化的挑战，提出有效的解决方案，实现了边缘设备友好的高质量去摩尔纹处理，相关方法和代码已开源。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [86] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 本文提出了TV-LoRA方法，结合扩散生成模型与多重正则化约束，实现了在极低剂量稀疏采样CT重建中的高质量、高效率；在多个数据集上超越主流基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有低剂量稀疏采样CT重建方法在采样极其稀疏时容易出现伪影、纹理缺失，并且很难兼顾高效推理与重建质量。因此，亟需创新性地结合生成模型与多种正则化约束来提升重建性能及效率。

Method: 提出TV-LoRA方法：结合扩散生成先验（NCSN++基于SDE建模）和多重正则化（各向异性TV与核范数LoRA），在ADMM框架下优化。采用2D切片、FFT加速与张量并行计算增强推理效率。

Result: 在AAPM-2016、CTHD和LIDC数据集，极低投影视角（N_view=8,4,2）下，TV-LoRA在结构相似度、纹理恢复、边缘清晰度及伪影抑制等指标均优于主流方法，且具良好鲁棒性和泛化能力。消融实验验证了LoRA正则化与扩散先验的互补性，FFT-PCG模块显著加速推理过程。

Conclusion: TV-LoRA能实现高保真、高效率的3D低剂量稀疏采样CT重建，具广泛临床应用前景。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [87] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 本文提出了评估拓扑地图领域系统的新指标和数据集，分析并公开了应对感知混淆挑战的基准方案。


<details>
  <summary>Details</summary>
Motivation: 拓扑地图在导航中的作用突出，但目前缺乏统一的评测标准和公开数据集，导致不同方法难以公平对比。此外，感知混淆（perceptual aliasing）这一核心难题未被充分量化，严重影响现有系统性能。

Method: (1) 形式化定义拓扑一致性，将定位精度作为高效且易于解释的替代评估指标；(2) 提出首个用于量化数据集模糊度（ambiquity）的指标，实现跨环境的公平对比；(3) 构建包含不同模糊度水平的多样化基准数据集，并实现、公开深度学习基线系统，与传统方法共同评测。

Result: 通过实验与分析，揭示了现有拓扑映射方法在感知混淆条件下的局限性，并验证了提出的新评测指标和数据集的有效性。

Conclusion: 本文提出的开放标准、基准数据集和评测工具，有助于推动拓扑地图领域公平、可复现的系统研究，并为解决感知混淆提供了新思路。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [88] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文关注事件相机下的MeshFlow估计问题，提出了高分辨率数据集和高效网络，在多个任务上达到出色表现并显著提升速度与性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在动态场景下有独特优势，但事件MeshFlow估计缺乏专门数据集和方法，尤其是在高分辨率与数据密度变化方面研究不足。

Method: 1）构建了高分辨率事件MeshFlow数据集（HREM/HREM+），提供光流及MeshFlow标签，且支持多种密度；2）提出轻量级EEMFlow网络（编码器-解码器结构），并加入CDC模块提升细节修复能力；3）进一步提出自适应密度模块ADM，针对不同密度数据自调整输入事件密度。

Result: EEMFlow模型在主流数据集上较新方法实现30倍速度提升，并在多项指标上达最好表现。ADM模块能使EEMFlow及其升级版EEMFlow+性能提升8%-10%。

Conclusion: 本文为事件MeshFlow估计领域提供了高质量数据集与高效模型，并提出了提升鲁棒性的新方法，在性能和泛化性上实现了突破，推动了事件相机低延迟动态视觉任务的发展。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [89] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的6D对象姿态估计扩散模型，显著提升了训练速度和准确率，并取消了繁琐的评估网络，实现了高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的6D物体姿态估计方法存在训练收敛慢、依赖额外评估网络等问题，影响了模型实用性和效率。

Method: 提出了两个关键技术：1. 首先采用姿态回归头对编码器进行预训练，并联合回归头与去噪扩散头进行联合训练，加速收敛并提升准确率；2. 设计了基于时间相关分数缩放的采样引导机制，有效平衡探索与利用，无需额外的评估网络。

Result: 在REAL275、HouseCat6D、ROPE等多个标准数据集上进行了大量实验，提出的方法在单次姿态推理下实现了SOTA的准确率，并在训练和推理阶段展现出更高的效率。

Conclusion: 新提出的6D姿态扩散估计方法兼具准确性与高效性，能够省去额外评估环节，在实际应用中展现出更好的性能与鲁棒性。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [90] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本文关注多模态大语言模型（MLLMs）蒸馏过程中由多个教师模型产生的推理轨迹概念漂移问题，提出了一种自主偏好优化方法，有效提升了学生模型在一致性、鲁棒性和泛化性上的表现。


<details>
  <summary>Details</summary>
Motivation: 在知识蒸馏中，多个教师模型由于推理分布随时间动态变化（概念漂移），会无意中将偏差传递给学生模型，导致其性能下降。这一问题在多模态大模型知识蒸馏领域长期被忽视。

Method: 作者建立了“概念漂移与知识蒸馏”的理论联系，将多教师模型的非平稳推理动态建模为多流推理轨迹的预测问题。提出“学习、比较、批判”的范式，其中自主偏好优化（APO）让学生模型先比较、学习多个教师模型，再进行批判性反思和概念对齐。

Result: 大量实验证明，在知识蒸馏任务中，所提方法提升了学生模型的一致性、鲁棒性和泛化能力。

Conclusion: 自主偏好优化及相关方法有效缓解了多教师模型概念漂移带来的负面影响，为多模态大语言模型蒸馏提供了新思路，同时贡献了大规模数据集和相关工具。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [91] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: 提出了SiteShield，一个多模态、基于LVLM和RAG的自动化建筑安全检查报告系统，集成视觉和语音输入，提升了安全报告生成的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查耗时低效，需要处理大量信息。现有LVLM应用存在回应不相关、输入受限和虚假信息等问题，且LLM受训练数据以及实时适应能力限制，急需更高效、智能的自动化工具。

Method: 提出SiteShield，结合多模态LVLM与检索增强生成（RAG）框架，能处理视觉与音频输入，实现自动化生成建筑安全报告。方法在真实世界数据集上评估，与传统单模态LLM无RAG系统进行对比。

Result: SiteShield在实际数据集上表现优异，F1分数0.82，hamming损失0.04，精确率0.76，召回率0.96，效果显著优于无RAG的单模态LLM。

Conclusion: SiteShield有效提升了建筑安全报告生成的检索能力和效率，为自动化安全检查提供了创新的技术路径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [92] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: 本文提出了BLADE，一种无需先验偏见知识或冲突样本的生成式去偏框架，在多个数据集上显著提升了神经网络的泛化能力和鲁棒性，尤其在CIFAR-10偏见最严重分组下提升幅度达18%。


<details>
  <summary>Details</summary>
Motivation: 神经网络在实际应用时容易学习到隐含偏见，即依赖训练数据中与目标标签无关的表面属性，从而影响泛化能力。现有去偏方法通常需要已知偏见信息或特定类型样本，但这些假设在现实中难以保证。本文动机是解决这一普遍但实际难题。

Method: BLADE通过以下步骤实现去偏：1）训练生成模型跨偏见域转换图像，保持与任务相关的特征不变；2）根据每张图像受偏见影响程度，自适应地用其生成的偏见域翻译图像进行精炼；3）在表示学习阶段，将原图与不同偏见但相同任务特征的生成对齐，并与相同偏见样本错开。整个方法无需先验知识或偏见冲突样本。

Result: 在多个基准数据集上，BLADE显著优于现有最先进方法。在腐蚀CIFAR-10数据集最不利分组下，BLADE比最近的最佳方法提升了约18%，确立了新的去偏基线。

Conclusion: BLADE实现了无需显式监督的端到端去偏，对开发更鲁棒的深度学习模型具有重要意义。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [93] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习框架SEG-MIL-CBM，实现了无需昂贵概念标注且具备空间归因能力的解释性视觉模型。通过对图像分割区域的多实例学习，该方法提升了模型的鲁棒性和解释性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然在计算机视觉中取得巨大成功，但其“黑盒”特性限制了解释性，尤其是在安全关键领域（如医疗、交通），难以建立信任。而传统的解释性模型（如CBM）需要大量昂贵的标注且缺乏区域归因能力。本文旨在解决无需标注、提升透明度和空间解释性的实际难题。

Method: 提出SEG-MIL-CBM，将概念引导的图像分割与基于关注（Attention）的多实例学习（MIL）框架结合。具体做法是把每个分割区域作为一个独立实例进行特征提取和聚合，模型能够自动在空间区域之间加权，最终输出受概念约束的区域解释，无需人工标注概念或分组。

Result: 该方法在包含伪相关（比如背景对标签的影响）、输入扰动（如视觉质量下降）、大规模数据集等各种实际场景下，取得了鲁棒性和性能提升，并能生成基于高层概念的空间解释。

Conclusion: SEG-MIL-CBM提升了深度视觉模型的解释性与鲁棒性，实现了无需额外标注下的概念解释和区域归因，为安全关键行业提供更加可信的AI决策基础。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [94] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对Diffusion Transformer模型推理过程加速的新方法HyCa，通过更智能的特征缓存策略，实现了在不需要重新训练的情况下的大幅加速。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers在生成图像和视频时表现优异，但其逐步采样过程计算成本高，主要瓶颈为每步都需Transformer前向计算。现有的加速方法多忽略了特征的动态异质性，采用统一的缓存策略，效率提升有限。

Method: 作者将模型中的隐藏特征变化视作各维度ODE（常微分方程）的混合，提出了HyCa，这是一种受混合ODE求解器启发的缓存框架，可以按照特征维度采用不同的缓存和预测策略。

Result: HyCa在多种主流Diffusion Transformer模型与任务上实验，均未需重新训练，且几乎无损失精度：如在FLUX上加速5.55倍，在HunyuanVideo上加速5.56倍，在Qwen-Image及Qwen-Image-Edit上加速6.24倍。

Conclusion: HyCa可广泛适用于主流扩散变换器模型，能够以近乎无损精度和无需重新训练的前提下大幅提升采样效率，对高效大模型推理具有重要意义。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [95] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为World-To-Image的新框架，可以通过利用智能体搜索真实世界知识，显著提升文本到图像（T2I）模型在处理新颖或分布外实体时的生成表现。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在面对未见或分布外实体时，因知识截止问题导致生成能力显著下降。因此，如何让T2I模型及时获取和利用最新的世界知识，是提升其实用性和泛化能力的关键难题。

Method: 该框架设计了一个智能体，能动态地在网络中搜索并检索与输入文本相关但模型未知的新概念的图像。随后，这些检索到的多模态信息会被用于优化提示，进而引导强大的生成主干网络更准确地合成图像。评估方式除传统指标外，还引入LLMGrader和ImageReward等现代指标，重点评价语义一致性。

Result: 实验结果表明，World-To-Image在语义一致性和视觉美感两个方面都远超现有SOTA方法。在作者自建的NICE基准上，准确性提升8.1%。同时，这一优化过程的效率也很高，通常少于三次迭代即可完成。

Conclusion: World-To-Image框架显著提升了T2I模型对新颖和真实世界概念的表达能力，在效率和生成质量上都达到了新的高度，有望应用于需要反映快速变化现实世界的场景。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [96] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了MASC（Manifold-Aligned Semantic Clustering）方法，通过引入层次化的语义结构，有效提升了自回归图像生成模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归图像生成模型使用的大词表结构平坦，未能利用token嵌入空间中的内在结构，这导致训练和生成任务复杂、效率低下。

Method: 提出MASC框架，依据token嵌入空间的几何特性，通过带几何感知的距离度量和密度驱动的层次聚类，自动构建token的语义树，将原本平坦的高维预测任务转化为结构化的层级任务。该模块可以直接应用于现有模型（plug-and-play）。

Result: MASC方法能加快训练速度高达57%，并以LlamaGen-XL为例，把生成质量指标FID从2.87降到2.58，表现明显增强。

Conclusion: 论文验证了通过结构化预测空间可显著提升自回归生成模型的表现，MASC使AR方法接近甚至超越最前沿的生成模型，强调了预测空间结构创新的重要性。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [97] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本文提出ZoomIn，一个模仿人类视觉检查流程的双阶段AI图像取证框架，用以更有效地检测高质量合成图像及解释检测结果。作者还构建了MagniFake数据集，显著提升了检测准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着AI合成图像迅速发展，真假内容界限日益模糊，数字信息完整性面临威胁。现有视觉-语言模型虽有解释能力，但难以识别高质量伪造图像中的细微伪迹，急需提升检测准确性和可解释性。

Method: 作者提出ZoomIn取证框架：分两步，先粗略扫描找出可疑区域，再聚焦这些‘放大’区做详细分析，并给出基于视觉证据的解释。为训练此框架，作者还构建了MagniFake数据集，包含2万张标注合成和真实图，并有伪造区域边框和文本解释，数据采用自动化VLM流程生成。

Result: ZoomIn框架的检测准确率达到96.39%，并在不同数据分布下表现出较强泛化能力，能够输出基于视觉证据、易于理解的检测解释。

Conclusion: ZoomIn框架在保证高检测准确率的同时，实现了可解释性与强泛化，迈出了解决AI图像取证困境的重要一步，为数字内容真实性保驾护航。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [98] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 本文提出了一种简单、端到端可训练的图像配准算法，仅需少量训练数据和代码即可实现，并在某些场景中取得了较高的配准精度。


<details>
  <summary>Details</summary>
Motivation: 图像配准通常要求复杂算法和大量数据与算力。为降低实现难度、数据需求和代码复杂度，作者希望提出一种高效、便捷的新方法。

Method: 作者开发了一种简单的可端到端训练的配准算法，并以少量Python代码实现。用非常有限的数据训练（如仅74张图片），在如立体视觉配准等任务中进行实验。

Result: 在输入仅为19x15窗口、训练集仅74张图片的情况下，算法实现了较高的配准精度，同时训练和实现都很高效。

Conclusion: 该方法适合对训练数据、时间和代码复杂度有严格限制的场景，可为相关应用的研究与实践提供简明有效的技术基础。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [99] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出ArConv新型卷积层，开发轻量高效神经网络模型，在眼病检测任务上优于MobileNetV2。


<details>
  <summary>Details</summary>
Motivation: 目前深度卷积神经网络在眼病等医学图像检测场景中应用广泛，然而现有方法大多计算复杂，不易在资源受限设备如手机上部署。提升模型在移动端可达性和实时性非常关键。

Method: 作者从卷积层设计入手，创新性提出名为ArConv的新型卷积层，优化计算效率，实现了一种参数精简、易部署的新型神经网络模型，对已有眼科数据集RfMiD进行了实验对比。

Result: 新模型参数量仅为1.3M，显著低于MobileNetV2的2.2M；在RfMiD测试集上准确率达到0.9328，优于MobileNetV2的0.9266。

Conclusion: ArConv层可有效提升轻量模型的准确率和可部署性，适合移动端眼病检测等实际应用，有望推动智能医学诊断普及。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [100] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: 本文提出了Kaleido，一种面向高质量照片级神经渲染的生成模型，可以统一实现对象级和场景级渲染。其核心创新在于无需显式3D表达即可进行多视角生成，并可利用大规模视频预训练提升性能。Kaleido在多个基准上创下新纪录，尤其在少视角条件下零样本推理表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经生成模型，大多需要显式的三维表示或依赖稀缺的、有标签的3D数据集，难以大规模扩展和泛化，也难以兼顾对象级与场景级渲染需求。研究者希望构建一个统一且端到端高效的生成框架，能够从丰富的视频数据中学习，从而提升生成质量与泛用性。

Method: 作者将3D视为视频的一个特定子域，将生成问题建模为序列到序列的图像合成。模型采用一种新的decoder-only rectified flow transformer架构，无需显式3D建模，通过mask自回归方式在任意数量参考视图条件下生成任意数量的目标6-DoF视图。此外，通过大规模视频数据进行预训练，增强了模型空间一致性，降低了对相机标注3D数据的依赖。

Result: Kaleido在多项视角合成基准任务上取得了最高性能，尤其在few-view条件下零样本任务表现远胜现有生成方法，在many-view任务下首次达到与按场景优化方法相当的生成质量。

Conclusion: Kaleido证明了在无需显式三维表示条件下，依托统一生成架构和大规模视频预训练，能够显著提升三维和视频建模的效果，有望推动神经渲染领域的发展。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [101] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种针对对比增强MRI肝脏分割的紧凑型框架（CoSSeg-TTA），利用半监督方法和风格迁移技术，在标注有限和跨中心情况下显著提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 在医学影像，特别是对比增强MRI中进行肝脏分割对于诊断和治疗非常关键。但受限于标注数据稀缺、不同扫描仪和机构之间的协议差异，导致分割任务存在显著域偏移，现有框架在该场景下效果有限。

Method: 作者基于nnU-Netv2，提出CoSSeg-TTA方法，整合半监督mean teacher策略以利用无标注数据；引入基于随机直方图的风格迁移和可训练的对比敏感模块以增强域适应能力；推理阶段采用持续测试时自适应，提高对未知域的鲁棒性。

Result: 在多项实验中，该方法在Dice分数和Hausdorff距离指标上显著优于nnU-Netv2基线，并且在低标注和跨中心场景下展现出较强泛化能力。

Conclusion: CoSSeg-TTA能有效缓解单模态MRI肝脏分割中的数据和域泛化挑战，并在实际临床应用和跨中心推广方面具备潜力。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [102] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文提出了一种不依赖补丁大小和位置信息的对抗补丁防御方法，通过基于概念的解释手段识别和抑制最有影响力的概念激活向量，实现对补丁攻击的防护，并在实验中超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 对抗补丁攻击在实际环境中对深度学习模型安全性造成重大威胁，现有防御方法依赖补丁的先验知识，适用性有限。为提升防御的泛化能力和实际应用价值，需提出无需补丁先验的防御手段。

Method: 提出了一种基于概念解释的防御方法，识别出受补丁影响最明显的概念激活向量并对其进行抑制，无需检测补丁的具体位置和大小。

Result: 在Imagenette数据集和ResNet-50模型上进行评测，该方法无论在对抗鲁棒性还是干净准确率上均优于现有的PatchCleanser方法，并且在不同补丁大小与位置下效果依然稳健。

Conclusion: 将可解释性与鲁棒性结合，通过概念驱动的防御为抵御对抗补丁攻击提供了可扩展和有效的新策略，对未来安全机器学习模型设计具有重要意义。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [103] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: 提出了一种新的Seq-VPR方法（Adapt-STformer），通过创新的循环可变形Transformer编码器实现了更高性能和更好的灵活性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列视觉定位方法在实际应用中缺乏灵活性和效率：不能灵活支持不同长度的视频序列，推理速度慢，内存消耗高，难以满足实时场景需求。因此，研究者希望设计出既高效、又灵活的Seq-VPR模型。

Method: 提出了Adapt-STformer方法，核心为新颖的循环可变形Transformer编码器（Recurrent Deformable Transformer Encoder, Recurrent-DTE），通过迭代循环机制聚合多帧时序信息，实现对变长序列的灵活支持、高速度推理及低内存占用。

Result: 在Nordland、Oxford、NuScenes等数据集上实验显示，Adapt-STformer相比第二优方法召回率提高最多17%，序列特征提取时间减少36%，内存占用降低35%。

Conclusion: Adapt-STformer实现了在灵活性、效率和性能上的兼得，适合实时视觉定位应用，优于当前主流的Transformer Seq-VPR方法。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [104] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: 本文提出了ChronoEdit方法，将图像编辑问题转化为视频生成，从而提升编辑后的物理一致性。通过利用视频生成模型和时序推理，ChronoEdit能够生成既真实又物理合理的图像编辑结果，并在新提出的PBench-Edit基准上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型生成模型虽然在图像编辑和上下文生成方面取得了进展，但在编辑对象的物理一致性（如物体变化的连贯性）方面依然存在缺陷。尤其对于需要世界模拟等任务来说，这种能力至关重要。作者因此提出新方法以弥补这一空白。

Method: ChronoEdit将输入和编辑后的图像视为视频的首尾帧，利用预训练大规模视频生成模型学习到的时序一致性与隐式物理规律进行中间帧生成。引入‘时序推理阶段’：在推理时同时对目标帧与推理token去噪，得出物理可行的编辑轨迹，早期借助推理token，后续逐步丢弃从而降低计算成本。

Result: 作者构建了新的PBench-Edit基准，对物理一致性要求高的图像-提示对进行验证。ChronoEdit在视觉保真度和物理合理性方面均显著优于现有方法。

Conclusion: ChronoEdit将图像编辑转化为视频生成问题，在确保编辑物体物理一致性的同时保持高视觉质量，在挑战性的新基准上实现了最优表现。代码和模型也将开源，便于社区验证和改进。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [105] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: 本文介绍了CARE-PD，这是目前最大的公开帕金森病（PD）3D步态数据集，涵盖来自8家临床中心的9个队列。该数据集支持基于3D网格的步态分析基准，促进临床评分预测和动作重建任务，并已公开用于学术研究。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的步态定量分析因缺乏大规模、多样化和带有临床标注的运动数据而受限，阻碍了机器学习在PD运动评估中的应用。

Method: 收集多中心PD步态影像（RGB视频、动作捕捉），通过统一预处理转为匿名化SMPL 3D人体网格，建立多任务测试基准，包括监督式临床评分预测和无监督动作重建任务，并设计多种泛化协议评估方法。

Result: 实验对比最新动作编码器和传统手工特征，发现编码器在临床预测任务中表现更佳。预训练数据能显著降低3D坐标误差（MPJPE从60.8mm降至7.5mm），且PD严重程度的宏F1提升17个百分点。

Conclusion: CARE-PD数据集和基准极大推动了精确PD步态分析的发展，验证了多样、临床校正数据对AI训练的重要价值，全套数据与代码已对学术界开放。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [106] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自回归框架GenAR，可直接从H&E染色图像预测空间转录组的基因表达，并显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 空间转录组虽然可精确测量空间基因表达，但实验成本高昂，而H&E染色图像广泛易得。如果能用图像低成本准确预测基因表达，可大幅降低分子测序成本。但现有方法忽视了基因间共表达关系，且一般采用连续回归拟合离散的基因计数，导致输出不合理。

Method: 提出GenAR算法：1）多尺度层次聚类基因暴露共表达结构，2）自回归式预测基因表达的离散计数（非连续回归），3）融合病理和空间特征条件预测。模型逐步从粗到细细化预测，有效建模基因间依赖。

Result: 在四种不同组织类型的空间转录组数据集上，GenAR显著优于现有方法。信息论分析也证实新方法能消除传统回归带来的偏差。

Conclusion: GenAR可高效低成本从H&E图像推断空间基因表达，有助于精密医疗及分子诊断普及。代码已发布，便于应用与复现。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [107] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文提出了Diffusion^2框架，在行人轨迹可观测数据极少时（即“刹那轨迹”）依然能精确预测未来轨迹，显著提升了交通安全场景中的行人运动预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法通常依赖于充足的历史观测数据，但现实交通中行人可能突然出现，仅有极少可用数据，使得传统方法难以精确预测，带来安全风险。亟需针对极端场景下的短时轨迹进行预测的有效方法。

Method: 提出Diffusion^2框架，包含两个级联的扩散模型：第一个反向预测历史未观测轨迹，第二个在生成的历史基础上正向预测未来轨迹。为应对生成历史带来的不确定性，设计了双头参数化机制估计随意性不确定性，及时间自适应噪声模块动态调整前向扩散噪声。

Result: 在ETH/UCY与Stanford Drone数据集上的实验表明，Diffusion^2在刹那行人轨迹预测任务中取得了新的最优结果。

Conclusion: Diffusion^2能有效提升极短轨迹观测下的行人未来轨迹预测准确率，有助于解决现实自动驾驶等场景下的行人安全问题。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [108] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本文提出了MorphoSim，一种能够基于自然语言生成可编辑、可控4D动态场景的模型，支持多视角一致性和对象级操作，从而为机器人等领域提供高质量、灵活的环境模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成视频模型虽然能模拟真实动态，但局限于2D视图且互动性差，难以满足机器人训练和评估对环境可控性、可编辑性的需求。因此，需要一种能够生成真实、可交互、可编辑的4D（空间+时间）动态场景的模型。

Method: 提出MorphoSim框架：用户用自然语言描述场景和动作，系统利用轨迹引导生成结合特征场蒸馏技术，生成具有多视角一致性和对象级控制能力的4D动态场景。用户可以在无需重新生成全场景的情况下进行交互式编辑（如指引、重着色、移除对象等），并从任意视角观察场景。

Result: 实验显示，MorphoSim能在保持高度场景保真度的同时，实现对象的可控与可编辑特性。

Conclusion: MorphoSim框架具备生成高质量、可控、可编辑4D动态场景的能力，为机器人等应用提供了强大的训练与评估环境。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [109] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为VLMCountBench的新基准，专门测试视觉-语言模型（VLMs）在基础几何图形上计数的能力，发现主流VLM在复杂计数任务下表现有限。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在多种视觉任务上表现出色，但其在基础计数能力上的表现尚未被系统检验。本研究旨在回答VLMs能否正确计数这一基础能力。

Method: 作者构建了只包含基础几何图形（如三角形、圆形）及其组合的VLMCountBench基准，通过严格控制变量（如颜色、尺寸、提示词等），系统性地研究这些因素对计数任务的影响。

Result: 实验结果显示，VLMs在只有一种图形的计数任务中表现良好，但在多种图形组合下（组合计数）则出现显著失败。

Conclusion: 当前主流VLM存在基础计数能力的局限，尤其是在组合计数场景下。该发现为未来提升VLM基础推理能力指明了重要研究方向。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [110] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: 本文提出一种新的无参考人脸修复方法——CodeFormer++，能在提升图像视觉质量的同时更好地保留人脸身份信息，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式无参考人脸修复方法在提升图像细节和保持人脸身份一致性时存在权衡，难以兼得高视觉质量和身份保真度，因此需要更优解决方案。

Method: 作者将无参考人脸修复分解为三步：1) 保身份修复；2) 高质量人脸生成；3) 动态融合身份特征与真实感纹理。具体创新包括：基于学习的可形变人脸配准模块对齐生成与修复结果；纹理引导修复网络动态抽取并迁移生成结果的纹理细节；融合深度度量学习以提升身份与生成特征的结合。

Result: 在多个真实与合成数据集上，CodeFormer++在人脸修复的视觉质量和身份一致性方面均超越现有方法，达到当前最优水平。

Conclusion: CodeFormer++有效解决了以往方法视觉质量与身份保持难以兼得的问题，实现了高质量且身份一致的人脸图像修复，为生成式无参考人脸修复提供了新的高效范式。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [111] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为A.I.R.的训练无关帧选择方法，在提升视频问答（VideoQA）准确性的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧选择方法在视频问答任务中难以兼顾效率与准确性。传统基于相似度的轻量方法难以捕捉复杂问题的深层语义，导致帧选择不准；而利用大型视觉-语言模型（VLM）的方法虽然准确，但计算成本极高，难以推广。

Method: 作者提出A.I.R.方法，通过强大的VLM对复杂问题进行深层语义分析，并采用自适应、迭代方式分批处理高潜力帧，避免对全部视频帧进行处理，从而在保证准确性的同时大幅降低计算资源消耗。该方法完全无需训练。

Result: 大量实验表明，A.I.R.方法在多个视频问答基准数据集上，相较于已有帧选择方法和VLM基线方法，在准确率与计算效率方面均有显著提升。

Conclusion: A.I.R.为在视频问答任务中高效且准确地选取帧提供了新思路，无需训练即可适用于自监督或无监督场景，有望推动基于视觉-语言模型的视频理解研究与应用。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [112] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出了一种名为reAR的新型训练策略，通过引入逐token正则化目标，显著提升了视觉自回归（AR）生成模型的性能，使其能够追平甚至超过更先进的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型有望实现视觉与语言模型的统一，但在实际性能上不如扩散模型。以往研究多将性能落后的原因归结于tokenizer和光栅化顺序，但本文从生成器-分词器不一致性角度识别了更深层次的瓶颈。

Method: 提出了reAR训练方法。在预测下一个token时，除了常规自回归目标外，还让模型学习恢复当前token的可视化embedding，以及在噪声环境下预测目标token的embedding。该方法无需更改分词器、生成顺序、推理流程或引入额外外部模型。

Result: 在ImageNet数据集上，reAR大幅降低了gFID（3.02降至1.86），提升了IS（达到316.9），当与先进分词器结合时，在仅使用177M参数的情况下获得了1.42的gFID，性能可与更大规模（675M参数）的扩散模型媲美。

Conclusion: reAR方法无需结构调整即可明显提升视觉AR生成模型的性能，为视觉/语言统一提供了简洁、高效的策略，可推动视觉自回归模型在实际视觉生成任务中的应用。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [113] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: 本文提出了一种新的伪装目标检测网络SPEGNet，通过统一的设计有效融合多尺度特征，提高边界检测和整体一致性，并具备实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法为提升性能，常常不断叠加复杂组件（如边界模块、注意力机制、多尺度处理器），导致推理负担增加，且细节信息因为降采样被损失，影响检测效果。

Method: SPEGNet采用统一的架构，将通道校准与空间增强结合，实现多尺度特征的有效融合。边界信息直接从富含上下文的表征中获得，保证了语义和空间信息对齐，通过逐步细化的尺度自适应边缘调制，有效在不同分辨率下兼顾边界精度和区域一致性。

Result: SPEGNet在CAMO、COD10K和NC4K伪装目标检测数据集上分别取得了0.887、0.890和0.895的$S_\alpha$分数，并实现了实时推理速度，在处理细小复杂目标和大范围相似目标、遮挡及模糊边界场景下均有优异表现。

Conclusion: SPEGNet兼顾了检测精度和效率，通过结构创新减少了依赖复杂组件的积累，提升了在多尺度、遮挡、边界模糊等复杂场景下的伪装目标检测能力。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [114] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: 本文提出了MedCLM，一种自动化流程，将医学检测数据集转化为具备推理链条（CoT）的医学视觉问答（VQA）数据，实现模型在多个VQA基准上的领先表现。


<details>
  <summary>Details</summary>
Motivation: 临床诊断推理与AI之间的桥接在医学影像领域仍是核心难题，亟需开发能更好模拟临床推理过程的多模态模型。

Method: 通过MedCLM流程，将检测数据集中的病变框与器官分割及结构化理由相联系，自动生成具备推理链条的VQA问答对，并引入分阶段的CoT-课程学习策略，包括易（显式病变定位）、中（隐式病变定位）、难（弱监督推理）三个难度阶段。

Result: 实验验证MedCLM在多个医学VQA基准任务上实现了最新最优性能。

Conclusion: MedCLM为开发临床推理对齐的医学视觉-语言模型提供了可扩展的解决方案，有助于提升AI系统的医疗诊断推理能力。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [115] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了VaseVQA-3D，这是首个用于分析古希腊陶罐的三维视觉问答数据集，并研发了针对性视觉语言模型VaseVLM。该方法显著提升了3D陶罐识别与理解能力，有助于文化遗产数字保存。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在特定文化遗产领域面临数据稀缺和领域知识不足，尤其对3D文物（如陶罐）分析表现欠佳。缺乏有针对性的数据集和适配模型，限制了其在数字文化遗产保护上的应用。

Method: 作者构建了VaseVQA-3D数据集，包含664个古希腊陶罐3D模型及问答数据，并搭建了完整的数据构建流程。此外，提出了VaseVLM模型，通过领域自适应训练增强了模型在陶罐分析上的表现。

Result: 实验结果显示，所提方法在VaseVQA-3D数据集上，R@1指标提升了12.8%，词汇相似度提升6.6%，大幅超越了以往的最优方法。

Conclusion: 本研究显著提升了3D陶罐等文化遗产的识别与理解，通过数据集和模型创新为数字文化遗产保护提供了新方向。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [116] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: 该论文提出了一种专为电商场景设计的图片编辑模型TBStar-Edit，通过数据工程、分层模型架构和两阶段训练策略，实现了在保持商品视觉一致性的同时，精准高保真地编辑图片，并在电商基准测试中优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 通用领域的图像生成与编辑模型在电商场景中使用时会遇到一致性问题，难以保证商品外观和布局的完整性，影响实际应用效果，因此需开发专为电商设计的编辑模型。

Method: 作者构建了全面的数据处理管道以获得高质量编辑数据；设计了包含基础模型、样式转移和一致性增强模块的分层架构；采用两阶段训练策略，分别针对样式转移和一致性增强训练不同模块，以提升模型一致性。

Result: 在自建的电商测试集上，TBStar-Edit在VIE分数（客观指标）和用户偏好（主观指标）方面均明显优于现有通用领域的图像编辑模型。

Conclusion: TBStar-Edit有效解决了电商图像编辑中一致性差的问题，实现了兼顾精确编辑及商品外观完整性的高性能，能更好地服务电商实际需求。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [117] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 该论文提出了一种异步扩散模型（Asynchronous Diffusion Models），通过动态分配像素去噪时间步来提升文本与图像的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本驱动的图像生成中，无法很好地让生成的图像内容与输入文本提示精准对应，主要原因是所有像素同步降噪，导致与提示词相关区域无法获得足够的上下文信息。

Method: 作者提出将传统的同步像素去噪过程改为异步进行，对不同像素分配不同的去噪步数。与文本提示相关的区域采用更缓慢的去噪节奏，使其能够利用更清晰的像素间上下文信息，从而实现更好的对齐效果。

Result: 实验表明，该异步扩散模型在多种类型的文本提示下都能显著提升生成图像的文本对齐度。

Conclusion: 异步扩散模型能够更好地结合文本提示信息，改善传统同步扩散模型在文本到图像对齐方面的不足。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [118] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 本文提出了一种高效的扩散模型采样引导方法——切向增强引导(TAG)，无需修改模型结构即可提升生成图像质量并减少语义不一致。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在图像生成任务上表现优异，但经常出现语义不一致或虚构内容。现有的推理时引导技术往往需借助外部信号或模型结构改动，造成计算开销增加，因此亟需一种直接有效且高效的引导方法。

Method: 提出TAG方法，利用采样过程中的一个中间样本作为投影基，放大与该基切向的估计分数分量，从而校正扩散采样轨迹。理论上，基于一阶泰勒展开证明放大切向分量可以将采样状态引导到更高概率区域，减少不一致性。该方法无需修改模型结构，可作为即插即用的模块应用于不同扩散模型。

Result: TAG能以极小的计算增加，提升扩散采样的一致性和图像质量。其方法具架构无关性，并且实验证明对多个扩散模型均有效。

Conclusion: TAG为扩散模型采样提供了一种新颖、有效且高效的引导方式，在无需结构改动的前提下显著提升生成质量和模型鲁棒性。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [119] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 提出一种名为CRL的条件表征学习方法，通过用户自定义标准生成更加契合特定任务的表征，实验效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有表征学习方法主要捕获主导语义，不一定适用于特定下游任务（例如动物栖息地分析更重视场景特征），现有通过监督微调提升效果的做法代价较高。

Method: 提出Conditional Representation Learning（CRL），用户指定标准后，利用大语言模型生成描述文本，构建语义基底，并通过视觉-语言模型将图像投影到条件特征空间，从而获得特定语义的表征。

Result: 在分类与检索任务上的大量实验显示，CRL方法具有更优性能和更强泛化能力。

Conclusion: CRL能根据用户需求提取更加契合实际应用的特征，减少了对大量标注和计算的依赖，是通用表征学习方法的有效补充。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [120] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 本论文提出了一种利用病理医师在全视野切片（WSI）浏览过程中的行为记录，实现“看哪里”与“为什么看”的行为归因标注，进而发展出以行为为基础的病理诊断智能体。该方法改进了模型的解释性和临床一致性。


<details>
  <summary>Details</summary>
Motivation: 当前病理大模型虽然性能强大，但缺乏可以模拟医生多阶段真实浏览行为的智能体系统，核心障碍在于缺乏与临床经验高度一致的行为级标注数据。医生的视野选择和关注点是高度经验性、隐性的，难以从教科书或大模型训练语料中提取。

Method: 提出AI Session Recorder工具，结合WSI浏览器自动记录医生导航行为，并将行为日志转换为标准化命令和标注框，通过人工快速复审（human-in-the-loop）生成结合“看哪里”和“为什么看”的Pathology-CoT数据集。基于该数据，训练Pathologist-o3双阶段智能体：先筛选兴趣区域，后基于行为进行推理。

Result: 在消化道淋巴结转移检测任务中，Pathologist-o3模型达到84.5%精度、100.0%召回率和75.4%准确率，超越了OpenAI o3等最先进模型，并表现出良好的泛化能力。

Conclusion: 本工作首次实现了基于行为监督的病理学智能体。方法能高效将日常行为日志转化为专家级标注，有助于临床AI的升级与人类对齐，推动实用化智能病理诊断系统发展。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [121] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了空间-光谱-频率交互网络（S$^2$Fin），通过在空间、光谱和频率域的特征融合提升多模态遥感影像分类性能，并在多个数据集上超越当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态遥感图像分类虽然借助深度学习和特征融合取得了进展，但在从异构和冗余图像中提取结构化和细节特征方面仍然存在不足。现有方法难以有效利用高频稀疏信息。论文意在引入频域学习，进一步提升模型对细节和结构特征的建模能力。

Method: 提出空间-光谱-频率交互网络（S$^2$Fin），主要包括：1）高频稀疏增强Transformer，通过稀疏空间-光谱注意力学习优化高频滤波参数；2）两级空间-频率融合机制，包括自适应频率通道模块（融合高低频结构与高频细节）和高频共振掩膜（利用相位相似度突出图像边缘）；3）在网络中间层引入空间-光谱注意力融合模块，增强特征提取。

Result: 在四个有标注数据有限的多模态遥感图像基准数据集上，所提方法在分类准确率等指标上均优于当前主流方法，展现出较强的泛化和细粒度区分能力。

Conclusion: 通过引入频域机制和多维度特征融合，S$^2$Fin显著提升了多模态遥感影像分类的效果，尤其在结构和细节特征提取方面表现突出。代码已经开源，便于社区复现和应用。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [122] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本论文分析了多模态语言模型（MLMs）在处理视觉任务中的表现，尤其关注其视觉key-value tokens的信息流和瓶颈，并发现模型的感知能力与信息流控制密切相关。


<details>
  <summary>Details</summary>
Motivation: 尽管已有关于视觉Transformer和激活的可解释性工作，但当前尚不清楚为何多模态语言模型在感知密集型任务表现不佳。因此，作者希望通过研究视觉key-value tokens在流经语言模型过程中的信息变化，揭示其限制和潜力。

Method: 作者系统分析了LLaVA-OneVision、Qwen2.5-VL和Llama-3-LLaVA-NeXT等主流MLM的视觉key-value tokens信息流。评测包括零样本分割、语义对应、时序对应和指代表达检测等任务，并对比了经过与未经过MLM微调的视觉编码器。同时研究了不同层的key tokens对感知能力的影响，以及通过文本前缀改善视觉表征感知能力。

Result: 结果发现：1）图像value tokens携带的信息足以支持多项感知任务的零样本推理；2）经过MLM微调的编码器在某些感知任务上反而丢失部分视觉信息，有别于未经微调的视觉编码器；3）语言模型后期层的key tokens产生的伪影（artifacts）削弱了全局感知；4）通过为图像输入增加文本前缀能提升模型对视觉信息的利用率和表现；5）许多实际任务模型即使内部包含了感知信息，但该信息并未被有效输出，如在BLINK数据集艺术风格问题中有33.3%此类情况。

Conclusion: 文中工作揭示了key-value tokens在多模态模型中的动力学，指明提升MLMs视觉感知能力的新方向，包括更优的视觉编码器与语言模型训练策略，并为后续机制可解释性研究打开了新路径。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [123] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 本文提出了一种结合变换器（Transformer）架构和基于纹理的方法的新型集成框架，有效提升了深度伪造检测的准确性与鲁棒性，并在多数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术（deepfakes）的快速发展，检测被篡改的多媒体内容变得越来越重要。而现有方法在不同数据集和生成技术间泛化能力较弱，因此亟需更加强健、通用的检测技术。

Method: 作者结合了Swin Transformer和Vision Transformer（ViT）等变换器架构与基于纹理的方法，提出集成框架。通过数据分割、序贯训练、频率分割、基于patch的注意力机制和人脸分割技术，重点增强眼睛和嘴巴等高关注区域，解决数据集不平衡，提升模型对多样性的适应能力。

Result: 在DFWild-Cup数据集（涵盖8种不同深度伪造数据集的多样子集）上，该方法取得了最先进的检测性能。

Conclusion: 变换器模型和纹理方法的互补优势在集成模型中被充分发挥，证明了混合模型能有效应对深度伪造检测的新挑战，并为实际应用提供了强健的解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [124] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了PaperTalker，一个用于自动生成学术展示视频的多智能体系统，并建立了首个包含101篇论文及相关演示视频的基准数据集，展示了其在提升视频质量上的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前学术展示视频制作高度依赖人工，费时费力，需要协调幻灯片、字幕、语音和讲述人等多种信息渠道。自动化生成此类视频存在独特挑战，如高密度多模态信息的抽取和同步表达。因此，研究如何高效自动化生成高质量学术展示视频具有重要意义。

Method: 1. 构建PaperTalker数据集，包含101篇论文及作者制作的展示视频、幻灯片和讲者元数据。
2. 设计四种专用评测指标（Meta Similarity、PresentArena、PresentQuiz、IP Memory），用于衡量视频信息传达效果。
3. 提出PaperTalker多智能体生成框架，实现幻灯片生成与布局优化（基于树搜索）、光标定位、字幕、语音合成及“讲头”渲染，并支持并行化高效处理。

Result: 在Paper2Video实验中，PaperTalker生成的视频在信息准确性和丰富性上优于现有基线方法。定量和定性评测均证明其成效。数据集与代码已开源。

Conclusion: PaperTalker为自动化生成学术展示视频提供了新的数据基准和方法论，提升了视频的可用性和信息传递效果，为广泛推广学术视频自动化制作奠定了基础。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [125] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 本研究对遥感图像分割用于热带森林监测中的影响进行了分析，比较了传统SLIC算法和其他四种表现更佳的分割方法对后续分类器训练和检测森林砍伐效果的影响。


<details>
  <summary>Details</summary>
Motivation: 在ForestEyes项目中，图像分割为志愿者标注和后续机器学习模型训练提供基础。虽然SLIC算法常被采用，但近期研究显示其他超像素分割方法在遥感方面表现更佳，因此有必要系统比较不同分割方法对森林砍伐检测任务的实际影响。

Method: 研究对比了SLIC与四种当前表现最佳的分割方法，通过PyCaret AutoML自动搜索最佳分类器并进行性能评估，随后引入分类器融合（多模型集成）方法，以评估分割方式和分类器集成对性能的综合影响。

Result: 尽管单一分割方法在分类性能上的差别较小，但采用分类器融合（集成多模型）后，分割方法之间的性能差异更加显著，分类精度获得明显提升。

Conclusion: 分割算法的选择以及采用分类器融合技术对提升森林砍伐检测的准确性都至关重要。推荐在实际应用中关注多种分割方案并结合集成学习，提高遥感图像分析的检测能力。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [126] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: 本文提出了EduPersona，这是一个面向虚拟学生代理的课堂主观能力评测基准数据集，涵盖两种语言、三门学科和十类人格，支持大规模、多维度评估，并显著提升大模型的教室表现能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）已应用于教育领域，虚拟学生代理在课堂仿真和教师培训中越来越重要。但现有模型的主观能力，如人格一致性、情感表达等，尚未有系统的评估工具，影响其可信部署。因此，亟需建立专门面向课堂主观能力的评测基准。

Method: 作者构建了EduPersona数据集，基于大五人格理论划分十类人格，包含1308轮真实课堂对话和超12万条师生问答，通过人格化风格扩展数据规模。评价体系拆分为三项任务：（1）基本一致性（行为、情感、表达与语境的匹配），（2）学生仿真度，（3）长期人格一致性。利用这一资源，对多种LLM及基于EduPersona微调后的模型在上述任务上进行系统实验比较。

Result: 经过EduPersona微调的模型在所有任务上均获得显著提升：任务1提升33.6%，任务2提升30.6%，任务3提升14.9%。同时分析了不同人格建模难度存在异质性。

Conclusion: EduPersona是首个聚焦课堂主观能力的基准数据集，建立了模块化、可验证的研究范式。其数据集和评测框架将开源，帮助推动可信赖、类人的教育AI发展。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [127] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 本文提出了一种用于从步态序列预测心理特质的多阶段混合专家模型MoME，在心理属性分析任务上超越现有方法，验证了多任务步态学习在心理特质估计的可行性。


<details>
  <summary>Details</summary>
Motivation: 步态可携带丰富的生物识别和行为信息，但如何利用人的步态推断其心理特质仍然是一个富有挑战且较少探索的问题。作者希望提升通过步态估计心理属性的能力，以便丰富无感知行为分析的应用。

Method: 作者提出了一种分层的多阶段混合运动专家（MoME）架构，分四个复杂度阶段处理步态周期，利用轻量级专家模型提取时空特征，并采用任务特定的门控模块自适应加权不同特质和阶段的专家输出，实现了多任务心理属性预测。

Result: 在PsyMo基准数据集（覆盖17项心理特质）上，该方法在步态片段和用户层面分别取得了37.47%和44.6%的加权F1分数，优于现有步态分析模型；辅以身份识别、性别预测和BMI估计等辅助任务，心理特质预测效果进一步提升。

Conclusion: 多任务步态学习能够有效进行心理特质估计，为基于运动的心理推断研究提供了新基础，并证明了利用步态行为推断心理属性的可行性和前景。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [128] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ConceptSplit的新框架，有效解决了文本到图像扩散模型中多概念混合带来的干扰和混淆问题，提高了多概念个性化生成的质量。


<details>
  <summary>Details</summary>
Motivation: 多概念个性化文本到图像生成模型越来越流行，但其主要挑战在于“概念混合”，即多个概念在生成图像时发生干扰或不良融合，难以保留各自独立特征。需要新的方法来改善这一问题。

Method: 提出ConceptSplit框架，包括两个主要部分：1）Token-wise Value Adaptation（ToVA），一种在交叉注意中只适配value投影、避免合并带来混淆的训练方法；2）Latent Optimization for Disentangled Attention（LODA），通过优化输入潜变量，在推理阶段分离各概念的注意力，减少干扰。

Result: 实验显示，ConceptSplit在质和量两方面显著缓解了多概念混合带来的干扰，生成的图像能更好地表达独立、清晰的多个个性化概念。

Conclusion: ConceptSplit框架有效实现了文本到图像扩散模型中的多概念个性化，显著减少了概念混淆问题，对相关任务提供了新解决思路与工具。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [129] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出了一种标签高效的多期MRI肝脏分割方法，无需空间配准，能在有限标注和多厂家、多模态条件下泛化，提升实际临床中的分割表现。


<details>
  <summary>Details</summary>
Motivation: 多期MRI在肝纤维化评估中很重要，但不同厂家、序列和期相的标注数据稀缺且分布不均，显著限制了高效、泛化的自动分割应用。

Method: 方法包括大型基础3D分割网络微调、跨模态伪监督协同训练以利用无标签数据，并采用标准预处理流程。模型无需空间配准，可适应多种MRI期相和厂家。

Result: 实验结果显示，无论是在有标注还是无标注数据域，所提方法均实现了稳健的分割性能，为多期、多厂家MRI下的肝脏分割提供了标签高效的强基线。

Conclusion: 结合基础模型的微调与协同训练能有效推广至真实临床应用，为标签稀缺与异质性影像条件下的自动分割任务提供新思路。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [130] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的人脸表情生成方法，能够在保证身份一致性的前提下，实现对表情的精细控制，并支持真实图像的表情编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式模型在维持人脸身份一致性和实现对表情的精细操控方面存在权衡难题：大多数方法要么身份失真、要么难以精准刻画细腻表情变化。本文旨在解决这一难题，实现更人性化、可控性强的AI驱动故事生成。

Method: 作者基于身份一致的人脸基础扩散模型，采用了组合式结构，并引入了基于FLAME blendshape参数的表情跨注意力模块，提升对表情的精确控制能力。模型训练时融合多类型丰富、带有细致表情变化的图像和视频数据。此外，设计了可插拔的Reference Adapter，支持在真实图像中转移参考帧的表情特征，实现个性化表情编辑。

Result: 实验结果显示，该方法能够生成既定身份且表情可控性高的人脸图像，不仅覆盖基本情绪，还能细致还原微表情和复杂表情过渡，且定量和定性测试均优于现有方法。

Conclusion: 该研究实现了身份一致、表情可控的人脸生成，为AI叙事提供了更高质量、更灵活的人物表情合成方案，可支持真实图像编辑等多种应用，推动相关领域发展。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [131] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成模型ReactDiff，用于自动生成多样且类人的面部反应，显著提升对话系统中的人机交互自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前的人机对话系统难以生成真实且多变的面部反应，主要因为缺乏对人类自然表情变化中随机性与动态性的建模。现有方法往往导致面部表情不自然或者过于单一，影响交互体验。

Method: 作者提出ReactDiff，一个基于时序扩散模型的面部反应生成框架。该方法在扩散过程中新引入两种关键先验：一是时序面部行为运动学，二是面部动作单元依赖关系。这些先验约束能够保证生成的面部表情平滑、连贯且受限于人体面部结构，极大减少了抖动、不稳定或不自然的表情。

Result: 在REACT2024数据集上的大量实验证明，ReactDiff不仅生成的反应质量达到当前最优水平，还在反应的多样性与适宜性上表现突出，优于现有方法。

Conclusion: ReactDiff实现了高质量、多样化的人类面部反应生成，显著提升人机对话系统中的反应自然性，为更真实、人性化的交互体验打下基础。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [132] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出了一种提升3D场景语义图预测精度的新方法，通过改进目标特征编码和对比预训练，有效提升对象与关系检测效果，在多个指标上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 以往的3D语义场景图预测方法过度依赖图神经网络，但这些网络在判别能力上存在不足，导致对象和关系的特征表征效果有限，影响整体精度。因此，作者希望改进对象特征表示，从根本上提升场景图预测效果。

Method: 作者设计了一种新的高判别性对象特征编码器，并引入了对比预训练策略，将对象表示学习与场景语义图预测任务解耦。该方法同时充分融合了几何和语义信息，以提升对象识别和关系预测的准确性。

Result: 通过在3DSSG数据集上进行综合实验，作者的方法显著提升了场景图预测的各种评价指标，对比当前最优方法表现突出。预训练特征编码器还可无缝集成到现有框架中，带来明显增益。

Conclusion: 研究表明，提升对象特征质量对整体3D语义场景图预测至关重要。提出的高判别性编码和对比预训练方案能有效提升对象和关系的识别效果，推动领域发展。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [133] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 本研究首次为野生动物监测场景下的单目深度估计（MDE）方法建立了基准测试，对当前四种主流深度估计方法进行了系统评估，并为深度估计在野保监测应用提供了实践指南。


<details>
  <summary>Details</summary>
Motivation: 尽管单目深度估计技术取得了重要进展，但在实际野生动物相机陷阱拍摄的自然环境下，相关方法的性能尚未被系统评估。缺乏深度信息一直是野生动物监测中影像分析的难点，因此亟需有针对性的基准测试与方法比较，为实践应用提供参考依据。

Method: 作者收集了93张经ChARUCO标定、含有真实距离信息的野生动物监测相机陷阱图片，针对Depth Anything V2、ML Depth Pro、ZoeDepth、Metric3D四种SOTA方法及一个几何学基线，评估了其单目度量深度估计的表现，并分析了均值与中位数提取方案的效果，还比较了各方法的计算效率。

Result: Depth Anything V2在总体表现最好（MAE为0.454米，相关系数为0.962），而ZoeDepth在户外自然环境表现大幅下降（MAE为3.087米）；采用中位数提取的深度估计普遍优于均值；速度上ZoeDepth最快（0.17秒/帧），但精度最低，Depth Anything V2精度和速度平衡最佳（0.22秒/帧）。

Conclusion: 本文建立的基准测试为今后的野生动物监测深度估计方法研究提供了可靠的性能基线，结果指明深度算法需充分考虑自然环境适应性，并为生态保护监测系统中深度估计方案的选型提供了实际指导。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [134] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: 本文提出了一种可自动量化体育直播中赞助商品牌曝光度的新系统ExposureEngine，能更精确识别旋转/倾斜的赞助商标志，相比传统方法提供更高效和准确的分析。


<details>
  <summary>Details</summary>
Motivation: 体育直播中品牌曝光度分析多依赖人工且主观、不易扩展，已有自动方法因仅用水平包围框导致遇到标志旋转或视角变化时精度降低，因此急需更准更自动的系统。

Method: 作者提出ExposureEngine，能预测旋转包围框（OBB），并设计了包含1103帧、670种标志的新数据集。模型以高精度检测各种视角下的标志，并整合到分析流水线，实现曝光时长、面积等指标的自动统计。系统还支持自然语言交互生成报告和摘要。

Result: 实验中，模型在logo检测任务上达到mAP@0.5 0.859，精度0.96，召回0.87，能稳定应对多样的广播环境。

Conclusion: ExposureEngine系统为体育媒体品牌测量提供了一套端到端、高精度且可审计、解释的解决方案，对传统分析方式有明显提升，具有实际应用和推广价值。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [135] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: 本文提出了一种新方法AA-YOLO，在YOLO检测头中集成了异常检测机制，用于提升红外小目标检测的准确性和鲁棒性，尤其在复杂背景下有效降低了误报率，并且对多种YOLO模型通用。


<details>
  <summary>Details</summary>
Motivation: 传统的目标检测器在红外小目标检测任务中，经常由于复杂背景和目标过小而出现许多误报。解决该痛点对于国防等领域具有重要意义。

Method: 在YOLO检测头中引入统计异常检测测试，将小目标当作背景中的异常模式检测，通过这一机制有针对性地抑制误警，并确保模型在训练样本有限、带噪声或出现跨域变换时依然保持良好性能。该方法仅需修改检测头，可应用于多种YOLO主干网络，包括轻量模型，也可与实例分割任务结合。

Result: AA-YOLO在多个红外小目标检测基准上表现出有竞争力的检测效果，对样本稀缺、噪声干扰及跨域情况显示出较强鲁棒性，并已在多种YOLO模型上成功应用。

Conclusion: AA-YOLO具有良好的适应性与部署价值，适合资源受限的实际场景。其检测头通用、性能优异，且代码将开源，便于进一步推广使用。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [136] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 该论文评估了基于transformer的架构在自然面对面对话场景下进行人物识别的效果，提出了结合空间和时间特征的两路模型。实验结果显示，空间信息优于时间动态，二者融合可进一步提升识别率。


<details>
  <summary>Details</summary>
Motivation: 随着自然交互应用的增多，如何在面对面的复杂场景下准确识别人物变得非常重要。以往基于视频中骨骼点特征的人物识别方法尚未充分利用transformer架构和多模态融合。本研究旨在探索更有效的结构，提升自然场景下人物识别的准确率。

Method: 作者实现并评估了一种两路transformer框架：一路处理COCO WholeBody的空间骨骼点配置，另一路挖掘运动模式。比较了预训练与从零训练，引入了速度特征，并提出多尺度时间transformer以层次化建模动作动态。最后在特征层面对空间和时间输出进行融合。

Result: 空间transformer单独识别准确率达95.74%，多尺度时间transformer为93.90%；两者特征融合后识别率提升至98.03%。空间配置比运动动态贡献更大，特定领域内训练优于跨领域迁移。

Conclusion: transformer结构能很好地处理自然交互场景下的人物识别问题，空间和动态特征互补，联合建模大幅提升识别效果。该工作为未来多模态与跨文化识别人机交互研究提供了重要参考。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [137] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Progressive Gaussian Transformer框架（PG-Occ），通过逐步精细化表示及自适应采样，实现了开放词表的3D占用预测，并在性能上取得大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D占用预测方法不是语义类别固定、不能支持开放词表查询，就是稀疏表达不能刻画小目标，或者高密度表示计算代价太高。因此亟需一种兼顾表达能力与计算效率的新方法。

Method: 提出Progressive Gaussian Transformer Framework（PG-Occ），利用递进式的在线密化策略，逐步提升3D Gaussian表示精细度，与各尺度各时刻自适应的各向异性采样和时空特征融合机制，使得对复杂场景的小细节与丰富语义信息均能有效建模。

Result: PG-Occ在大规模实验中表现优异，mIoU指标相比上一最好方法提升14.3%，展现了显著的先进性。

Conclusion: PG-Occ框架首次实现了精细、可扩展的3D开放词表占用预测，有效平衡了精度和效率，对自动驾驶等实际应用具有重要推动意义。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [138] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出通过生成未知类别数据来更好地估计开放环境下的分布，从而提升开放词表学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅以已知类别数据建模开放环境的分布，但缺乏未知类别数据导致分布估计误差无法被辨识和调整。因此，需要突破只依赖已知类别，探讨包含未知类别的分布估计。

Method: 作者提出结合类别-领域数据生成流程和分布对齐算法。通过层次化语义树和领域信息，引导生成未知类别数据，然后利用这些数据进行分布估计和后验概率最大化对齐。

Result: 在11个数据集上的大量实验证明，本文方法在性能上相较于现有基线最多提升14%。

Conclusion: 生成未知类别数据能够有效上界分布估计误差，提出的方法大幅提升了开放词表学习的泛化能力，显示出明显的优越性。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [139] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛首次建立了联邦学习（FL）在外科手术视频分类领域的基准，通过多中心数据评估了各方法的泛化与本地适应能力，暴露了现有方法在泛化与模型稳定性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 推动联邦学习在医疗外科AI中的实际应用，并制定统一的评测基准，以不共享患者数据的前提下，实现多中心协作，提高模型泛化与适应能力。

Method: 参赛团队基于Appendix300多中心视频数据，开发了多种外科炎症分级分类方法。评测任务包括：1）对未见过中心的泛化能力；2）本地微调后的中心特异性适应能力。方法涵盖基础模型线性探查、三元组损失度量学习、不同FL聚合策略（FedAvg、FedMedian、FedSAM）、时空建模与上下文预处理。主要评估指标为F1分数和预期代价，排名稳健性通过自助法和统计检验。

Result: 各模型在泛化任务中整体表现有限，适应任务经微调后均有提升，但排名不稳定。ViViT（视频视觉Transformer）模型综合表现最佳。发现当前方法在泛化、类别不平衡敏感性和去中心化超参数优化等方面仍有明显短板。时空建模和上下文感知预处理策略表现出潜力。

Conclusion: FedSurg挑战建立了外科手术视频分类领域的首个FL基准，突出了本地个性化与全局稳健性的权衡，强调了模型结构、数据预处理与损失函数设计的重要性。该基准为未来设计更适合临床实际、能处理类别不均衡和具适应性的联邦学习方法提供了参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [140] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出了一种无需人工干预的双机器人高分辨率3D扫描系统，实现了文化遗产文物的高保真、高效率数字化。


<details>
  <summary>Details</summary>
Motivation: 当前文化遗产3D扫描普遍依赖人工或半自动操作，过程繁琐且需要专业技能，制约了高质量数字化的普及。

Method: 设计了一个由两台机器人协作的自动化扫描系统，一台负责操控高分辨率扫描仪，另一台操控文物托盘。通过空间参数化分区和协同运动规划，实现全面、高效的表面扫描。优化轨迹规划和采样点分布，提升覆盖率和重建精度。

Result: 实验显示，该系统获得更低的Chamfer Distance和更高的F-score，数字化几何精度和效率优于基线方法，显著降低对操作人员的专业要求。

Conclusion: 该双机器人系统在文化遗产数字化领域，兼顾了高保真度、自动化与高效率，减少了对人工经验的依赖，具有实际应用前景。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [141] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: 本文系统比较了大规模CNN与ViT模型在两类几何估计任务（二维图像变换估计与基础矩阵预测）及不同数据量场景下的表现，发现ViT在大数据下优于CNN，但在小数据场景中CNN性能更佳，ViT具备更强跨域泛化性。


<details>
  <summary>Details</summary>
Motivation: 以往ViT和大规模CNN作为预训练骨干能提升迁移学习性能，但其在低数据情境下图像几何估计算法中的效率和适用性尚未有系统研究。为推动如自动驾驶、机器人、三维场景重建等实际任务，对于模型选择和结构改进提出科学依据。

Method: 选取ResNet、EfficientNet、CLIP-ResNet（CNN代表）及CLIP-ViT、DINO（ViT代表），在2D刚性变换估计与基础矩阵预测两类任务中，实验对比预训练模型（分类或对比学习预训练）在大/小数据量及跨域测试条件下的性能表现。

Result: 实证发现：在大数据下，ViT在细化任务中优于CNN。在小数据量环境中，CNN的归纳偏置和较小容量使其表现好于ViT或媲美ViT。此外，ViT在跨域情境下表现出更强的泛化能力。

Conclusion: 模型骨干的选择应结合具体任务需求与数据量状况，单一结构并非最佳。建议未来探索可结合局部与全局特征的混合结构，以提升几何估计等任务的表现与泛化能力。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [142] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DiT-VTON的虚拟试穿新框架，利用扩散Transformer（DiT）实现高质量、多品类产品的图像试穿，显著提升了细节还原、鲁棒性和编辑能力。


<details>
  <summary>Details</summary>
Motivation: 随着电商快速发展，用户对可视化虚拟试穿（VTO）需求猛增，但现有方法在细节保留、实际应用鲁棒性、多样商品支持等方面效果有限。

Method: 作者将Diffusion Transformer（DiT）适配到以图像为条件的试穿任务，系统探索了多种DiT结构（如token拼接、通道拼接、ControlNet集成）。模型在包含多背景、非服装类别等的大型数据集上进行训练，提升了模型泛化和鲁棒性，并具备局部编辑、姿态保留等高级图像编辑功能。

Result: 实验表明，DiT-VTON在VITON-HD数据集及千类商品多样化数据集上，细节保留和鲁棒性优于现有技术，同时无需额外条件编码器，且具备更强的VTA和编辑能力。

Conclusion: DiT-VTON有效拓展了VTO任务边界，不仅在传统服装试穿上取得更好效果，还支持多种商品类别和复杂编辑任务，对虚拟电商体验有积极推动作用。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [143] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: 本论文提出了一种能够从手术室固定摄像头视频重建任何医护人员视角（即第一视角）的方法框架EgoSurg，不需要中断实际临床流程，即可合成高逼真度的任意和本体视角影像。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察多依赖固定视角或回忆，缺乏第一视角资料，无法真实重现影响手术安全、教学和流程优化的决策过程。现有壁挂摄像头虽然能捕捉流程，但看不到团队成员真实所见，极大限制了洞察深度。

Method: EgoSurg结合基于几何的神经渲染与扩散式视图增强技术，利用房间内固定的摄像头采集视频，无需干预手术流程，实现高保真度任意和本体视角的动态重建。

Result: 在多机构、真实和受控手术案例评测下，EgoSurg成功高质量重建了医护个体化和任意视角影像，重现特定成员的视野，并获得高视觉保真度。

Conclusion: EgoSurg将现有手术室摄像系统转变为可交互的动态3D记录基础，为沉浸式手术数据分析与教学开启新方向，使手术实践可从任意视角可视化、体验与分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [144] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: 本文提出AvatarVTON，实现只需单张服装图片即可在虚拟人身上进行4D试穿，具备自由姿态、视角切换和多服装选择等多种功能。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术存在难以实现动态服装交互、需要多视图输入或依赖物理先验等局限，难以适应现实应用中的高互动性和多样性需求。

Method: 提出AvatarVTON框架，包含：(1) Reciprocal Flow Rectifier模块，对无先验的光流进行校正，实现稳定的拟合和时间一致性；(2) Non-Linear Deformer，将Gauss分解为视角-姿态不变和特定成分，实现自适应、非线性的服装变形。并扩展相关基线，建立规范评测。

Result: AvatarVTON在多项实验中表现出高保真度、多样性和动态服装逼真度，优于当前主流方法。

Conclusion: AvatarVTON突破了单视图监督下的动态服饰试穿难题，适用于增强现实、虚拟现实、游戏及数字人应用，推动4D虚拟试穿领域发展。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [145] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 本论文提出了一种基于3D Flow Matching (FM) 框架的方法，从MRI或CBCT生成合成CT(sCT)图像，用于提升放射治疗的精确性并减少患者辐射。


<details>
  <summary>Details</summary>
Motivation: 生成sCT可支持MRI-only和基于CBCT的自适应放疗，这不仅提高治疗精度，还能减少患者接受的辐射剂量。现有方法在高图像质量和效率之间存在平衡难题。

Method: 采用全3D Flow Matching框架，利用高斯噪声体经过学习到的FM速度场转化为sCT图像。速度场由轻量级3D编码器提取的MRI或CBCT特征来调节。方法在SynthRAD2025挑战基准上进行评估，并分别针对不同模态及三个解剖区域训练模型。

Result: 结果显示，该方法能精准重建全局解剖结构，但由于受限于内存和运算速度，训练分辨率较低，对细节保留有限。

Conclusion: 该方法在重建全局结构方面表现良好，但细节表现需要改进。未来将研究基于patch的训练和潜空间流模型以改善分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [146] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 本文提出AT-BPTT框架，通过动态调整截断位置和窗口大小，有效提升了数据集蒸馏过程中的效率与模型性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏的内循环优化方法多采用随机截断，灵活性不足且通常效果不佳。作者观察到神经网络在训练的不同阶段呈现不同的学习动态，随机截断难以适应，容易导致次优结果，因此需要更智能的截断机制。

Method: 提出了AT-BPTT框架，包括：（1）基于概率的分阶段时间步长选择机制；（2）根据梯度变化自适应调整窗口大小；（3）采用低秩Hessian近似以降低计算开销。这些方法使截断方式随训练动态调整，提高效率和效果。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K等数据集上，AT-BPTT平均提升准确率6.16%，同时内循环优化加速3.9倍，节省63%内存。

Conclusion: AT-BPTT能根据训练阶段和梯度变化动态调整截断与窗口策略，在保证高精度的同时实现高效的数据集蒸馏与资源利用优化。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [147] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 本论文提出了一种利用航拍图像实现光伏电站自动化建模的新方法，可细致建模到单独光伏组件级别，无需依赖第三方数据。


<details>
  <summary>Details</summary>
Motivation: 光伏电站的精准且动态的模型对其运维至关重要，但现有方法难以获得实时且详尽的电站结构信息。

Method: 方法基于航拍概览图像，运用视觉分割识别光伏组件，通过结构推断将组件划分到具体支架、排、列，并借助关键点信息融合多张图像的数据以保留结构完整性，最终获得3D地理参考模型。

Result: 方法在两座不同光伏电站上进行了实验验证，能够高效并准确地生成包含语义结构的地理参照紧凑模型。

Conclusion: 该方法为光伏电站的自动建模和运维提供了可靠工具，可提升自动化水平并减少对外部数据依赖。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [148] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 本文提出了一种通过3D骨架关节点数据自动识别人类活动表达心理状态（肢体语言）的框架，有效保护隐私且适用性强。


<details>
  <summary>Details</summary>
Motivation: 在环境心理学与强化学习等领域，理解人类与环境间动态关系很重要。但现有方法采集人类心理状态数据常依赖理论模型或问卷，费时费力且无法保护隐私，难以泛化。

Method: 作者提出了结合空间-时间图卷积网络（ST-GCN）与卷积神经网络（CNN）的肢体语言识别框架。该框架通过迁移学习，无需手动定义动作与心理类别映射，直接从3D骨架数据推理活动所反映的心理和情感状态，并保护用户身份匿名。

Result: 在Dyadic User EngagemenT (DUET) 数据集上实验证明，该方法具有高准确率、可扩展性和良好的人本建模能力，有效识别人机交互中的肢体语言。

Conclusion: 该方法为强化学习驱动的人类-环境交互建模提供了新途径，能够保护隐私、自动化、精确地捕捉和还原行为中的心理与情感信息。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [149] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 本文提出将社会目标融入到传统的网络-物理系统（CPS）中，形成“网络-物理-社会基础设施系统”（CPSIS），并通过人体骨架动作实现对双人互动的自动识别与分析，提升对人类社交行为的理解。


<details>
  <summary>Details</summary>
Motivation: 传统CPS系统主要关注性能与安全等经济目标，忽略了对人的社会性需求。因此，研究如何将社会目标纳入系统架构，提升基础设施的人类中心价值变得十分迫切。

Method: 本文聚焦于现实世界数据下的人际（双人）互动识别，采用深度传感器的骨架动作信息，比较了五种基于骨架的互动识别算法，并在包含12类双人互动的数据集上进行了实验，涵盖情感与文化交流等类型。

Result: 五种骨架动作算法在该12类双人互动识别数据集上的表现得到了量化比较，为理解人类互动行为的自动分析提供了对比基准与参考。

Conclusion: 通过使用深度传感器实现的骨架动作分析，可以在保障隐私的同时，有效识别人类双人互动，为将社会目标真正集成到智能基础设施系统奠定了基础。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [150] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 本文提出了一种结合早期退出（early exits）和知识蒸馏（knowledge distillation）的新型神经网络优化方法，用于提升模型在受限算力场景下的效率，同时保持图像分类准确率。通过引入基于熵的新损失函数，提高了学生模型对教师模型错误预测样本的学习能力。实验证明，在CIFAR10、CIFAR100和SVHN数据集上，该方法在不损失准确率的前提下显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络尽管分类性能优异，但高昂的计算成本限制了其在实时和边缘设备上的应用。因此，亟需开发高效的压缩和动态结构技术，以实现模型的高效部署。

Method: 方法将两种优化技术相结合：一是早期退出机制，允许模型提前输出分类结果以减少计算；二是知识蒸馏，即用复杂教师早退模型指导简化学生早退模型学习。对于教师模型错误分类的图片，提出基于熵的特殊损失函数，进一步提升学生模型的泛化能力。

Result: 该方法在CIFAR10、CIFAR100和SVHN等多个数据集上均取得了优秀的实验结果。与传统知识蒸馏方法相比，实现了更好的准确率与计算复杂度权衡，验证了新损失函数的有效性。

Conclusion: 结合早期退出和知识蒸馏，同时创新引入基于熵的损失，对低算力环境下的高效神经网络设计有重要价值。未来该方法有望推广到更广泛的知识蒸馏场景中。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [151] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文提出了一种深度学习方法（μDeepIQA）用于光学显微镜图像的质量评估，能够更快、更稳定地预测图像质量，并适应不同类型的图像数据。


<details>
  <summary>Details</summary>
Motivation: 传统的显微图像质量评估方法在处理大规模数据集时，往往计算量大、耗时长，并且对非理想类型的图像表现不稳定。为了提高评估速度和准确性，需要一种更通用且高效的方法。

Method: 本文受前期研究启发，基于深度卷积神经网络（CNN），对自然图像IQA架构进行了再训练，使其能预测显微镜图像的个体质量指标和全局质量分数，并支持图像块级别的空间质量可视化。

Result: μDeepIQA模型在显微镜图像的质量评估上表现出高泛化能力，不仅提供了快速、稳定的图像质量预测，还能有效处理异常、非理想图像，并可用于图像空间质量的局部分析。

Conclusion: 深度学习方法（μDeepIQA）能为光学显微镜图像提供高效、稳定、泛化性强的质量评估，尤其适合大规模与非标准数据，为生命科学与生物医学领域的图像数据处理和分析提供了可靠支持。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [152] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 本文提出了一套端到端的物联网（IoT）机器人系统，可用于葡萄园中无损、实时、空间分辨地对葡萄的产量与品质（糖度Brix和酸度）进行测量和制图。系统融合了高性能的葡萄检测与重量估算模型和一个新颖的深度学习品质评估框架。


<details>
  <summary>Details</summary>
Motivation: 田间高效、准确地获得葡萄产量与品质数据对精准农业和葡萄酒生产至关重要，但传统方法缺乏实时、无损、空间解析能力。特别是高光谱图像（HSI）应用中，不同光照造成“域偏移”，严重影响品质评估的准确性。

Method: 该系统主要由两大模块构成：1）葡萄串检测与重量估算的高效模型；2）基于高光谱数据的品质评估深度学习框架。创新点在于提出了Light-Invariant Spectral Autoencoder（LISA），可学习照明不变的特征，有效解决不同光照下的域偏移问题。实验在三种不同照明下（人工控光、早晨日照、下午日照）采集的HSI数据上验证了系统鲁棒性。

Result: 系统实现了葡萄串检测召回率0.82、重量预测R^2为0.76。LISA模块让品质预测的泛化能力比传统方法提升超过20%。最终可产出高分辨率、地理参考的葡萄产量和品质数据。

Conclusion: 本文的IoT机器人系统集成了鲁棒的检测、重量与品质评估模块，能实时、高分辨地为葡萄种植提供可靠的数据支持，为精准葡萄栽培提供决策依据。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [153] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 该论文发布了一个包含大量侧扫声呐（SSS）数据、测深图和光学图像的多模态海底栖息地数据集，可用于机器学习模型的开发和测试，并配套开源工具，推动海底自动分类技术的发展。


<details>
  <summary>Details</summary>
Motivation: 目前海底栖息地映射受限于标注数据集不足，影响了机器学习方法和基准的进展。该领域缺乏统一、丰富的数据支撑海洋生态保护及可持续管理。

Method: 作者收集了约一百万枚加泰罗尼亚（西班牙）沿岸的SSS切片数据，并提供测深图及由自主水下机器人（AUV）采集的光学图像。约36000张SSS切片被人工标注分割掩模；同时，作者研发了配套的开源预处理与标注工具，并实现了光学与SSS图像的配准，用于跨模态表示学习。

Result: 生成了一个高质量、公开的大型多模态数据集和配套开源工具，支持监督和自监督的多模态学习；为AUV传感器数据融合、海底栖息地自动分类等研究方向提供了重要资源。

Conclusion: 该数据集作为海底栖息地映射领域的标准基准，将促进多传感器信息融合和水下自动分类算法的研究进展，有助于生态保护、自动化勘测及可持续管理领域的发展。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [154] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 本研究评估了四种主流目标检测模型（YOLOv5、Faster R-CNN、SSD、RetinaNet）在卢旺达基加利对摩托车检测的表现，重点关注其在资源受限环境中实时应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 基加利的摩托车出租是主要交通方式，但驾驶不规范，给自动驾驶系统带来极大挑战。现有检测模型大多缺乏针对当地实际场景的数据与测试，因此亟需比较不同模型在该环境中的表现。

Method: 收集了198张基加利摩托车的自定义数据集，采用PyTorch实现四种目标检测模型的迁移学习。对比模型的检测准确性、定位精度及推理速度，并分析了数据集受限及模型复杂性等实际部署中的难题。

Result: 对比发现各模型在准确性和速度表现上有差异。指出数据量和模型复杂性是实现的主要挑战，复杂模型虽然准但资源消耗大，简化模型更适合本地实时场景。

Conclusion: 建议未来采用结构更简单的检测模型以适应发展中国家资源受限自动驾驶系统的实际需求，这有助于提升相关技术的落地和可及性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [155] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种新的层次语义感知分类方法（SAHC），通过引入层次结构特定的分类头和一致性机制，实现对遥感图像多层次标签的准确分类。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习遥感图像分类方法普遍忽略了标签之间的层次结构关系，只关注细粒度分类，无法利用类别语义关系提升分类表现。

Method: 设计了在深度网络中集成多个服务于不同细粒度层次的分类头，并通过可训练的层次结构矩阵自监督引导网络学习不同层级的特征和关系。同时，提出层次共识机制，确保各层级的分类输出概率分布保持一致，实现信息整合与权重调配。

Result: 在三个具有不同层次复杂度的标准数据集上，采用不同主干网络进行测试。结果显示该方法在提升网络学习效率与层次一致性方面表现优异，取得更鲁棒的分类准确率。

Conclusion: SAHC方法有效提升了遥感图像多层次分类的能力，具备良好的通用性和适应性，对层次结构丰富的分类任务具有推广意义。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [156] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了区域专家网络（REN），一种结合解剖信息的MoE模型，用于提升医学影像分类准确性，尤其在肺部疾病识别上取得显著优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构在医学影像中未能结合区域或解剖结构信息，难以有效捕捉疾病在解剖结构上的异质性；医学影像分类迫切需要结合区域或解剖先验以提高分辨复杂病理变化的能力。

Method: 构建了区域专家网络（REN），每个专家针对肺叶及其组合进行训练，并引入多模态门控机制，整合影像组学和深度学习（包括CNN、ViT、Mamba）特征，对专家输出进行加权，实现区域病理特征的精细建模。

Result: REN在间质性肺疾病分类任务中达到平均AUC 0.8646（±0.0467），比SwinUNETR基线提升12.5%（AUC 0.7685）；区域专家在下肺叶的AUC达0.88-0.90，显著优于传统深度学习方法（0.76-0.79），且与疾病进展规律一致。

Conclusion: REN方法具备良好的泛化性与临床可解释性，是一种可扩展的、基于解剖结构的医学影像AI框架，有望应用于其它具备结构先验的医学影像问题。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [157] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督主动学习（UAL）方法，称为Natural Feature Progressive Framework (NFPF)，解决了现有UAL方法在样本选择和性能方面的不足，并达到了与有监督主动学习方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型高度依赖大规模人工标注数据，但数据标注费时费钱。虽然主动学习（AL）可以降低标注成本，但需要多次人参与。无监督主动学习（UAL）通过只在样本筛选后进行标注，有望进一步节省人力，但传统方法性能不佳。因此，亟需提出一种UAL方法，既高效又接近有监督方法的性能。

Method: 作者提出NFPF框架，核心为Specific Feature Learning Machine (SFLM)，用于精确衡量每个样本对模型性能的贡献，并利用重构差异(Reconstruction Difference)指标进行初始样本选择，从而提升样本表示和选择质量。

Result: 在视觉领域的数据集上，NFPF显著优于所有现有UAL方法，并且其性能接近最新的有监督主动学习方法。

Conclusion: NFPF不仅提升了无监督主动学习的准确性和鲁棒性，还提高了对数据分布的覆盖度，为降低人力标注成本提供了更优解决方案。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [158] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对乳腺双视图（CC与MLO）影像缺失时的互译方法——CA3D-Diff，通过创新的扩散模型结构实现了更高质量且结构一致的视图合成，对临床乳腺癌辅助诊断具有实际价值。


<details>
  <summary>Details</summary>
Motivation: 在实际临床中，乳腺X线摄片常因拍摄失误或伪影导致某一视图缺失，这会影响后续病灶检测与分析效果。针对目前跨视图处理方法难以应对非刚性变形和组织重叠的问题，研究提出新方法以克服这些挑战。

Method: 1) 设计了基于条件扩散模型的双向乳腺视图转换框架CA3D-Diff；2) 提出列感知的跨注意力机制，利用乳腺解剖结构在不同视图会落在相似列上的几何属性，并用高斯衰减机制增强局部列相关，抑制长距离错配；3) 引入隐式三维结构重建模块，将噪声2D特征投影至粗略3D体，再反馈至UNet去噪网络，提升生成的视图的解剖一致性。

Result: 大量实验表明，CA3D-Diff在双向乳腺视图生成任务上，比当前先进方法在视觉质量和结构一致性上均有明显提升。生成的合成视图还能有效提升单视图乳腺癌筛查的恶性肿瘤分类准确率。

Conclusion: CA3D-Diff为乳腺影像双视图互译提供了一套有效的新方案，不仅提升了影像合成质量，也为实际单视图诊断缺陷场景下的临床应用带来直接价值。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [159] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: 作者提出了SSDD，这是一种新的像素扩散解码器，能够在比KL-VAE更高效、更快的情况下，实现更高质量的重建，无需使用对抗损失。


<details>
  <summary>Details</summary>
Motivation: 传统的生成式图像模型采用基于KL-VAE的编码器进行特征提取和降维，但它们需要对抗损失和多步解码，存在解码慢以及训练不稳定等问题。作者希望解决这些不足，实现更高效、更高质量的图像生成。

Method: 提出了一种基于transformer组件的新型像素扩散解码器结构，无需对抗损失（GAN-free）。通过蒸馏技术把扩散解码器的能力转移到高效的单步解码器里，即SSDD，优化单步重建能力。

Result: SSDD在单步重建过程中，将FID从0.87提升至0.50，吞吐量提高1.4倍，同时保持DiTs的生成质量且采样速度提升3.8倍，超越了KL-VAE。

Conclusion: SSDD能够作为KL-VAE的直接替代方案，显著提升生成模型的质量和速度，无需复杂的对抗损失或多步采样，推动了生成式模型的发展。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [160] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 本文提出了一种通过微调视觉基础模型（VFMs）部分层及小型编码-解码网络，将数字水印嵌入模型内部表征，实现对VFMs所有权验证的方法。嵌入的水印能在下游任务微调后的模型中依然可检出。


<details>
  <summary>Details</summary>
Motivation: 由于训练VFMs需要大量数据和算力，开发者希望通过许可授权来保护模型知识产权。然而不诚信用户仍可能非法分发受保护模型。因此，可靠的模型所有权验证工具变得极其重要，可辨目录受保护模型与独立模型。

Method: 该方法通过微调VFMs的一小部分表达性强的层，并结合小型编码-解码网络，在输入图片的内部表征中嵌入数字水印。训练后，这些水印在模型的功能性拷贝中仍然可以被检测，即使模型被进一步用于下游任务的微调。

Result: 理论分析和实验表明：该方法对未被水印的模型检测的误报率低，对已水印模型的漏报率也低，具备良好的区分能力和实际可行性。

Conclusion: 该方法为视觉基础模型的所有权验证提供了一种有效解决思路，能保护模型开发者的知识产权，并在模型被下游任务微调后依旧保持水印的可检性。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [161] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 本文提出了基于深度神经网络(DNN)的全新不确定性估计方法，通过引入变换层生成多个潜在表示，用于视频驾驶员行为和意图识别中的分布外(OOD)检测，并与主流方法对比验证有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限且安全关键的任务(如驾驶员行为识别)中应用越来越广，但现有基于最后一层概率推断的不确定性检测方法性能不稳定，且效率较低，因此需要新的高效且易调优的不确定性估计方法。

Method: 提出在预训练DNN基础上，加入变换层以生成多重潜在表示，从而对模型不确定性进行估计，分别命名为LUR与通过repulsive训练提升的RLUR；并与8种主流概率深度学习方法在4个驾驶员视频行为识别数据集上进行分类、校准、OOD检测等对比实验。

Result: LUR与RLUR在内部分布分类上与现有LL-PDL方法持平，且在不确定性驱动的OOD检测任务上达到主流方法水平，训练更高效，调优更简单。此外，开源了NuScenes数据集的2.8万帧动作标注和1,194段视频意图标注。

Conclusion: 新方法LUR和RLUR实现了高效、易用且准确的不确定性估计，在驾驶场景OOD检测上具有实际应用价值，相较部分主流方法大大简化了训练和调参流程。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [162] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 本研究提出了一种利用手绘螺旋和波浪图像结合机器学习对帕金森病（PD）进行早期检测的方法，显著提升了诊断准确率，为非侵入性、低成本的PD早筛提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断对改善患者预后至关重要，然而传统诊断手段繁琐且成本高昂，因此亟需一种高效、低成本的新型辅助诊断技术。

Method: 本研究采用卷积神经网络（CNN）、迁移学习及注意力机制分析手绘螺旋和波浪图像，并通过数据增强提升数据丰富性，模型结构分为预训练CNN、定制卷积层及集成投票三阶段，最终采用hard voting整合多模型预测结果。

Result: 在实验中，螺旋图像的加权平均值precision、recall和F1分数均为90%，波浪图像为96.67%；结合hard voting集成后，总体准确率达93.3%。

Conclusion: 手绘螺旋和波浪图像结合机器学习方法，能为帕金森病的早期筛查提供准确、非侵入且低成本的解决方案，具备广阔应用前景。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [163] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文系统综述了Video-LMM（视频大型多模态模型）在后训练阶段的三大核心方法，并提出了分类体系和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 视频理解任务因需复杂的时空关系、多模态信息整合而极具挑战。随着Video-LMM崛起，其后训练如何使模型从基本感知跃升为推理引擎，文献中却缺乏系统总结。

Method: 提出将Video-LMM后训练方法分为三大类：1）链式思维的有监督微调（SFT）；2）基于可验证目标的强化学习（RL）；3）通过增强推理计算的测试时扩展（TTS），并对代表性方法进行系统性梳理和比较。

Result: 归纳总结了主要方法的特点、相互联系，以及面向视频的特殊适应性；系统剖析了它们在时间定位、时空对齐、长视频处理和多模态集成等挑战上的表现。

Conclusion: 本综述提出了后训练方法的完整分类和比较框架，指明当前的难点（奖励设计、可扩展性、性能与成本权衡），同时整理出评测基准和重要资源，为后续研究者提供指导。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [164] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 本文提出利用3D基础模型进行跨视角段匹配，有效提升了极端视角变化下图像区域的对应能力，且在ScanNet++和Replica数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 段匹配任务比关键点匹配更能应对遮挡、光照和视角变化，但在大视角差条件下仍具挑战性。本文旨在借助3D基础模型的空间理解能力，解决极端视角变化下的段匹配难题。

Method: 提出一种新架构，利用3D基础模型的归纳偏置，将其空间理解能力应用于极端（最高180度）视角变化下的图像对段匹配。与SOTA方法如SAM2 video propagator和局部特征匹配法进行了对比实验。

Result: 在ScanNet++和Replica数据集上，所提方法在AUPRC指标上相较SOTA方法提升最高达30%。同时，在3D实例分割和目标导航等下游任务中也展现出优势。

Conclusion: 利用3D基础模型的空间信息能显著提升大视角变化下段匹配的鲁棒性和精度，对相关视觉任务具有重要推动作用。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [165] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 本文提出了一种针对对比度失真的无参考图像质量评估方法，通过生成伪参考图像来实现更高精度的质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估方法多数集中在模糊、噪声等失真类型上，对比度失真的视觉效应和特性与其他常见失真不同，且被忽略，因此需要针对对比度失真的专门评估方法。

Method: 首先利用多种对比度增强算法，生成与真实参考图像视觉上相近的伪参考图像，将无参考质量评估问题转化为全参考评估问题。为此，构建了一个大型的对比度增强图像数据集，并训练分类网络，根据图像内容和失真类型选择最合适的对比度增强算法生成伪参考图像。最后，通过全参考方式评估伪参考与失真图像的质量差异。

Result: 在三个包含对比度失真的公开数据库（CCID2014、TID2013、CSIQ）上测试，所提方法表现优异，显示出良好的评估能力。

Conclusion: 提出的基于伪参考生成的无参考对比度失真图像质量评估方法有效提升了评估准确性，填补了对比度失真图像质量评价领域的空白。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [166] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的神经可塑性模块化分类器，用于垃圾和工业表面缺陷的图像分类，在实际环境中表现优于传统静态模型。


<details>
  <summary>Details</summary>
Motivation: 高效和准确的垃圾分类及工业表面缺陷检测对实现可持续垃圾管理与高标准质量控制至关重要，但现有静态模型难以应对动态和复杂的实际环境需求。

Method: 该模型融合了ResNet-50作为局部特征提取器，以及Vision Transformer(ViT)用于捕捉全局语义信息。同时结合FAISS相似性检索机制，丰富模型特征空间。另外，提出神经可塑性模块化结构，其可在训练过程中动态扩展模块，灵感来源于生物学习系统。

Result: 在垃圾图像分类和KolektorSDD2金属表面缺陷检测数据集上的实验结果均显示，所提模型在准确度和适应性方面优于传统静态模型。

Conclusion: 神经可塑性模块化分类器兼具高性能和扩展性，为实际环境中的垃圾和工业表面缺陷分类提供了可行高效的解决方案，适用于环境与工业领域。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [167] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一套系统性方法，旨在提升AI模型在结构化视觉（如图表、数学图等）生成与编辑方面的能力，并通过数据集、模型训练和评测基准推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型虽能生成美观的自然图片，但面对结构化视觉内容时，如图表、流程图和数学图形，常因需要复杂的布局规划、文本渲染和多模态推理，导致事实正确性和编辑性较差。因此，研究者有必要专门针对结构化视觉场景进行改进和系统研究。

Method: 作者首先构建了一个包含130万对高质量结构化图片对的大型数据集，所有图片对均来源于可执行绘图程序，并辅以逐步推理注释。基于此数据集，作者设计并训练了一个融合VLM和FLUX.1 Kontext的统一模型，通过轻量连接器加强多模态理解。采用三级课程式训练策略，实现特征逐步对齐、知识注入和推理增强生成，并在推理阶段引入外部推理器提升表现。最后，作者提出了新的评测基准StructBench和对应指标StructScore，支持对复杂结构化视觉生成与编辑进行精细化评测。

Result: 在对15个主流模型的评估中，发现即便是最先进的闭源系统，在结构化视觉生成方面表现仍不理想；所提模型则在编辑等任务上表现突出，并且推理增强在多种架构下都带来一致提升。

Conclusion: 该工作通过发布结构化视觉大数据集、统一模型与测评基准，为多模态结构化视觉内容的统一基础研究提供了重要推动，将有助于后续相关技术的发展和应用。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [168] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种框架，实现来自不同世界（如卡通和写实）的角色在文本驱动视频生成中的自然互动，有效解决身份混淆和风格失真问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本驱动的视频生成难以让风格差异大、历史上从未共存的角色进行真实自然的互动，角色身份和行为容易丢失，并产生风格“串味”（如写实角色变得卡通化）。亟需方法突破这一生成瓶颈。

Method: 提出了两项关键技术：（1）跨角色嵌入（Cross-Character Embedding, CCE），用于学习和保持每个角色的身份与行为逻辑，（2）跨角色增强（Cross-Character Augmentation, CCA），通过合成共存和混合风格数据扩充训练集，提高系统泛化与鲁棒性。

Result: 在包含10个卡通与写实角色的专门基准集上评测，结果显示框架在角色身份保持、互动质量和风格鲁棒性上均明显优于现有方法，有效防止风格串味。

Conclusion: 该框架首次实现在文本驱动下，让从未共存且风格差异巨大的角色自然互动，且保留各自风格，为多角色生成故事和新型视频内容创作开辟可能。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [169] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一种结合多模态推理与视频生成的框架，通过关键帧指导提升视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管当前视频生成模型能生成流畅、视觉效果好的视频片段，但在模拟复杂动态和因果连贯性方面仍有不足，尤其难以准确建模视觉结果和状态转移。而大语言和多模态大模型在视觉推理和未来预测方面表现强大。因此，作者希望结合两者优势，提升视频生成的连贯性和复杂动态模拟能力。

Method: 提出VChain框架，通过在推理阶段利用多模态大模型（如GPT-4o）推理出关键视觉状态，生成一组稀疏的关键帧，并以这些关键帧为锚点，对预训练视频生成器只在这些时刻进行稀疏性微调。这样既高效，又避免了密集监督。

Result: 在多个复杂、多步骤的视频任务上进行广泛实验，结果显示VChain能显著提升所生成视频的整体质量。

Conclusion: VChain有效提升了视频生成模型在复杂动态和因果连贯性上的表现，并通过高效、低开销的方法结合了多模态推理能力和视频生成技术。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [170] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 本文分析了Transformer语言模型中注意力分数对内容依赖较弱、关注范围较广的注意力头，并提出了一种可通过一段"校准文本"，仅依靠模型权重和该文本来近似推断第一层的输出表征方式。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型内部不同注意力头在处理语境时的表征方式，尤其是识别哪些头关注范围更广、对具体词汇内容不敏感，由此揭示早期层神经网络对文本高级特性编码的机理。

Method: 1. 分析GPT2-Small第一层多个注意力头的注意力分布和分数对内容的敏感性。
2. 证明在词分布固定时，这些头的softmax分母（归一化因子）比较稳定。
3. 用一段校准文本采样softmax分母，并结合各头输出来对周围文本做近似线性总结。
4. 仅依靠权重和校准文本，追溯和识别出数百个对高级语境属性有响应的神经元。

Result: 实验证明，可以不用广泛数据，仅用第一层参数和一段校准文本，即可联立推断出许多响应高级语境特征的神经元，包括那些在校准文本中未被激活的神经元。

Conclusion: 该方法揭示了Transformer模型早期层存在大量表征语境属性的神经元，为自动理解或解释深度神经网络内部机制提供了新工具，并有助于在不依赖大规模数据的情况下挖掘模型表征能力。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [171] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种面向文本图的图检索与推理方法Graph-$S^3$，在大语言模型（LLM）辅助下进行复杂图问答任务，显著优于现有检索方法。


<details>
  <summary>Details</summary>
Motivation: 许多真实世界的数据表现为文本型图结构，将这类数据与LLMs结合有助于复杂的图问答应用。但当前基于LLM的文本图问答的主要瓶颈在于检索阶段，如何高效找到既有信息量又精简的子图内容。现有方法或依赖浅层嵌入相似度，或需高昂标注与训练成本，检索表现不佳。

Method: 作者提出Graph-$S^3$框架，以LLM驱动的智能体作为检索器，采用合成的逐步监督训练（而非单次结果导向），通过离线提取的“黄金子图”来为每一步检索提供奖励。主要技术包括黄金子图的数据合成、奖励生成流程，以及基于此的双阶段交互图探索策略学习。

Result: 在3个主流数据集上，与7个强力基线方法相比，Graph-$S^3$平均准确率提升8.1%，F1分数提升9.7%，在复杂多跳推理任务上提升更为显著。

Conclusion: Graph-$S^3$通过采用逐步合成奖励的LLM自监督检索策略，有效提升了文本图问答的检索效果，为后续更复杂的图问答场景奠定基础。研究成果即将开源。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [172] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 论文研究了大语言模型(LLM)在日常任务中表现出的隐性价值观，并与人类和不同LLM之间进行了比较，发现LLM在价值观上的表现经常与人类和其他LLM不一致。


<details>
  <summary>Details</summary>
Motivation: 动机在于，虽然AI助手可辅助用户完成日常任务，但对于它们在执行带有主观性的任务时表现出的内隐价值观知之甚少，这些价值观如环保、慈善、多样性等在人类决策中很重要。作者希望揭示LLM在日常任务中的价值观取向，并与人类进行对比。

Method: 作者选取了六个主流LLM和100名美国众包人类参与者，设计了30项包含主观性的日常任务，系统性地比对LLM及人类在这些任务中的取向和表征的隐性价值观的异同。

Result: 研究结果显示，LLM在隐性价值观方面表现并不总是与人类一致，也不总是与其它LLM相符，呈现出较大差异性。

Conclusion: 作者认为目前的LLM在处理具备主观判断色彩的日常任务时，其表现出来的价值观与人类及其它模型差异明显，这对于后续提升AI助手的人类价值观契合度具有重要启示意义。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [173] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本文提出了CSAR算法用于从新兴语言语料库中诱导形态素，并在多个人工和实际数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 语言学和人工智能领域对自动化获取词素（morpheme）的方法有很高需求，尤其是在新兴语言和构造语言上，缺少先验知识时的自动化分析手段尤为重要。

Method: CSAR是一种贪心算法，利用词形与词义之间的互信息对形态素进行加权评分。每次选出权重最高的词-义配对，从语料库中移除已选对，然后重复上述操作，逐步诱导出所有形态素。该算法的有效性在程序生成的数据集和真实人类语言数据上进行了测试，并与相关任务的基线方法做了对比。

Result: 实验结果表明，CSAR在人工生成数据集上优于现有基线；在人类语言数据上的表现也合理地反映了实际语言现象。同时，还通过分析新兴语言的结果，量化了同义性和多义性等语言学特征。

Conclusion: CSAR算法能有效自动诱导新兴语言语料中的形态素，并能反映出语言学上的关键信息，为自动语言结构分析提供了新思路。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [174] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron是一种统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态与联合模态检索，在复杂真实场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界的信息检索变得越来越复杂，传统基于文本的检索方法难以处理包含丰富视觉、语义信息的文档（如PDF、幻灯片、视频）；已有成果表明，利用文档布局和多模态信息可以提升检索效果，因此需要一个能统一处理多模态内容的模型。

Method: 提出了Omni-Embed-Nemotron模型，将检索范围从文本和图像扩展到音频和视频，实现了跨模态（如文本与视频之间）和多模态联合检索。论文描述了模型架构、训练流程及其评测方案。

Result: 通过在文本、图像和视频检索任务上的实验，验证了该模型在多模态检索场景中的有效性与优越表现。

Conclusion: Omni-Embed-Nemotron能够高效统一地处理多种模态内容，为复杂信息需求提供了新的解决方案，推动了多模态检索技术的发展。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [175] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本论文提出了一种基于信号博弈的突现通信环境，通过优化超参数，以提升人工语言与人类语言的相似性，并利用XferBench作为度量标准。


<details>
  <summary>Details</summary>
Motivation: 当前突现通信研究中的人工语言通常与人类语言存在较大差异，缺乏系统性方法去优化和评估人工语言与人类语言的相似度。因此，研究一种方法来产生更加拟人化的突现语言，促进人工智能与人类更好沟通，具有重要意义。

Method: 作者设计了一个基于信号博弈的通信环境，并针对其中的超参数开展优化。以XferBench作为客观函数，系统量化了突现语言迁移至人类语言过程中与人类语言的统计相似性。此外，还分析了熵（entropy）对迁移学习性能的预测能力，并验证了突现通信系统的熵最小化性质。

Result: 通过实验，论文表明合适的超参数设定可以生成具有更好迁移能力、且与人类语言更加相似的突现语言。研究还发现熵的高低可以较好地预测迁移学习效果，并进一步确认了突现语言系统趋向熵最小化的特性。

Conclusion: 论文归纳指出，选择和优化恰当的超参数有助于产生更贴近人类语言的突现语言体系，熵在理解和构建高效通信系统中的作用值得进一步重视。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [176] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: 该论文提出了SEER基准，用于检测大模型能否定位文本中具体表达情感的片段，而非仅对句子整体判别情感类别。作者通过新标注的数据和多模型评测，发现现有大模型在处理较长文本时表现下降。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别任务往往只为整句打标签，忽略了情感通常只由特定词组表达。实际应用如共情对话或临床支持更需要知道“情感是如何表达”的，而不只是“是什么情感”。因而需要能精准识别情感证据的基准。

Method: 作者构建了SEER基准，包含1200条真实语句，分别标注了情感类型和表达情感的具体证据（文本片段）。涵盖单句和5句短文两种任务。评测了14个开源大模型在证据句段识别上的表现，并进行了错误分析。

Result: 实验发现部分大模型在单句任务上接近平均人类水平，但在处理5句长文时准确率下降。错误多表现为过度依赖情感关键词、在中性文本中产生假阳性。

Conclusion: LLMs在细粒度情感证据识别上仍有提升空间，尤其在跨句长文本中易出错，未来模型和标注标准需进一步优化。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [177] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文发布并分析了首个大规模阿拉伯语LLM检测数据集ALHD，用于区分人类与大模型生成文本，覆盖不同体裁和方言，推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前用于区分人类与大模型生成文本的数据集大多局限于英语等少数语言，且规模有限。阿拉伯语缺乏高质量、全面、用于检测的公开数据集。鉴于阿拉伯语使用广泛，且存在多种体裁和方言，该领域急需可靠数据资源来支持LLM检测、减少虚假信息、学术不端及网络威胁等风险。

Method: 作者构建了ALHD数据集，覆盖新闻、社交媒体、评论三类体裁，包含MSA与多种方言，精选自三款主流LLM及多种真实人类来源，总计40万样本，均衡分布。进行了严格的预处理和详细标注，保证实验可复现。用该数据集，对传统分类器、BERT系模型及大模型（零样本/少样本条件）进行基准测试，深入分析各模型优劣及跨体裁泛化能力。

Result: 微调后的BERT模型整体表现优异，超越零样本/少样本LLM，但所有模型在跨体裁泛化任务上均出现困难，尤其面对新闻文本，LLM生成内容更像人类文本，检测难度提升。

Conclusion: ALHD奠定了阿拉伯语领域LLM检测研究的基础，有助于应对相关风险，并明确指出模型在跨体裁、体裁特异性泛化方面亟需突破，为后续研究指明方向。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [178] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出了一种名为TS-Reasoner的新方法，有效融合时间序列基础模型（TSFM）和大语言模型（LLM）以提升时间序列推理能力，并在多个基准任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM虽然能很好地做预测，但缺乏推理和理解能力，而LLM则理解和推理强但对时间序列数据感知弱。两者的有效结合是提升时间序列推理能力的关键，却缺乏对齐与融合的高效训练方法。

Method: 该研究提出TS-Reasoner，核心方法是在TSFM的潜在表示与LLM的文本输入之间进行对齐。具体做法包括：构建多样的合成时间序列与文本描述对进行对齐训练，然后通过两阶段训练（对齐预训练后指令微调），且训练过程中冻结预训练好的TSFM参数。

Result: 实验证明，TS-Reasoner在多个基准测试中都优于主流LLM、视觉语言模型（VLM）及时间序列LLM，且数据利用效率高（所需训练数据量少于一半）。

Conclusion: TS-Reasoner有效提升了时间序列推理能力，且具备很高的数据效率，对相关领域有较强的应用与推广价值。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [179] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 本文针对在特定领域使用RAG进行推理时输出过于通用的问题，提出了一种对比性推理方法，通过比较类似案例以获得更专业、具体的答案，并在金融领域实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法虽然能提取事实信息，但在专业推理、如金融分析中，结果往往过于宽泛、缺乏针对性，无法像人类专家一样发现细微差异并做出有针对性的分析。为弥补这一不足，作者希望让RAG能够检索和对比类似案例，实现更深入的专业推理。

Method: 作者提出在RAG系统之上加入对比推理层（peer-aware comparative inference layer），该模块能够检索、比较与当前问题相关的类似案例或问题，从而生成更具针对性和洞见性的答案，而非一味输出通用事实。

Result: 经过实验验证，该对比推理方法在文本生成指标（如ROUGE和BERTScore）上优于传统RAG，特别是在对比人类专家生成的股权研究和风险分析材料时表现更佳。

Conclusion: 相比单纯的信息检索，加入对比推理机制能使RAG系统在专业领域的推理任务中生成更具体、更贴合实际需求的解答，有望提升RAG在金融等专业领域的实用性与可信度。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [180] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出了一种新的数据结构Consensus Graphs (ConGrs)，用于整合和挖掘语言模型在同一提示下多次采样所得响应中的共识与变异性，从而合成更准确与稳健的最终内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效综合多轮采样生成的长文本中丰富的不确定性信号，亟需更灵活、有力的机制整合多样化输出以提高生成文本的可靠性和一致性。

Method: 设计了一种基于有向无环图（DAG）的数据结构ConGrs，利用轻量级的生物信息序列比对算法对不同响应进行对齐和合并，辅以二级语言模型判别器优化判断。同时，根据具体任务设计了针对性的解码方法用以从ConGr中合成最终输出。

Result: 在两项人物传记生成任务中，基于ConGr的合成响应提升了31%的事实准确率，并比现有方法减少80%以上对判别器的依赖。在三个需要拒答无解问题的任务中，弃答率提升了56%。在MATH和AIME推理任务中，准确率相比自验证和多数票提升了6个百分点。

Conclusion: ConGrs能够有效捕捉和利用语言模型响应中的变异信号，通过整合多样化采样信息显著提升生成内容的可靠性、事实准确性和拒答能力，提供了比现有方法更灵活且高效的方案。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [181] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文探讨在指令微调数据中引入扰动（如去除停用词、打乱单词顺序）是否能提升大语言模型（LLMs）对噪音指令的抵抗能力，并发现这种方法有时可以提升模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，LLMs对指令措辞的细微变化很敏感，这影响了模型的鲁棒性和泛化能力。为提升LLMs对用户输入噪音的容忍度，作者希望验证通过扰动指令微调数据可否解决这一问题。

Method: 在微调数据中引入不同类型的扰动（如去除停用词、打乱词序），并用这些扰动数据对LLMs进行指令微调。之后，分别在原始和扰动后的主流评测集（MMLU、BBH、GSM8K）上测试模型表现，并分析其学习动态与行为变化。

Result: 实验结果显示，经过扰动指令微调的模型，在某些情况下，其性能优于只用原始指令微调的模型。模型的下游任务表现和对噪音输入的鲁棒性有所提升。

Conclusion: 在指令微调过程中加入扰动，不仅能提升LLMs面对不规范指令时的表现，还可能整体增强模型对下游任务的适应能力。建议今后的模型开发中，将扰动指令纳入微调流程，提高模型实用性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [182] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: 本文提出TriMediQ框架，将患者对话转化为三元组并集成到知识图谱中，从而提升大模型在多轮医疗问答中的推理能力，并在基准数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽在单轮医疗问答任务中表现良好，但与实际临床多轮互动的需求不符，而且在需要处理对话日志进行推理时表现显著下降。这一差距限制了其在真实临床场景中的应用。

Method: 作者提出TriMediQ方法，将患者响应转化为三元组，再集成到知识图谱（KG）中。包括一个冻结的三元组生成器（保障事实一致性）和一个可训练的投影模块（图编码器+投影器），用于捕捉KG中的关系信息。训练时仅微调投影模块，LLM权重冻结。在推理时，利用微调模块增强多跳推理能力。

Result: 在iMedQA等交互式医疗问答基准数据集上进行实验，TriMediQ较五个主流基线方法在iMedQA上准确率提升最高达10.4%。

Conclusion: 将患者多轮对话结构化成三元组知识图谱，能大幅提升大模型在多轮医疗推理场景下的准确性，为LLM作为医疗助手提供了有效的工程路径。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [183] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 本文针对在大型语言模型（LLM）应用于文本分类前后的关键环节展开讨论，指出概念化和后续统计推断的重要性与易被忽视的问题，以及由此产生的系统性偏差，这些偏差无法仅通过提升 LLM 准确率或事后校正消除，并提出实际建议。


<details>
  <summary>Details</summary>
Motivation: LLM 在计算社会科学中的广泛使用让研究者可能跳过对待测概念的系统定义，从而产生概念化偏差，影响所有后续推断。作者希望揭示这一问题并强调概念化阶段的重要性。

Method: 通过模拟实验，分析忽略概念化过程带来的偏差，并评估典型的提高模型准确率或事后偏差校正方法是否足以消除这些问题。

Result: 模拟显示，概念化错误产生的偏差无法单纯依靠模型精度提升或事后修正方法消除。

Conclusion: 研究提醒 CSS 领域研究者在 LLM 时代同样要重视概念化工作，并提出具体、低成本、无偏、低方差的估算建议以指导实际操作。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [184] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: 本文提出了CCD-Bench基准，用于评估大语言模型（LLM）在跨文化价值冲突情境下的决策能力，发现现有模型更倾向于北欧和德意志欧洲的价值观，缺乏对其他文化群体价值冲突的真实多样性处理。


<details>
  <summary>Details</summary>
Motivation: 现有的文化相关基准主要关注文化知识、价值预测或单一偏见评测，未能评估LLM在多元文化价值观直接冲突时的决策过程。因此需要新的基准来研究当不同文化价值观相冲突时模型如何进行决策。

Method: 作者构建了CCD-Bench基准，包含2182个跨越七个领域的开放式两难问题，每个问题都附有十个匿名的回应选项，代表十个GLOBE文化集群。采用拉丁方设计分层排列以减少顺序效应，并在17个非推理型LLM上进行了评测。

Result: 模型明显偏向北欧和德意志欧洲的选项，对东欧和中东北非的选项代表性不足。尽管大部分模型给出的理由会引用多个文化维度，但实际上仅表面多元，鲜有深刻的权力、权益、性别平等等角度的分析。排列效应影响极小，开发商谱系而非地理区域决定了模型偏好。

Conclusion: 当前的模型对多元文化价值冲突的应对不足，仅呈现表面多元化，亟需更能包容真实多元世界观的对齐与训练方案。CCD-Bench为评价模型的多元决策能力提供了新工具，并指出模型在权力、权益及性别等议题下的不足。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [185] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: 本文提出了一种新型Reactive Transformer (RxT)架构，通过集成短期记忆(STM)机制，实现了对对话历史的高效管理，将大规模语言模型在长对话中的推理成本从二次方降为线性，并大幅提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer结构虽然在语言模型领域表现优异，但由于其无状态（stateless）特性以及对长序列的二次方复杂度，导致在长对话场景下效率低、成本高。现有方法通过重复处理长对话历史，导致推理延迟和资源消耗增加，亟需新的架构来提升效率和可扩展性。

Method: 设计了一种事件驱动的新架构RxT，将对话的每一次交换视为独立事件，每轮通过生成器-解码器基于当前输入和前一记忆状态生成响应；随后，内存编码器与专用注意力网络异步更新固定大小的STM，实现状态管理。这样解耦的设计使得响应生成与记忆更新相互独立，提高了对话流畅性和计算效率。

Result: 在合成数据上的原型实验表明，RxT在推理速度和性能上优于同等规模的无状态模型，实现了常数时间的推理延迟，并大幅减少了用户侧的总推理成本。

Conclusion: RxT有效突破了传统Transformer在长对话管理中的计算瓶颈，实现实时、可扩展且经济高效的对话系统，为未来大语言模型在对话AI领域的应用奠定了基础。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [186] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了参与EvalLLM 2025法语生物医学命名实体识别（NER）和健康事件抽取竞赛的三种方法，主要结合LLM、注释规范、合成数据和后处理。最佳结果由GPT-4.1通过精心设计提示获得。


<details>
  <summary>Details</summary>
Motivation: 在法语生物医学领域数据稀缺的情况下，如何提升NER和健康事件抽取的性能。尤其关注乎少量标注或合成数据下的大模型应用效果。

Method: 提出三种方法：(1) 基于GPT-4.1的in-context learning，自动选择范例结合注释规范摘要作为提示；(2) 微调后的GLiNER在合成数据上训练，并用LLM后处理结果；(3) LLaMA-3.1-8B-Instruct同样在合成数据上微调。事件抽取采用类似的ICL策略。

Result: GPT-4.1结合精心设计的提示，在NER任务上获得宏F1为61.53%，事件抽取为15.02%。

Conclusion: 良好提示设计对于极低资源场景下提升大模型性能至关重要。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [187] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Deco-G的新型解码框架，可将格式遵循与任务求解解耦，提升大语言模型在复杂指令下的表现，兼顾格式与内容的正确性。实验表明在多项任务中该方法均优于常规提示方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理含复杂任务描述与严格格式要求的指令时，常因两者混合导致无法同时满足内容与格式的规范。研究者希望通过将这两类指令进行解耦，提高模型在执行复杂任务时的整体表现。

Method: 提出Deco-G解码框架，通过另外的概率模型TPM处理格式规范，确保输出格式合规；同时仅对LLM给出任务相关指令，解耦格式与内容。新方法包括指令感知蒸馏、灵活的trie算法和HMM状态剪枝，提升工程效率。

Result: Deco-G在数学推理、LLM评判、事件参数抽取等多样化任务与多种格式约束下均取得1.0%-6.0%的相对性能提升，保证了输出严格的格式合规。

Conclusion: Deco-G有效提升了大语言模型对复杂格式和任务指令的执行能力，证明解耦格式与内容能带来更优输出，为多格式复杂任务提供了切实可行的方法。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [188] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 本文指出现有LLM评测基准过于简单，难以反映模型在高信息密度场景下的表现，因此提出使用需要结构化关系推理的任务进行评估，结果表明LLM在此类任务中出现记忆滑移与遗忘的临界长度比现有基准评测结果更短。


<details>
  <summary>Details</summary>
Motivation: 当前主流的LLM评测任务（如'needle in a haystack'或连续预测）场景单一，无法充分评估模型在复杂、高密度信息推理任务中的能力。实际应用中，模型需从含噪声自然语言中推导结构化知识，这要求更高的推理与记忆能力，因此有必要探索更贴近实际推理需求的评测方法。

Method: 作者提出让LLM从包含噪声与长距离相关线索的自然语言文本中归纳出关系结构（如图谱），以此替代简单的续写或检索任务，测试模型在复杂推理场景下的有效上下文长度和遗忘特征。通过实验比较了当前LLM及OpenAI o1等推理专用模型的表现。

Result: 实验发现，在结构化关系推理任务下，LLM的记忆滑移和遗忘发生得更早，有效上下文长度比现有基准测试中更短。即使是面向推理优化的专用模型也暴露出相似的缺陷。

Conclusion: 现有大语言模型在从非结构化输入中抽象结构化知识、长程推理等方面存在明显局限，未来需通过架构层面对其进行改进，以增强复杂推理能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [189] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 本文提出了一种基于音节级掩码语言模型的无监督语音识别（UASR）方法，无需复杂资源且提升了效果，尤其适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 现有无监督语音识别方法多依赖音素级建模，需要昂贵的字母到音素（G2P）转换器；且对音素边界不明确的语言效果不佳。为了降低资源消耗并适应更多语言，需要新的方法。

Method: 作者提出用音节级别的UASR框架，并采用掩码语言建模，不依赖G2P，也规避了生成对抗网络（GAN）带来的训练不稳定问题。

Result: 在LibriSpeech数据集上，方法使字符错误率（CER）相对降低了40%；对中文普通话等音素边界模糊的语言也表现出良好的效果。

Conclusion: 所提音节级掩码语言模型的UASR系统在消除依赖、提升鲁棒性及拓展适用语言范围方面有显著价值，对低资源语言和更复杂多模态任务有很大推动作用。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [190] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了UniDoc-Bench，这是第一个大规模、真实场景下用于多模态检索增强生成（MM-RAG）的评测基准，基于来自8个领域、7万页PDF文档，生成了1600个多模态问答任务，以系统衡量多模态检索生成的实际效果。


<details>
  <summary>Details</summary>
Motivation: 现有的MM-RAG评估要么只关注文本或图像，要么采用过于简化的多模态设置，无法反映实际以文档为中心的多模态使用场景，因此需要开发更真实、系统的基准来推动该领域发展。

Method: 构建包括文本、表格和图像的文档数据集，通过管线自动提取并关联证据，生成多类型（事实性检索、比较、摘要、逻辑推理）多模态问答对，并通过统一的协议和度量标准支持不同检索范式（文本、图片、多模融合、多模联合检索）对比。部分问答对经多位标注者和专家复核确保可靠性。

Result: 实验证明，多模态文本-图像融合RAG系统在基准上性能优于仅用文本、仅用图片或者基于联合多模嵌入的检索；同时发现当前多模态嵌入还有不足，且文本和图像信息互补；分析还揭示了系统性失效模式。

Conclusion: 单一文本或图片信息不足以支撑实际多模态检索生成任务，当前多模态嵌入尚需提升。UniDoc-Bench为后续多模态RAG技术发展和评价提供了重要参考和实用建议。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [191] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了基于QLoRA的微调框架，提升对Roman Urdu-English混合文本中攻击性语言的检测效果，通过将数据自动翻译为英文并使用LLM进行微调，在低资源场景下取得优异表现。


<details>
  <summary>Details</summary>
Motivation: Roman Urdu等代码混合语言中的贬损词语检测因语法不明、拼写不统一及标注数据稀缺，对NLP系统构成挑战。解决该问题对于网络内容审核和用户保护至关重要。

Method: 作者首先将Roman Urdu-English混合语料数据用Google Translate自动翻译为英文，然后利用QLoRA（一种高效微调技术）分别对多种Transformer和LLM（如Meta LLaMA 3 8B、Mistral 7B v0.1等）进行微调。训练和评估基于人工标注数据，任务为攻击性与非攻击性内容分类。

Result: 在所有测试模型中，Meta LLaMA 3 8B取得了最高的F1分数（91.45），Mistral 7B紧随其后（89.66），均优于传统Transformer基线。显示QLoRA在低资源代码混合文本场景下微调高性能模型的有效性。

Conclusion: 该方法为Roman Urdu内容审核提供了可扩展的方案，并验证了大语言模型在多语言、低资源攻击性检测中的潜力，为未来多语种攻击性检测系统奠定了基础。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [192] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedReflect的新框架，通过模拟医生反思式的推理模式，提升大语言模型（LLM）在医学问题解决上的表现，且无需外部检索和大量标注即可实现显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在医学问题解决中通常需要依赖外部知识检索或大量标注数据，这导致检索过程开销大、标注成本高，而且效果仍有限。为此，作者希望通过让模型自身实现反思推理，减少对外部资源的依赖。

Method: MedReflect框架让LLM模拟医生思维，包括初步假设生成、自我提问、自我回答和决策优化，形成单轮反思链，不依赖外部检索，也不需要重度标注，并将其应用于医学数据集构建。

Result: 通过仅使用2,000个随机训练样本和轻量微调，MedReflect在多个医学基准上实现了准确率的大幅提升，且显著减少了数据标注需求。

Conclusion: MedReflect显示了LLM具备通过自我反思提升医学问题解决能力的潜力，能减少对外部监督和专用数据的依赖，为医学AI领域带来成本效益更高的解决方案。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [193] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 该论文提出TreePrompt方法，通过学习大语言模型（LLM）的偏好，在树状结构中选取高质量、相关性的示例，用于提升少样本机器翻译任务效果，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的示例选择方法主要关注查询和示例的相似性，而忽视了示例本身的质量，这限制了LLM在翻译任务中的表现。因此，作者希望寻找一种能够兼顾相关性和示例质量的新示例选择方法。

Method: 作者提出了一种新颖的TreePrompt方法，其在树状结构下通过学习模型偏好，选择高质量且语境相关的示例。此外，将TreePrompt与K-NN和自适应少样本提示（AFSP）结合，探究相关性和质量的平衡，并与随机选择等方法进行对比。

Result: 在英-波斯语（MIZAN）和英-德语（WMT19）这两个翻译任务中，将TreePrompt与AFSP或随机示例选择相结合，均能提升翻译性能，证实了方法有效。

Conclusion: TreePrompt方法能够有效结合LLM示例的高质量与相关性，显著改善少样本翻译任务的结果，优于仅关注相似性的传统方法。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [194] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 该论文提出了一种基于语音细粒度分析的帕金森病多语言检测方法，通过分析意大利语、西班牙语和英语数据集，发现音素级别特征对于疾病检测的诊断性能最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的语音检测帕金森病方法多分析整句，容易遗漏特定语音单元的诊断信息。研究人员希望通过更细粒度的语音特征（音素、音节、词）来提升帕金森病跨语言检测的准确率。

Method: 该研究开发了自动管道，能从录音中提取与时间对齐的音素、音节和单词。基于意大利语、西班牙语和英语数据集，采用双向LSTM和多头注意力机制，对不同细粒度的特征进行诊断性能比较。

Result: 音素级别分析效果最佳，达到93.78%的AUROC和92.17%的准确率，且优于音节和词级别分析。注意力机制显示最具诊断价值的特征与临床常用语音测试相符，如音素级的长元音、音节级的顺念音节、词级的/pataka/序列。

Conclusion: 通过精细语音单元分析能显著提升帕金森病的多语言检测能力，特别是音素级最有效。该方法与临床检测方法一致，具有较强实用价值。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [195] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）中的few-shot提示在词义消歧（WSD）任务中的应用，发现样本分布不均会影响多语言词义消歧表现，但对英语影响较小。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，few-shot提示因其实用性广受关注。研究者希望进一步了解该方法在词义消歧中的效果及其局限性，尤其是样本分布不均带来的偏差。

Method: 采用GLOSSGPT提示方法测试英语、德语、西班牙语、法语和意大利语五种语言上的词义消歧任务，并比较两种主流LLM（GPT-4o和LLaMA-3.1-70B）在不同样本分布下的表现。

Result: 实验发现，在多语言WSD任务中，如果few-shot样本分布不均，则会引发错误的词义预测。而这种问题在英语任务中并不明显；两种大模型对样本分布均较敏感。

Conclusion: few-shot学习中的样本分布平衡对于提升多语言词义消歧的准确性至关重要，应确保提示样本的多样性和代表性。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [196] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 本文介绍了Rezwan，这是一个包含超过120万条圣训的大规模AI辅助圣训语料库，通过全自动流程实现数据提取和结构化，并在各环节进行了多语言、智能加标与语义分析等丰富性增强。


<details>
  <summary>Details</summary>
Motivation: 当前数字伊斯兰研究缺乏大规模、结构化且方便人工智能应用的数据资源，传统的人工处理耗时耗力且成本高昂。作者希望通过AI技术极大提升圣训语料加工效率和数据质量，拓宽数字人文与伊斯兰学的研究能力。

Method: 作者基于大型语言模型（LLMs）搭建了全自动处理管道，实现了圣训分段、链-正文分离、校验、多层增强（含多语言翻译、智能加标、摘要、主题标引和语义关联）。通过与Maktabat Ahl al-Bayt等数字资源对接，并嵌入多语言和智能摘要等AI技术，完成了超大规模语料建设。最终用专家抽样校验核心环节并与Noor Corpus等人工语料进行对比。

Result: 在1213条圣训的专家评测中，链-正文分离和摘要等结构化任务的准确率接近人工水平（9.33/10），但在智能加标和语义相似性检测仍有提升空间。与人工构建的Noor Corpus相比，Rezwan整体数质俱优，平均得分8.46/10远高于Noor的3.66/10，并大幅降低人工成本。

Conclusion: 该工作证明了AI技术能极大提升宗教文本处理的效率和质量，为伊斯兰研究和数字人文领域建立了新的数据和方法范式，大规模、多语言、智能丰富的语料基础极大拓展了研究者的工具箱。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [197] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）在社会政治语境下生成和识别深层认知框架的能力，并揭示其隐藏表示中与特定认知框架高度相关的特征维度。


<details>
  <summary>Details</summary>
Motivation: 在人类沟通和认知中，理解语境和认知框架至关重要。作者希望探究LLM是否能够捕捉和表达这些复杂的人类认知结构，尤其是在社会政治语境下。

Method: 研究者首先评估LLM生成和识别特定认知框架（如'严格父亲'与'关爱父母'框架）的能力；随后借鉴可解释性研究方法，分析模型隐藏层中与这些框架对应的特征维度。

Result: LLM在文本生成和识别认知框架能力上表现优异，即便在零样本（zero-shot）任务中也能有效识别。研究还定位了模型内部与特定认知框架高度相关的单一特征维度。

Conclusion: LLM不仅能够生成和识别复杂的人类认知框架，还在其内部表示中以可辨识方式捕捉这些语义，有助于更深入理解LLM如何表达有意义的人类概念。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [198] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 本文针对大型推理模型（LRMs）在复杂任务中过于啰嗦（overthinking）的问题，提出了Step Pruner（SP）方法，有效减少推理冗余步骤并降低回复长度，同时保持甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）方法通过惩罚生成的token数来促使模型简洁，但少token不等于少推理步骤，且模型会通过合并或删除必要步骤来“hack”奖励机制，影响推理质量。因此需有新方法专注于推理步骤本身的紧凑性和正确性。

Method: 作者提出Step Pruner（SP）框架：用“step-aware”奖励函数，将主要奖励分配给紧凑且正确的推理步骤，对冗余和错误推理进行惩罚，并设计动态停止机制——一旦单步输出长度超出上限即停止更新，防止模型作弊。

Result: 在四个推理任务基准上，SP方法在保持甚至提升准确度的同时，大幅度压缩模型输出长度。例如，在AIME24数据集上，使token用量减少了69.7%。

Conclusion: Step Pruner不仅提高了推理效率，还能防止模型训练中的作弊行为，为大模型推理优化提供了可靠且实用的新范式。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [199] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 本文比较了人工与基于大模型的体育新闻语篇修辞关系标注，发现DistilBERT具有最佳性能。


<details>
  <summary>Details</summary>
Motivation: 自动和准确地识别和分类文本中的修辞关系对于提升语篇理解和自然语言处理任务具有重要意义。人工标注耗时且主观，亟需借助先进的自动化方法提升效率与一致性。

Method: 研究者利用INCEpTION工具对板球新闻进行修辞关系标注，将人工标注与BERT、DistilBERT、Logistic Regression等自动模型进行对比，并评估不同模型在区分elaboration、contrast、background、cause-effect等修辞关系的表现。

Result: DistilBERT模型在自动修辞关系分类任务中取得了最高的准确率，优于BERT和Logistic Regression。

Conclusion: DistilBERT在语篇修辞关系识别方面比其他模型表现更佳，展示了基于transformer的NLP模型在语篇分析中的应用潜力，推动了自动语篇解析和自然语言处理的结合。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [200] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 本论文首次构建并发布了涵盖孟加拉语政治立场标注的大型新闻数据集，同时对28种大语言模型在该任务上的性能进行了全面评估，为低资源语言媒体立场识别研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 南亚地区媒体偏见检测尤为重要，但目前针对孟加拉语政治立场的标注数据集和计算方法研究稀缺，影响了针对该语言的相关工作进展。

Method: 作者收集了200篇具有重大政治意义和争议性的孟加拉语新闻，并标注为亲政府、批评政府和中立三种立场；同时，提出诊断性分析方法，评估28个专有及开源大语言模型在该任务上的表现。

Result: 大语言模型在检测批评政府立场上表现良好（F1最高达0.83），但在处理中立新闻时表现不佳（F1最低为0.00）；模型存在明显的亲政府立场预测偏差，对语义模糊的表述易产生误判。

Conclusion: 该数据集和相关分析工具为孟加拉语媒体立场检测提供了首个系统性资源，有助于提升低资源语言大模型的性能与公正性，并为未来相关研究奠定了基础。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [201] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: 本研究提出了PsychoLexTherapy，这是一个用于波斯语心理治疗推理模拟的对话系统框架，特别优化用于小模型和本地部署，通过结构化记忆支持多轮、多文化对话，结果在多项基准评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 波斯语等小语种在心理健康AI对话系统领域缺乏代表性，现有系统难以兼顾文化适应性与本地隐私需求。该研究旨在解决如何在小语种环境下开发兼具隐私保护、持续对话能力、心理治疗知识和文化适应性的对话框架。

Method: 开发流程分三步：1）用PsychoLexEval测试小语言模型心理学知识；2）设计、实装面向推理的PsychoLexTherapy框架，包含结构化记忆用于多轮对话；3）构建真实波斯语用户问题数据集（PsychoLexQuery）和混合模拟对话数据集（PsychoLexDialogue），并与多种方法（如简单提示、多智能体辩论、结构化推理路径）做基准对比。

Result: 在PsychoLexQuery数据集上，PsychoLexTherapy在自动与人工评价中优于所有基线方法；多轮对话中，结构化长时记忆显著提升连贯、同理性与个性化，避免了简单历史拼接的信息丢失与逻辑混乱。

Conclusion: PsychoLexTherapy是一套实际可用、保护隐私且具有文化适应性的波斯语心理治疗对话系统基础方案，贡献了高质量数据集和评测流程，并为结构化记忆支持的心理推理对话系统提供了有力的实证依据。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [202] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大语言模型（LLM）的自动分析流程，对美国近23万名医生的410万份患者评价文本进行了大规模的人格特质和患者主观评分分析，揭示了医生人格、专业性及患者满意度之间的关联，并发现了性别、专业和医生类型等系统性差异。


<details>
  <summary>Details</summary>
Motivation: 患者如何看待医生对医疗信任、沟通和满意度有重要影响，但传统方法难以在大规模数据中客观、系统地衡量医生的人格和患者的主观判断，因此需要新的自动化分析工具。

Method: 研究利用大语言模型开发了一套分析流程，自动从大量患者文本评价中推断医生的五大人格特质和五项与患者相关的主观评价。方法通过多模型对比和专家标注校验，并与患者满意度数据做相关性分析。

Result: LLM评估与专家标注一致性高（相关系数0.72-0.89），与患者满意度相关（r=0.41-0.81，均显著）。全国分析发现：男医生在所有人格和能力评分上均高于女医生；儿童科和精神科更突出同理心特质；所有特质均能正向预测总体满意度。聚类还发现了四种医生类型。

Conclusion: 使用大语言模型自动从患者叙述中提取医生特质是可解释、有效的，可以大规模洞察医患关系，推动医疗质量衡量、偏见检测和医务人员发展。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [203] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了首个用于评估大语言模型（LLM）在复杂、多回合任务环境下欺骗行为的仿真框架，并发现欺骗行为具有模型依赖性、在压力下更容易发生且会削弱信任。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究发现LLM在特定压力下可能发生欺骗，但大多数评估局限于单轮提示，难以反映现实中欺骗通常在长期、多回合互动中逐步展开的实际情况。缺乏系统手段来检测和分析LLM在多步骤任务中的欺骗行为及其对信任的影响。

Method: 作者设计了多智能体仿真系统，包括一名任务执行者（负责多步任务）、一名监督者（评估及提供反馈，并维护信任状态），以及一名独立的欺骗审计者（审查整个行为轨迹并识别欺骗）。该框架适用于多轮任务，有动态情境压力。实验覆盖了11个前沿的闭源和开源LLM模型。

Result: 实验表明，不同模型表现出不同的欺骗倾向；在情境压力增加时，欺骗发生率上升，同时会持续削弱监督者对执行者的信任。质性分析发现，模型会采用隐瞒、模棱两可甚至伪造信息等多种欺骗策略。

Conclusion: 该研究表明，在多回合交互中，LLM欺骗是一个新兴风险，对实际应用中的信任和安全构成挑战。所提出的评估框架为未来LLM在现实世界中进行信任敏感型应用的评测提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [204] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的实体知识增强方法，提升了社交媒体和生物医学文本中COVID-19相关命名实体识别（NER）的效果。


<details>
  <summary>Details</summary>
Motivation: COVID-19相关社交媒体文本内容非正式、标注稀缺，且需要丰富领域知识，现有NER方法难以应对。

Method: 提出了一种用于COVID-19的实体知识增强方法，可用于提升对非正式和正式文本中生物医学命名实体的识别能力。

Result: 在COVID-19推特数据集和PubMed数据集上实验表明，所提方法在全监督和小样本设置下均显著提升了NER性能。

Conclusion: 实体知识增强方法能有效改善疫情相关及一般生物医学NER任务，适用于不同文本环境，具有较强实用性。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [205] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 该论文提出了AgriGPT-VL套件，一个专门面向农业领域的多模态大模型框架，包括最大规模的农业视觉-语言数据集、定制的农业多模态模型及配套评测基准，对比主流大模型在农业任务上实现了性能突破。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型快速发展，但农业领域因缺乏领域专属模型、数据集和客观评测体系，难以实现落地应用。因此，作者希望解决农业领域多模态应用的痛点，推动AI在农业的专业化发展。

Method: 1) 构建Agri-3M-VL农业视觉-语言语料库，包含大量图像-文本对、视觉问答、专家标注和强化学习样本。2) 研发AgriGPT-VL农业专用视觉-语言模型，通过分阶段的文本结合、多模态深浅层对齐和GRPO强化优化提升推理能力。3) 建立AgriBench-VL-4K评测集与多维评测策略，涵盖开放性与图像相关问题，并利用大模型评分机制。

Result: AgriGPT-VL在农业多模态评测基准AgriBench-VL-4K上大幅优于各类通用VLMs，在文本-only测试集AgriBench-13K上也表现优异，无语言能力损失。消融实验证明模型对齐和强化优化显著提升性能。

Conclusion: AgriGPT-VL套件为农业多模态智能提供全方位资源和工具，在数据、模型和评测上均填补了行业空白，并承诺开源，有望推动低资源环境下的农业智能研究和应用。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [206] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 本文旨在通过分析大语言模型（LLM）的内部激活，预测其输出结果的正确性，并评估外部上下文对模型生成的有效性。作者发现，仅用模型中间层的激活特征，就可较高准确率预测输出是否正确，同时能判别上下文信息的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型用途广泛，但由于其有时会高置信地产生错误信息，可信度依然受质疑。随着应用的拓展，迫切需要有方法提前判断模型输出是否正确，以及外部提供给模型的上下文是否有效且合适。

Method: 作者利用可解释性技术，分析模型激活（特征），并训练简单分类器识别输出正确性。设计新指标判别外部上下文是正确、错误还是无关，并对六个模型进行了实验评估。

Result: 实验表明，基于输出首token的中间激活训练的分类器可75%左右准确率预测输出正误；提出的基于模型内部状态的指标显著优于提示词基准，能更好区分上下文类型。

Conclusion: 通过分析LLM内部特征能较早发现和预警不准输出与不良上下文，为提高模型可信度和理解其决策机制提供了新手段。代码已公开。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [207] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文首次系统性研究了泰语文本端实时对话中的回合结束（EOT）检测，提出了基线方法并优化了准确率和延迟之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于音频静音的回合结束检测延时高且容易受犹豫、语言特有现象影响，不能满足流畅、及时的语音交互需求。针对泰语，相关研究极少，缺乏有效的EOT检测基线。

Method: 将EOT检测建模为分词边界上的二分类问题，使用YODAS数据集字幕文本及泰语语义特征（如句末助词），比较了小型LLM模型的zero-shot和few-shot提示方法，以及轻量化transformer的有监督微调；同时提出可公开实现方案。

Result: 实验发现，不同方法在准确率和延迟间存在清晰权衡。小型且经过微调的模型能达成接近即时的EOT判定，适合本地设备部署。

Conclusion: 本研究为泰语实时对话EOT检测提供了公开基线，证明小型微调模型可满足低延迟、高准确率的需求，为实际应用打下基础。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [208] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 本文研究了将反事实推理融入大语言模型（LLMs）决策解释中的作用，并提出了一种新的评估方法来量化分类任务中关键词的重要性。实验证明，引入反事实推理能够有效提升识别关键字词的能力。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在文本分类任务表现优越，但通常作为黑盒使用且调用成本高，如何有效解释其分类决策成为一个实际且紧迫的问题。作者希望探索借助反事实推理提升解释效果。

Method: 作者提出并使用了一种名为decision changing rate的框架，通过更改输入的关键词来观察LLM分类决策的变化，从而量化关键词对于模型决策的重要性，并在方法中引入了反事实推理。

Result: 实验结果显示，引入反事实推理后，LLM在识别对决策有贡献的高重要性词汇时表现更佳，具有明显提升。

Conclusion: 利用反事实推理配合决策变化率框架，有助于更好地解释LLM的分类决策机制，提高了模型可解释性。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [209] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文评估了小型语言模型（SLMs）在急诊科（ED）决策支持中的应用潜力，并构建了相关基准；结果显示，一般领域训练的SLMs在多项医学任务中优于医学专调模型。


<details>
  <summary>Details</summary>
Motivation: 大模型（LLM）虽表现出色，但在硬件资源、运营成本、隐私等方面难以落地于急诊等高压力场景；小模型（SLM）因推理能力强、效率高，有望更好支持急诊医生。

Method: 编制综合基准，涵盖医学知识及通用推理能力，选用通用和医学混合语料训练的小模型，并在MedMCQA、MedQA-4Options、PubMedQA等数据集上评估其在急诊场景下的表现。

Result: 综合实验证明，通用领域训练的小模型在急诊相关任务中，整体优于医学专业微调小模型。

Conclusion: 在急诊场景下，通用领域训练的小型语言模型性能更优，专门医学微调并非必需，有助于降低部署门槛。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [210] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 本论文研究了如何让大型语言模型（LLMs）具备可引导的多元价值输出能力，提出并评估了多种结合链式思维（CoT）方法，其中基于可验证奖励的强化学习效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型通常只反映单一、均质的价值观，难以胜任那些需要理解复杂人类多元观点的任务。因此，需要开发能够灵活采纳和对齐不同视角的模型。

Method: 论文尝试了多种实现模型多元可引导性的方式，包括：CoT提示工程、人类撰写CoT的微调、基于合成解释的微调，以及采用可验证奖励的强化学习（RLVR）。主要在Value Kaleidoscope和OpinionQA数据集上进行实验。

Result: 不同方法中，基于可验证奖励的强化学习（RLVR）在多项评测中表现最好，且训练样本效率高。对模型生成的推理链条进一步从忠实性（faithfulness）和安全性（safety）角度进行分析。

Conclusion: 链式思维技巧可以有效提升模型灵活表达多元观点的能力，其中RLVR最为突出。这为实现更加“可引导多元输出”的大模型提供了有力支撑。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [211] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在数据有限时表现出优秀的数据效率，这主要归功于输入token的随机掩码，以及其他随机正则方法的共同作用。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现扩散语言模型在小数据集上也能取得很好的训练效果，但其背后的机制尚未明了，因此本研究旨在系统分析和解释其高数据效率的来源。

Method: 作者进行了大量消融实验，比较不同训练技术（如输入token随机掩码、MLP dropout和权重衰减）对模型数据效率的影响，以分离和量化每个因素的效用。

Result: 实验发现，输入tokens的随机掩码对数据效率的提升起到主导作用；同时，MLP dropout和权重衰减等随机正则方法也能带来相似的提升。

Conclusion: 随机正则化措施（如随机掩码、dropout和权重衰减）在多轮训练时能普遍提升模型的数据效率，这为理解扩散语言模型的高效机制提供了新的视角。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [212] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 本文提出了PoLi-RL，一种用于条件语义文本相似性任务（C-STS）的新型点到列表强化学习框架，在C-STS基准上获得了Spearman系数新SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统语义文本相似性（STS）存在歧义，C-STS引入条件减少歧义，但现有方法多为判别式模型，未充分利用大模型与强化学习的最新进展。

Method: 提出PoLi-RL，两阶段课程训练：先用点式奖励训练基本能力，再用点式、对式、列表式混合奖励精细化判别；创新提出并行片段排序奖励（PSRR），为每个完成单独分配排名反馈。

Result: 在C-STS官方基准上，PoLi-RL在cross-encoder结构上实现Spearman 48.18的新SOTA。

Conclusion: 率先将RL成功应用于C-STS，验证了该方法在复杂排序类条件判断任务中的有效性，为LLM训练引入了更精细且强大的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [213] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了Caco框架，通过代码驱动自动合成高质量、多样化、可校验的大型语言模型推理数据，并证明其在数学推理任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂任务时，推理能力至关重要，但现有的链式思维（CoT）方法存在生成不受控、质量和多样性不足等问题。借助代码增强CoT已显示改善，但受限于特定数学场景，难以通用。作者希望构建一个可扩展、自动化且高质量的推理数据生成框架，推进LLM推理能力和泛化能力。

Method: 提出了Caco（Code-Assisted Chain-of-ThOught）框架，使用代码格式统一现有数学和编程解答，微调生成器后自动扩展多样推理轨迹。采用代码执行验证和规则过滤，确保逻辑正确和结构多样，最终将筛选输出反向转换为自然语言指令和CoT，构建数据集，实现完整自动闭环推理数据合成。

Result: 实验构建了Caco-1.3M数据集，Caco训练模型在数学推理基准上表现优越，超越主流强基线。分析表明，代码校验和指令多样性提升了对新任务的泛化能力。

Conclusion: Caco框架实现了无需人工参与的自动化推理数据合成，提升了大语言模型在数学推理等任务中的能力和泛化，确立了自动、自校验、高可信推理系统的新范式。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [214] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究系统评估了大语言模型（LLM）对隐喻理解的能力，发现其在概念映射、隐喻-字面知识及句法敏感性等方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在知识整合、上下文推理和创造性生成方面表现突出，但其理解隐喻的机制尚未被充分研究。隐喻分析对语义理解具有挑战性，因此需探究LLM在此方面的表现及局限。

Method: 从三方面评估LLM的隐喻处理能力：（1）通过嵌入空间投影评估其在目标域的概念映射能力；（2）通过分析隐喻词与对应字面词，识别模型的固有隐喻知识；（3）考研模型对隐喻性句法结构的敏感性。

Result: 研究发现，LLM在15%−25%的情况下给出与概念无关的解释，对隐喻指示词依赖较多而非上下文，对句法异常较为敏感，但对结构理解不够强。

Conclusion: 现有LLM在隐喻分析中存在多方面限制，因此需要发展更健壮的计算方法以提升对隐喻的理解能力。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [215] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 本文介绍了涵盖斯里兰卡议会记录、法律判决、政府出版物、新闻及旅游统计的多语种开放文档数据集。数据集包含三种官方语言，随时更新，并便于学术研究及NLP应用。


<details>
  <summary>Details</summary>
Motivation: 斯里兰卡多语言环境下，缺乏开源、高质量文档语料库限制了语言学、法律、社会和NLP等领域的研究与发展。作者旨在解决这一数据资源匮乏问题。

Method: 作者系统性地整理和收集了斯里兰卡主要官方文件、新闻和统计信息，涵盖三种语言（僧伽罗语、泰米尔语、英语），统一格式处理，建立可公开访问的数据集，并设立自动更新机制。

Result: 当前共整理出215,670份文档（60.3GB），囊括13个子数据集，通过GitHub和Hugging Face开放获取。数据集每日更新，内容覆盖广泛，适合多领域、跨语言研究。

Conclusion: 该开放多语种数据集大大丰富了斯里兰卡语境下的研究资源，为计算语言学、法律分析和多语言NLP等方向提供了有力支撑，还考虑了数据的伦理和许可问题。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [216] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 本文针对当前大型语言模型主要基于英语及其文化语境训练，导致在其他语言和文化背景下表现不足的问题，提出了一种通用的数据集准备与模型再训练方法，并以Gemma 2模型和某些弱势语言为实验案例，提升了模型在这些语言下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型主要受众是英语及其文化圈，导致模型在其他语言和文化下表现不佳，无法满足多元文化和弱势语言社群的实际需求。研究动机是为了弥补这一缺陷，实现生成式AI的文化包容性，促进非主流文化和语言的数字内容生成与保护。

Method: 本文开发了一套通用的数据集准备方法，用于构建与特定文化及弱势语言相关的训练数据，并将这些数据用于对Gemma 2模型进行后训练（post-training），以提升该模型在目标语言和文化下的表现。

Result: 对Gemma 2模型进行所述后训练后，在目标弱势语言和文化背景下其生成内容的表现明显提升，证实了所提方法的有效性。

Conclusion: 提出的方法为提升大型语言模型在非主流语言和文化中的表现提供了范例和工具，可推广至其他国家和语言，帮助各地开发符合自身文化传统的生成式AI，助力本地语言保护与文化传承。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [217] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 作者提出了一种新方法SSD（Self Speculative Decoding），可以在不影响生成质量的情况下大幅提升基于扩散的大模型（dLLMs）的推理速度。


<details>
  <summary>Details</summary>
Motivation: 目前的扩散式大型语言模型（dLLMs）具有并行生成和双向关注的优势，但现有的并行解码方法与逐步解码结果有差异，可能导致性能下降，影响实际应用。

Method: 提出了一种无损耗的推理加速方法SSD。SSD利用dLLM本身，同时作为草稿和验证者。模型在一次前向传递中对多个位置进行生成和多层次验证，无需额外的草稿模型或辅助模块，减少冗余与内存开销。

Result: SSD方法在开源模型LLaDA、Dream上达到了最高3.46倍的推理速度提升，而生成结果与逐步解码完全一致。

Conclusion: SSD显著提升了基于扩散的大模型推理效率，无需额外模型或内存增加，生成质量无损，为相关模型的实际部署提供了更优方案。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [218] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: 本文提出了Latent Thought Policy Optimization（LTPO），一种在不更新模型参数情况下，通过优化潜在向量提升大模型推理能力的方法，并在多个推理基准任务中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的潜在推理虽然效率高，但在复杂、分布外任务中表现脆弱，亟需提升其稳健性与泛化能力。

Method: LTPO将中间'潜在思维'向量视作可优化参数，使用基于自信度的奖励信号，通过在线策略梯度方法在测试时动态优化，无需外部监督和文本生成，也不更新模型参数。

Result: 在五个主流推理基准上，LTPO不仅与强基线持平甚至超越，尤其在极具挑战性的AIME基准下，显著优于其他潜在推理方法，准确率大幅提升。

Conclusion: LTPO展现出在困难推理任务上无与伦比的稳健性和复杂推理能力，为提升LLM推理可靠性和实用性提供了新方法。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [219] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出了CALM框架，利用专家提示提升大型推理模型（LRM）在优化建模任务中的表现，并在此基础上开发了STORM模型，表现优于绝大多数同类产品。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型（LRM）虽然具备复杂多步推理能力，但现有的领域自适应方法难以充分挖掘这些能力，尤其是在传统的非反思性（non-reflective）数据集上的直接微调效果不佳。因此，作者希望提出更适合LRM本身推理模式的新型适应方法，以提升其在优化建模任务中的实际表现。

Method: 作者提出了CALM框架（Corrective Adaptation with Lightweight Modification），流程包括：1）专家识别模型推理过程中的缺陷并给出简要校正性提示；2）模型接收提示后修正推理轨迹，生成带有高质量校正数据；3）通过监督微调和后续强化学习，逐步提升模型性能。通过微小干预（不超过2.6%的生成内容），即可显著改善适应数据质量。基于该过程训练的模型被命名为STORM。

Result: 基于CALM方法得到的STORM模型（4B参数）在五个主流优化建模基准上取得68.9%的平均准确率，达到了当前最佳水平，并能媲美超大规模（671B参数）模型。

Conclusion: 动态、基于提示的数据合成方法能够保护并放大现代LRM的推理能力，比传统数据集微调更高效、更具可扩展性，为解决复杂优化建模任务提供了专家级新路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [220] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reward-Enhanced Policy Optimization（REPO）的强化学习后训练框架，显著提升了大语言模型在OTA平台中作为商务谈判代理的说服力、合规性和响应质量。


<details>
  <summary>Details</summary>
Motivation: 在OTA实际场景中，价格谈判需要在满足旅客付款能力与酒店盈利目标之间平衡，且需多轮、口语交流、严格遵守流程与合规，不得过度承诺或出现编造。现有SFT或单一奖励优化方式，不能充分捕捉说服风格，也难以保障各类业务规则精准落实。

Method: 作者提出REPO框架，结合三类异质奖励信号：1）基于偏好的人类对齐奖励模型（RM），2）高阶行为与流程合规的奖励裁判（RJ），以及3）确定性数值、格式和合规检查的程序化奖励函数（RF）。通过简单机制融合三者，防止奖励投机，提升对实际谈判场景的适用性和结果可控性。

Result: 在约150轮真实和225轮问题对话集测试下，REPO平均评分达4.63，显著高于基础模型和多种强化学习基线，提升高质量对话比例，并大幅改善失败案例修复率，解决的修复干净度也最高。

Conclusion: REPO能够有效提升大语言模型在OTA等商谈场景中的实际表现，不仅更好捕捉人类说服策略和合规要求，还展现出主动共情、本地化推理等新能力，优于传统训练技术。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [221] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型生成文本在词汇、语义和风格上趋于同质化，可能导致‘知识坍缩’，即可获取的信息范围变窄。通过提出新方法测量LLM输出中的知识多样性，并在多模型、多主题、多国家和多提示条件下，实证分析了这一问题。结果显示，与网络搜索及维基百科相比，大部分模型在知识多样性方面存在明显不足，尤其大模型更为突出，而检索增强生成（RAG）方法对提升多样性有效但效果因文化语境而异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型趋于生成同质化内容，长远看可能导致信息多样性缺失，危及知识生态。然而，现有研究评估同质化的方式有限，缺乏对时间和文化背景下的系统分析，因此需要提出更科学的方法衡量知识多样性并做广泛实证检验。

Method: 提出衡量知识多样性（epistemic diversity）的新方法，通过对27个LLM、覆盖12国的155个主题、200种来源于真实用户的提问提示，系统评估和比对不同模型输出多样性。并分析了模型规模、RAG策略及跨文化差异影响。

Result: 新一代LLM虽多样性略有提升，但绝大多数模型输出的知识多样性仍低于基本网络搜索和维基百科。模型规模越大，知识多样性越低。采用RAG能够提升多样性，但不同国家背景表现不一。模型生成的本地话题回答更偏向英语语境，未能很好反映本地知识特征。

Conclusion: 大语言模型存在输出知识趋同带来的多样性危机，尤其是模型愈大问题愈严重，RAG是改善手段但非万能。此外，现有模型未能有效代表多元文化知识。本研究提示未来LLM需关注知识多样性和全球代表性。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [222] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出了一种创新的双语链式思维(Language-Mixed CoT)推理范式，并在韩语场景下验证其有效性，显著提升了多语言大模型的推理能力，模型与数据已开源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理场景中多采用长链式思维方式，但相关知识蒸馏方法主要集中在英语，针对非英语，特别是各具体语言的推理能力提升研究较少。因翻译可能带来信息丢失或噪声，存在现实需求填补多语种推理能力提升的方法与资源空白。

Method: 提出Language-Mixed CoT，将英语作为推理支点，与目标语言动态切换，减少翻译带来的人为痕迹。以韩语为例，构建了大规模Yi-Sang多领域Prompt&推理过程数据集，基于Qwen等六种主流大模型家族，训练了9个不同规模模型，并对比Mono CoT等基线方法进行消融分析与评测。

Result: 最优模型KO-REAson-35B在五项基准任务上取得第一，总分领先，平均提升18.6分。消融实验表明，双语链式思维推理在单语、多语言及多模态场景下均优于单语链式思维。

Conclusion: Language-Mixed CoT有效弥补了当前多语言推理大模型的短板，显著提升了模型在非英语语言上的推理能力。相关数据与模型开源，促进该领域研究发展。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [223] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文提出了LT-Swap基准，用于测试语言模型在极少数新词学习上的能力，发现目前主流模型对罕见词表现较差，且不同架构差异显著。


<details>
  <summary>Details</summary>
Motivation: 尽管儿童能以极少数据学会新词，但现有语言模型的评估指标更关注常见词，对少见词（长尾词）学习能力的评估不足。该工作旨在填补这一研究空白。

Method: 作者提出了LongTail-Swap（LT-Swap）基准，通过构建基于训练语料的特定测试集，设计语法和语义上可接受与不可接受的配对句子，专门用来考察模型对罕见词的泛化。评估方式为零样本，计算每对句子的平均对数概率，适配于10M和100M词规模的标准训练集，并在16个主流模型进行对比。

Result: 实验表明，主流语言模型在罕见词（长尾词）测试集上的表现普遍较差，并且不同模型架构在长尾任务上的表现差距比在常见词任务上更大。

Conclusion: 该基准能够更好地揭示语言模型对低频新词的泛化能力，为模型架构设计及评估提供了新视角；相关代码也已开源，供社区使用。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [224] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 本文提出了一个累积量展开方法，用于量化大语言模型在下一个词预测中对高阶统计结构的学习过程，并通过实验证明该方法可有效揭示模型捕捉不同类型上下文与特征的能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型内部如何捕捉和利用高阶统计相关性一直是个开放性问题，现有分析手段难以分辨模型内部关于高阶结构的学习动态。因此，作者希望提出一种数学上严谨、计算开销小的新方法，来理解大语言模型在不同训练阶段学习的数据特征。

Method: 作者提出将每层softmax输出的熵视为其分布中心的扰动，并通过累积量展开推导出逐阶累积量的封闭公式。这能够分离出各阶统计相关（如方差、偏度、峰度等），通过实验证明：在结构化prompt下，累积量随层数呈现上升-平台特征，而随机排列prompt保持平坦，指示不同上下文结构对模型表示的影响；训练过程中，累积量单调上升并饱和，直观展示了模型从捕捉低阶到高阶特征的内在过程；数学类prompt与自然语言prompt具有不同签名，揭示了不同任务处理机制。

Result: 实验证明，该累积量指标可以敏感地区分模型对有意义上下文和随机顺序的处理，并能揭示不同内容（如数学和一般文本）在模型中的处理差异。此外，该方法可清晰展示模型学习过程中高阶统计特征的变化轨迹。

Conclusion: 累积量分析方法为研究高维神经网络内部特征学习动态提供了一个轻量、高效且数学上扎实的定量工具，有助于深入理解大语言模型中的表征与推断机制。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [225] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: 本文提出SliceMoE，一种将token隐藏向量切分为多个子片段并分别路由到不同专家的新型MoE结构，实现更高的推理效率和专家利用率。


<details>
  <summary>Details</summary>
Motivation: 传统MoE通过token级路由导致单个专家需处理较宽泛的语义，容易导致瓶颈、负载失衡和专家特化能力有限。为解决这些问题，作者引入更细粒度的切片级路由策略。

Method: SliceMoE将token的高维embedding分割为若干切片，每个切片由一个轻量共享路由器选定前k个专家处理。专家独立处理后，切片重新组装为完整输出。除基本框架外，还引入切片级容量损失、跨切片dropout和高效的批量GEMM算子。

Result: 在WikiText-103语言建模、WMT英德翻译和三个文本分类数据集上，SliceMoE推理速度较稠密基线快1.7倍，困惑度比同参数token-MoE低12-18%，专家分工和负载均衡显著改善，且专家掌握更具可解释性的语法与语义子空间。

Conclusion: SliceMoE通过切片级路由有效利用专家容量，提高推理效率及任务性能，同时促进专家间更合理的分工和负载，实现MoE架构的进一步突破。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [226] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 本文提出了一种结合机器学习和深度学习的混合方法，用于波斯语面向方面的情感分析，并通过多语言BERT和决策树提升了准确率，取得了当前最佳表现。


<details>
  <summary>Details</summary>
Motivation: 因波斯语标注数据集稀缺、预处理工具有限、高质量嵌入及特征提取手段缺乏，使波斯语情感分析难度较高。

Method: 提出一种混合（机器学习+深度学习）方法：利用多语言BERT提取极性分数作为特征，并引入到决策树分类器中。同时构建了波斯语同义词和实体词典，用于文本增强（同义词及命名实体替换）。

Result: 在Pars-ABSA数据集上，实现了93.34%的准确率，超过了现有基准。混合建模和特征增强在低资源语言情感分析中表现出良好效果。

Conclusion: 混合算法和特征增强方法可有效提升低资源语言（如波斯语）情感分析的准确性，所提出的新词典和训练方式推动了该领域前沿发展。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [227] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG（检索增强生成）框架RDR2，充分利用文档结构信息，提升了大模型的事实准确性，并在多个数据集取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）能力突出，但容易产生事实错误。现有RAG方法检索到的文档常被当做孤立片段处理，未充分利用文档结构，影响信息整合与准确性。该文旨在解决检索内容结构信息缺失问题，提升RAG质量。

Method: 提出RDR2（Retrieve-DocumentRoute-Read）框架，引入基于LLM的路由系统，动态遍历文档结构树，根据内容相关性和层次结构选择最优证据片段，将文档路由设为可训练任务，自动进行动作遴选与结构感知检索，模拟人类阅读策略。

Result: 在五个具有挑战性的数据集上全面评估RDR2，结果显示其取得了SOTA（state-of-the-art）性能，相比传统RAG系统有显著提升，特别是在需要多个文档综合推理的复杂场景中。

Conclusion: 显式地引入和利用结构化文档信息，能显著提升RAG系统的知识获取和利用能力，RDR2为结构感知型RAG方法提供了新的范式，特别适用于复杂检索与综合任务。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [228] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言模型评估指标——分布式正确性得分（DCS），用于更全面地衡量模型对答案分布的不确定性表达，相较传统准确率等指标更加细致且能更好抑制幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型评估主要采用单一答案的准确率等指标，忽略了模型对不同选项不确定性的表达，容易导致模型偏向于随意作答而不是表达‘不知道’等真实不确定性，进而产生幻觉现象。

Method: 作者提出了DCS指标，能够对模型在多个答案分布上的置信度进行精细区分，尤其能区分对错误答案的过度自信与选择‘我不知道’之间的区别。通过理论分析和案例对其有效性进行说明，并将12个现有评测基准数据集用DCS变体进行适配测试。

Result: 在六个主流语言模型和12个评测基准上用DCS评估，发现有一半基准下，所有模型的DCS得分均为负值，显示出强烈的幻觉倾向。

Conclusion: DCS是一种更细致、能激励表达真实不确定性的评测方法，有助于降低模型幻觉风险。传统准确率无法捕捉模型不确定性和过度自信，未来评测应更关注概率分布特征而非单一答案。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [229] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 本文指出当前大语言模型在安全对齐方面存在“后果盲点”问题，即仅凭表面词汇特征判断风险，导致易被越狱或过度拒绝无害请求。作者提出了CB-Bench基准评估和CS-Chain-4k数据集以改进模型的后果推理。经新数据微调后，模型安全性和实用性均有所提升。


<details>
  <summary>Details</summary>
Motivation: 虽然当前大语言模型具有较好的安全对齐机制，但在实际使用中仍易被规避安全限制（越狱），或对带有敏感字面信号的无害请求出现过度拒绝。作者认为，这两种失效模式共同源于模型对行动与后果之间关系推理能力不足。希望通过更细致建模后果推理，提高模型安全判断的鲁棒性。

Method: （1）提出Consequence-blindness（后果盲点）概念，指出主流模型过度依赖表面词汇/风格线索，而忽视实际后果评分。（2）构建CB-Bench基准，涵盖4类风险情景，测试模型在语义风险与结果风险是否一致时的表现，包含匹配与未匹配场景。（3）提出CS-Chain-4k数据集，专门用于提升后果推理能力，并据此对模型进行微调。

Result: 实验证明，主流大模型在CB-Bench测试下普遍未能正确区分语义与实际风险，表现出广泛的后果盲点。用CS-Chain-4k微调后，模型在抵抗语义伪装越狱和减少对无害请求的过度拒绝上表现更好，且未损失其它基准上的通用性和效用。

Conclusion: 后果盲点是大模型安全对齐的系统性难题。提升模型后果推理能力是未来安全对齐的核心目标。CB-Bench和CS-Chain-4k为更实际、可复现的安全评估与能力提升提供了工具和路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [230] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文评估大语言模型用CONSORT标准判定临床试验研究报道质量的能力，提出了评测数据集，并比较了不同模型和提示方法的表现。最佳组合准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 临床试验报道质量直接影响临床决策，现有评估方式主观性强且效率有限。因此，探索用大语言模型自动评估报道质量有重要意义。

Method: 作者构建了基于CONSORT标准的评测数据集（CONSORT-QA），对来自通用和生物医学领域的大语言模型进行评测，采用包括Chain-of-thought在内的不同提示方法，评估其识别CONSORT标准条目的准确率和推理过程。

Result: 最佳模型与提示方法组合取得了85%的准确率。Chain-of-thought方法提供了更多关于模型推理过程的信息，提升了评估透明度。

Conclusion: 大语言模型具备较高的能力自动判定临床试验文章的报告质量，配合合适的提示技巧可达较高准确率。Chain-of-thought不仅提升了结果，还帮助理解模型的推理过程，有助于后续改进和应用。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [231] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 论文提出了一种名为“接种提示”（inoculation prompting）的方法，通过在微调数据前加上短指令，引导模型学习并抑制不想要的特征，从而提升语言模型在微调后的表现。


<details>
  <summary>Details</summary>
Motivation: 在微调语言模型时，通常会同时学到期望与非期望的行为特质，导致模型泛化出一些不良特质。本研究旨在控制或消除不良特质的学习。

Method: 在训练（微调）时，向训练样本前添加一条特定系统指令，明确诱导模型产生不良特质，测试时则不加该指令，通过比较“接种”与未接种模型对不良特质的表现，更好地选择性学习期望特质。

Result: 接种提示能有效减少不良特质的表达，在多个场景中都优于未修改数据训练；不仅能抑制不良特质，还能提升模型对特定特征的选择性学习能力，并可减少任务微调过程中的异常行为、后门注入和隐性学习带来的负面影响。

Conclusion: 接种提示是一种简单有效的选择性学习方法，不仅有助于实际模型训练，也揭示了语言模型泛化的内部机制，对理解和指导后续微调技术有理论和实际意义。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [232] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 本文针对预训练语言模型易受后门攻击的问题，提出了一种基于注意力与梯度信息的推理阶段检测方法，有效识别并缓解后门攻击，提高NLP模型安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型在各种NLP任务中取得了巨大成功，但它们在面对后门攻击时依然脆弱。现有防御方法效果有限，因此亟需新方法检测和减缓后门风险，确保模型安全可靠。

Method: 作者研究了带后门的预训练编码器模型，在处理带触发词的“投毒”输入时，发现注意力和梯度归因都明显偏向触发词。基于该观测，提出结合注意力和梯度信息，在推理阶段生成异常分数，用于检测潜在后门攻击。

Result: 大量关于文本分类任务和不同后门攻击场景的实验显示，所提方法相比现有基线能显著降低攻击成功率。同时，作者通过可解释性分析展示了异常分数机制对于定位触发器和防御鲁棒性的优势。

Conclusion: 本文方法能够有效检测并缓解预训练语言模型中的后门攻击，提高了模型在实际应用中的安全性和可解释性，为NLP模型防御后门攻击提供了新思路和有力工具。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [233] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 本文针对RAG系统在高风险场景下输出一致性不足的问题，提出了新的评估框架和用RL提升一致性的方法，并取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: RAG系统在语义等价的查询下，输出往往不一致，影响用户信任，尤其在需要高可靠性的应用中问题突出。当前缺乏系统化分析与效果提升手段。

Method: 1. 构建了一个可分解RAG信息一致性的评估框架，将一致性来源拆分为检索器层、生成器层和端到端整体；2. 提出以强化学习为基础的PS-GRPO方法，通过多路复述组一致性奖励，训练生成器结果在等价提问下保持一致；3. 针对大规模训练计算瓶颈，提出了高效的奖励近似计算方法。

Result: 在短文本、多跳推理和长文本等QA基准测试中，所提Con-RAG方法在一致性和准确率上均超越了强大基线，并且无需人工真值监督。

Conclusion: 提出的评估和训练方法有效提升了RAG系统在安全关键领域的可靠性和一致性，为实际部署提供了实用手段。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [234] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 本文提出了一种以人为本的评估语法纠错（GEC）工具的方式——通过分析编辑所需时间（PEET）衡量工具实用性，并发布了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 文本编辑通常需要多次修订，初始使用高效GEC工具能极大影响最终文本质量及编辑效率。为量化GEC工具的实际用户节省编辑时间的问题，作者开展了本研究。

Method: 作者构建了首个关于后期编辑（PE）时间的英文GEC测试数据集（BEA19与CoNLL14），并引入了PEET指标来评估和排名GEC工具，通过真实编辑时间数据对各类GEC工具进行比较分析。

Result: 实验证明，多数GEC工具能显著缩短人工编辑时间。分析发现，判断句子是否需修改、改写、标点调整等操作对编辑时间影响最大。同时PEET指标与人工评测结果高度一致。

Conclusion: 作者展示了PEET作为以用户为中心的新型GEC工具评估方式的有效性，并公开了数据与工具，拓展了此领域评测新方向。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [235] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: 本文提出了一种新方法（SECA），能够通过对提示词进行语义等价且语义连贯的现实化修改，有效诱发大型语言模型（LLM）产生幻觉，并通过实验证明其攻击成功率高、约束违规率低。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在高风险领域常部署，但容易产生幻觉，影响可靠性。之前的对抗性攻击方法往往依赖不真实的提示词修改，如填充胡言乱语或改变原意，导致这些方法难以揭示现实中幻觉出现的真实机制。相比于计算机视觉领域中真实、可感知的对抗攻击，文本领域缺乏类似现实而语义保真的攻击方法，因此有必要探索此方向。

Method: 提出了一种称为SECA的攻击方法，将诱发幻觉的真实攻击建模为受语义等价和语义连贯约束的优化问题，利用一种新的零阶约束保持搜索方法，在输入提示词空间中找到既对抗又合理的攻击样本。

Result: 通过在开放式多选问答任务中的实验，SECA方法在几乎不违反语义约束的情况下，实现了比现有方法更高的攻击成功率，适用于开源和商用的大语言模型，甚至包括梯度不可访问的商用模型。

Conclusion: SECA证明了LLM对现实语境下提示词的微小变化极为敏感，暴露出幻觉发生的潜在风险，为理解和改进LLM的鲁棒性提供了新工具。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [236] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）生成文本时对语义同序（semantic isotopy）的保持能力，设计了基于故事续写的大规模实验，证明LLM可在一定范围内保持文本语义结构。


<details>
  <summary>Details</summary>
Motivation: 过去对于分布式语义与结构语义之间的联系已有研究，但LLM生成文本是否保持细致且深层的语义结构（如语义同序），尚未被充分探讨。作者希望填补这一空白，明确LLM在语意连贯性和结构保持上的表现。

Method: 作者提出了基于10,000条ROCStories的故事续写实验，分别用五个LLM模型完成任务。首先用GPT-4o在一个标准语言学基准上验证提取语义同序的能力，再用其分析所有生成故事的语义同序结构，并从覆盖率、密度、分布等角度进行结构和语义属性分析。

Result: 实验表明，在设定的文本延展范围内，LLM生成的续写能够在多个属性上很好地保持原有的语义同序，即保持了一定的语义结构一致性。

Conclusion: LLM在自动生成文本时，可以有效保留语义同序的结构，这为今后LLM在文本语义和结构性生成任务中的应用提供了支持。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [237] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 本文探究了自然语言处理（NLP）领域中与社会公益相关研究（NLP4SG）的分布，分析了作者和会议的角度，发现针对社会公益的NLP研究大多出现在ACL体系外，并由非ACL作者完成。


<details>
  <summary>Details</summary>
Motivation: 近年来，NLP在促进社会公益方面的重要性和关注度逐渐提升，尤其是在联合国可持续发展目标的背景下，研究者希望了解NLP社区在社会公益领域的实际参与情况及趋势，以为今后的研究方向和政策制定提供参考。

Method: 作者从作者和发表会议（期刊）的两个层面出发，对NLP4SG领域的文献进行了数据量化分析，统计ACL社区和非ACL社区在社会公益相关议题上的研究发布比例，涵盖ACL主流作者及其它领域作者的行为对比。

Result: 主要发现有两点：1）ACL社区作者在ACL以外的其他领域更倾向于发表社会公益相关研究；2）绝大部分应用NLP解决社会公益问题的研究是由非ACL作者在ACL社区之外的会议和期刊发表的。

Conclusion: 这些结果反映出ACL社区对社会公益议题的关注度和参与度相对有限，且有较多相关成果出现在外部社区。研究呼吁ACL需更多关注NLP4SG议题，将其纳入未来的议程和发展策略。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [238] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 论文强调大语言模型(LLM)的输出不确定性量化对安全至关重要，并指出当前方法忽视了未观测序列的概率。作者建议在未来的不确定性量化方法中融入对未观测序列概率的估计，以提升检测模型幻觉的能力。


<details>
  <summary>Details</summary>
Motivation: 在安全敏感场景下，LLM输出不确定性直接影响应用安全性。目前主流的不确定性量化方法主要关注已观测输出的概率分布，忽视了尚未被观测的输出序列，其可能对整体不确定性评估有重要影响。

Method: 作者分析了现有基于输出分布熵的不确定性量化方法，指出其依赖多次采样得到的序列及其概率，实验研究了未观测序列概率的作用，并提出考虑该部分概率的重要性。

Result: 实验表明，未观测序列的概率对评估LLM输出不确定性十分关键。现有方法若不考虑这部分概率，易低估潜在不确定性。

Conclusion: 作者建议未来相关研究应重视并融合未观测序列的概率信息，从而提升LLM不确定性量化的准确性与适用性，增强安全场景下幻觉检测能力。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [239] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种高效结合有监督微调（SFT）与强化学习（RL）用于提升大型语言模型（LLMs）推理能力的方法，大幅减少数据用量并取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习和有监督微调各自对提升LLMs推理能力有优势，但单独使用存在高数据需求与过拟合等问题，现有结合方法还面临数据低效、算法依赖性强和灾难性遗忘等挑战。

Method: 提出了一个可插拔框架，动态将SFT与RL结合。方法通过挑选具有挑战性的样本用于SFT，降低数据需求，并采用高熵token参与损失计算、冻结RL关键参数，减缓灾难性遗忘影响。

Result: 在仅用先前最佳方法1.5%的SFT数据和20.4%的RL数据情况下，实现了最优推理表现。

Conclusion: 该方法为SFT与RL结合在推理后训练领域提供了高效、灵活和高性能的新解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [240] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: 本文提出了一种新的注意力机制——压缩卷积注意力（CCA），大幅降低了注意力的参数量、KV-cache和计算量，并进一步与分组查询注意力（GQA）结合成CCGQA，实现更优的性能与资源压缩。


<details>
  <summary>Details</summary>
Motivation: 传统的多头注意力（MHA）在处理长序列时计算量呈二次增长，KV-cache线性增长，导致训练和推理效率低下。现有方法如GQA和MLA虽然缩小了缓存，但未显著改善计算速度，限制了长上下文Transformer的应用优化。

Method: 作者提出压缩卷积注意力（CCA）：将query、key、value投影到低维潜空间，在潜空间内完成注意力计算，从而整体压缩参数、缓存和计算量。此外，通过与GQA结合形成CCGQA，以进一步优化计算与存储的折中，使用户可根据软硬件条件调整压缩比。

Result: 实验证明：在等KV-cache压缩情况下，CCGQA在稠密和MoE模型上均优于GQA和MLA。在MoE模型上更是能以仅GQA/MLA一半的KV-cache，提供8倍压缩且性能无损。CCA/CCGQA也显著降低了训练和prefill计算量，在H100 GPU上可将prefill延迟降低至MHA的1.7倍，加速反向传播至1.3倍。

Conclusion: CCA和CCGQA不仅显著压缩了注意力机制的参数、缓存占用和计算量，还在长序列任务中性能优异，极大提升了训练、推理效率，可广泛应用于大规模Transformer加速。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [241] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了一个名为PsySET的心理学驱动的基准，用于全面评估大语言模型（LLM）在情感和人格调控方面的有效性和可信度。


<details>
  <summary>Details</summary>
Motivation: 在实际社会交互中，控制LLM的情感和人格状态对于实现更人性化的互动体验非常关键，但目前缺乏系统的评估方法。

Method: 作者开发了PsySET基准，涵盖情感与人格领域，对多种LLM及其不同调控方式（如提示工程、微调和向量注入）进行对比评测，并综合考察模型安全性、真实性、公平性和伦理性。

Result: 提示工程虽稳定有效但调控强度有限，向量注入能实现更精细的控制但会小幅影响输出质量。不同情感调控会带来副作用，如积极情绪可能导致抗干扰性下降和偏见提升，愤怒则增强抗泄露但提高毒性。

Conclusion: PsySET实现了首次针对LLM情感与人格调控的系统性评估，为相关应用的可解释性和可靠性提供了重要参考。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [242] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest 利用大语言模型，创造沉浸式的交互式文字冒险游戏，以促进英语作为外语的学习。通过分支剧情和个性化内容生成，配合词汇助理工具，提升大学生英语学习效果。初步试点结果显示词汇能力提升和用户积极反馈。


<details>
  <summary>Details</summary>
Motivation: 当前外语学习缺乏个性化和沉浸体验，难以激发学习兴趣和效果。作者希望结合大语言模型和生成式故事，提升交互性和个性化，增强英语学习动力并提高词汇掌握能力。

Method: 设计了基于大语言模型的互动式“选择你自己的冒险”文本游戏，针对每位学习者生成符合其能力的内容，并嵌入词汇助理以解释生词。引入剧情分支和里程碑机制，保证故事连贯及开放性。进行了中国大学EFL学生活动试点并收集反馈。

Result: 试点表明，参与者在词汇学习方面有显著提升，对该系统持积极态度。同时也有用户希望改进叙事长度质量，并提出增加插图等多模态内容的建议。

Conclusion: GenQuest 展现了大模型驱动的生成式游戏在外语学习中的应用潜力，不仅能提升词汇能力，还能带来良好的学习体验。今后可改进内容丰富度与多模态呈现，以满足更多学习需求。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [243] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的训练大语言模型用于文本表征的方法GRACE，利用生成能力和对比学习，通过生成可解释自然语言推理提升模型表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型编码器多采用对比损失，将模型视为黑箱，忽视其生成和推理能力，缺乏可解释性。需要一种方法将生成和推理能力结合进来，提高表征质量和透明度。

Method: 提出GRACE，将对比学习信号视为奖励，用作生成策略优化，模型生成自然语言推理过程（rationale），通过均值池化获得高质量表征，采用多元奖励函数，用策略梯度优化，使正例相似而负例区分。模型既输出表征又输出可解释推理过程。

Result: 在MTEB基准测试上，跨类别评测显示，四个主干模型的有监督设置平均提升11.5%，无监督提升6.9%。同时保持了模型的通用能力。

Conclusion: GRACE将对比学习作为生成推理奖励，结合表征学习与生成，使大模型表征更强且可解释性更高。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [244] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALC的辅助学习策略，通过学习更精细的向量嵌入来提升产品推荐系统的覆盖率，并在两个产品推荐数据集上验证了其效果，达到了最新的覆盖率水平。


<details>
  <summary>Details</summary>
Motivation: 实际生产环境中的产品推荐系统要求自动覆盖率高，但常规方法在满足这一要求上存在不足。尤其在高自动化需求下，提升相关产品被自动检索到的比例是核心挑战。

Method: 提出了ALC（Auxiliary Learning strategy for Coverage），利用困难负例训练，设置细致的嵌入区分目标，包括两个新的训练目标，结合极端多标签分类方法和threshold-consistent margin loss实现优化。

Result: 在LF-AmazonTitles-131K和Tech and Durables两个数据集，以及三种极端多标签分类方法组合下，ALC都显著提升了推荐系统的自动覆盖率，达到了当前最优（state-of-the-art）水平。

Conclusion: 通过引入辅助学习、困难负例和新损失函数，ALC方法有效提升了产品推荐的自动化覆盖能力，为大规模推荐系统部署提供了更优解决方案。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [245] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在处理复数指涉时，是否能展现出类似人类的表现，包括对歧义性和非歧义性情境的理解。通过一系列实验发现，LLM对于复数代词的歧义识别和指涉解释与人类存在差异，表现不一致。


<details>
  <summary>Details</summary>
Motivation: 由于人类在语言交流中经常需要理解和使用复数指涉，尤其是在含糊或歧义的代词出现时，理解其指向是谁对交流至关重要。作者关注LLM在这一复杂语言现象上的能力，以评估其可解释性和实用性。

Method: 设计了多组实验，包括对LLM进行下一个词预测任务（以评估其代词生成能力）、代词解释和歧义识别任务，并采用不同的提示策略（prompting strategies）。实验结果与人类表现对比分析。

Result: LLM有时能够识别歧义代词的可能指代对象，但在解释复数指涉时，特别是当潜在的解释未被明确提及时，其选择未必和人类一致。此外，在未被直接提示的情况下，LLM难以发现歧义性。不同类型实验中，模型表现也存在不一致性。

Conclusion: LLM在处理复数指涉和歧义检测方面具有一定能力，但与人类相比仍存在明显差距，且在具体任务间表现波动大，说明其理解复杂语言现象的稳定性和一致性还有待提升。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [246] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 本文深入分析了当前大音频语言模型（LALMs）在多项选择问答（MCQA）框架下的评估方法，发现其评测结果对选项顺序和措辞非常敏感，并提出改进的评估方案。


<details>
  <summary>Details</summary>
Motivation: 随着LALMs的发展，人们通常利用MCQA框架来评估模型表现，但现有MCQA评测方式对题目或选项的微小变动过于敏感，评估结果不稳定，容易产生误导，因此有必要系统梳理这种评测框架的不足。

Method: 作者针对三大MCQA基准测试（MMAU、MMAR、MMSU）和四款主流LALMs（Audio Flamingo 2/3、Qwen2.5-Omni-7B-Instruct、Kimi-Audio-7B-Instruct），系统测试了模型对选项顺序及题目/选项措辞变动的敏感性。

Result: 实验结果显示，当前模型不仅对选项的排列顺序高度敏感，即使是问题及选项表述方式的轻微调整，也会导致模型表现有明显波动。

Conclusion: 传统MCQA评估方式未能反映模型真实、全面的能力。本文提出了一种更简单、能反映细微变化和具有更丰富报告能力的评估方案，为今后的LALM性能评估提供了更合理的方法。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [247] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: 本文提出FedSRD框架，通过稀疏化、重构和分解的方式，提高大语言模型在联邦学习中的通信效率，在降低高达90%通信开销的同时，还提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前高质量Web数据趋于枯竭，传统基于网络公开数据的大模型训练方式难以为继。联邦学习（FL）因其分布式、隐私保护特性，成为AI演进的新方案。但现有的低秩适应（LoRA）方法在联邦环境下通信开销大，且参数聚合时冲突严重，亟需创新优化。

Method: 提出FedSRD框架，分为三步：1）客户端对LoRA更新进行基于重要性的稀疏化，减少上传参数量；2）服务器端在全秩空间重构并聚合这些更新，缓解冲突；3）最终将全局更新低秩化处理后以稀疏低秩格式广播，提高通信效率。还设计了FedSRD-e高效变体，降低算力消耗。

Result: 在10个基准测试上，与现有方法相比，FedSRD框架通信成本降低最高达90%，且在数据异质性环境下还能提升模型精度。

Conclusion: FedSRD有效解决了LoRA在联邦学习通信瓶颈和参数冲突问题，为高效、安全地微调大模型提供了实用新方法，对AI模型的隐私协作和高效部署具有重要推动意义。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [248] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 本文展示了一种基于知识图谱的对比学习方法（SciNCL），能够有效提升过程工业领域文本嵌入的性能，且模型体积更小。


<details>
  <summary>Details</summary>
Motivation: 当前NLP模型常通过融合知识图谱信息来增强对领域术语或特定文档关系的理解，弥补仅靠文本学习可能被忽视的关联。但在工业过程领域，文本日志稀疏且结构特殊，如何有效利用其知识图谱信息提升模型性能仍未充分探索。

Method: 本文将原用于科学出版物的SciNCL（一种图感知邻域对比学习方法）引入到过程工业领域。具体做法是利用从稀疏知识图谱中提取的三元组对预训练语言模型进行微调，使其更好地理解领域特定的实体及关系。

Result: 微调后的语言模型在专有过程工业文本嵌入基准（PITEB）上，相较于SOTA的mE5-large编码器，性能提升9.8-14.3%（5.4-8.0百分点），且模型体积仅为其1/3到1/5。

Conclusion: 将SciNCL方法应用于过程工业领域，不仅能显著提升文本嵌入表现，还有效降低了模型尺寸，验证了领域知识图谱增强方案的有效性和实用价值。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [249] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本论文提出并测试了一种全面评估大语言模型（LLMs）检测面向不同群体社会偏见能力的框架，并通过多种规模及方法（如提示、上下文学习、微调）在跨类型与多样人群数据集上进行了系统实验。结果显示，经过微调的小模型在可扩展偏见检测方面表现出色，但在处理多群体复合偏见时仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有网络语料中存在面向人口群体的社会偏见，给AI模型引发监管压力。目前偏见检测研究大多局限于单一内容或人口维度，未能全面评估各种大语言模型的潜力和局限。这种不足阻碍了更大规模更有效的AI数据审计及合规发展。

Method: 作者提出了一个以英文为目标、面向多标签人口维度的偏见检测框架，并采用多模型（不同规模、提示、上下文学习、微调等）方案。在12个覆盖多种内容与群体的公开数据集上，系统比较这些模型的自动化偏见检测表现。

Result: 微调的小规模模型在多个数据集上表现优越，显示其作为高效自动偏见检测工具的潜力，但多群体复合偏见的检测与人口轴覆盖仍存在明显不足。

Conclusion: 当前LLMs偏见检测能力虽有进步，但在多群体和复杂偏见维度上存在局限性，需要开发更高效、更全面的审计框架以满足实际和合规需求。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [250] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 提出了一种新的低秩自适应方法 PI-LoRA，可高效自动地从临床指南和教材中提取医学决策树，不仅提升了准确性，还显著降低了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有医学决策树构建方法依赖人工标注，耗时且费力，亟需自动化、参数高效的解决方案以适用于计算资源有限的临床决策支持系统。

Method: 提出 Path-Integrated LoRA（PI-LoRA）方法，将梯度路径信息融入低秩适应机制，用于更智能地分配秩，同时裁剪次要模块，提高提取医学决策树的效率与准确度。

Result: 在医学指南数据集上的大量实验表明，PI-LoRA 在Text2MDT任务中明显优于现有高效微调方法，在保持更低模型复杂度的同时实现更高准确率。

Conclusion: PI-LoRA 能高效准确地从文本中抽取医学决策树，拥有轻量化架构，非常适合资源有限的临床决策支持系统，具备实用推广价值。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [251] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文针对在线医疗平台中存在的信息冗余与非专业术语影响问诊效率的问题，提出了一种基于大语言模型（LLM）并结合核心关注点引导的医学问题摘要（MQS）优化框架，实现了对医疗健康问题的高质量摘要。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型直接微调仍存在难以准确提取核心关注点和生成虚假内容（幻觉）的问题，导致医学问答摘要效果不理想，因此需要新的方法提升问诊摘要的准确性和可靠性。

Method: 作者设计了一个包括三部分的优化框架：1）通过特殊提问模板引导LLM从原始提问中提取可信的核心关注点；2）结合原始问答对构建微调数据集，强化模型聚焦能力；3）提出多维评估与筛选机制，从不同维度整体提升摘要质量。

Result: 在两个主流医学问题摘要数据集和三项评测指标上进行了实验，所提出方法在所有指标上均达到最新最优（SOTA）表现，有效提升了对问题核心的识别能力并明显减少幻觉内容。

Conclusion: 本文提出的基于关注点引导的大语言模型微调及多维评测框架，可显著提升医疗健康类问答自动摘要的准确性与可靠性，相关代码已开源。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [252] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 本文提出了一种新的多智能体工具集成型强化学习算法（MATPO），在单个大语言模型（LLM）内部通过不同角色的提示实现多代理合作，实现更高效和鲁棒的推理。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多轮推理和工具调用任务中，多依赖单智能体框架，受限于上下文长度和工具响应噪声，性能有限。目前缺乏有效的强化学习训练方法用于多代理工具集成的场景，有效解决这一痛点意义重大。

Method: 作者提出MATPO，包括规划者和工作者两种代理角色，通过角色专项提示在单LLM内实现分工协作。加强信任分配机制，有效区分并优化不同子任务的奖励归因，从而实现强化学习训练。该方法无需多LLM部署，节省计算资源。

Result: 在GAIA-text、WebWalkerQA、FRAMES等基准数据集上，MATPO相较单智能体基线平均提升18.38%的性能，并能更好应对工具输出噪声。

Conclusion: 实验表明，在一个LLM内部统一多智能体角色，可提升工具集成推理的效能与稳定性，为高效稳定的多代理强化学习提供了实用的新方案。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [253] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: 提出TiTok框架，通过Token级知识迁移，实现LoRA参数的高效迁移，无需额外模型，在多个基准上带来4~8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: PEFT（如LoRA）减低大模型微调成本，但其参数只能在同一模型结构下使用，不能跨模型迁移。现有使用数据合成或知识蒸馏的做法要么依赖训练数据，要么增加模型复杂度。

Method: 提出TiTok框架，通过比较使用和不使用LoRA的源模型，对比其输出生成“对比冗余”，识别出任务相关的关键信息token，用于高效筛选合成数据，无需训练额外判别器。

Result: 在多个迁移场景和三个基准上实验，TiTok方法在整体性能上比对比方法平均提升4~8%。

Conclusion: TiTok方法实现了LoRA参数高效跨模型迁移，无需增加模型或计算开销，验证了其实用性和优越性。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [254] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文深入分析了Mixture-of-Experts (MoE) 模型在多语言环境下的稀疏路由动态，揭示了不同层的语言特异与共性路由现象，并提出了一种通过调整中间层专家激活方式显著提升多语言性能的方法。


<details>
  <summary>Details</summary>
Motivation: MoE 架构虽然被广泛用于大规模LLM扩展，但其在多语言任务下不同语言之间的路由机制尚不清楚，理解其对多语言数据的响应机制对于提升模型泛化能力和性能具有重要意义。

Method: 利用平行多语言数据集，详细分析MoE模型在解码器各层的路由分布与语言相关性，挖掘各层在不同语言token的专家选择模式。进一步，基于发现的中间层路由趋同现象，提出推理时通过优先激活中间层英文高频专家来增强多语言路由一致性，验证对多语言任务的性能提升。

Result: 作者发现MoE模型在解码器早晚层呈现语言特异性路由，中间层则在多语言间路由高度一致。通过提升中间层的跨语言专家一致性，可显著提升多语言任务表现，相关干预方法在两项任务、三种模型、超过15种语言中带来1-2%的性能提升。非中间层或专注于多语专家的干预则导致性能下降。

Conclusion: MoE模型处理非英语文本时，其泛化能力受制于是否能在各语言中充分利用通用专家。通过设计合适的中间层路由机制可持续提升多语言应用效果，这一发现为MoE结构今后多语言及泛化能力优化提供了新路径。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [255] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: 本文提出了JSON Whisperer框架，使大语言模型（LLMs）能通过自然语言指令高效编辑JSON文档，仅输出所需的RFC 6902差异补丁（diff patch），而非重生整个文档，提升了效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法每次编辑JSON文档都需LLM重生全量结构，计算效率低下。因此，研究如何让LLM只生成必要的补丁，成为提升效率的重要课题。

Method: 本文提出JSON Whisperer框架，包括EASE（Explicitly Addressed Sequence Encoding）方案，将数组转为带稳定key的字典，绕过了索引移位相关难题，解决了LLM生成补丁时常漏相关操作和处理数组索引困难的问题。

Result: 实验显示，配合EASE，补丁生成方案可减少31% token消耗；编辑质量与全量重生相差5%以内，对复杂指令和列表操作提升尤为明显。

Conclusion: 借助JSON Whisperer和EASE，LLM可以更高效且准确地补丁式编辑JSON文档，对开发高效AI编辑工具，高效处理结构化数据有重要意义。数据集也公开供研究。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [256] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本论文针对淡研究和服务不足的成人阅读障碍，专门为僧伽罗语（Sinhala）开发了一套辅助系统，集成了语音识别、错误检测和文本纠正，并实现多模态反馈。系统在有限的数据下，取得了较好准确率，验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍，尤其是在非英语语境如僧伽罗语，长期被忽视。这种障碍对个人和职业生活有重大影响，但缺乏有效的辅助工具和相关研究。作者希望填补这一空白。

Method: 系统整合了Whisper语音转文本、SinBERT（专为僧伽罗语微调的BERT模型）检测常见阅读障碍错误，再用结合mT5和Mistral模型的文本纠正，最后用gTTS将文本转回语音，形成完整的多模态辅助流程。

Result: 在僧伽罗语低资源条件下，系统获得了0.66的转录准确率、0.7的纠错准确率，以及0.65的整体系统准确率，验证了系统的有效性和可行性。

Conclusion: 本研究首次为僧伽罗语的成人阅读障碍者构建了有效的辅助系统，强调了NLP技术在弱势语言群体中包容性的重要意义，具有较强的实际推广和应用价值。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [257] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 本文提出了一种针对生物医学领域的两阶段检索架构，将高效的ModernBERT与精细的ColBERTv2重排序结合，用于提升RAG系统中的检索与回答准确率，并在MIRAGE基准上获得了最新最高的性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗）应用RAG时，检索模块的质量直接影响答案的准确性。当前普适或专用检索器各有局限：专用模型虽准但计算开销大，通用模型对领域特定语言处理不足，因此亟需兼顾效率与表现的新方案。

Method: 设计并评估了一个两阶段检索架构：第一阶段用ModernBERT做高效初筛，第二阶段利用ColBERTv2对候选文档重排序；检索器在PubMedQA数据上微调，并系统性分析和消融不同组件及训练方式的影响。

Result: ColBERT重排序显著提升Recall@3高达4.2个百分点。集成后的RAG系统在MIRAGE五项任务上平均准确率0.4448，超越了强基线MedCPT (0.4436)。联合微调检索器和重排序器是性能提升的关键。

Conclusion: 结合高效浅层和强大深层检索模型可以为专业领域RAG带来显著性能提升，且需注重联合微调，以充分发挥各组件的配合优势。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [258] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 本研究提出了一个新基准，用于测试小规模预训练语言模型（BabyLMs）在区分Grice会话准则（含违背）的能力，并与儿童和大型语言模型表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 隐含意义在交流中极为重要，但目前尚不清楚在小数据集上预训练的语言模型是否能够像儿童一样理解这些含义。研究目的是系统测试这些“小模型”对Grice会话准则（包括其违背行为）的敏感性。

Method: 通过构建一个新的基准，将小模型（基于低于10M和100M token预训练）在五种Grice准则下的表现与儿童以及大型预训练模型（3T token）进行了对比分析。

Result: 100M token训练的模型优于10M token模型，但还没有达到儿童或者大型模型的水准。数据适度增长提升了部分语用推理能力，使模型更细致地区分不同语用维度。

Conclusion: 小模型随着预训练数据增多，其理解Grice准则（含隐含意义）的能力有所提升，但仍不足以比肩儿童或大型模型。未来可通过扩大数据量或其它策略进一步提升模型的语用理解能力。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [259] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文系统评估了自注意力与结构化状态空间模型（如Mamba）结合的混合架构，并比较了不同融合策略对语言建模与长上下文任务的影响，提出了针对不同策略的最优设计建议。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多采用自注意力机制，但其在长上下文建模和计算效率上存在局限。将其与结构化状态空间模型结合能在建模质量和效率之间取得平衡，但缺乏对混合策略系统性比较和关键有效因素的分析。

Method: 作者系统评估了基于层间（顺序）和层内（并行）两种融合方式的混合架构，从语言建模、长上下文能力、可扩展性以及训练和推理效率等多角度进行对比分析。同时，分析了各策略中计算原语的核心特性，探索了最关键的实现要素，并据此提出最优设计建议。

Result: 比较揭示了不同融合策略在各项指标上的优劣，明确了每种混合方式中影响效果的关键因素，提出了针对不同策略的最优模型设计方案。

Conclusion: 混合架构在多项任务上展示出性能与效率的优异平衡。本文的系统分析和设计建议为开发高效的混合语言模型提供了实践指导和理论依据，有助于该领域的进一步优化和创新。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [260] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 论文探讨了开发极危语言自动语音识别（ASR）系统时对数据的实际需求，发现仅需很少量且不标准格式的数据即可实现可用的ASR效果，这对于资源匮乏的濒危语言社区尤为重要。


<details>
  <summary>Details</summary>
Motivation: 全球近半语言面临消失风险，现代语音技术对这些语言的保护和复兴至关重要，但传统ASR系统对标准格式、量大的有标注语音数据要求，阻碍了濒危语言的技术支持。论文以马恩岛盖尔语为例，探讨如何以极少数据（且非传统格式）建立ASR系统。

Method: 本文使用短格式的发音资源（如录制的单词或短语，而非完整句子），分别对马恩岛盖尔语和康沃尔语实验，在仅用约40分钟数据的条件下，训练并评估ASR系统。

Result: 利用短格式的发音资源，40分钟数据即可在马恩岛盖尔语上实现可用的ASR效果（WER低于50%）。同方法在康沃尔语上也取得类似成效，显示门槛远低于此前认知。

Conclusion: 极危语言开发ASR系统所需的数据量和要求比想象中低得多，且不必拘泥于传统的语句级标注数据。这为无法满足高门槛要求的濒危语言社区带来新希望，推动更多极危语言的数字化保护和应用。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [261] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLM）在训练过程中下游任务的性能表现波动较大，提出通过检查点集成方法可以提升模型表现的稳定性。


<details>
  <summary>Details</summary>
Motivation: 常规做法是在训练过程中追踪下游任务表现并选取验证分数最高的检查点，但实际发现这些分数波动很大，难以选到真正最佳的模型，影响最终模型效果。本文旨在分析并缓解这一波动问题。

Method: 作者对在多样的大规模网页语料训练的LLM进行下游任务表现的稳定性分析，发现无论总体还是单例层面，分数波动都很显著。为此，研究了两种检查点集成方法：检查点平均（checkpoint averaging）和集成（ensemble），将临近的多个训练检查点聚合，以降低波动。作者还给出了理论分析支持。

Result: 通过实证和理论方法，证实无须更改训练过程，仅采用检查点平均或集成就能明显提升下游任务表现的稳定性。

Conclusion: 简单的后处理检查点集成技术能够有效改善下游任务表现不稳定的问题，为LLM训练流程提供了实用的优化手段。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [262] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 本文提出了PsiloQA，一个涵盖14种语言、具有细粒度span级标注的多语言幻觉检测数据集，通过自动流程构建，并展示了其跨语言泛化能力及对幻觉检测方法的支持。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准多为英文、序列级标注，缺乏多语言、细粒度人工监督，无法全面评估LLMs hallucination问题。

Method: 通过自动三阶段流程：用GPT-4o从Wikipedia生成问答对、多模型无上下文生成可能幻觉回答；随后利用GPT-4o自动比对golden答案和检索上下文，对幻觉span自动标注，最终形成多语言大规模数据集。

Result: 对多种幻觉检测方法评估，发现encoder模型在多语言环境下表现最佳。PsiloQA数据集具有良好的跨语言迁移和知识迁移能力，且比人工标注数据代价低。

Conclusion: PsiloQA为可扩展、多语言场景下的细粒度幻觉检测提供了新基准，推动该方向研究进展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [263] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 文章介绍了一种检测推理蒸馏训练数据的新方法Token Probability Deviation（TBD），有效提升了对于“见过的”和“未见过的”问题的区分能力，在公开数据集上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏提升大模型推理能力，但容易引入基准数据污染（蒸馏数据包含评测集，结果被高估）。为了解决蒸馏数据检测这一难题，作者提出了新的研究任务和检测方法。

Method: 提出Token Probability Deviation（TBD）方法。通过分析蒸馏模型对“见过的问题”生成结果的高确定性，TBD量化模型生成的token概率与高参考概率的偏离程度，用以区分见过与未见问题。

Result: TBD方法在S1数据集上获得了0.918的AUC和0.470的TPR@1% FPR，表明检测效果突出。

Conclusion: 提出的TBD方法可以有效检测蒸馏数据，缓解推理蒸馏导致的基准污染问题，对大模型评估可靠性具有较大意义。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [264] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文提出了SocialHarmBench数据集，用于评估大型语言模型（LLM）在政治操控、虚假信息等社会敏感领域的安全漏洞，并发现现有模型在这些领域表现出高度脆弱性。


<details>
  <summary>Details</summary>
Motivation: 传统的安全基准测试很少关注于政治操控、宣传、虚假信息生成等具有重大社会后果的领域。鉴于LLM越来越多地被用于此类场景，评估其在这些高风险领域的稳健性和潜在危害性变得十分必要。

Method: 作者构建了涵盖7个社会政治类别、分布于34个国家的585条提示词新基准SocialHarmBench，并对当前主流开源模型进行评估，系统性检验模型在历史修正、宣传等领域的易攻击性与顺从性。

Result: 实验证明，开放权重模型（如Mistral-7B）在历史修正主义、宣传及政治操控等领域的攻击成功率高达97%-98%。此外，模型在应对21世纪、20世纪以前以及特定区域（如拉美、美国、英国）相关问题时表现尤为脆弱。

Conclusion: 当前主流LLM的安全机制无法应对复杂多变的社会政治场景，存在系统性偏见，对人权和民主价值保障构成潜在威胁。作者开放了SocialHarmBench数据集，推动相关领域进一步研究。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [265] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 本文发现，在将大语言模型（LLMs）用于自然语言到SQL任务时，训练数据和目标数据在SQL结构特征上的一致性对微调效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管监督微调（SFT）在下游任务中有效，但训练数据的多样性有时会限制模型在不同领域的泛化能力。针对自然语言到SQL（NL2SQL）任务，作者关注训练数据与目标查询在结构上的匹配如何影响微调表现。

Method: 作者提出通过比较训练集、目标数据及微调前模型输出的SQL结构特征分布，来衡量数据的一致性，并在三个大型NL2SQL跨领域基准和多个模型上进行了系统实验。

Result: 实验表明，结构一致性高时，SFT显著提升模型准确率和SQL生成质量；一致性低时，提升不明显甚至无效。结构对齐度可以作为微调效果的强预测因子。

Conclusion: 数据的结构特征对齐对NL2SQL任务微调至关重要。为有效泛化和提升性能，微调前应重视数据选择的结构对齐度。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [266] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: 本文提出了一种新型本体内检测大模型幻觉的方法LSD（Layer-wise Semantic Dynamics），能够高效、准确地识别大模型产生的事实性错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）生成流畅但事实错误内容（即“幻觉”）在高风险领域带来严重风险。现有幻觉检测大多依赖多次采样或外部数据源，效率低下且不具备通用性。作者试图实现更高效、通用的幻觉识别方法。

Method: LSD方法分析transformer各层隐藏状态的语义变化，采用基于margin的对比学习，使模型隐藏层激活与事实编码器提取的真实语义对齐。与传统需多次采样不同，LSD仅需一次前向传播，结合层级语义轨迹漂移来识别幻觉。

Result: 在TruthfulQA和合成事实-幻觉数据集上，LSD取得F1=0.92、AUROC=0.96、聚类准确率0.89的表现，优于SelfCheckGPT和语义熵等基线方法，且检测速度提升5-20倍。

Conclusion: LSD是一种高效、可扩展、模型无关的幻觉实时监测工具，不仅实现高精度，还揭示了事实一致性在大模型表征空间中的几何特征。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [267] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文提出了一种用于Nawatl语的上下文无关文法（CFG），以生成人工句子扩充语言资源稀缺的Nawatl语料库，初步实验显示生成语料有助于提升一些语义任务模型表现。


<details>
  <summary>Details</summary>
Motivation: 由于Nawatl语言属于数字资源极度匮乏的原住民语言，可用于机器学习的语料几乎不存在，因此亟需扩大语料库促进相关研究。

Method: 作者设计并引入了Nawatl语的上下文无关文法，通过自动生成大量符合语法规范的人工句子，极大扩充了Nawatl语的训练语料库（命名为π-yalli），并利用扩充后的语料训练如FastText等算法，并在句子层面的语义任务上进行评估。

Result: 初步实验结果表明，利用文法生成方法扩充的语料库训练模型后，在相关语义任务上相比某些大型语言模型(LLMs)具有提升。但作者也发现，如需进一步改进，需开发能更好刻画Nawatl语言特性的文法。

Conclusion: 通过CFG自动生成结构化句子可以在极度低资源语言的Nawatl语下大幅扩充可用语料，有助于提升模型表现；但进一步改进文法的表达能力对实现更大性能提升至关重要。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [268] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究不同礼貌程度的语言提示（prompt）如何影响大语言模型（LLM）的回答准确性，发现无礼提示反而表现更好。


<details>
  <summary>Details</summary>
Motivation: 已有研究证明自然语言提示的措辞会影响LLM表现，但礼貌与语气如何影响模型结果尚未深入探究。

Method: 构建包含数学、科学和历史共50道题，每题改写为非常礼貌、礼貌、中性、粗鲁、非常粗鲁五种语气，共250条prompt；用ChatGPT 4o作答，采用配对t检验分析统计显著性。

Result: 与常识和过往研究相反，无礼语气的prompt准确率高于礼貌语气（非常礼貌80.8%，非常无礼84.8%）。

Conclusion: 新一代LLM对语气变化的响应可能与旧模型不同。研究提示cd应关注prompt语用层面，并引发对人机交互社会性的新思考。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [269] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过增强对学生反思文本中特定主题的感知能力，有效提升了文化资本主题的自动识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型常常因只关注句级信息，难以抓住学生反思中隐含的文化资本主题，如理想目标、家庭支持等。这些主题常被编织进叙述中，而不是以直接关键词出现，标准预训练模型对这类领域特定及叙事上下文信息缺乏感知。

Method: 提出AWARE框架，包含三个核心部分：1）领域感知，使模型词汇适应学生反思写作风格；2）上下文感知，获得能表达全文脉络的句子嵌入；3）主题重叠感知，通过多标签策略捕捉句中共现的主题。

Result: AWARE框架在检测文化资本主题方面比强基线模型Macro-F1提升2.1个百分点，并显著提升所有主题检测表现。

Conclusion: AWARE是一种稳健且可泛化的方法论，可应用于任何依赖叙事上下文理解的文本分类任务，有助于更好地理解学生反思、促进教育公平。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [270] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 本论文提出了一种高效资源利用的微调方法，通过采用LoRA和QLoRA技术，在内存和GPU资源有限的情况下，优化LLaMA-3.2-3B模型，使其在医学推理任务中表现优异，同时显著降低显存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型具备强大推理能力，但全面微调需要大量计算资源，限制了其在资源有限环境中的应用，特别是在医学领域。

Method: 采用参数高效的微调方法，如LoRA和QLoRA，对LLaMA-3.2-3B模型基于公开医学推理数据集进行适配，无需标准的全量微调即可提升模型能力。

Result: 通过这些高效微调技术，模型在医学推理的连贯性和事实准确性上取得改进，并将显存消耗降低了最多60%。

Conclusion: 轻量级的参数高效微调可以在保持强医学推理能力的同时，极大地减少资源消耗，为医学AI系统的低资源环境部署提供了实用策略和新的平衡思路。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [271] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出一种新型的不可感知越狱攻击方式，通过利用Unicode变体选择符，使有害的提示在视觉上与原提示无异，却在底层令语言模型产生有害输出。


<details>
  <summary>Details</summary>
Motivation: 过去视觉模态下的越狱攻击常用不可感知扰动，但文本模态攻击多需明显可见的变化（如添加无意义后缀）。本研究旨在发现并突破文本模态越狱对“可见修改”的既定假设，提高攻击隐蔽性。

Method: 作者提出使用Unicode变化选择符（variation selectors）作为不可见后缀，通过链式检索流程自动生成可用于越狱的对抗性后缀。这些后缀插入后，表面上看不出变化，实际却改变了模型分词结果，实现越狱。并在四种对齐大模型和提示注入攻击任务上进行实验评估。

Result: 实验显示，所提出的不可感知越狱方法对四种对齐大模型均取得高攻击成功率，并且能够泛化至提示注入攻击，所有攻击均未在提示内容上留下任何可见痕迹。

Conclusion: 研究表明，通过不可见的Unicode变体选择符，可以实现极具隐蔽性且高效的大模型越狱攻击。这对现有检测手段带来新挑战，并提示未来防护需顾及这类隐形威胁。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [272] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文将成语理解与方言理解任务结合，提出了针对魁北克法语方言的新基准数据集，并通过实验证明这些数据集可有效衡量语言模型的方言能力。


<details>
  <summary>Details</summary>
Motivation: 传统的成语理解和方言理解都是自然语言处理的经典任务，但二者往往被分别研究。该文提出区域性成语可以作为特定方言理解的测试手段，弥补了方言理解评测工具不足的现状。

Method: 作者构建了两个魁北克法语方言的成语数据集（QFrCoRE和QFrCoRT），并详细说明了数据集的构建方法，以便他人复现。随后用94个大语言模型（LLM）开展实验，测试它们在这些数据集上的表现，从而评估模型对方言成语的理解能力。

Result: 实验证明，作者提出的区域成语基准能够有效地测试大语言模型在特定方言下的理解能力。数据集具有较强的可靠性和代表性。

Conclusion: 这种结合成语与方言理解的基准，对推动自然语言处理模型方言能力的公平测评提供了新方向，并为其他方言的数据集构建提供了方法借鉴。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [273] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 提出了一种创新的混合检索方法GQR，在提升视觉文档检索性能的同时显著减小了模型规模与资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有多模态编码器已极大推动视觉文档检索发展，但大规模表示导致实际应用中的部署和可扩展性受限。同时，单纯依赖视觉信息的模型，受限于模态间的鸿沟，未能充分利用多模态信息。作者希望在提升效率和效果的前提下，突破这两大瓶颈。

Method: 提出了Guided Query Refinement（GQR）方法：通过在测试时借助轻量级密集文本检索器对主视觉模型的查询表达进行优化，并根据辅助模型的得分进行细致调整。与传统混合（如排序/分数级别融合）不同，GQR更深入挖掘不同模型表示空间的交互。

Result: 在多个视觉文档检索基准上，GQR方法不仅性能媲美大规模模型，还实现了最多14倍加速和54倍内存减省。

Conclusion: GQR显著提升了多模态检索的效果-效率边界，为实际部署带来新突破，为今后多模态检索研究指明了新方向。源码已开源。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [274] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文介绍了COLE，一个包括23项任务、专注于法语自然语言理解（NLU）多样性的全新基准，并对94个大语言模型进行了系统评测，发现当前LLM在某些任务上明显存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 法语作为世界重要语言之一，在自然语言理解评测方面缺乏系统、全面的基准，因此需要开发一个覆盖面广、能反映法语语言现象的评测工具，以推动法语NLU研究进展。

Method: 作者构建了COLE基准，涵盖了情感分析、复述检测、语法判断、推理等23类任务，特别关注法语相关的语言现象。利用COLE，对94个大语言模型进行对比评测，系统分析这些模型在各种NLU任务上的表现。

Result: 大模型在法语NLU任务表现良莠不齐，特别是在零样本抽取式问答、细粒度词义消歧、地区语言变化理解等任务上表现不佳。闭源模型与开源模型之间存在显著性能差距。

Conclusion: COLE的发布为法语NLU评测提供了重要资源，有助于发现和推动当前大模型的研究难点和进步方向，将促进法语语言建模领域的进一步发展。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [275] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: 本文提出SwiReasoning框架，能在无需额外训练的情况下，自适应切换大型语言模型的显式与隐式推理模式，从而提升推理的准确率和token效率。


<details>
  <summary>Details</summary>
Motivation: 已有研究显示，LLMs既能通过链式显性推理，也能在隐空间连续推理，提高token效率。但隐式推理存在概率分布分散、噪声大、收敛慢、准确率下降以及无文本“过度思考”等问题。为解决这些挑战，本文提出了新的推理方式。

Method: SwiReasoning框架无需训练，包括两大创新：1）根据下一个token分布的熵变化动态地在显性（Chain-of-Thought）与隐性推理之间切换，实现探索与收敛的平衡；2）限制思考方式切换的最大次数，减少“过度思考”，提升token利用率。

Result: 在主流数学和STEM基准测试上，SwiReasoning平均提升1.5%-2.8%的准确率，在不同模型家族和规模下均有效。在token预算受限时，token效率提升56%-79%，预算越紧增益越显著。

Conclusion: SwiReasoning可在不重新训练LLM的前提下，通过智能切换推理方式，有效提升复杂推理任务的准确率和token效率，对实际应用具备广泛意义。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [276] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 本文提出了一种三阶段的方法SLM-MUX，能高效协同多种小语言模型（SLMs），使其整体准确率超越单一模型，并优于现有多模型编排方法。


<details>
  <summary>Details</summary>
Motivation: 小语言模型数量迅速增长，虽然单独性能不如大模型，但效率更高且在特定任务中表现突出。现有的方法主要面向大模型，对SLMs编排效果不佳，因此需要针对SLMs的高效协作方案。

Method: 提出三阶段框架：1）设计SLM-MUX架构，实现多SLMs高效协同；2）采用模型选择搜索，自动选出互补性最强的SLMs；3）在推理时引入适配SLM-MUX的缩放策略。

Result: 在MATH、GPQA和GSM8K任务上，相比已有方法分别提升最高13.4%、8.8%、7.0%。仅用2个SLM，主方法在GPQA和GSM8K超越Qwen 2.5 72B，在MATH持平。并提供理论分析佐证优势。

Conclusion: 多SLM可以通过该方法高效编排，显著提升整体准确率和效率，为小模型实际部署和协作打开新局面。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [277] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: 本文提出了TeachLM，一种通过参数高效微调优化教学的大型语言模型，利用10万小时真实师生对话数据，显著提升了模型的对话和教学能力。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型在教育领域发展受限，很大原因是缺乏真实学生学习过程的数据，且现有的提示工程难以有效编码复杂的教学策略。为此，需要新的方法提升AI的教学表现。

Method: 作者利用Polygence平台采集的10万小时真实一对一师生互动对话，经过严格匿名处理后，用于参数高效微调主流预训练大模型，生成TeachLM。此外，提出多轮对话评测协议，通过生成高质量合成对话来评估模型的教学表现。

Result: 微调后的TeachLM在多项教学对话指标上显著优于原始模型：学生发言时长翻倍，提问风格更佳，对话轮次提升50%，教学个性化能力增强。

Conclusion: 基于真实学生学习数据进行微调，能极大改善大型语言模型的教学对话和个性化能力，为AI教育应用带来更高潜力。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [278] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 本文提出了一种新的扩散式大语言模型（dLLMs）解码策略“Tolerator”，能修正早期预测的错误，相比以往方法在多个基准测试上取得更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs的离散解码策略中，一旦接受的token无法在后续步骤中修改，导致早期错误无法纠正，影响最终生成质量。为了解决这一关键问题，亟需一种能够反复修正预测的解码方法。

Method: 提出Tolerator解码策略，无需额外训练，采用两阶段流程：第一步是序列填充，第二步是通过反复遮蔽-解码部分token并把剩余token作为上下文，实现已接受token的反复修正。

Result: 在包含语言理解、代码生成和数学等五个标准基准上的实验表明，Tolerator能在相同计算预算下，持续超过现有基线方法。

Conclusion: 高效的解码算法对dLLMs潜力的充分发挥至关重要，Tolerator通过提升解码可修正性，有效提升了最终输出质量。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [279] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: 该论文介绍了Gemini Robotics 1.5及其ER 1.5模型，这是一组先进的感知-推理-动作一体化机器人模型，带来在多机器人泛化、动作转移、推理与任务分解等方面的突破。


<details>
  <summary>Details</summary>
Motivation: 通用型机器人需要在物理感知、复杂推理和灵巧控制等方面实现重大进步，现有模型难以同时胜任多任务与推理-执行协作，因此需要更强大的多模态和推理模型。

Method: 1) 提出新型架构及“动作转移”机制，使模型能够从多类、多形式机器人数据中学习，提高泛化性；2) 采用多层自然语言内在推理过程，机器人实现“先思考后行动”，提升任务分解、执行能力及行为可解释性；3) ER 1.5聚焦于提升具身推理（视觉-空间理解、任务规划、进度估计等）。

Result: 1) Gemini Robotics 1.5能够实现多机器人、多模态任务的泛化和复杂任务分解；2) ER 1.5在具身推理相关基准上达到了新SOTA水平，有效提升了机器人在现实环境中执行复杂任务的能力。

Conclusion: 本模型家族让机器人进一步具备“感知-思考-行动”三位一体能力，为未来物理智能体能自主解决复杂多步任务打开了新局面。

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [280] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 本文提出了可形变的三连杆仿生游动系统模型，结合几何力学理论，优化并预测了高阻尼环境下具柔顺性的机械体或动物的运动表现，并通过实物机器人实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有几何力学在分析和优化爬行、游动等动作时普遍假定运动轨迹能精确执行。但是现实中，动物或机器人柔顺的身体在环境中与介质作用常产生干扰，使动作轨迹偏离设计路径。为弥合理论与实际差距，有必要发展适用于柔顺实体的新运动建模与优化框架。

Method: 对经典的Purcell三连杆游动模型进行扩展，在关节处接入串联弹簧以表示系统柔顺性。采用电缆驱动三连杆无肢机器人作为物理平台，在松散介质中验证模型。以阻力理论建模动力学，将几何力学方法引入动作预测与最优控制框架，用于寻找最佳柔顺参数及动作模式。

Result: 提出的模型可准确预测具柔顺性的机器人于不同顺应性和环境情形下的运动表现，并能优化运动轨迹，实现最大位移。物理实验与理论分析吻合。

Conclusion: 研究为柔顺游动机构建物理驱动、可优化的建模与控制系统，系统性地将柔顺性纳入设计与控制考虑，证明柔顺性有助于在均匀或异质环境中实现更健壮、高效的运动。

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [281] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习的快速机器人运动生成方法，能在动态环境中高效生成可行轨迹，提高人机协作系统中机器人的响应效率。


<details>
  <summary>Details</summary>
Motivation: 目前采样型和优化型运动规划方法各有缺陷，前者在高维空间效率不高且需后处理平滑，后者虽然能融合多约束直接生成平滑轨迹，但对初始值敏感且易陷入局部最优。亟需一种能兼具高效、平滑、广泛适应性的运动规划初始化方案。

Method: 利用基于Flow Matching的生成模型，在无须环境已知信息（例如障碍物位置和形状）的情况下，仅依靠单视图点云输入，为优化规划提供近最优的初始化轨迹，从而提升轨迹优化的效果和效率。

Result: 在UR5e机械臂的仿真实验中，该生成式初始化方法自身高成功率，并显著提升轨迹优化的成功率与效率，相比传统和现有学习型初始化器需更少优化迭代，同时对未见环境具有良好泛化能力。

Conclusion: 提出的方法为高维复杂空间中的机器人运动规划提供了一种更高效、稳健且泛化能力强的初始化途径，有助于提升HRC系统的实时响应与任务完成效率。

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [282] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: 本文提出了一个基于RotorPy的四旋翼无人机仿真测试平台，用于系统性评估不同控制方法在多种干扰下的鲁棒性与适应性，并实现了标准化、可复现的比较环境。


<details>
  <summary>Details</summary>
Motivation: 现有针对四旋翼的鲁棒自适应控制方法评价碎片化，难以系统比较，不同任务、仿真器和实现方式导致结果不统一，阻碍了方法的发展和优化。

Method: 搭建了一个易部署、模块化的仿真测试平台，集成了多种典型的自适应和非自适应控制器，支持风力、负载变化、旋翼故障和控制时滞等多种扰动，并提供通用的任务相关性能指标；各控制方法可重复、统一地进行评估。

Result: 测试平台支持多种干扰场景和多类型轨迹的自动化压力测试，并通过案例展示了其灵活性与广泛适用性，显著简化了比较流程，提高了评估效率。

Conclusion: 该测试平台为四旋翼自适应与鲁棒控制方法的系统性分析和比较提供了统一工具，方便复现，大大推动了领域发展，技术社区可获取开源包进行扩展和深入研究。

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [283] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 该论文研究了如何通过优化目的地与分拣滑槽之间的任务映射，提高机器人分拣系统（RSS）的吞吐量。作者提出并评估了基于进化算法和混合整数线性规划的优化方法，并通过仿真验证了优化方案的优越性。


<details>
  <summary>Details</summary>
Motivation: 实际的机器人分拣系统中，如何将运输包裹的机器人任务合理分配到目的地滑槽直接影响整体效率。任务映射不仅与路径和目标分配互相关联，还受到滑槽关闭机制及滑槽分散对下游处理影响的限制。因此，提出更优的映射策略对提升系统吞吐量有重要意义。

Method: 作者先形式化定义任务映射与任务映射优化（TMO）问题，开发了一个模拟器用于评估不同任务映射方案。提出一种基于进化算法和混合整数线性规划的TMO方法，并用贪心算法作对比实验。同时，引入质量多样性算法分析不同任务映射对系统吞吐量的影响。

Result: 仿真结果显示，提出的TMO优化方法在不同系统规模、滑槽数量和目的地配置下，均能显著提升吞吐量，优于贪心生成的映射方式。质量多样性算法揭示了多样化映射方案在性能上的分布情况。

Conclusion: 优化目的地与滑槽的任务映射对于提升机器人分拣系统效率至关重要。基于进化算法和混合整数线性规划的优化方案在多个场景下表现突出。该研究为实际分拣系统的任务分配提供了有效的新工具。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [284] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: 本文提出了一种适用于不确定动力学情况下的IMDP（区间马尔可夫决策过程）的鲁棒宽容控制器综合方法，可合成多策略控制器，提升机器人在未知环境下的适应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的控制器综合方法主要针对确定性策略，并假设系统的转移概率已知。然而，实际机器人系统普遍存在传感噪声、执行误差与粗略建模等不确定性，这要求能处理转移概率区间的模型与方法，同时提升系统在运行时的灵活性和鲁棒性。

Method: 作者提出在IMDP框架下合成鲁棒的宽容控制器（multi-strategy）。为保证合成策略在所有允许的转移概率下均能满足指定的可达性或奖励规格，问题被建模为混合整数线性规划（MILP）。针对求解效率，提出了基于顶点枚举的基线方法和利用对偶性避免枚举的可扩展方法。

Result: 在四个标准测试领域的实验表明，提出的方法可合成鲁棒且最大限度宽容的控制器，且能扩展到拥有十万量级状态的大规模IMDP系统。

Conclusion: 本文首次实现了对IMDP上鲁棒宽容控制器的自动合成，为现实中存在高度不确定性的机器人控制系统提供了有效工具，也为后续研究提供了理论与方法基础。

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [285] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: 提出了一种结合人类运动预测和数字孪生的安全规划系统，能更精确地实现人机协作过程中的碰撞预防。


<details>
  <summary>Details</summary>
Motivation: 传统的规划器大多只依赖动力学模型，难以对人类动作做长时域预测，难以及时主动预防碰撞。为提升人与机器人协作的安全性与效率，需要更精准的人体动作预测与规避方法。

Method: 1. 利用深度相机获得人体3D骨骼姿态；2. 用CNN-BiLSTM预测各关节的未来轨迹；3. 在数字孪生环境中验证和评估预测与碰撞风险；4. 如果风险超阈值，基于胶囊体人工势场（APF）转化为碰撞风险，触发自适应RRT*（A-RRT*）重新规划轨迹。

Result: 在50次实验中，实现了100%主动避障，安全距离大于250mm，重新规划时间小于2秒，相比仅用运动学规划器更加精确和可靠。

Conclusion: 融合预测性人体建模与数字孪生验证，可显著提升人机协作时的安全性和规划精度，优于传统方法。

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [286] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: 本文提出了一种用于多机器人系统的实时时分布式轨迹生成与控制框架，能够在障碍物密集环境中维持和恢复机器人间的连通性。


<details>
  <summary>Details</summary>
Motivation: 多机器人协同应用中，保持机器人间的通信和连通性非常关键，但实际环境中的障碍物和视觉遮挡容易使连通性中断。现有方法普遍缺乏在连通性临时丢失后自动恢复的能力，亟需更鲁棒的连通性控制解决方案。

Method: 框架基于高阶控制屏障函数（HOCBFs）来控制机器人间距，确保避碰与连通性维持。同时引入控制Lyapunov函数（CLFs）使系统即使在连接暂时丢失或初始不连通的情况下也能恢复连通。轨迹采用Bezier曲线参数化，实现平滑且高阶导数连续的并行规划和控制。核心贡献是提出了统一的MPC-CLF-CBF框架，将模型预测控制、CLF、CBF结合为连续时间的轨迹生成与连通性控制方法。

Result: 框架在大量仿真实验和4台Crazyflie纳米四旋翼的实体实验中验证，证明了其在障碍物密集环境中有效实现连通性维护与恢复的能力。

Conclusion: 该方法能够实时高效地生成轨迹，实现多机器人系统在复杂环境中的稳定连通性维护和鲁棒的连通性恢复，为实际协作机器人应用提供了有力的理论与实践支持。

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [287] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: 该论文提出了LapSurgie系统，这是首个基于仿人机器人的腹腔镜远程操作框架，可利用普通腹腔镜手术器械实现精准操作，有望解决外科机器人在低资源地区难以部署的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人腹腔镜手术具有高效率和精准等优势，但高昂的设备和部署要求导致其仅限于资源丰富的医疗中心，加剧了医疗资源不均。解决机器人外科手术在低资源和偏远地区的可用性问题十分紧迫。

Method: 作者开发了一种仿人机器人操作系统LapSurgie，采用逆向映射的方法实现对普通腹腔镜仪器的精准远程操作，无需额外设备改动，并配备立体视觉实时反馈控制台。

Result: 通过跨平台的用户实验，系统证明了良好的人机协作效果，展现了方案在腹腔镜手术中的实用性和可行性。

Conclusion: LapSurgie框架为仿人机器人在标准化手术环境下的远程腹腔镜操作提供了新方法，为外科机器人在基层及低资源地区的普及奠定了基础。

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [288] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 本文提出了一种适用于微创手术机器人相机-机器人标定的新型特征检测与姿态估计算法。该方法在挑战性外科环境中实现了快速且高精度的性能。


<details>
  <summary>Details</summary>
Motivation: 微创手术机器人需要精确的视觉-机械手协作，但现有的标定方法因机器人结构柔顺、机械臂部分可见性差而难以直接应用。需开发更高效、鲁棒性更强的标定方法，以支持此类手术机器人的在线控制。

Method: 作者设计了一种融合几何特征（关键点与轴边缘）统一编码的深度网络架构，只需一次推理即可检测两种特征，实现高效投影几何姿态估计。该方法基于大规模合成数据及投影标签训练。

Result: 在特征检测和姿态估计任务上进行了定性与定量评估，结果显示该方法在复杂外科环境中表现出极快的推理速度和行业领先的精度。

Conclusion: 该框架为微创手术机器人提供了高效、准确的新型标定方案，适用于实际的在线机器人控制，有望提升手术机器人的操作精度与效率。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [289] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: 本论文提出了一种基于图的路径规划工具，用于模拟大象鼻软体机器人，实现高效且可靠的避障规划，并兼顾能耗优化。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因能大幅度弯曲和伸展，在复杂环境下具有刚性机器人无法比拟的灵活性，但其高非线性和无穷维的运动学为运动规划带来了极大挑战，尤其是在障碍物密集场景中。

Method: 设计了一款三根人工肌肉纤维驱动的软体机械臂，利用形态弹性学和有源丝理论开发生物力学模型，预先计算出形状库，并在形状空间中构建K近邻图。通过有符号距离函数剔除与障碍物碰撞的节点和边，并引入基于几何距离与驱动能耗的多目标边权重，实现能耗敏感的避障路径规划。

Result: 算法能够在毫秒级时间内完成避障路径规划，确保运动物理可行且能效高。事实表明，考虑能耗可显著减少驱动功，但可能增加末端路径长度。

Conclusion: 形状空间图搜索为软体机器人路径规划提供了一种快速、可靠的方法，具备实时应用前景，可广泛用于手术、工业和辅助等领域。

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [290] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出了一种接触显式的统一多任务行走与操作策略学习框架，实现了多样化机器人任务下的单一策略控制。


<details>
  <summary>Details</summary>
Motivation: 以往在多任务控制中需为不同任务设计不同策略，难以利用任务间的共性，且泛化能力有限，因此亟需一种能够统一并泛化多种机器人运动与操作任务的策略学习方法。

Method: 提出以接触目标（包括位置、时序和末端执行器）序列为任务统一描述方式，将多任务学习归一化到接触层面。利用目标条件的强化学习训练单一策略，使机器人根据给定的接触计划完成各种任务，并在多种机器人系统（四足、双足人形、双手操作）上进行验证。

Result: 在不同类型的机器人和多种任务上，单一策略能够可靠完成多种任务，展现出对未见场景的泛化能力。显式引入接触推理显著提升了策略的泛化表现。

Conclusion: 基于接触显式的策略学习为可扩展的机器人行走-操作任务提供了一种有前景的基础，未来能更好支持多样且复杂的机器人任务。

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [291] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: 本文提出了一种双层控制框架，通过时间相关的网格投影增强道路边界，实现自适应的路径规划和障碍物规避，提高自动驾驶汽车在动态环境下的安全性与实时性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在动态环境中需要高水平的路径规划和避障能力，以保障行车安全。然而，现有方法在实时应对复杂动态障碍和路径优化时仍面临挑战。

Method: 提出了一种双层控制框架：主控制环采用非线性模型预测控制（NMPC）结合同伦约束松弛，实现实时路径优化；独立的备份环路并行运行，在主环路无法及时计算最优轨迹时，提供安全的备用轨迹。道路边界则通过障碍物时变网格投影自适应扩展。

Result: 方法在多种驾驶场景下进行了评估，验证了其实时性、适应性和高鲁棒性，表明新方法提升了复杂、动态环境下自动驾驶的可靠性和安全性。

Conclusion: 该框架为复杂动态环境下自动驾驶的路径规划和安全保障提供了有效解决方案，是向更安全、更可靠自动驾驶迈出的重要一步。

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [292] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种新型无坐标静态模型，用于高效分析和控制软磁壳体（如磁性夹持器和软体步行机器人）的形变行为。该模型基于SE(3)群上的Cosserat壳理论，并通过有限元方法，能有效解决传统壳体建模中奇异性和锁定等问题。模型经实验和理论验证，特别适用于大幅度旋转和位移的场景。


<details>
  <summary>Details</summary>
Motivation: 传统的Cosserat杆理论多用于近似一维的细长软体机器人，但随着使用壳体结构（宽度与长度比大）的软机器人发展，现有理论已无法准确、高效描述其复杂行为，尤其是软磁材料的大变形和转动。

Method: 提出基于SE(3)群的Cosserat壳静态建模方法。将磁性壳体看作二维流形，每点具有六自由度（位置和旋转），利用基于Lie群结构的新颖局部变形梯度定义，推导虚功原理下的强/弱形式平衡方程，给出线性化弱形式用于有限元数值实现。

Result: 模型通过有限元数值解法有效避免了传统方法在应对大形变时出现的奇异性和锁定现象。理论和实验测试表明，该方法在壳体发生剧烈旋转、位移时，比现有方法更有效。

Conclusion: 提出的方法为宽体软磁机器人建模与分析提供了高效、健壮的新手段，对壳状结构的柔性机器人研究和应用具有重要推动作用。

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [293] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的完全无线软体机器人，借鉴尺蠖生物的结构和运动方式，具备多种地面和水下运动能力。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人受限于外部连接与控制，无法实现真实环境下多任务部署。推动软体机器人实际应用，急需开发可完全自持、具多模式运动能力的机器人。

Method: 设计了具有弯曲柔性结构且通过磁力驱动的软体机器人，集成了轻量级无线控制电路和摄像头，并进行了结构优化和系统集成。通过实验对机器人的动态性能和运动方式进行验证。

Result: 机器人总质量102.63克，实现了最大步行速度3.74厘米/秒和游泳速度0.82厘米/秒，无需外部基础设施即可行走、转向、游泳及运输负载。

Conclusion: 经过系统测试，该软体机器人在运动方式和动态性能上表现优越，具备在真实环境中多任务部署的潜力。

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [294] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文系统性研究了视觉退化对机器人自我建模能力的影响，并提出结合去噪和结构约束的新框架以提升鲁棒性，有效应对真实环境下的噪声与复杂背景问题。


<details>
  <summary>Details</summary>
Motivation: 具备视觉自我建模的机器人具有更强的适应能力，但目前的自动化建模流程在噪声和杂乱环境中表现较差，亟需提高其鲁棒性以适应真实应用需求。

Method: 作者设计了一系列仿真和真实实验，量化模糊、椒盐噪声、高斯噪声等视觉退化对最新机器人自我建模管线（如形态推断、轨迹规划、损伤恢复）的影响。为增强鲁棒性，提出了结合经典图像去噪与保形态结构约束的任务感知去噪框架，并引入语义分割以分离出复杂背景中的机器人。

Result: 实验结果显示所提方法能在模拟和实体平台上恢复接近基线的建模性能，而现有方法在视觉退化下大幅退化。

Conclusion: 该工作显著提升了视觉自我建模的鲁棒性，为自适应机器人在不可控、现实环境中的部署打下了实践基础。

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [295] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 本文提出了EmbodiSwap方法，通过在真人视频上生成逼真的机器人叠加图像，为零样本模仿学习搭建桥梁，帮助机器人学习人体动作。此外，还创新性地将V-JEPA用作视觉主干网络，大幅提升机器人的模仿能力。实验表明，基于V-JEPA的模型在实际操作中表现优异。相关代码、数据集与模型已开源。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习受限于人类与机器人外形和感知的差异（embodiment gap），尤其是难以利用真实人类视频数据直接迁移到机器人。该研究试图消除这一差距，提高机器人学习人类动作的能力。

Method: 开发了EmbodiSwap方法，将人类视频替换为对应的机器人三维模型叠加，生成合成数据，支持机器人模仿。同时首次将V-JEPA视觉主干网络用于机器人模仿学习，替代传统视觉网络。

Result: 基于EmbodiSwap与V-JEPA的零样本模仿系统，在现实测试中取得了82%的成功率，优于少样本训练的对比方法。V-JEPA主干显著优于机器人领域常见的视觉主干。

Conclusion: EmbodiSwap与V-JEPA的结合显著改善了机器人对人体动作的模仿学习，推动了机器人利用人类视频进行高效学习的发展。该研究的开源数据、代码与模型有望促进更多相关研究与应用。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [296] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的平面推物方案，采用单一学习模型在无需重训练的情况下解决多种非抓取桌面推物任务，并结合MPPI控制器实现多任务自适应控制，实验证明该方法具备良好的泛化性、精度及仿真到真实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动推物方法常针对特定任务，缺乏通用性和可扩展性，难以在多个任务间直接迁移，需要频繁重建或调整模型，手动工程量大。作者希望设计一种通用、高效的推物方案，能处理多任务、多变化场景。

Method: 采用GRU递归神经网络结合非线性层，学习平面推物的对象-环境动力学。在状态-动作表征方面针对不确定动力学、推长变化等设计专门结构。通过域随机化的仿真训练提升模型的仿真到实际（sim-to-real）迁移能力。将学习得来的动力学模型与采样式MPPI控制器结合，实现根据不同控制目标适配任务（如路径跟踪、避障、精确定位等），无需重训练。

Result: 模型在消融实验中展现出更优预测准确性与稳定性。在Franka Panda机械臂仿真及真实实验中取得高成功率，能在严格阈值下精准定位，对长周期轨迹跟踪与避障效果同样良好，只需调整目标函数即可支持多任务。拓展实验表明通过改变训练推长及优化控制器，可进一步提升长时任务表现。

Conclusion: 本文提出的方法支持多任务和任务切换，无需重训练，具备良好的泛化能力和实际可用性。通过学习动力学模型与高效控制器结合，为非抓取推物等机器人操作任务提供了通用解决思路，在仿真和真实环境中均表现优秀。

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [297] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 本文探讨了在复杂动态环境下，移动机器人等智能系统通过引入类别标签（任务、角色等）进行运动轨迹预测的方法，并比较了不同方法在不同数据集下的效果，发现不同方法在数据分布不均衡和数据量有限时表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，机器人要高效导航和避免碰撞，需要准确预测周围多样化的智能体行动。由于不同类别的智能体（如行人、车辆等）运动模式不同，基于类别的预测能显著提升预测精度，因此提出探索和系统分析基于类别的运动预测方法。

Method: 作者在两个数据集（THOR-MAGNI和Stanford Drone Dataset）上，分别使用了基于条件模式的预测方法和高效的深度学习方法作为基线，设计实验评估两类方法在不同类别标签下的表现，关注数据平衡性及数据量有限场景。

Result: 实验证明，无论是基于模式还是深度学习的方法，在引入类别标签后预测精度都有提升。但是，在数据集类别不平衡或新环境缺乏充足数据时，模式方法表现更优；而在数据平衡的场景下，深度学习方法效果更佳。

Conclusion: 在多智能体导航场景下，采用类别条件预测可以有效提升轨迹预测准确率。对于新环境或数据稀缺、类别不均衡的情形，推荐使用基于模式的方法；而数据充分且平衡时建议采用深度学习方法。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [298] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: 提出了一种新的COVER框架，可实现半静态环境下固定时间约束内的运动规划，并提供比以往方法更广的覆盖和更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，机器人需要在有限时间内完成运动规划。但半静态环境下，障碍物大多静止，只有少量变化，存在可利用的结构化变异。现有方法要么无严格保证，要么依赖高度离散化，限制了实际使用。急需兼顾效率与理论保障的新方法。

Method: 作者提出COVER框架，对障碍物配置空间进行分区，并在每个分区内规划可行路径，对每个分区的覆盖路网进行系统性可行性验证。这样可在被验证区域内，实现固定时间约束的运动规划查询。

Result: 通过大量实验，作者在7自由度仿真Panda机器人上、面对桌面和货架任务，对COVER进行了验证。结果表明，COVER相比现有技术，覆盖范围更广，查询成功率更高。

Conclusion: COVER能有效利用半静态环境的结构特征，兼顾理论保证与实际性能，推动了运动规划领域在真实场景中的应用能力。

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [299] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 提出基于3D潜在特征地图的移动操作策略SBP，相较于仅依赖图像的策略，在空间与时间推理能力上更强，并在多个移动与桌面操作任务上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统移动操作方法多依赖单帧图像，难以进行全局、长期的空间与时间推理，限制了机器人在复杂环境下的表现，因此需要结合长期记忆与全局感知的策略。

Method: 提出Seeing the Bigger Picture（SBP）方法：通过多视角观测，渐进式融合成具有场景特征的3D潜在特征网格地图，并能在任务执行中在线优化。预训练的解码器可从地图中重建目标嵌入。策略网络（可用行为克隆或强化学习训练）将地图当作状态输入，利用3D特征聚合器获得全局信息。

Result: 在场景级移动操作和顺序桌面操作任务上评估SBP，结果显示SBP能够实现全局推理、长期记忆，且在分布内和新场景上均优于基于图像的策略。例如，在顺序操作任务上成功率提升25%。

Conclusion: 基于3D潜在特征地图的SBP方法极大改善了移动操作机器人的空间和时间推理能力，提升了多任务执行的效率和泛化性。

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [300] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 论文提出了NoTVLA框架，通过聚焦于稀疏轨迹、优化轨迹规划，显著缓解了VLA模型在多任务学习中的灾难性遗忘问题，实现更低算力消耗与更好泛化。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型在多任务环境下极易发生灾难性遗忘，主要由于过度依赖连续、密集的动作轨迹导致各任务间知识难以共享和迁移，阻碍了实际落地。

Method: NoTVLA通过关注稀疏轨迹，结合临时压缩与空间推理剪枝，对机器人末端执行器的轨迹进行高效规划。训练过程中仅使用稀疏轨迹以减少信息冗余和干扰，提升泛化能力。

Result: NoTVLA在多任务评测中展现出比pi0更优的性能和泛化能力，同时算力消耗降低一数量级，并无需手腕摄像头。单任务表现接近专家模型，并保留语言能力，支持多平台部署。

Conclusion: NoTVLA架构有效避免了灾难性遗忘，实现了高效的知识迁移与泛化，提升多任务机器人的实用性与部署灵活性。

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [301] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: 本论文提出了一种新的穿戴式感测系统WAFFLE，通过学习用户的自然进食时机，有效预测和控制机器人辅助喂食的动作，实现了更自然和个性化的人机交互，显著提升了用户体验和控制感。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人喂食系统受限于技术难题，特别是难以准确估算用户的进食时机，导致人机协作不自然、效率低下，从而影响残障人士的自理能力和生活质量。因此，研究如何基于用户自然行为，提升机器人喂食时机感知和反应能力，成为亟待解决的问题。

Method: 作者设计了WAFFLE系统，利用可穿戴传感器采集用户头部动作、咀嚼和说话等自然信号，通过监督回归模型预测进食时机，并引入自适应参数将预测结果转化为机器人执行或停止的指令。系统在15名无运动障碍参与者和2名有障碍者的真实环境中开展了实验验证，还在不同类型的机器人及实际家庭环境中测试了通用性。

Result: 实验结果显示，WAFFLE在用户控制感、机器人理解和工作负载等指标上与或优于现有方法，大部分用户更偏好本系统，无论是个人还是社交用餐场景下均表现优异。同时在有运动障碍参与者实际家庭中也展现出较好泛化能力。

Conclusion: WAFFLE系统能够实现自然、主动的机器人辅助喂食，能适应不同用户、机器人设备、喂食轨迹和环境，提升用户自主性和生活质量，有助于推动智能喂食系统的普及和实际应用。

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [302] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: 本文提出了一种新型高帧率VIO系统（TCB-VIO），极大提升了视觉惯性里程计的精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉惯性里程计（VIO）由于视觉传感器与处理器间数据传输瓶颈，导致延迟增加且容易出现空间漂移和时间漂移问题。新兴的焦平面传感-处理阵列（FPSP）能将处理器集成到每个像素点上，有效解决上述瓶颈，提升算法执行效率。本文希望借助FPSP，提升VIO系统的性能，降低漂移。

Method: 作者提出了TCB-VIO方法，采用多状态约束卡尔曼滤波（MSCKF），利用FPSP，使视觉和惯性数据以高帧率（图像250FPS、IMU 400Hz）紧密融合处理，并实时估计六自由度的位姿。

Result: 实验结果显示，TCB-VIO在多项指标上优于当前主流方法（如ROVIO、VINS-Mono、ORB-SLAM3），表现出更高的精度和实时性。

Conclusion: 将FPSP技术引入VIO系统能显著提升性能。TCB-VIO是实现高精度、高实时性视觉惯性里程计的有效方法，对相关视觉计算及机器人导航有重要意义。

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [303] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: 本论文提出了一种适用于越野环境的新型全局路径规划方法，有效解决了大规模地图下实时性、运动学可行性和内存效率等问题。


<details>
  <summary>Details</summary>
Motivation: 传统的全局路径规划方法在越野环境下表现不佳，尤其在大范围地图和复杂地形中难以兼顾实时性、运动学可行性以及内存效率。必须有更适应这些应用场景的新方法。

Method: 作者构建了基于像素坐标系的中间地图，融合地理特征（如小路、水道、可通行与限制区域、树木），然后将路径规划分为图搜索、运动学可行性检测和路径平滑三步，兼顾实时性和内存效率。

Result: 新方法在数平方公里的大规模越野地图上进行了测试，能够在平均1.5秒内完成可行路径规划，极端条件下内存使用约1.5GB，展现出优异的性能。

Conclusion: 提出的方法具有通用性，适用于包括搜救和农业等多种越野自主导航任务，能够有效平衡实时性、运动学可行性及内存利用率。

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [304] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: 本文提出了一种增强型的视觉-语言-动作（VLA）模型推理框架SITCOM，通过集成基于模型的多步规划和奖励机制，大幅提升机器人长时序规划和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人控制策略学习面临数据收集昂贵、泛化能力弱及长时序任务规划难等问题。虽然VLA模型能将自然语言转化为单步控制指令，但缺乏前瞻能力，难以应对复合错误和动态任务。

Method: 作者提出SITCOM框架，可对任何预训练VLA模型进行推理增强，核心做法是利用基于变换器（Transformer）的动力学模型进行多步动作预测和模拟（rollouts），通过奖励函数从候选动作轨迹中选出最佳路径，其中动力学模型在BridgeV2大规模数据集上训练，并在SIMPLER环境微调，用以缩小真实和模拟环境的差距。

Result: 在SIMPLER环境下多任务综合评测显示，结合优秀奖励函数后，SITCOM搭配训练好的动力学模型将任务完成率从48%显著提升至72%。

Conclusion: SITCOM框架显著增强了VLA在长时序任务和动态环境下的鲁棒性与泛化能力，为机器人控制策略学习开辟新思路。

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [305] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 本文提出了一种基于反馈的自主组织剥离框架，通过端到端图像推理拓扑变化，并引入了可见性度量和优化的控制器设计，大幅提升了手术机器人的自主性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的外科手术机器人在组织剥离任务中，面对组织拓扑和感知方面的挑战时，反馈机制仍有局限，难以应对高度动态环境下的变化。

Method: 提出一种在每次剥离动作后，通过内窥镜图像推理组织拓扑变化的反馈框架，并设计可见性度量和最优控制器，主动调整组织以优化可见性。这些反馈机制与基于规划和学习的方法相结合，用于指导后续操作。

Result: 实验表明，引入该反馈机制后，系统的自主性、错误率和鲁棒性在复杂手术场景下均有显著提升。

Conclusion: 基于反馈的拓扑感知与可见性优化方法，能够有效提高自主手术系统在复杂动态环境下的表现，为外科机器人自主操作提供了新思路。

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [306] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: 本论文系统总结并评估了八种旨在降低数据驱动控制策略计算复杂性的技术，提升其在实际机器人与车辆运动控制中的实用性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有的模型预测控制（MPC）和数据驱动控制方法在确保安全高速运动方面已取得进展，但由于计算量大、速度慢、对内存和算力要求高，难以在快动态或资源受限的实际系统中应用。该文旨在寻找并验证高效的解决方案。

Method: 系统梳理了减少数据驱动控制算法计算复杂性的主流方法，如降阶建模、函数近似政策学习、凸松弛等，并归纳出八种具体技术。随后将这些方法应用于典型案例（如机械臂、软体机器人及车辆运动控制）进行实验验证。

Result: 实验结果显示，提出的八种方法能在多种实际系统（如机器人和车辆）中有效降低所需的计算资源，提高控制速度，并保持或提升安全性与性能。

Conclusion: 总结证明，上述高效方法可显著提升数据驱动最优控制策略在实际机器人与车辆运动控制场景中的可用性，为未来智能机器人与自动驾驶等领域推广提供了有力支撑。

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [307] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 提出了一种针对多异质机器人在不明环境中自主探索的路径规划新方法HEHA，通过分层规划与新型路由算法提升效率，有效减少探索时间。


<details>
  <summary>Details</summary>
Motivation: 多异质机器人在复杂未知环境协作探索时，如何智能分配、路径规划与约束规划是一个大规模、需快速迭代优化的难题。

Method: 采用分层探索策略（HEHA），分为全局与局部规划；全局规划中提出了PEAF路由算法，能快速找到约束下的次优解。局部规划中考虑机器人异质性以提升协同效率。

Result: 实验结果表明，HEHA方法相较基线方法最多可减少30%的探索时间。

Conclusion: HEHA方法通过高效的分层与异质性协作规划，有效提升了团队机器人在未知环境下的探索效率，对大规模、异构机器人自主探索具有现实意义。

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [308] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 本论文提出了一种基于强化学习的数据驱动控制框架，实现了使用标准挖掘机铲斗进行岩石抓取任务，并在复杂且不结构化的工地环境下取得了与人工操作相媲美的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化挖掘方法主要针对连续介质（如泥土），或者依赖专用夹持器，这限制了它们在真实工地岩石抓取任务中的应用。因此，提出一种能够适用于标准挖掘机和实际工况的自动化岩石抓取方法具有很大实际意义。

Method: 采用基于PPO（Proximal Policy Optimization）算法的无模型强化学习方法，在AGX Dynamics模拟器中训练智能体。通过对岩石几何、密度、质量及初始配置进行显著域随机化，增强策略对环境变化的适应性。最终，策略直接输出挖掘机各关节的速度指令。

Result: 实验表明，该方法对未见过的岩石和不同土壤条件具有良好泛化能力，在保持挖掘机稳定性的同时，抓取成功率与人工操作者相当。

Conclusion: 证明了无需专用硬件或详细材料建模，也能通过学习型方法实现高效的离散物体操控，为自动化挖掘技术提供了新思路。

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [309] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的方法（VBM-NET），通过上视正交投影图像直接进行移动机器人基座位姿选择，实现高效抓取。


<details>
  <summary>Details</summary>
Motivation: 在移动操作中，精准选择抓取时的基座位置对任务成功至关重要。但以往方法依赖高精度的状态信息（如物体位置及环境模型），在实际应用中不易获得，因此亟需一种基于简单感知数据（如场景俯视图像）即可实现基座选址的方法。

Method: 作者提出了VBM-NET方法。首先通过上视正交投影图获取场景全局感知。然后用具空间等变性的TransporterNet 高效提出抓取候选基座位姿。接着，采用图神经网络表达可变数量的候选位姿，并基于强化学习选择最优位姿。

Result: VBM-NET的方法仅需较少计算时间便可输出与经典方法相当的解决方案，并且验证了其在仿真到实际环境中的可迁移性。

Conclusion: VBM-NET结合视觉投影图、等变模型及图神经网络，实现了高效、可泛化且低依赖先验信息的移动基座位姿选择，对移动操作实际应用有现实意义。

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [310] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本论文探讨了在二尖瓣边缘修复手术中应用机器人辅助，以简化操作并提高准确性。结果表明，机器人系统在减少手术时间和错误及提高夹片定位准确性方面优于人工操作。


<details>
  <summary>Details</summary>
Motivation: 经导管瓣膜修复手术由于手动导管系统的机械限制和陡峭的学习曲线存在显著挑战，亟需新方法提升安全性与操作便捷性。

Method: 本研究用机器人系统替代传统手工修复装置的复杂手柄控制，通过手柄进行直观关节控制；并将手术过程拆分为多个运动步骤，在心脏与血管的模型上逐步对比人工与机器人系统的表现，考察手术时长和夹片定位准确性等指标。

Result: 实验表明，机器人系统能有效减少手术时间和运动错误，并提升夹片定位的准确性。

Conclusion: 机器人辅助有望克服人工系统的关键局限，为复杂的经导管手术提供更加可靠和用户友好的平台。

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [311] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: 本研究提出一种社交机器人系统，能实时巡逻、识别并通报违停行为，采用GPT-4o多模态模型实现无需预处理的车牌识别，在室内停车场实验中达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 在现实生活中，违停问题普遍且难以及时发现与处理。传统的违停监控依赖大量人工和固定摄像头，存在效率和灵活性不足的问题。因此，本研究旨在通过社交机器人和先进的多模态AI方法，提高停车场违规停车检测的自动化与智能化水平。

Method: 本研究搭建了一台社交机器人，能够在室内停车场模拟环境中自动导航，并使用摄像头抓取车牌图像。系统采用对比实验，评估了传统双模型管线方法与大型多模态模型（如GPT-4o）在车牌识别任务中的表现，最终选择免预处理的GPT-4o模型进行识别。被识别为违规停车后，自动通过Line消息通知管理员。

Result: 实验结果显示，GPT-4o模型能高准确率地识别摄像头抓取的车牌信息，并有效识别违停情况。机器人能顺利自动导航并自主调整视角，及时、准确地发送违停通报。

Conclusion: 本研究证明了一种创新的多模态深度学习识别方法在实际场景中的有效性，并展示了社交机器人在智能停车管理中的应用前景，适用于室内停车场的违停检测与通报。

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [312] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种结合扩散生成模型与MPC的灵活步态规划方法，实现了新颖的腿式机器人运动控制，并在实际环境中验证了强适应性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的方法难以在测试时适应新任务，MPC方法对模型精度要求高，不易在复杂环境中应用；因此，需要一种兼具鲁棒性、灵活性且易适配新约束及奖励的方法。

Method: 提出Diffusion-MPC，将学习到的扩散生成模型作为近似动力学先验，结合奖励和约束规划，引入联合预测未来状态与动作、约束投影、奖励引导等机制，并通过环境交互和基于回报的样本重加权改进模型。

Result: Diffusion-MPC能在实际腿式机器人中实现高质量运动控制，展现出在不同奖励与约束下的较强适应能力，无需重新训练即可灵活应对新的任务需求。

Conclusion: Diffusion-MPC为复杂环境下腿式机器人提供了无需高精度动力学模型即可灵活适配多任务的新方法，兼具鲁棒性和适应性，对实际机器人控制具有重要意义。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [313] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 本文提出了一种名为ContextVLA的新策略模型，通过高效利用多帧时序观测信息，大幅提升了机器人在部分可观测任务中的表现，并显著降低了多帧输入下的训练和推理计算量。


<details>
  <summary>Details</summary>
Motivation: 现有行为克隆方法在利用多帧观测（即时序上下文）提升机器人任务表现时效果不稳定。原因之一是视频高维输入导致VLA模型在多帧情况下训练和推理效率低下。该工作希望构建一种既能有效利用时序上下文，又能具备高效推理和训练能力的策略模型。

Method: 作者观察到基于Vision-Language Model（VLM）的VLA模型具有较强的时序理解能力，能更有效地处理多帧观测。为此，提出ContextVLA方法，把以往的多帧观测压缩为一个上下文token，既保留时序信息，又降低多帧处理带来的计算开销。该方法通过这种上下文表示，将多帧信息高效融入动作生成过程。

Result: 实验表明，ContextVLA在多项机器人任务上稳定超越单帧VLA，同时能达到完整多帧训练的效果，并显著缩短了训练和推理时间。

Conclusion: ContextVLA能够有效、高效地利用时序上下文，提升部分可观测机器人任务中的策略学习表现，为多帧观测机器人决策提供了实用方法。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [314] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于因子图的模型预测控制（MPC）工具包FactorMPC，专为在非线性流形上运行的系统而设计，在高维、受约束、具有安全要求的场景下实现了高效、实时的优化与控制。


<details>
  <summary>Details</summary>
Motivation: 传统欧式空间下的MPC难以直接用于流形（如机器人姿态、受限轨迹规划等）上的动态系统，遇到奇异性、参数冗余和收敛性差等问题，因此亟需更自然支持流形状态的新方法。

Method: 作者提出FactorMPC工具包，利用因子图将系统动力学、约束和优化目标统一建模，并原生支持流形值状态及其高斯不确定性（在切空间中建模）。同时，提出基于速度扩展的在流形上的CBF（控制屏障函数）因子，实现安全约束和障碍规避。该方法利用因子图的稀疏和概率结构，提升大规模系统的实时性能。

Result: 在四旋翼上的仿真和实验显示，FactorMPC在轨迹跟踪和避障性能上优于传统基线方法，并能支持高维、复杂约束等实际挑战。

Conclusion: FactorMPC为流形动力学系统提供了可扩展且在几何上自洽的集成规划与控制框架，兼具高效、安全和易用的特点，还提供了开源实现以促进后续研究和应用。

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [315] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: 本论文提出了一种基于质心稳定性的遥操作重定向方法，可提升人形机器人在复杂任务中的稳定性，尤其是在手部接触和非共面地面条件下。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人遥操作在涉及手部接触和非共面地面时，容易出现电机扭矩过载或打滑丧失稳定，限制了机器人任务能力。作者希望通过一种新方法提升机器人的稳定性和任务执行范围。

Method: 提出了一种基于质心稳定性边界梯度的计算方法。在遥操作过程中，动态地根据稳定性边界梯度，调整接触点和姿态，使得关键任务过程中稳定性得到增强。该方法能够高效分析哪些操作指令可能引起稳定性剧烈变化，并对此进行适应性调整。

Result: 通过仿真和硬件实验，在实际人形机器人操控任务中验证了所提框架的有效性。实验表明，所提出的方法能够提升稳定裕度，且更高的稳定裕度与更强的冲击抗扰和关节扭矩裕度相关。

Conclusion: 该工作证明了基于质心稳定性重定向方法能够有效提升人形机器人在复杂遥操作任务中的稳定性，有助于拓展其任务能力与实际应用范围。

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [316] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 本文提出SureSim框架，通过将有限量真实世界测试与大规模仿真结合，为机器人操作策略在现实世界中的表现提供可靠推断和置信区间，降低硬件验证成本。


<details>
  <summary>Details</summary>
Motivation: 传统上，机器人操作策略往往只通过少量硬件实验进行评估，缺乏统计保证。如何在减少真实硬件测试的前提下，获得对政策在真实环境中的表现进行严格和可靠评价，是该领域的痛点。

Method: 提出SureSim框架，将少量配对的真实和仿真评测数据，应用于修正大规模仿真评估的偏差，并采用非渐近均值估计算法，对策略表现输出统计上的置信区间。

Result: 在物理仿真环境下评估扩散策略（diffusion policy）和多任务微调策略(π_0)，实验证明该框架在保证性能评估边界一致的前提下，可减少20-25%的硬件验证工作量。

Conclusion: 通过与仿真结果结合，SureSim显著提升了对机器人操作策略现实表现的推断效率与统计可靠性，对该领域的评估方法有重要意义。

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [317] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的直接轨迹优化方法，通过创新的投影机制保证动力学约束，在无人机导航任务中显著提升了动态可行性和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型在轨迹优化领域虽能建模多峰概率分布，但难以解决动力学可行性等非线性等式约束，现有方法往往无法显式保证约束，导致最优性不足。因此，需要新的方法确保生成轨迹动态可行。

Method: 本文提出一种基于模型的直接轨迹扩散优化方法，直接生成状态序列。为保证动力学可行性，提出无梯度投影机制，将其整合到反向扩散过程当中，从而强制状态满足约束。

Result: 实验在密集障碍无人机航点导航场景中展开，对比最新基线方法，新方法实现了零动力学可行性误差，成功率大幅提升（约为对手的4倍）。

Conclusion: 实验结果证明，结合无梯度投影机制的直接轨迹扩散优化方法能显著提升轨迹的可行性及任务成功率，在处理含复杂约束的轨迹优化任务时具有优势。

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [318] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的速度型DeePC方法，在无须已知外部负载情况下，实现了软体机器人的鲁棒优化控制，并在实验中优于标准DeePC。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动控制方法如DeePC虽能有效控制软体机器人，但在处理未知外部负载和扰动时，容易性能下降，导致控制偏差和效率降低。针对这一问题，需开发更鲁棒且能抵抗未知扰动的方法。

Method: 作者提出了一种基于速度形式的DeePC框架，核心思路是利用输入输出数据的增量表达。这种方法无需引入加权数据集或扰动估算器，通过数据驱动直接抑制外部未知负载带来的影响。

Result: 在平面软体机器人实验中，该方法在面对未知负载时，表现出比标准DeePC更优的控制性能和鲁棒性。

Conclusion: 提出的速度型DeePC框架能在未知外部负载情况下，有效提升软体机器人的控制性能，为实际控制任务提供了更强鲁棒性和更优表现。

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [319] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 本论文提出了一种名为 Everything-Grasping (EG) Gripper 的软体末端执行器，能够通过结合分布式表面吸附和内部颗粒夹持，实现对各种尺寸和物理状态（固体和液体）物体的抓取。该装置配合触觉感知框架和自适应夹持算法，实现在不同尺度、不同状态物体的自主抓取。


<details>
  <summary>Details</summary>
Motivation: 当前软体机器人抓取器难以在不进行气密密封的情况下，实现对不同尺寸和固液态物体的通用抓取，存在应用上的重大限制。

Method: EG Gripper 采用分布式表面吸附和内部颗粒夹持相结合的方法，并引入液体检测和压力基吸附反馈的触觉感知框架，再结合触觉推断抓取模式选择算法（TIGMS），实现模式自主切换和抓取操作。

Result: 该夹持器能够抓取表面积从0.2 mm²到62000 mm²的各类物体，抓取对象的尺寸覆盖范围达到自身接触面积的3500倍小到88倍大。在多种任务环境（包括水下抓取、脆弱物体和液体抓取）的实验中，该夹持器表现出高度的鲁棒性和可重复性。

Conclusion: EG Gripper 是首个能够使用统一柔性结构可靠抓取不同尺度的固体和液体物体的软体抓手，推动了软体机器人在通用物体操作领域的发展。

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [320] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: 本文提出了一个名为MobRT的数字孪生框架，用于自动生成高质量、复杂移动操作机器人的演示数据，从而提升机器人在动态与高维环境下的操作能力。经实验验证，该框架生成的数据能有效提升策略泛化与执行表现。


<details>
  <summary>Details</summary>
Motivation: 现有仿照学习在机器人领域取得进展，但高质量大规模演示数据收集难度高，尤其对于需协调底盘移动与手臂操作的移动操作机器人更为困难，导致相关研究多聚焦于简单场景，复杂移动操作较少被研究。

Method: MobRT框架采用数字孪生技术，通过虚拟运动学控制和全身运动规划相结合，能够自动模拟并生成两类主要的复杂任务演示数据：1）与铰接物体互动（如开门、拉抽屉）和2）带移动底盘的抓取搬运任务。

Result: MobRT生成的数据在多种基线算法上进行了测试，并建立了全面的基准。结果显示，任务成功率与生成轨迹数量密切相关；使用该框架生成的模拟演示与真实演示结合，机器人策略的泛化性能和实际表现均有明显提升。

Conclusion: MobRT有效拓展了移动操作机器人在复杂任务场景下的研究与应用，显著提高了在模拟和现实环境中的泛化能力和鲁棒性。

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [321] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X是一个多传感器的全新SLAM系统，可实时、大规模地构建高精度的稠密体素地图，支持多种传感器输入，并在多项公开数据集上性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人对地图的可用性、定位精度和鲁棒性有更高的需求；而现有SLAM方法通常处理多传感器融合与大规模、稠密地图构建存在不足。本文旨在开发性能更优、适用更广的SLAM系统。

Method: 提出OKVIS2-X，统一融合视觉、惯性、深度（测量或学习获得）、激光雷达和GNSS等多传感器数据。系统采用稠密体素地图表达、创新的高效子地图策略保证大规模环境的实时性及可扩展性，通过地图对齐因子实现精确鲁棒的紧耦合框架，同时支持相机外参在线标定。

Result: 系统在EuRoC数据集上取得最高轨迹精度，在Hilti22 VI-only基准中超越所有竞品，在Hilti22 LiDAR基准上表现有竞争力，在VBR大规模多样数据集上具备领先精度。展示了强大的可扩展性（最高9公里序列）、准确性和鲁棒性。

Conclusion: OKVIS2-X系统能够为移动机器人提供可用于自主导航的全局一致、高精度的稠密地图，融合多模态传感器信息，且在业界标准数据集上具备领先或竞争力的性能，为SLAM研究和实际应用带来重要进展。

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [322] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文提出了一种仿生学智能机器人平台，能高仿真模拟母性小鸨，实现野外生态学研究中动物-机器人自然交互，并已通过实地测试验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在野外研究鸟类行为存在形态真实性、高耐用性与智能感知等技术难题，尤其是在不可控自然环境下。为解决这些问题，作者开发具备高度仿真、耐用和智能感知能力的仿生机器人，支持生态学与保护相关研究。

Method: 开发了具女性小鸨外形和视觉特征的仿生机器人。构建流程融合了3D结构光扫描、参数化CAD建模、关节3D打印和UV贴图涂装，突出高仿真与耐用性。底盘采用六轮摆动设计，适合复杂地形，集成NVIDIA Jetson实现RGB/热成像感知与YOLO检测及自驾视觉伺服（自动朝向目标）。引入热-可见光融合模块增强弱光识别能力。

Result: 在沙漠鸟舍实地测试，机器人可实现15-22帧/秒的实时运作、低于100毫秒的延迟，并能被活体小鸨所识别和正常互动，表现出稳定性与生态适应性。

Conclusion: 该平台整合高仿真数字化制造、视觉智能与生态学验证，为动物-机器人交互研究、保护机器人及科普提供了可复制蓝本，推动了仿生野外机器人发展。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [323] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 该论文提出了一种分散式基于梯度的方法，用于快速、灵活地解决双臂装配中的任务规划与动作控制难题，尤其应对极小公差组装时的动态变化。


<details>
  <summary>Details</summary>
Motivation: 双臂装配任务面临高层次序列、多机器人协同及低层次复杂操作。传统的任务与动作规划方法虽然有效，但在需要快速适应干扰和重新序列化时，收敛速度慢且缺乏灵活性，难以应对实际紧公差组装所需的频繁重新规划和重试。现有方法还需人工显式定义任务序列，缺少通用性和自适应性。

Method: 提出了一种分布式、基于梯度的规划框架，利用自动组合的自适应势函数构建分段连续的能量函数，仅通过局部最优（myopic）优化生成中间目标，而不是依赖长远规划，从而实现快速、结构化且可适应的任务/动作规划。

Result: 该方法能够在物理双臂紧公差组装任务中实现任务自动重试、协调动作与自主交接，并表现出良好的伸缩性和效率。实验表明，该方法利用能量函数结构和适应性，能有效解决长时序、多变环境下的装配任务。

Conclusion: 分散式基于梯度的规划方法能高效应对紧公差双臂装配过程中频繁发生的扰动，无需明确任务序列定义，具有高灵活性、自主性和良好的实际应用前景。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [324] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种针对任务优化多旋翼微型空中飞行器设计的方法，使用强化学习、贝叶斯优化和CMA-ES，仅通过飞行器在任务中的闭环性能来引导设计选择。实验和实物测试显示，该方法优于传统设计。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器的常规设计常常无法针对特定复杂任务达到最优表现，因此需要一种能够自动针对具体任务优化飞行器设计的方法，以提升性能。

Method: 作者联合使用强化学习、贝叶斯优化和协方差矩阵自适应进化策略（CMA-ES），只根据飞行器在特定任务中的闭环表现，对旋翼位置进行系统化优化，同时确保可制造性和气动干扰最小化。

Result: 优化得到的新设计相比传统多旋翼结构和已有文献中的全驱动结构，在灵活路径导航任务上表现更佳。作者还制造并测试了一个优化设计，验证其从仿真到现实的迁移能力。

Conclusion: 本文提出的方法能够高效针对具体任务设计多旋翼微型飞行器，提升其性能并确保设计具有实用可行性和良好的仿真到现实迁移效果。

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [325] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 本论文提出了一种自组织神经系统（SoNS），用于提升机器人集群的行为设计便利性和全局感知能力，并引入了与外部大语言模型（LLM）结合实现自动化代码生成。实验表明，SoNS增强的机器人集群在遇到障碍时能调用外部LLM即时生成并运行代码，从而以85%的成功率完成任务。


<details>
  <summary>Details</summary>
Motivation: 机器人集群的行为设计复杂、适应性较差且缺乏对当前配置和环境的全局感知，因而难以实现智能、自主和高效的任务完成。为解决这一问题，作者引入了SoNS，旨在提升行为设计的自动化和集群的环境感知能力。

Method: 作者在6台真实机器人及30台以上仿真机器人上实验，搭建SoNS系统，使其可感知集群配置和环境状况。当机器人集群遇到任务瓶颈（如陷入困境）时，能够自动请求外部LLM生成新的应对代码，随后自动集成和执行新的代码以尝试完成任务。

Result: 通过真实和仿真环境测试，SoNS增强的机器人集群在遇到障碍时能够调用外部LLM生成新代码，并以85%的成功率完成被卡住的任务。

Conclusion: SoNS有效提升了机器人集群的自适应能力和任务完成率，并展示了机器人系统与大语言模型结合以实现在线自动代码更新的可行性。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [326] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: 本文提出了一种适用于机器人在线惯性参数估计的新方法TAG-K，提升了计算速度和鲁棒性，在实际任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统递归最小二乘（RLS）和卡尔曼滤波（KF）方法在参数突变时跟踪能力弱，或计算开销大，难以满足动态环境和资源受限机器人的需求。因此需要高效、低复杂度、又能适应参数剧变的估计算法。

Method: 提出TAG-K方法：基于Kaczmarz算法，通过贪心随机选择加速收敛，并引入尾部均值提高噪声和不一致数据下的鲁棒性，维持每步低计算复杂度。与RLS、KF及其他Kaczmarz变体在仿真和实际四旋翼跟踪任务中对比性能。

Result: TAG-K在笔记本级CPU上求解速度提升1.5-1.9倍，在嵌入式MCU上提升4.8-20.7倍。同时噪声鲁棒性更强，估计误差降低25%，整体跟踪性能提升约2倍。

Conclusion: TAG-K方法能以极低计算开销实现快速、稳定的机器人惯性参数自适应估计，适合动态变化和资源有限环境，优于主流传统方法。

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [327] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 该论文提出一种基于U-Net的深度学习方法，能够去除红外(IR)图像中的发射源噪声，在低光环境下提升机器人感知能力，并显著优于现有的增强技术。


<details>
  <summary>Details</summary>
Motivation: 夜间或弱光环境下，常规RGB视觉传感器易受噪声影响，而IR流虽然噪声更小但存在发射器干扰，影响机器人高层次感知任务。因此亟需提升黑暗环境下的IR图像质量。

Method: 提出了一种基于U-Net网络结构的图像增强方法，将含有发射器干扰的IR图像输入，通过神经网络重建为干净的IR图像，从而提升后续识别、跟踪与定位等任务的表现。

Result: 提出的方法在IR图像质量提升和下游机器人任务表现上，均优于目前已有的IR图像增强技术，实现了从充足光照到极低光照场景下的鲁棒视觉。

Conclusion: 基于U-Net的IR图像干净重建方法，有效增强了机器人在黑暗环境下的视觉能力，提高了系统的实用可靠性。

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [328] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: 本论文提出了一种新的用于机器人通用策略学习的视觉-语言-动作（VLA）模型HyperVLA，通过使用超网络结构，大幅降低了推理成本，同时保持或提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型尽管具备良好的通用能力，但在推理时成本极高，限制了实际机器人应用。本工作旨在解决高推理成本问题。

Method: HyperVLA采用基于超网络（HN）的新结构，仅在推理时激活小型、任务特定的策略网络，而训练时保留大模型容量；同时，设计了多项算法细节，包括利用视觉基础模型先验、超网络归一化和动作生成策略，增强训练效果。

Result: HyperVLA在零样本泛化和小样本自适应任务上取得了与甚至超过现有VLA模型的成功率，推理参数量减少90倍，推理速度提升120倍（对比OpenVLA）。

Conclusion: 提出的HyperVLA模型在大幅降低运算和资源消耗的同时，依然实现了高水平的任务泛化和适应能力，为通用机器人政策推理提供了更高效率的新方案。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [329] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉-语言模型（VLM）的高层规划框架，有效提升了自主机器人在存在大量死路的未知室内环境中的导航效率，相比传统方法平均路径缩短约10%。


<details>
  <summary>Details</summary>
Motivation: 传统的室内探索算法，由于只具备有限的全局推理能力且依赖局部启发式方法，常常导致机器人走弯路甚至陷入死路，导航效率低下。

Method: 方法上，作者利用VLM模型零样本地在占用地图（occupancy map）上进行推理，自动选择更优的子目标。具体流程为：将三维占用网格转换为部分二维地图，生成若干子目标候选，并利用VLM对候选子目标进行评估排序。整个规划方案集成进了当前先进的轨迹规划器DYNUS，形成完整系统。

Result: 在仿真环境下，该方法能识别出结构性分区（如房间、走廊），兼顾前进进度与风险规避，相较于贪婪方法显著减少了小房间等不必要绕行，平均路径缩短约10%。

Conclusion: 通过引入VLM对场景结构的理解，增强了高层路径规划，能够减小导航过程中的低效绕行和失败概率，从而显著提高未知复杂室内环境中的自主导航效率。

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [330] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: 该论文开发了一种受电影《星际穿越》中TARS机器人启发的方块型多模式机器人TARS3D，并通过解析建模和深度强化学习探索其复杂步态。


<details>
  <summary>Details</summary>
Motivation: 现有机器人多模仿生物结构，但很多实际环境中非人形结构更具优势。作者受TARS机器人启发，旨在探索这种超越仿生形态的多模式运动能力及潜力。

Method: 首先，作者将TARS机器人重现为TARS3D平台，具备7个自由度。对其两种典型步态（类双足行走和高速滚动）建立降阶动力学模型，推导极限环条件，并硬件实验验证。滚动步态建模为八辐条无轮毂轮。通过深度强化学习在模拟中进一步搜索未探索的步态可能。

Result: 实验表明TARS3D能在硬件上执行两种主步态，满足机械结构和步态极限。深度强化学习不仅复现了解析步态，还发现了新的运动模式，展现灵活丰富的步态能力。

Conclusion: TARS3D展示了电影虚构启发的仿生超越结构能够实现多种以往未见的本体运动方式。分析建模结合强化学习为多模态机器人研究开辟了新路径。

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [331] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出了一种创新的紧凑状态表示方法StaMo，在无需监督的情况下，用轻量编码器和预训练Diffusion Transformer（DiT）解码器，实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能系统状态表示要么冗余、要么缺乏关键任务信息，很难兼顾表达性与紧凑性。论文旨在解决高效、可解释、带有效生成先验的状态表示问题。

Method: 利用无监督学习策略，将状态压缩为两个token，再用预训练DiT作为解码器，提取有效状态表达，并可与当前基于视觉-语言-动作（VLA）模型无缝融合。通过token间插值直接生成可执行的机器人潜在动作。

Result: 在LIBERO基准上性能提升14.3%，实际机器人任务成功率提升30%，Co-training时优于之前方法10.4%。方案还可泛化至模拟、真实及人类视角数据，各项任务表现出色，且推理开销极低。

Conclusion: StaMo能够从静态图像学习到结构化动力学，突破对复杂模型与视频的依赖，提升操作策略表现与可解释性，具有极强的通用性和实用价值。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [332] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 该论文提出了一种名为Automaton Constrained Q-Learning（ACQL）的新算法，有效解决了机器人任务中既要实现有序目标序列，又需遵循时变安全约束的难题。ACQL结合了线性时序逻辑（LTL）与目标条件值学习，展现出超越以往方法的表现，特别是在连续控制和实际机器人实验中。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器人任务通常需要在满足安全约束的同时，依次完成多个目标。传统强化学习方法难以同时支持灵活的时序目标与动态安全约束。现有RL与LTL结合的方法在复杂、连续场景表现不佳，亟需一种新方法突破这一局限，应对实际机器人任务的需求。

Method: 作者提出了ACQL算法，将强化学习中的目标条件值学习与LTL公式的自动机表示结合，通过自动机引导强化学习过程。该方法可表达并处理多阶段任务进展、静态与动态安全约束，支持大多数LTL任务规范，并通过自动机显式追踪任务执行过程和约束满足情况。

Result: ACQL在多种连续控制任务中表现优异，不仅实现了目标到达，还能满足严格的安全约束，且在此前方法失效的复杂场景下依然有效。实际部署在6自由度机械臂进行目标到达实验，同样取得积极成效，验证了方法的现实可用性。

Conclusion: ACQL提供了一种稳健且可扩展的解决方案，能依照复杂时序规范学习和执行机器人行为，显著提升了LTL约束下强化学习应用于真实机器人任务的实际能力。

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [333] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic提出了一种两阶段残差学习框架，从人类动作数据中实现类人精确控制，有效提升了人形机器人在复杂场景下的运动与操控能力。


<details>
  <summary>Details</summary>
Motivation: 当前通用动作追踪（GMT）虽能让人形机器人模仿多样的人类动作，但在操作任务中，存在精度及物体感知不足的问题，限制了机器人在服务和仓储等实际场景中的应用。

Method: 提出ResMimic框架，分为两阶段：第一阶段通过大规模人类动作数据训练的GMT策略生成类似人类的基础全身运动，第二阶段通过高效精确的残差策略微调GMT输出，提升运动能力并整合物体交互。并设计了基于点云的物体追踪奖励、促进精准交互的接触奖励及课程化虚拟物体控制器提升训练效率和稳定性。

Result: 在仿真和真实Unitree G1人形机器人上验证，ResMimic在任务成功率、训练效率和鲁棒性方面均显著优于现有强基线方法。

Conclusion: ResMimic大幅提升了人形机器人的体态操控能力，使其更适用于日常服务和仓储等复杂场景，在实际部署中展现出广阔的应用潜力。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>
