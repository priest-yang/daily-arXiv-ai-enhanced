<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.CL](#cs.CL) [Total: 51]
- [cs.RO](#cs.RO) [Total: 29]
- [eess.IV](#eess.IV) [Total: 6]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

TL;DR: 本论文提出无需真实世界训练数据即可实现水下3D目标检测的两种方法，并首次建立了无训练的水下3D检测基准。结果显示，模板匹配法在实际数据中优于深度学习法，具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下3D物体检测由于声学环境复杂和标注数据稀缺面临巨大挑战，传统基于深度学习的方法受到真实标注数据获取成本高昂的限制。研究动机是寻找在缺乏训练数据情况下实现水下3D目标检测的新途径。

Method: 作者提出两种无需真实数据训练的检测方案：(1) 使用声纳物理仿真生成训练数据，训练主流神经网络进行目标检测；(2) 基于几何先验实现的模板匹配系统，直接在点云中检测目标。两者均在波罗的海真实水深调查数据上进行评估。

Result: 仿真数据训练的深度学习模型在仿真场景上mAP高达98%，但在真实声纳数据上mAP骤降至40%，反映出领域偏移问题；而模板匹配方法在真实数据上达到83% mAP，无需训练且对噪声与环境变化更鲁棒。

Conclusion: 结果表明，传统深度学习方法在数据稀缺的水下环境中表现不佳，而模板匹配凭借几何先验表现优越。本工作为水下训练无关3D检测建立了新基准，无需大量数据，拓展了水下导航与监测等应用前景。

Abstract: Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [2] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

TL;DR: 本研究提出了一种名为MobileDenseAttn的融合模型，有效提升了脑肿瘤MRI影像自动检测的准确率、效率和可解释性，为临床实际应用提供了有力的工具。


<details>
  <summary>Details</summary>
Motivation: 传统的脑肿瘤MRI检测方法存在泛化能力弱、计算效率低、可解释性差等问题，难以在临床中广泛推广，因此需要一种高效、准确且可解释的检测方案。

Method: 作者设计了MobileDenseAttn模型，将MobileNetV2和DenseNet201的特征通过特征级融合结合，并利用GradCAM热力图增强模型解释性。在包含6020张MRI扫描的增强数据集上进行5折交叉验证，并与主流基线模型进行对比。

Result: MobileDenseAttn模型获得了99.75%的训练准确率、98.35%的测试准确率，F1分数为0.9835，较VGG19模型准确率提升3.67%，训练时间减少39.3%。GradCAM可清晰定位肿瘤区域，提升了临床可用性。

Conclusion: MobileDenseAttn模型兼具高准确率、计算效率和良好解释性，为实际临床脑肿瘤检测提供了有前景的解决方案。

Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [3] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 本文通过实验发现，视觉语言模型（VLMs）在用图片引用实体时难以正确关联事实知识，其视觉表征与内部联系薄弱。作者还提出了可检测失败的探针方法，有效提高了视觉问答任务下模型的准确覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs虽然能通过文本准确完成事实回忆，但在多模态输入尤其是依赖纯视觉信号时表现欠佳。探索和识别VLM的此类不足，旨在提升多模态理解与应用的可靠性。

Method: 作者设计了对比实验，分别测试VLM在有文本引用与纯图片引用情况下的事实回忆能力，并分析模型内部状态。进一步提出针对内部状态的探针，用于无须重新训练的情况下检测模型何时会在多模态任务失败。

Result: 结果显示VLM在依赖图片时，事实回忆能力减半。提出的内部状态探针准确率达92%以上，并在视觉问答任务中将模型覆盖率提升7.87%，错误风险降低0.9%。

Conclusion: 探测VLM内部状态能有效识别多模态问答失效情形，是提升多模态语义基础能力的重要方向。建议未来研究关注改善视觉和知识的关联机制，提升模型多模态理解能力。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [4] [SERES: Semantic-aware neural reconstruction from sparse views](https://arxiv.org/abs/2508.18314)
*Bo Xu,Yuhu Guo,Yuchao Wang,Wenting Wang,Yeung Yam,Charlie C. L. Wang,Xinyi Le*

Main category: cs.CV

TL;DR: 本文提出了一种能够从稀疏图像生成高保真3D模型的语义感知神经重建方法。该方法通过引入基于patch的语义logits，并联合优化显式距离场和辐射场，有效缓解了因特征错配导致的辐射不确定性，并通过几何原语掩码正则化以减弱形状不确定性，在DTU数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对当前利用稀疏图片进行3D重建时，由于图像间特征匹配有限，导致辐射和形状存在模糊和不确定性，现有方法难以生成高保真模型。因此，作者希望引入更多先验和正则化机制，提高稀疏视角下的重建精度。

Method: 作者提出将patch级的语义logits作为附加信息，联合隐式有符号距离场（SDF）和辐射场共同优化，并设计了基于几何原语掩码的正则项，以缓解形状歧义，从而丰富隐式表达并提升重建质量。

Result: 在DTU数据集上，该方法搭配SparseNeuS和VolRecon可将重建的平均chamfer距离分别降低44%和20%；作为NeuS和Neuralangelo等稠密重建方法的插件时，误差可下降69%和68%。

Conclusion: 本文证明了通过融合语义信息和几何正则，可以显著减少稀疏图像3D重建时的辐射和形状歧义，提高重建精度，且方法通用，可作为现有重建框架的增强模块使用。

Abstract: We propose a semantic-aware neural reconstruction method to generate 3D
high-fidelity models from sparse images. To tackle the challenge of severe
radiance ambiguity caused by mismatched features in sparse input, we enrich
neural implicit representations by adding patch-based semantic logits that are
optimized together with the signed distance field and the radiance field. A
novel regularization based on the geometric primitive masks is introduced to
mitigate shape ambiguity. The performance of our approach has been verified in
experimental evaluation. The average chamfer distances of our reconstruction on
the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When
working as a plugin for those dense reconstruction baselines such as NeuS and
Neuralangelo, the average error on the DTU dataset can be reduced by 69% and
68% respectively.

</details>


### [5] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

TL;DR: 本文针对难以手动识别的非法垃圾填埋场，采用轻量级深度学习模型结合融合技术，在AerialWaste数据集上取得了92%+的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 非法垃圾填埋场对环境和公众健康造成严重威胁，但由于人工识别难度大，许多填埋场未被及时发现。通过自动化手段高效、准确地识别这些地点具有重要实际意义。

Method: 利用包含10434张意大利伦巴第大区航拍照片的AerialWaste数据集，实验了多种轻量级深度学习网络（如Mobilenetv2、Googlenet、Densenet、MobileVit等），对模型表现进行比较，并用融合方法集成表现最好的模型，提升分类性能。

Result: 轻量级模型表现优于复杂模型，能有效避免过拟合，最终通过集成和融合技术，二分类任务达到了92.33%的准确率、92.67%的精确率、92.33%的召回率、92.41%的F1值和92.71%的特异性。

Conclusion: 轻量级深度学习模型和模型融合技术能有效提升非法垃圾填埋场识别的准确性和泛化能力，具有现实应用推广价值。

Abstract: Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [6] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态情感分析框架SSU，有效结合结构与语义信息，在CMU-MOSI和CMU-MOSEI等数据集上取得了更高性能和更低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法在处理模态间结构依赖与语义对齐上存在不足，导致表现、可解释性和鲁棒性受限。

Method: 提出了SSU框架，动态构建基于语言语法的文本结构图，以及基于文本引导机制的声学和视觉图，捕捉细致的模态内结构关系。同时引入以全局语义为锚点的跨模态对齐机制，并设计多视角对比学习目标以增强辨别性和一致性。

Result: 在CMU-MOSI和CMU-MOSEI数据集上，SSU取得了新的最优性能，同时大幅降低了计算资源需求。

Conclusion: SSU不仅提升了多模态情感分析的准确率与效率，还提高了模型的可解释性和对细微情感模式的捕捉能力。

Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [7] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: 本文提出了FastAvatar，一种能够从单张任意姿态人脸图片近乎实时生成3D高斯喷溅模型（3DGS）的神经网络方法，兼具高准确性与高速性，并支持实时身份和属性编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D人脸重建方法存在两大主要问题：一是从单张不同姿态图片生成高质量3D模型困难；二是现有方法要么速度慢（如逐人优化法），要么保真度低且缺乏编辑能力（如现有快速前馈法）。因此需要一种既快速又高质量，并能灵活编辑人脸属性的新方法来满足实际应用需求。

Method: FastAvatar首先用多视角人脸数据集训练生成一个模板3DGS人脸模型。之后，通过新设计的编码-解码神经网络，把输入的人脸图片编码到姿态无关、身份相关的隐空间，再解码预测每个高斯点的结构和外观参数残差，实现对模板模型的高效个性化调整。全流程为前馈式推理，仅推断残差，因此极大提升了速度与鲁棒性。此外，创新的隐空间设计使实时身份插值和属性编辑成为可能。

Result: FastAvatar在重建质量上显著优于现有前馈式3D人脸GS方法（如GAGAvatar），而在推理速度上比逐人优化方法（如FlashAvatar、GaussianAvatars、GASP）快1000倍，且具备此前前馈式方法不具备的实时身份插值与属性编辑能力。

Conclusion: FastAvatar结合了极佳的重建质量与极速推理速度，极大拓展了3DGS在消费级和交互式系统中逼真化身场景的应用范围。

Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

</details>


### [8] [Securing Face and Fingerprint Templates in Humanitarian Biometric Systems](https://arxiv.org/abs/2508.18415)
*Giuseppe Stragapede,Sam Merrick,Vedrana Krivokuća Hahn,Justin Sukaitis,Vincent Graf Narbel*

Main category: cs.CV

TL;DR: 本文提出了一种适用于人道主义和紧急场景的移动生物识别系统，采用了高效且安全的生物模板保护方案PolyProtect，对面部和指纹数据均进行了实验验证，结果表现良好。


<details>
  <summary>Details</summary>
Motivation: 在人道主义和紧急场景下，生物识别技术能提升行动效率，但同时带来了个人隐私和安全风险，特别是在弱势群体中。本研究的动机在于平衡效率提升与数据保护需求，提出适合上述情境的低风险解决方案。

Method: 作者系统性分析了该场景下的功能、操作、隐私和安全需求，对现有生物模板保护（BTP）技术进行了广泛对比，最终选择适用于神经网络人脸特征的PolyProtect方法。实验方面，作者基于来自埃塞俄比亚的真实数据，结合EdgeFace特征提取器，对PolyProtect在验证、识别、不可逆性和不可关联性等方面进行了评估，并首次扩展到指纹数据。

Result: PolyProtect在采用EdgeFace提取的人脸特征和指纹特征上，均显示出良好的验证和识别准确性，以及在不可逆性和不可关联性上的优异表现，结果令人鼓舞。

Conclusion: PolyProtect方法不仅高效且具备良好的跨模态能力，适用于人道主义和紧急场景的生物识别保护需求。本文为指纹场景和大规模身份识别提供了首个实验结果，展示其广泛应用潜力，相关代码预计公开。

Abstract: In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code

</details>


### [9] [Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?](https://arxiv.org/abs/2508.18421)
*Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 该论文主张将动态关系图显式嵌入到视觉基础模型中，以提升其在需要实体、关系和时空推理任务中的能力，如细粒度人类活动识别和医学图像分析。作者通过跨领域实验表明，用轻量级的关系推理模块增强模型能有效提升语义表现、泛化性、解释性以及硬件效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型虽然迁移能力强，但在需要精细关系推理和时空、语义依赖的复杂任务（如第一人称视频理解、医疗影像分析）上表现有限。因此，探索显式引入关系推理机制来提升模型能力具有重大意义。

Method: 方法是在视觉基础模型的基础上，引入动态关系图推理模块，这些图根据输入和任务上下文动态生成拓扑结构和边语义，实现稀疏且自适应的关系推理，从而增强对实体、角色和关系的建模。

Result: 论文通过在人类操控动作识别和脑肿瘤分割等任务上的实验，证实了增强后的混合模型在细粒度语义表达、域外泛化、可解释性和计算资源消耗方面均优于单纯的基础视觉模型。

Conclusion: 论文呼吁未来研究应聚焦于动态图关系学习、多层次关系推理、跨模态融合及直接衡量模型关系推理能力的新评测协议，以推动结构化视觉任务中基础模型与关系图的深度结合和突破。

Abstract: Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.

</details>


### [10] [LPLC: A Dataset for License Plate Legibility Classification](https://arxiv.org/abs/2508.18425)
*Lucas Wojcik,Gabriel E. Lima,Valfride Nascimento,Eduil Nascimento Jr.,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: 本文提出了一个用于车牌可读性分类的新数据集（LPLC），并使用三种主流图像识别模型进行基线测试。结果表明，通过机器学习判别哪些车牌需要图像增强处理是一个困难的问题，现有方法的表现还有很大的提升空间。


<details>
  <summary>Details</summary>
Motivation: 在自动车牌识别中，低质量和难以辨认的车牌极大影响识别准确率。现有基于超分辨率等重建方法虽然可以提升图像清晰度，但仍难直接判断实际可识别性。缺乏标准数据集进一步限制了该方向的发展。

Method: 作者构建了一个包含10,210张车辆图像、12,687个带细致可读性注释的车牌（LPLC）数据集，考虑了多种车辆类型、光照条件和摄像头质量。注释包括遮挡情况、四分类可读性等级，并为部分车牌标注具体字符。以三种主流网络（ViT、ResNet、YOLO）对车牌可读性进行分类，并对超分辨率及识别方法表现进行对比分析。

Result: 所有三种网络在车牌可读性三分类任务中的整体F1分数均未超过80%，表明问题本身具有较高难度。基于数据集的基线实验和后续分析同样显示，当前方法难以令人满意地解决低质量车牌的判别和识别。

Conclusion: 数据集和基线测试构建证明了判别车牌可阅读性的挑战性，并为后续研究提供了公开资源和标准。后续深入研究和新方法的提出是提升自动车牌识别实际落地能力的关键。

Abstract: Automatic License Plate Recognition (ALPR) faces a major challenge when
dealing with illegible license plates (LPs). While reconstruction methods such
as super-resolution (SR) have emerged, the core issue of recognizing these
low-quality LPs remains unresolved. To optimize model performance and
computational efficiency, image pre-processing should be applied selectively to
cases that require enhanced legibility. To support research in this area, we
introduce a novel dataset comprising 10,210 images of vehicles with 12,687
annotated LPs for legibility classification (the LPLC dataset). The images span
a wide range of vehicle types, lighting conditions, and camera/image quality
levels. We adopt a fine-grained annotation strategy that includes vehicle- and
LP-level occlusions, four legibility categories (perfect, good, poor, and
illegible), and character labels for three categories (excluding illegible
LPs). As a benchmark, we propose a classification task using three image
recognition networks to determine whether an LP image is good enough, requires
super-resolution, or is completely unrecoverable. The overall F1 score, which
remained below 80% for all three baseline models (ViT, ResNet, and YOLO),
together with the analyses of SR and LP recognition methods, highlights the
difficulty of the task and reinforces the need for further research. The
proposed dataset is publicly available at
https://github.com/lmlwojcik/lplc-dataset.

</details>


### [11] [CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering](https://arxiv.org/abs/2508.18430)
*Aranya Saha,Tanvir Ahmed Khan,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.CV

TL;DR: 本文提出了一种专为皮肤病视觉问答任务设计的专家-通才框架CLARIFY，通过结合轻量级专家模型与压缩型通才模型，有效提升诊断准确率并显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在医学任务中表现出潜力，但受限于通用性不足以高精度诊断且模型庞大导致推理成本高，因此临床实际应用受限。

Method: 设计了CLARIFY框架，包含：1）轻量级、专门训练的图像分类器作为专家，负责快速准确地做出诊断预测；2）压缩型但功能强大的对话式VLM作为通才，负责自然语言回答用户问题。专家的预测直接引导通才的推理路径，并通过知识图谱检索模块，确保回答基于准确的医学知识。

Result: 在策划的多模态皮肤病数据集上，CLARIFY诊断准确率比最强基线（即精调且未压缩单路VLM）提升了18%，平均显存需求和延迟分别至少降低20%和5%。

Conclusion: 专家-通才协同系统的层次化设计，既提高了诊断准确度，又大幅提升了效率，为轻量级、可信赖且适于临床的医学AI系统提供了有效范式。

Abstract: Vision-language models (VLMs) have shown significant potential for medical
tasks; however, their general-purpose nature can limit specialized diagnostic
accuracy, and their large size poses substantial inference costs for real-world
clinical deployment. To address these challenges, we introduce CLARIFY, a
Specialist-Generalist framework for dermatological visual question answering
(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image
classifier (the Specialist) that provides fast and highly accurate diagnostic
predictions, and (ii) a powerful yet compressed conversational VLM (the
Generalist) that generates natural language explanations to user queries. In
our framework, the Specialist's predictions directly guide the Generalist's
reasoning, focusing it on the correct diagnostic path. This synergy is further
enhanced by a knowledge graph-based retrieval module, which grounds the
Generalist's responses in factual dermatological knowledge, ensuring both
accuracy and reliability. This hierarchical design not only reduces diagnostic
errors but also significantly improves computational efficiency. Experiments on
our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an
18\% improvement in diagnostic accuracy over the strongest baseline, a
fine-tuned, uncompressed single-line VLM, while reducing the average VRAM
requirement and latency by at least 20\% and 5\%, respectively. These results
indicate that a Specialist-Generalist system provides a practical and powerful
paradigm for building lightweight, trustworthy, and clinically viable AI
systems.

</details>


### [12] [VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results](https://arxiv.org/abs/2508.18445)
*Sizhuo Ma,Wei-Ting Chen,Qiang Gao,Jian Wang,Chris Wei Zhou,Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai,Baoying Chen,Xiongwei Xiao,Jishen Zeng,Wei Wu,Tiexuan Lou,Yuchen Tan,Chunyi Song,Zhiwei Xu,MohammadAli Hamidi,Hadi Amirpour,Mingyin Bai,Jiawang Du,Zhenyu Jiang,Zilong Lu,Ziguan Cui,Zongliang Gan,Xinpeng Li,Shiqi Jiang,Chenhui Li,Changbo Wang,Weijun Yuan,Zhan Li,Yihang Chen,Yifan Deng,Ruting Deng,Zhanglu Chen,Boyang Yao,Shuling Zheng,Feng Zhang,Zhiheng Fu,Abhishek Joshi,Aman Agarwal,Rakhil Immidisetti,Ajay Narasimha Mopidevi,Vishwajeet Shukla,Hao Yang,Ruikun Zhang,Liyuan Pan,Kaixin Deng,Hang Ouyang,Fan yang,Zhizun Luo,Zhuohang Shi,Songning Lai,Weilin Ruan,Yutao Yue*

Main category: cs.CV

TL;DR: 本文介绍了VQualA 2025面部图像质量评估挑战赛，旨在推动能高效评估各种退化情况下人脸图像质量的轻量级模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现实场景下，人脸图像常常存在噪声、模糊和压缩伪影等质量退化问题，影响图像处理任务效果。因此需要能够自动、准确评估人脸图像质量的方法。

Method: 组织了一项国际竞赛，要求参赛者在计算资源受限（小于0.5 GFLOPs和500万参数）的前提下，开发针对任意分辨率和真实退化情况的人脸图像质量评价（FIQA）模型。采用了基于相关性度量指标的评估方法，对比真实主观评分（MOS）。

Result: 挑战赛共吸引了127支队伍，收到了1519份最终模型提交，对各模型在野外人脸图像数据集上的表现进行了全面评测与总结。

Conclusion: 通过本次挑战，汇集并评测了多种轻量级高效FIQA方法，为实际人脸图像质量评估模型的研发和应用提供了研究基础和方向。

Abstract: Face images play a crucial role in numerous applications; however, real-world
conditions frequently introduce degradations such as noise, blur, and
compression artifacts, affecting overall image quality and hindering subsequent
tasks. To address this challenge, we organized the VQualA 2025 Challenge on
Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.
Participants created lightweight and efficient models (limited to 0.5 GFLOPs
and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on
face images with arbitrary resolutions and realistic degradations. Submissions
underwent comprehensive evaluations through correlation metrics on a dataset of
in-the-wild face images. This challenge attracted 127 participants, with 1519
final submissions. This report summarizes the methodologies and findings for
advancing the development of practical FIQA approaches.

</details>


### [13] [Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling](https://arxiv.org/abs/2508.18463)
*Md. Rashid Shahriar Khan,Md. Abrar Hasan,Mohammod Tareq Aziz Justice*

Main category: cs.CV

TL;DR: 提出一种结合时空建模与语义理解的零样本异常检测新框架，能在无异常样本训练下检测监控视频中未见过的异常事件。


<details>
  <summary>Details</summary>
Motivation: 异常检测因异常事件的不确定性和依赖场景特性而具挑战性。传统方法需异常数据训练，难以应对新型或未知异常。目标是开发无需异常样本即可检测异常的新框架。

Method: 提出结合TimeSformer（抽取时空特征）、DPC（未来预测，捕捉时间异常）、CLIP（语义推理，基于文本提示检测概念异常）的混合架构。用InfoNCE和CPC联合损失联合训练，视觉输入与时序和语义表征对齐，并通过上下文门控机制融合场景特征提升决策。

Result: 该方法能泛化检测复杂环境中未见过的异常行为，弥补了时序推理与语义理解间的空白。

Conclusion: 将预测模型与视觉-语言理解结合，为监控领域零样本异常检测提供了上下文感知解决方案，具备较好泛化能力。代码已开源。

Abstract: Detecting anomalies in surveillance footage is inherently challenging due to
their unpredictable and context-dependent nature. This work introduces a novel
context-aware zero-shot anomaly detection framework that identifies abnormal
events without exposure to anomaly examples during training. The proposed
hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal
dynamics and semantic context. TimeSformer serves as the vision backbone to
extract rich spatial-temporal features, while DPC forecasts future
representations to identify temporal deviations. Furthermore, a CLIP-based
semantic stream enables concept-level anomaly detection through
context-specific text prompts. These components are jointly trained using
InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic
representations. A context-gating mechanism further enhances decision-making by
modulating predictions with scene-aware cues or global video features. By
integrating predictive modeling with vision-language understanding, the system
can generalize to previously unseen behaviors in complex environments. This
framework bridges the gap between temporal reasoning and semantic context in
zero-shot anomaly detection for surveillance. The code for this research has
been made available at
https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.

</details>


### [14] [DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance](https://arxiv.org/abs/2508.18506)
*Ajinkya Khoche,Qingwen Zhang,Yixi Cai,Sina Sharif Mansouri,Patric Jensfelt*

Main category: cs.CV

TL;DR: 本文提出了一种全新的自监督LiDAR场景流估计方法DoGFlow，无需人工标注，通过雷达多普勒数据生成伪标签，并在LiDAR域中完成运动估算。


<details>
  <summary>Details</summary>
Motivation: 当前三维场景流估计严重依赖大规模手动标注的数据集，这极大限制了自动驾驶系统的发展，而自监督方法效果不足，尤其在长距离及恶劣天气场景中表现较差。

Method: 提出DoGFlow框架，利用4D雷达多普勒数据直接计算运动伪标签，然后通过动态感知关联与消歧传播算法，将其跨模态、高效准确地迁移到LiDAR域，实现自监督场景流估计。

Result: 在MAN TruckScenes数据集上，DoGFlow大幅优于现有自监督方法，并且只需10%的标注数据即可让LiDAR骨干网络达到超过90%全监督方法的性能。

Conclusion: DoGFlow实现了高效、低成本且鲁棒的3D场景流估计，可极大提升感知模型的标注效率，是无人驾驶等领域的有力工具。

Abstract: Accurate 3D scene flow estimation is critical for autonomous systems to
navigate dynamic environments safely, but creating the necessary large-scale,
manually annotated datasets remains a significant bottleneck for developing
robust perception models. Current self-supervised methods struggle to match the
performance of fully supervised approaches, especially in challenging
long-range and adverse weather scenarios, while supervised methods are not
scalable due to their reliance on expensive human labeling. We introduce
DoGFlow, a novel self-supervised framework that recovers full 3D object motions
for LiDAR scene flow estimation without requiring any manual ground truth
annotations. This paper presents our cross-modal label transfer approach, where
DoGFlow computes motion pseudo-labels in real-time directly from 4D radar
Doppler measurements and transfers them to the LiDAR domain using dynamic-aware
association and ambiguity-resolved propagation. On the challenging MAN
TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised
methods and improves label efficiency by enabling LiDAR backbones to achieve
over 90% of fully supervised performance with only 10% of the ground truth
data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/

</details>


### [15] [SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors](https://arxiv.org/abs/2508.18531)
*Zhangyu Jin,Andrew Feng*

Main category: cs.CV

TL;DR: 该论文提出了SatSkylines，一种结合卫星图像和粗略几何先验进行3D建筑生成的新方法，能够在没有高精度几何输入的情况下，实现高质量建筑结构建模，并发布了Skylines-50K大规模3D建筑数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D生成方法难以从卫星图像的俯视视角准确恢复建筑结构，而依赖体素等高精度输入的3D细化方法则无法很好适应简单几何形状（如立方体等）的情况，因此需要一种能结合简单几何先验与卫星影像的通用高效3D建筑生成方法。

Method: 该方法提出建模粗糙且带噪声的几何先验到详细3D建筑几何的转化过程，在不增加计算成本的前提下，实现了对几何结构的灵活控制；同时，作者建立并发布了Skylines-50K大规模建筑资产数据集，支持详细3D模型的生成和训练。

Result: 实验表明，该方法在多个评测标准下都取得了优异效果，并展现出强大的泛化能力，能够对不同形状与风格的建筑实现有效3D重建。

Conclusion: SatSkylines无需高精细几何输入，能够有效利用卫星影像和简单先验生成准确、风格多样化的3D建筑模型，并保持高泛化能力和计算效率，对3D城市建模等实际应用具有重要意义。

Abstract: We present SatSkylines, a 3D building generation approach that takes
satellite imagery and coarse geometric priors. Without proper geometric
guidance, existing image-based 3D generation methods struggle to recover
accurate building structures from the top-down views of satellite images alone.
On the other hand, 3D detailization methods tend to rely heavily on highly
detailed voxel inputs and fail to produce satisfying results from simple priors
such as cuboids. To address these issues, our key idea is to model the
transformation from interpolated noisy coarse priors to detailed geometries,
enabling flexible geometric control without additional computational cost. We
have further developed Skylines-50K, a large-scale dataset of over 50,000
unique and stylized 3D building assets in order to support the generations of
detailed building models. Extensive evaluations indicate the effectiveness of
our model and strong generalization ability.

</details>


### [16] [Adaptive Visual Navigation Assistant in 3D RPGs](https://arxiv.org/abs/2508.18539)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.CV

TL;DR: 本文提出在3D游戏环境下自动识别地图关键连接点（STP和MSTP）的新任务，基于深度学习方法实现检测与排序，并构建了数据集及基线，为日后AI辅助导航和关卡设计提供基础。


<details>
  <summary>Details</summary>
Motivation: 在复杂的3D游戏地图里，玩家需凭视觉线索找到区域间的关键通道点（如门、出口）。但自动化识别这些点较为困难，影响自动导航和地图设计评估。因此，系统化、客观地检测这些关键连接点，对于提升玩家体验和设计工具智能化极为重要。

Method: 提出从单个游戏画面自动识别和筛选关键空间转换点（STPs和主要STP-MSTP）的任务。方法分两阶段：首先用Faster R-CNN检测可能的STPs；再用融合局部与全局特征的轻量级排序器选出唯一主要STP。两阶段均采用高效参数适配器，并提出了检索增强融合模块。全过程在五款动作RPG自建多样化数据集上验证。

Result: 实验表明，大数据量下全网络微调的STP检测效果最佳，但当数据较少时，参数适配器迁移（adapter-only transfer）对STP检测与MSTP筛选更稳定有效。建立了基准性能指标。

Conclusion: 本文首次系统定义了空间过渡点自动识别任务，提出完整基线系统与数据集，为未来AI导航辅助及数据驱动关卡设计工具奠定基础，并指出模型高效适配方向。

Abstract: In complex 3D game environments, players rely on visual affordances to spot
map transition points. Efficient identification of such points is important to
client-side auto-mapping, and provides an objective basis for evaluating map
cue presentation. In this work, we formalize the task of detecting traversable
Spatial Transition Points (STPs)-connectors between two sub regions-and
selecting the singular Main STP (MSTP), the unique STP that lies on the
designer-intended critical path toward the player's current macro-objective,
from a single game frame, proposing this as a new research focus. We introduce
a two-stage deep-learning pipeline that first detects potential STPs using
Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses
local and global visual features. Both stages benefit from parameter-efficient
adapters, and we further introduce an optional retrieval-augmented fusion step.
Our primary goal is to establish the feasibility of this problem and set
baseline performance metrics. We validate our approach on a custom-built,
diverse dataset collected from five Action RPG titles. Our experiments reveal a
key trade-off: while full-network fine-tuning produces superior STP detection
with sufficient data, adapter-only transfer is significantly more robust and
effective in low-data scenarios and for the MSTP selection task. By defining
this novel problem, providing a baseline pipeline and dataset, and offering
initial insights into efficient model adaptation, we aim to contribute to
future AI-driven navigation aids and data-informed level-design tools.

</details>


### [17] [Wan-S2V: Audio-Driven Cinematic Video Generation](https://arxiv.org/abs/2508.18621)
*Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: 提出了一种名为Wan-S2V的音频驱动角色动画模型，大幅提升了影视级别场景中的动画表现，相较现有先进方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动角色动画方法在复杂影视制作（如细致互动、真实肢体动作与动态镜头）中的表达力不足，难以满足电影级动画需求。

Method: 提出了在Wan基础上改进的Wan-S2V音频驱动模型，针对影视级动画需求优化，并与主流方法（如Hunyuan-Avatar和Omnihuman）进行了广泛对比测试。同时探索其在长视频生成和精准口型同步编辑上的应用。

Result: 实验表明，Wan-S2V在电影级动画表现、表达力和真实性方面，显著优于现有SOTA模型，并在多场景下表现出更强的通用性和适应性。

Conclusion: Wan-S2V在影视级音频驱动动画方面实现了新突破，可用于高质量动画创作、长视频生成与精细口型编辑，推动了该领域技术进步。

Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation
demonstrate promising performance for scenarios primarily involving speech and
singing. However, they often fall short in more complex film and television
productions, which demand sophisticated elements such as nuanced character
interactions, realistic body movements, and dynamic camera work. To address
this long-standing challenge of achieving film-level character animation, we
propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.
Our model achieves significantly enhanced expressiveness and fidelity in
cinematic contexts compared to existing approaches. We conducted extensive
experiments, benchmarking our method against cutting-edge models such as
Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate
that our approach significantly outperforms these existing solutions.
Additionally, we explore the versatility of our method through its applications
in long-form video generation and precise video lip-sync editing.

</details>


### [18] [Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction](https://arxiv.org/abs/2508.18632)
*Huayi Wang,Haochao Ying,Yuyang Xu,Qibo Qiu,Cheng Zhang,Danny Z. Chen,Ying Sun,Jian Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的癌症生存分析多模态融合框架DeReF，通过随机特征重组与动态门控专家混合融合，有效提升了特征泛化性和信息交互，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法（如拼接、注意力和MoE等）存在两大问题：一是固定融合方案导致过度依赖预定义特征组合，二是MoE方法下专家网络之间特征交互不足，影响生存分析效果。

Method: 提出DeReF（Decoupling-Reorganization-Fusion）框架，将特征解耦、随机重组和动态MoE融合结合起来，并在特征解耦阶段引入区域交叉注意力网络，以增强特征表达。随机特征重组策略增加特征组合多样性和粒度，改善MoE中信息闭合问题。

Result: 在内部肝癌（LC）数据集和三大基准TCGA公开数据集上进行了大量实验，结果验证了该方法在癌症生存分析中的有效性，优于经典融合方法。

Conclusion: DeReF框架能更好地融合和挖掘多模态特征，提高癌症生存预测性能，对未来多模态医学信息分析具有推广价值。代码将公开。

Abstract: Cancer survival analysis commonly integrates information across diverse
medical modalities to make survival-time predictions. Existing methods
primarily focus on extracting different decoupled features of modalities and
performing fusion operations such as concatenation, attention, and MoE-based
(Mixture-of-Experts) fusion. However, these methods still face two key
challenges: i) Fixed fusion schemes (concatenation and attention) can lead to
model over-reliance on predefined feature combinations, limiting the dynamic
fusion of decoupled features; ii) in MoE-based fusion methods, each expert
network handles separate decoupled features, which limits information
interaction among the decoupled features. To address these challenges, we
propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which
devises a random feature reorganization strategy between modalities decoupling
and dynamic MoE fusion modules.Its advantages are: i) it increases the
diversity of feature combinations and granularity, enhancing the generalization
ability of the subsequent expert networks; ii) it overcomes the problem of
information closure and helps expert networks better capture information among
decoupled features. Additionally, we incorporate a regional cross-attention
network within the modality decoupling module to improve the representation
quality of decoupled features. Extensive experimental results on our in-house
Liver Cancer (LC) and three widely used TCGA public datasets confirm the
effectiveness of our proposed method. The code will be made publicly available.

</details>


### [19] [ROSE: Remove Objects with Side Effects in Videos](https://arxiv.org/abs/2508.18633)
*Chenxuan Miao,Yutong Feng,Jianshu Zeng,Zixiang Gao,Hantang Liu,Yunfeng Yan,Donglian Qi,Xi Chen,Bin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: ROSE提出了一种系统性消除视频中物体及其带来的阴影、反射等“副作用”的框架，通过3D渲染合成配对数据，显著提升了视频物体去除及副效应消除的效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频物体去除方法在消除物体本身外，对其阴影、反射等环境副作用处理效果较弱，根本原因在于缺乏带有这些副效应的配对监督数据。为了解决这一难题，论文提出系统性分析并解决物体带来的多类别环境副效应。

Method: 利用3D渲染引擎合成含阴影、反射、光照、半透明和镜面等多种副作用的大规模配对视频数据；构建自动化的数据处理管线；在Diffusion Transformer上实现端到端的视频修复（inpainting）模型，将整段视频输入模型以定位物体及相关副作用区域，并引入额外监督信号揭示副效应掩模；提出新的评价基准ROSE-Bench涵盖常见及五类特殊副效应场景。

Result: ROSE不仅在合成数据集上优于现有方法，并且在真实世界视频场景中也表现出良好的泛化能力和更优的物体及副效应去除效果。

Conclusion: ROSE在视频中物体和副效应去除任务上实现了领先效果，通过系统性数据合成和建模显著提升了相关任务的准确性与通用性，为后续研究提供了数据和评测基准支持。

Abstract: Video object removal has achieved advanced performance due to the recent
success of video generative models. However, when addressing the side effects
of objects, e.g., their shadows and reflections, existing works struggle to
eliminate these effects for the scarcity of paired video data as supervision.
This paper presents ROSE, termed Remove Objects with Side Effects, a framework
that systematically studies the object's effects on environment, which can be
categorized into five common cases: shadows, reflections, light, translucency
and mirror. Given the challenges of curating paired videos exhibiting the
aforementioned effects, we leverage a 3D rendering engine for synthetic data
generation. We carefully construct a fully-automatic pipeline for data
preparation, which simulates a large-scale paired dataset with diverse scenes,
objects, shooting angles, and camera trajectories. ROSE is implemented as an
video inpainting model built on diffusion transformer. To localize all
object-correlated areas, the entire video is fed into the model for
reference-based erasing. Moreover, additional supervision is introduced to
explicitly predict the areas affected by side effects, which can be revealed
through the differential mask between the paired videos. To fully investigate
the model performance on various side effect removal, we presents a new
benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five
special side effects for comprehensive evaluation. Experimental results
demonstrate that ROSE achieves superior performance compared to existing video
object erasing models and generalizes well to real-world video scenarios. The
project page is https://rose2025-inpaint.github.io/.

</details>


### [20] [OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward](https://arxiv.org/abs/2508.18634)
*Chunlin Zhong,Qiuxia Hou,Zhangjun Zhou,Shuang Hao,Haonan Lu,Yanhao Zhang,He Tang,Xiang Bai*

Main category: cs.CV

TL;DR: 该论文为解决视频描述中运动与细节不均衡问题，提出了新数据集HMD-270K和新的优化方法CSER，并开发了OwlCap模型，大幅提升了视频理解与生成的一致性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述方法在提取运动与细节特征时常常出现偏向，导致生成的描述不完整，影响下游视频理解及生成的准确性。提升运动与细节的平衡性，有助于促进视频理解与生成技术的发展。

Method: 作者提出从数据和优化两方面入手：1）构建包含运动与细节融合的HMD-270K数据集，通过两阶段流程：Motion-Detail Fusion（运动-细节融合）和Fine-Grained Examination（细粒度检查）；2）优化方面，提出基于Group Relative Policy Optimization（GRPO）的Caption Set Equivalence Reward（CSER），支持单元到集合的匹配及双向验证。最后通过监督微调及GRPO+CSER后训练，开发了OwlCap多模态大模型。

Result: OwlCap模型在VDC（注重细节）和DREAM-1K（注重运动）两个基准上分别提升了+4.2 Acc和+4.6 F1，效果优于现有基线模型。

Conclusion: 该方法有效缓解了视频描述任务中的运动-细节失衡问题，显著提升了视频内容表征的完整性和一致性。HMD-270K数据集与OwlCap模型的开源有望进一步推动领域发展。

Abstract: Video captioning aims to generate comprehensive and coherent descriptions of
the video content, contributing to the advancement of both video understanding
and generation. However, existing methods often suffer from motion-detail
imbalance, as models tend to overemphasize one aspect while neglecting the
other. This imbalance results in incomplete captions, which in turn leads to a
lack of consistency in video understanding and generation. To address this
issue, we propose solutions from two aspects: 1) Data aspect: We constructed
the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage
pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)
Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)
based on Group Relative Policy Optimization (GRPO). CSER enhances completeness
and accuracy in capturing both motion and details through unit-to-set matching
and bidirectional validation. Based on the HMD-270K supervised fine-tuning and
GRPO post-training with CSER, we developed OwlCap, a powerful video captioning
multi-modal large language model (MLLM) with motion-detail balance.
Experimental results demonstrate that OwlCap achieves significant improvements
compared to baseline models on two benchmarks: the detail-focused VDC (+4.2
Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap
model will be publicly released to facilitate video captioning research
community advancements.

</details>


### [21] [Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection](https://arxiv.org/abs/2508.18641)
*Ye Tao,Xinran Fu,Honglin Pang,Xi Yang,Chuntao Li*

Main category: cs.CV

TL;DR: 本论文提出了一种基于聚类的特征空间表征学习方法，用于提升甲骨文自动检测的准确性。通过引入甲骨文字库数据，并在主流检测框架上进行验证，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 甲骨文对研究中华古代文明具有重要意义。然而，受图像噪声、裂纹等影响，现有自动检测网络效果有限，因此急需提高甲骨文检测的准确性和鲁棒性。

Method: 作者提出一种将甲骨文字库作为先验知识引入检测网络的特征表征学习方法，通过聚类获得判别性的特征空间。创新性地使用基于聚类结果的特殊损失函数优化特征表示，并将其整合进总损失函数中。在Faster R-CNN、DETR和Sparse R-CNN等主流框架上进行实验验证。

Result: 方法在两个甲骨文检测数据集和三种主流目标检测框架上均获得显著性能提升，证明了聚类特征表征方法的有效性。

Conclusion: 本文提出的基于聚类的特征空间表征学习方法能有效提升甲骨文字检测的准确性和鲁棒性，为数字考古领域的甲骨文自动识别提供了新思路。

Abstract: Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient
Chinese civilization. The automated detection of OBIs from rubbing images
represents a fundamental yet challenging task in digital archaeology, primarily
due to various degradation factors including noise and cracks that limit the
effectiveness of conventional detection networks. To address these challenges,
we propose a novel clustering-based feature space representation learning
method. Our approach uniquely leverages the Oracle Bones Character (OBC) font
library dataset as prior knowledge to enhance feature extraction in the
detection network through clustering-based representation learning. The method
incorporates a specialized loss function derived from clustering results to
optimize feature representation, which is then integrated into the total
network loss. We validate the effectiveness of our method by conducting
experiments on two OBIs detection dataset using three mainstream detection
frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive
experimentation, all frameworks demonstrate significant performance
improvements.

</details>


### [22] [SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain](https://arxiv.org/abs/2508.18664)
*Xin Tian,Yingtie Lei,Xiujun Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合傅立叶变换与信噪比(SNR)先验的Transformer网络（SFormer），用于水下图像增强，显著提升了图像色彩、纹理和对比度重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法在深度网络中引入物理先验（如SNR），但空间域SNR存在两个缺陷：无法有效区分通道间的干扰，且难以同时放大信息结构并抑制噪声。

Method: 作者提出在频率域中利用SNR先验，将特征分解为幅度谱和相位谱，有助于更好地调节各通道。提出Fourier Attention SNR-prior Transformer（FAST）模块，结合频谱交互与SNR提示增强重要频谱。此外，通过Frequency Adaptive Transformer（FAT）瓶颈，用门控注意力机制融合低高频分支，在U型架构中与传统RGB流、SNR引导分支结合，统一为SFormer模型。

Result: 在UIEB、EUVP、LSUI三个数据集的4800对配对图像上训练，SFormer在PSNR指标上比最新方法提升3.1 dB，在SSIM上提升0.08，有效恢复了水下场景中的色彩、纹理和对比度。

Conclusion: SFormer通过结合频域SNR先验和谱间交互注意力机制，解决了空间SNR的局限性，实现了更优秀的水下图像增强性能，验证了相关创新结构的有效性。

Abstract: Recent learning-based underwater image enhancement (UIE) methods have
advanced by incorporating physical priors into deep neural networks,
particularly using the signal-to-noise ratio (SNR) prior to reduce
wavelength-dependent attenuation. However, spatial domain SNR priors have two
limitations: (i) they cannot effectively separate cross-channel interference,
and (ii) they provide limited help in amplifying informative structures while
suppressing noise. To overcome these, we propose using the SNR prior in the
frequency domain, decomposing features into amplitude and phase spectra for
better channel modulation. We introduce the Fourier Attention SNR-prior
Transformer (FAST), combining spectral interactions with SNR cues to highlight
key spectral components. Additionally, the Frequency Adaptive Transformer (FAT)
bottleneck merges low- and high-frequency branches using a gated attention
mechanism to enhance perceptual quality. Embedded in a unified U-shaped
architecture, these modules integrate a conventional RGB stream with an
SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB,
EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and
0.08 in SSIM, successfully restoring colors, textures, and contrast in
underwater scenes.

</details>


### [23] [Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos](https://arxiv.org/abs/2508.18681)
*Dongfang Wang,Jian Yang,Yizhe Zhang,Tao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种分层时空分割网络用于超声心动图视频中左心室心内膜的自动分割，旨在提升EF（射血分数）估算的准确性。新方法融合了单帧细节与多帧全局动态信息，通过创新的时空交叉扫描模块，提升了对时空关系的建模能力，减少了由图像噪声引起的EF估算偏差。


<details>
  <summary>Details</summary>
Motivation: 现有心脏超声分割方法尽管分割性能较好，但在EF估算上的表现有限，主要原因在于单纯依赖单帧或多帧数据分别容易导致局部误差积累或细节损失。因此，需要一种兼顾局部细节和全局动态的新方法，来提高临床上关键的EF估算精度。

Method: 提出了一种分层时空分割网络：底层采用卷积网络处理单帧，保留细节；高层结合Mamba架构，捕获视频中的时空关系。同时，创新提出时空交叉扫描（STCS）模块，通过跨帧和位置的跳跃扫描，整合长距离上下文信息，增强模型对时空特征的捕获。

Result: 所提方法能够更好融合局部细节与全局动态，对EF估算的表现显著优于现有方法，并有效缓解了由超声图像噪声等问题带来的估算偏差。

Conclusion: 分层时空分割结合STCS模块能够显著提升超声心动图视频中EF的估算准确性，为心脏功能评估提供了更为可靠的自动工具。

Abstract: Automated segmentation of the left ventricular endocardium in
echocardiography videos is a key research area in cardiology. It aims to
provide accurate assessment of cardiac structure and function through Ejection
Fraction (EF) estimation. Although existing studies have achieved good
segmentation performance, their results do not perform well in EF estimation.
In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network
(\ourmodel) for echocardiography video, aiming to improve EF estimation
accuracy by synergizing local detail modeling with global dynamic perception.
The network employs a hierarchical design, with low-level stages using
convolutional networks to process single-frame images and preserve details,
while high-level stages utilize the Mamba architecture to capture
spatio-temporal relationships. The hierarchical design balances single-frame
and multi-frame processing, avoiding issues such as local error accumulation
when relying solely on single frames or neglecting details when using only
multi-frame data. To overcome local spatio-temporal limitations, we propose the
Spatio-temporal Cross Scan (STCS) module, which integrates long-range context
through skip scanning across frames and positions. This approach helps mitigate
EF calculation biases caused by ultrasound image noise and other factors.

</details>


### [24] [Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency](https://arxiv.org/abs/2508.18693)
*Zhitong Cheng,Yiran Jiang,Yulong Ge,Yufeng Li,Zhongheng Qin,Rongzhi Lin,Jianwei Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FPS（Feature-space Planes Searcher）的新型领域自适应方法，主张只优化决策边界而冻结特征提取器，以解决迁移任务中的域迁移问题，取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 域迁移问题致使模型从有标签源域过渡到无标签目标域时性能下降，现有方法主要依赖微调特征提取器，但存在效率低、可解释性差、难以扩展等不足。论文通过分析发现，预训练模型提取的特征空间具有强鲁棒性和迁移性，域迁移主要表现为决策边界不准，而非特征退化。

Method: 提出FPS框架，冻结预训练模型的特征编码器，仅通过几何特征对决策边界进行优化。该方法依托特征空间中的几何结构实现离线特征提取，优化过程高效、易解释，可在一次计算周期内对全数据集进行优化。

Result: 在多个公共数据集和领域（包括蛋白结构预测、遥感分类、地震检测等）上，FPS均取得了与SOTA方法相当或更优的性能；同时展示了其在多模态大模型中的高扩展性与通用性。

Conclusion: FPS为迁移学习领域适应任务提供了一种高效、简单且具有通用性的范式，兼具可解释性与计算效率，有望广泛应用于不同领域。

Abstract: Domain shift, characterized by degraded model performance during transition
from labeled source domains to unlabeled target domains, poses a persistent
challenge for deploying deep learning systems. Current unsupervised domain
adaptation (UDA) methods predominantly rely on fine-tuning feature extractors -
an approach limited by inefficiency, reduced interpretability, and poor
scalability to modern architectures.
  Our analysis reveals that models pretrained on large-scale data exhibit
domain-invariant geometric patterns in their feature space, characterized by
intra-class clustering and inter-class separation, thereby preserving
transferable discriminative structures. These findings indicate that domain
shifts primarily manifest as boundary misalignment rather than feature
degradation.
  Unlike fine-tuning entire pre-trained models - which risks introducing
unpredictable feature distortions - we propose the Feature-space Planes
Searcher (FPS): a novel domain adaptation framework that optimizes decision
boundaries by leveraging these geometric patterns while keeping the feature
encoder frozen. This streamlined approach enables interpretative analysis of
adaptation while substantially reducing memory and computational costs through
offline feature extraction, permitting full-dataset optimization in a single
computation cycle.
  Evaluations on public benchmarks demonstrate that FPS achieves competitive or
superior performance to state-of-the-art methods. FPS scales efficiently with
multimodal large models and shows versatility across diverse domains including
protein structure prediction, remote sensing classification, and earthquake
detection. We anticipate FPS will provide a simple, effective, and
generalizable paradigm for transfer learning, particularly in domain adaptation
tasks. .

</details>


### [25] [Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection](https://arxiv.org/abs/2508.18729)
*Melanie Wille,Tobias Fischer,Scarlett Raine*

Main category: cs.CV

TL;DR: 本文探讨了水下目标检测中不同物种检测性能差异的原因，并针对表现不佳类别提出改进策略。作者将检测任务拆分为定位与分类，深入分析了扇贝类别的劣势，指出主要瓶颈在于前景-背景区分阶段及特征本身。


<details>
  <summary>Details</summary>
Motivation: 水下生态监测需要精准目标检测，但常见问题如图像质量下降、类别分布不均和独特视觉特征导致检测效果参差不齐。尤其部分水生物种（如扇贝）检测明显落后，导致整体系统欠佳，因此需要深入剖析性能差异背后的根本原因并探索提升路径。

Method: 作者对DUO数据集操作，将目标检测任务分别进行定位（采用YOLOv11和TIDE工具分析）与分类实验，专注分析扇贝类的表现。通过调整数据分布权衡精度和召回率，并观察均衡和不均衡数据下的分类结果。

Result: 定位结果显示，不论数据量多少，前景-背景分辨始终是主要挑战。分类实验中，即使数据均衡，各类间依然存在精度差距，表明问题不仅仅源自样本不均或类别间依赖，更受特征本身影响。

Conclusion: 若强调检测精度，建议使用不均衡数据分布；若重视召回率，则推荐均衡分布。提升表现不佳类别（如扇贝）需关注定位算法与模块优化，而非仅靠增加数据样本。作者已公开方法代码和数据集。

Abstract: Underwater object detection is critical for monitoring marine ecosystems but
poses unique challenges, including degraded image quality, imbalanced class
distribution, and distinct visual characteristics. Not every species is
detected equally well, yet underlying causes remain unclear. We address two key
research questions: 1) What factors beyond data quantity drive class-specific
performance disparities? 2) How can we systematically improve detection of
under-performing marine species? We manipulate the DUO dataset to separate the
object detection task into localization and classification and investigate the
under-performance of the scallop class. Localization analysis using YOLO11 and
TIDE finds that foreground-background discrimination is the most problematic
stage regardless of data quantity. Classification experiments reveal persistent
precision gaps even with balanced data, indicating intrinsic feature-based
challenges beyond data scarcity and inter-class dependencies. We recommend
imbalanced distributions when prioritizing precision, and balanced
distributions when prioritizing recall. Improving under-performing classes
should focus on algorithmic advances, especially within localization modules.
We publicly release our code and datasets.

</details>


### [26] [A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition](https://arxiv.org/abs/2508.18695)
*Wasi Ullah,Yasir Noman Khalid,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种优化的混合深度学习框架，用于解决人类活动识别（HAR）在实时应用中的计算成本高、特征冗余和可扩展性差等难题，通过集成定制的InceptionV3、LSTM和创新特征选择方法，实现了高效、鲁棒的活动识别。


<details>
  <summary>Details</summary>
Motivation: 现有的HAR系统在智能监控、医疗、辅助技术等领域应用广泛，但在实时环境下面临着高计算资源消耗、特征冗余、可扩展性差的问题，亟需一种既高效又能在边缘设备部署的解决方案。

Method: 提出了一种集成定制InceptionV3（用于提取多级上下文空间特征）、LSTM（建模帧间时序关系）和一种创新的集成遗传算法（结合自适应动态适应度共享与注意力机制的ADFSA），可动态平衡准确率、冗余、独特性和复杂度等目标，选出紧凑且优质的特征集，便于轻量级分类器处理。

Result: 在UCF-YouTube等具有遮挡、杂乱背景、运动动态、光照不佳等挑战性的视频数据集上，所提方法获得了99.65%的识别准确率，将所需特征降至7个以内，大幅提升了推理效率。

Conclusion: 该方法结构轻量、可扩展，适用于树莓派等边缘设备，支持智能、资源感知环境下的真实场景部署，如公共安全、辅助技术和自动监控等，推动了HAR系统的实际应用。

Abstract: Human Activity Recognition (HAR) plays a pivotal role in various
applications, including smart surveillance, healthcare, assistive technologies,
sports analytics, etc. However, HAR systems still face critical challenges,
including high computational costs, redundant features, and limited scalability
in real-time scenarios. An optimized hybrid deep learning framework is
introduced that integrates a customized InceptionV3, an LSTM architecture, and
a novel ensemble-based feature selection strategy. The proposed framework first
extracts spatial descriptors using the customized InceptionV3 model, which
captures multilevel contextual patterns, region homogeneity, and fine-grained
localization cues. The temporal dependencies across frames are then modeled
using LSTMs to effectively encode motion dynamics. Finally, an ensemble-based
genetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)
is employed to select a compact and optimized feature set by dynamically
balancing objectives such as accuracy, redundancy, uniqueness, and complexity
reduction. Consequently, the selected feature subsets, which are both diverse
and discriminative, enable various lightweight machine learning classifiers to
achieve accurate and robust HAR in heterogeneous environments. Experimental
results on the robust UCF-YouTube dataset, which presents challenges such as
occlusion, cluttered backgrounds, motion dynamics, and poor illumination,
demonstrate good performance. The proposed approach achieves 99.65% recognition
accuracy, reduces features to as few as 7, and enhances inference time. The
lightweight and scalable nature of the HAR system supports real-time deployment
on edge devices such as Raspberry Pi, enabling practical applications in
intelligent, resource-aware environments, including public safety, assistive
technology, and autonomous monitoring systems.

</details>


### [27] [PseudoMapTrainer: Learning Online Mapping without HD Maps](https://arxiv.org/abs/2508.18788)
*Christian Löwens,Thorben Funke,Jingchao Xie,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 提出了一种完全不依赖高精地图标注数据训练在线制图模型的方法，通过从无标注传感器数据自动生成伪标签，并结合高效算法提升在线制图的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有在线制图模型在训练时需依赖昂贵且分布受限的高精地图标签，制约了泛化和大规模应用。该工作旨在消除对人工标注地图的依赖，实现无监督或半监督在线制图。

Method: 提出PseudoMapTrainer方法：1）利用多视角相机图像通过高斯云点重建路面，并结合预训练2D分割网络的语义信息生成伪标签；2）创新性地设计掩膜感知分配算法和loss函数，使模型能适配部分伪标签缺失场景；3）将大量无标注众包数据用于模型训练和预训练，实现半监督学习。

Result: 该方法首次实现无真值高精地图标注参与下的在线制图模型训练，并证明伪标签在大规模数据的半监督预训练中有效提升制图模型性能和泛化能力。

Conclusion: PseudoMapTrainer为自动驾驶等场景提供了更低成本、高泛化性的在线制图模型训练新方案，有望推动大规模应用，相关代码已开源。

Abstract: Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.

</details>


### [28] [ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting](https://arxiv.org/abs/2508.18696)
*Qun Ji,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为ColorGS的新框架，通过改进的颜色编码和形变建模，实现了内窥镜手术场景中可变形组织的高保真三维重建，在视觉精度和实时性能上均达到业界领先。


<details>
  <summary>Details</summary>
Motivation: 现有内窥镜3D重建方法在捕捉复杂颜色变化和建模全局形变方面存在局限，尤其是3D Gaussian Splatting技术在表达复杂纹理和非线性形变时表现不足。手术场景对模型还要求兼顾高精度和实时效果。

Method: 提出采用Colored Gaussian Primitives，实现空间自适应的颜色编码，通过动态锚点和可学习色彩参数提升对复杂纹理和光照变化的表达能力；同时设计增强形变模型（EDM），结合时序高斯基函数与可学习的时间无关形变，精确捕捉局部和全局组织运动。

Result: 在DaVinci手术视频和多个基准数据集上实验，ColorGS的PSNR提升至39.85（比3DGS高1.5），SSIM达97.25%，并保持实时渲染速度，明显优于现有方法。

Conclusion: ColorGS实现了高保真且高效的手术场景三维重建，提升了组织细节捕捉与形变一致性，为术中导航和AR/VR等医疗应用提供了更可靠的技术支持。

Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos
remains challenging due to the limitations of existing methods in capturing
subtle color variations and modeling global deformations. While 3D Gaussian
Splatting (3DGS) enables efficient dynamic reconstruction, its fixed
per-Gaussian color assignment struggles with intricate textures, and linear
deformation modeling fails to model consistent global deformation. To address
these issues, we propose ColorGS, a novel framework that integrates spatially
adaptive color encoding and enhanced deformation modeling for surgical scene
reconstruction. First, we introduce Colored Gaussian Primitives, which employ
dynamic anchors with learnable color parameters to adaptively encode spatially
varying textures, significantly improving color expressiveness under complex
lighting and tissue similarity. Second, we design an Enhanced Deformation Model
(EDM) that combines time-aware Gaussian basis functions with learnable
time-independent deformations, enabling precise capture of both localized
tissue deformations and global motion consistency caused by surgical
interactions. Extensive experiments on DaVinci robotic surgery videos and
benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves
state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior
3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time
rendering efficiency. Our work advances surgical scene reconstruction by
balancing high fidelity with computational practicality, critical for
intraoperative guidance and AR/VR applications.

</details>


### [29] [Interpretable Decision-Making for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.18898)
*Mona Mirzaie,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: 提出了一种能够提升自动驾驶系统决策可解释性的深度学习方法，同时保证驾驶性能优异。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶虽然能直接从原始数据生成控制命令，但决策过程难以解释，尤其在城市复杂场景中更是如此，这阻碍了自动驾驶的广泛应用和信任。

Method: 提出了一种新型损失函数，通过生成稀疏且局部化的特征图来提高模型的可解释性，使得能直观说明哪些图像区域影响了决策，并在CARLA模拟环境中进行了特征提取步骤的消融实验。

Result: 方法不仅提高了决策可解释性，而且在CARLA基准测试中显著降低了违规率、提升了路线完成率，表现优于CARLA排行榜上的顶尖模型。

Conclusion: 所提出方法实现了可解释性与自动驾驶性能的兼得，有助于打造更安全、更可被信任的自动驾驶系统。

Abstract: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.
Although end-to-end approaches derive control commands directly from raw data,
interpreting these decisions remains challenging, especially in complex urban
scenarios. This is mainly attributed to very deep neural networks with
non-linear decision boundaries, making it challenging to grasp the logic behind
AI-driven decisions. This paper presents a method to enhance interpretability
while optimizing control commands in autonomous driving. To address this, we
propose loss functions that promote the interpretability of our model by
generating sparse and localized feature maps. The feature activations allow us
to explain which image regions contribute to the predicted control command. We
conduct comprehensive ablation studies on the feature extraction step and
validate our method on the CARLA benchmarks. We also demonstrate that our
approach improves interpretability, which correlates with reducing infractions,
yielding a safer, high-performance driving model. Notably, our monocular,
non-ensemble model surpasses the top-performing approaches from the CARLA
Leaderboard by achieving lower infraction scores and the highest route
completion rate, all while ensuring interpretability.

</details>


### [30] [Class-wise Flooding Regularization for Imbalanced Image Classification](https://arxiv.org/abs/2508.18723)
*Hiroaki Aizawa,Yuta Naito,Kohei Fukuda*

Main category: cs.CV

TL;DR: 本文提出了基于类别的Flooding正则化方法，通过为不同类别设置不同的Flooding阈值，提升不平衡数据集中少数类别的识别性能，从而改善整体泛化效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络在不平衡数据集训练时，往往导致模型倾向于多数类别，严重影响少数类别的识别表现。亟需正则化方法缓解过拟合并平衡各类别学习。

Method: 将Flooding正则化扩展为类别级别，对每个类别根据其样本频率分配特定Flooding阈值，通过限制训练损失最低点，抑制多数类别过拟合，同时保证少数类别的学习充足。

Result: 在不平衡图像分类任务中，与传统Flooding正则化方法相比，该方法提升了少数类别的分类性能，并增强了整体模型的泛化能力。

Conclusion: 基于类别的Flooding正则化方法能有效解决数据不平衡中少数类别性能下降问题，是改进不平衡分类的重要策略。

Abstract: The purpose of training neural networks is to achieve high generalization
performance on unseen inputs. However, when trained on imbalanced datasets, a
model's prediction tends to favor majority classes over minority classes,
leading to significant degradation in the recognition performance of minority
classes. To address this issue, we propose class-wise flooding regularization,
an extension of flooding regularization applied at the class level. Flooding is
a regularization technique that mitigates overfitting by preventing the
training loss from falling below a predefined threshold, known as the flooding
level, thereby discouraging memorization. Our proposed method assigns a
class-specific flooding level based on class frequencies. By doing so, it
suppresses overfitting in majority classes while allowing sufficient learning
for minority classes. We validate our approach on imbalanced image
classification. Compared to conventional flooding regularizations, our method
improves the classification performance of minority classes and achieves better
overall generalization.

</details>


### [31] [VibES: Induced Vibration for Persistent Event-Based Sensing](https://arxiv.org/abs/2508.19094)
*Vincenzo Polizzi,Stephen Yang,Quentin Clark,Jonathan Kelly,Igor Gilitschenski,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出了一种为事件相机持续产生事件的新方法，通过机械振动而非复杂硬件实现，并结合运动补偿提升数据质量，验证了其在真实数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 事件相机需要像素强度变化才能产生事件，在静止或低动态环境下无法产生有效数据，极大限制了其应用范围。现有方案多需复杂硬件或额外光学器件，因此亟需简单、低成本的解决方法。

Method: 作者提出利用简单旋转偏心质量产生周期性振动，激发事件相机生成事件，并通过运动补偿算法去除人工注入的运动影响，输出运动校正后的清洁事件流。

Result: 实际搭建了硬件原型，并在真实采集的数据集上进行了测试，结果表明该方法能精确恢复运动参数，改善事件相机在图像重建和边缘检测等任务中的性能。

Conclusion: 该方法为事件相机在静态或低动态场景下持续工作提供了简单有效的新途径，有助于扩展事件相机在计算机视觉领域的应用。

Abstract: Event cameras are a bio-inspired class of sensors that asynchronously measure
per-pixel intensity changes. Under fixed illumination conditions in static or
low-motion scenes, rigidly mounted event cameras are unable to generate any
events, becoming unsuitable for most computer vision tasks. To address this
limitation, recent work has investigated motion-induced event stimulation that
often requires complex hardware or additional optical components. In contrast,
we introduce a lightweight approach to sustain persistent event generation by
employing a simple rotating unbalanced mass to induce periodic vibrational
motion. This is combined with a motion-compensation pipeline that removes the
injected motion and yields clean, motion-corrected events for downstream
perception tasks. We demonstrate our approach with a hardware prototype and
evaluate it on real-world captured datasets. Our method reliably recovers
motion parameters and improves both image reconstruction and edge detection
over event-based sensing without motion induction.

</details>


### [32] [Flatness-aware Curriculum Learning via Adversarial Difficulty](https://arxiv.org/abs/2508.18726)
*Hiroaki Aizawa,Yoshikazu Hayashi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，将课程学习（CL）和扁平化极小值训练（SAM）的优点结合，通过一种名为对抗性难度度量（ADM）的新指标动态评估样本难度，进一步提升模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经网络容易过拟合，导致泛化能力差。课程学习可通过样本难度排序改善泛化，但与SAM结合时，由于损失和梯度都变小，难以评估样本难度。因此，急需一种在SAM优化下仍有效的样本难度评估方法。

Method: 提出ADM，度量样本对抗性脆弱程度，用模型原始样本与对抗样本的归一化损失差来评估难度。将ADM嵌入CL-SAM训练过程，在不同训练阶段动态选择训练样本。

Result: 在图像分类、细粒度识别和领域泛化多项任务中实验证明，该方法兼具CL和SAM各自优势，显著优于已有的基于课程或扁平化的训练方法。

Conclusion: ADM为CL与SAM提供了有效的融合新范式，提高了模型泛化和鲁棒性，为后续相关研究提供了有价值的思路。

Abstract: Neural networks trained by empirical risk minimization often suffer from
overfitting, especially to specific samples or domains, which leads to poor
generalization. Curriculum Learning (CL) addresses this issue by selecting
training samples based on the difficulty. From the optimization perspective,
methods such as Sharpness-Aware Minimization (SAM) improve robustness and
generalization by seeking flat minima. However, combining CL with SAM is not
straightforward. In flat regions, both the loss values and the gradient norms
tend to become uniformly small, which makes it difficult to evaluate sample
difficulty and design an effective curriculum. To overcome this problem, we
propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial
vulnerability by leveraging the robustness properties of models trained toward
flat minima. Unlike loss- or gradient-based measures, which become ineffective
as training progresses into flatter regions, ADM remains informative by
measuring the normalized loss gap between original and adversarial examples. We
incorporate ADM into CL-based training with SAM to dynamically assess sample
difficulty. We evaluated our approach on image classification tasks,
fine-grained recognition, and domain generalization. The results demonstrate
that our method preserves the strengths of both CL and SAM while outperforming
existing curriculum-based and flatness-aware training strategies.

</details>


### [33] [Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings](https://arxiv.org/abs/2508.18733)
*Feiwei Qin,Shichao Lu,Junhao Hou,Changmiao Wang,Meie Fang,Ligang Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的方法（Drawing2CAD），能够自动将2D工程矢量图转换为参数化CAD模型，并为此构建了配套数据集和实验验证。


<details>
  <summary>Details</summary>
Motivation: 目前主流的生成建模方法，大多从点云、网格或文本描述生成CAD模型，但工业流程实际多以2D工程图为起点。如何从2D矢量工程图直接生成参数化CAD模型，尚未被充分研究，而该步骤对工程设计具有重大意义。

Method: 作者将CAD生成建模为序列到序列问题。具体方法包括：基于矢量图元的精准几何表示；采用双解码器Transformer结构，将命令类型与参数生成解耦，并保持严格对应关系；设计软目标分布损失函数，以容纳CAD参数固有的灵活性。此外，作者还构建了CAD-VGDrawing数据集，将工程图与参数化CAD模型配对。

Result: 实验结果表明，Drawing2CAD在将2D工程矢量图转换为参数化CAD模型任务上表现优异。方法在新构建的数据集上进行了彻底的实验验证，并取得了良好的效果。

Conclusion: Drawing2CAD为2D工程图到参数化CAD模型的自动生成提供了一种高精度、高保真的解决方案。提出的框架及数据集为工业CAD自动化设计与智能化提供了重要基础。

Abstract: Computer-Aided Design (CAD) generative modeling is driving significant
innovations across industrial applications. Recent works have shown remarkable
progress in creating solid models from various inputs such as point clouds,
meshes, and text descriptions. However, these methods fundamentally diverge
from traditional industrial workflows that begin with 2D engineering drawings.
The automatic generation of parametric CAD models from these 2D vector drawings
remains underexplored despite being a critical step in engineering design. To
address this gap, our key insight is to reframe CAD generation as a
sequence-to-sequence learning problem where vector drawing primitives directly
inform the generation of parametric CAD operations, preserving geometric
precision and design intent throughout the transformation process. We propose
Drawing2CAD, a framework with three key technical components: a
network-friendly vector primitive representation that preserves precise
geometric information, a dual-decoder transformer architecture that decouples
command type and parameter generation while maintaining precise correspondence,
and a soft target distribution loss function accommodating inherent flexibility
in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,
a dataset of paired engineering drawings and parametric CAD models, and conduct
thorough experiments to demonstrate the effectiveness of our method. Code and
dataset are available at https://github.com/lllssc/Drawing2CAD.

</details>


### [34] [Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion](https://arxiv.org/abs/2508.18734)
*DongHoon Lim,YoungChae Kim,Dong-Hyun Kim,Da-Hee Yang,Joon-Hyuk Chang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的音频-视觉语音识别（AVSR）方法，通过动态调整音频和视觉特征的权重，提高在嘈杂环境下的识别鲁棒性，实现了相对于主流方法显著更低的词错误率。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR系统在嘈杂环境下难以准确估算音频的可靠性，导致无法灵活切换或整合音视频信息，影响识别效果。因此需要一种能够动态调节多模态依赖度的方法。

Method: 提出了一种基于路由器门控的跨模态特征融合框架。具体地，依据音频的token级别的损坏分数，动态对音视特征进行加权；在解码器每一层，通过带门控的交叉注意力机制弱化不可靠的音频token、增强视觉特征，从而实现自适应特征融合。

Result: 在LRS3数据集上的实验表明，该方法相对AV-HuBERT在词错误率上获得了16.51-42.67%的相对下降。消融实验进一步验证了路由器和门控机制对于抗噪鲁棒性的贡献。

Conclusion: 所提出的方法显著提升了AVSR系统在真实嘈杂环境下的鲁棒性，并证明了动态加权及门控交互机制的有效性。

Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains
challenging, as existing systems struggle to estimate audio reliability and
dynamically adjust modality reliance. We propose router-gated cross-modal
feature fusion, a novel AVSR framework that adaptively reweights audio and
visual features based on token-level acoustic corruption scores. Using an
audio-visual feature fusion-based router, our method down-weights unreliable
audio tokens and reinforces visual cues through gated cross-attention in each
decoder layer. This enables the model to pivot toward the visual modality when
audio quality deteriorates. Experiments on LRS3 demonstrate that our approach
achieves an 16.51-42.67% relative reduction in word error rate compared to
AV-HuBERT. Ablation studies confirm that both the router and gating mechanism
contribute to improved robustness under real-world acoustic noise.

</details>


### [35] [Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods](https://arxiv.org/abs/2508.18753)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新的评测标准，将人-物交互（HOI）检测任务重新设计为多答案、多选题，以更准确地评估并比较现有大型生成式视觉-语言模型（VLMs）与传统HOI方法的表现，解决了旧有基准中惩罚合理预测的弊端。


<details>
  <summary>Details</summary>
Motivation: 早期的HOI检测方法只把VLMs（如CLIP）作为辅助，而最新大规模生成式VLMs在理解复杂图像的能力大大提升，因此迫切需要评价它们是否能胜任HOI检测这一通用性任务，并与专用方法作对比。然而旧有的HOI检测评测（如HICO-DET）对预测要求严格一致，无法容纳多种合理解释，从而低估了新模型的表现。

Method: 作者设计了一个新评测基准，把HOI检测任务改为多项多答案选择：每个问题只包含真实标签和精心挑选、减少歧义的负例（如标注“接飞盘”时不将“扔飞盘”作为负例），避免对合理但未被标注的预测进行惩罚。

Result: 该新评测可首次直接、公平地比较大型生成式VLMs与专用HOI检测方法，揭示两类方法在当前进展中的表现优劣，并能反映VLMs在真实图像多义性理解上的强项。

Conclusion: 通过该评价准则，不仅促进了通用VLMs在HOI检测领域的应用，也为后续方法的开发和更公平横向对比提供了理论与操作基础，有助于推动HOI理解研究的进一步进步。

Abstract: Prior human-object interaction (HOI) detection methods have integrated early
vision-language models (VLMs) such as CLIP, but only as supporting components
within their frameworks. In contrast, recent advances in large, generative VLMs
suggest that these models may already possess strong ability to understand
images involving HOI. This naturally raises an important question: can
general-purpose standalone VLMs effectively solve HOI detection, and how do
they compare with specialized HOI methods? Answering this requires a benchmark
that can accommodate both paradigms. However, existing HOI benchmarks such as
HICO-DET were developed before the emergence of modern VLMs, and their
evaluation protocols require exact matches to annotated HOI classes. This is
poorly aligned with the generative nature of VLMs, which often yield multiple
valid interpretations in ambiguous cases. For example, a static image may
capture a person mid-motion with a frisbee, which can plausibly be interpreted
as either "throwing" or "catching". When only "catching" is annotated, the
other, though equally plausible for the image, is marked incorrect when exact
matching is used. As a result, correct predictions might be penalized,
affecting both VLMs and HOI-specific methods. To avoid penalizing valid
predictions, we introduce a new benchmark that reformulates HOI detection as a
multiple-answer multiple-choice task, where each question includes only
ground-truth positive options and a curated set of negatives that are
constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing"
is not selected as a negative to avoid penalizing valid predictions). The
proposed evaluation protocol is the first of its kind for both VLMs and HOI
methods, enabling direct comparison and offering new insight into the current
state of progress in HOI understanding.

</details>


### [36] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（CmOS）用于自动生成带有视觉选项的多项选择题，有效提升了选项的语义合理性和视觉相似度。


<details>
  <summary>Details</summary>
Motivation: 传统多项选择题（MCQ）大多关注文本选项的生成，很少涉及视觉选项。同时，高质量干扰项的自动生成一直是个难题，手工制作成本高、难以扩展。

Method: 提出了跨模态选项合成（CmOS）框架，结合了多模态链式思维（MCoT）推理和检索增强生成（RAG）技术，用于生成语义合理、视觉上相似的正确答案和干扰项。同时，加入判别模块筛选适合视觉选项的内容。

Result: 实验结果表明，CmOS在内容判别、问题生成以及视觉选项生成方面都优于现有方法，且适用于多个学科和不同教育层次。

Conclusion: CmOS显著提升了带有视觉选项的教育MCQ的自动生成水平，有望广泛应用于教育评测和内容创作中。

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>


### [37] [Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring](https://arxiv.org/abs/2508.18787)
*Constantino Álvarez Casado,Sasan Sharifipour,Manuel Lage Cañellas,Nhi Nguyen,Le Nguyen,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本论文提出并实现了一种适用于低功耗设备的实时远程光电容积描记（rPPG）系统，可通过面部视频流提取心率、呼吸频率和血氧饱和度等生命体征。系统架构多线程，保证了在资源受限的设备上流畅运行和高效实时处理。


<details>
  <summary>Details</summary>
Motivation: 随着智能环境、低功耗计算设备和传感器技术的普及，远程无接触生理监测需求增长。但将此类系统应用于资源有限的平台会面临可扩展性、互操作性与性能方面的挑战。

Method: 系统以Face2PPG流程为核心，顺序处理视频帧以提取rPPG信号，并采用多线程架构实现视频采集、实时处理、网络通信和GUI同步运行。结合FRP和Actor Model实现高效事件驱动处理和任务并行化，并设计了协同式用户界面与网络接口（HTTP服务器及RESTful API）。

Result: 在资源受限设备上实现了每秒30帧连续、可靠运行，有效地降低了计算开销。系统在实时约束下表现出较强的鲁棒性和准确性。

Conclusion: 本系统成功解决了实时生理信号监测在低功耗平台上的主要技术难题，为现代医疗及人机交互场景下的性能优化提供了实用解决方案。

Abstract: The growing integration of smart environments and low-power computing
devices, coupled with mass-market sensor technologies, is driving advancements
in remote and non-contact physiological monitoring. However, deploying these
systems in real-time on resource-constrained platforms introduces significant
challenges related to scalability, interoperability, and performance. This
paper presents a real-time remote photoplethysmography (rPPG) system optimized
for low-power devices, designed to extract physiological signals, such as heart
rate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial
video streams. The system is built on the Face2PPG pipeline, which processes
video frames sequentially for rPPG signal extraction and analysis, while
leveraging a multithreaded architecture to manage video capture, real-time
processing, network communication, and graphical user interface (GUI) updates
concurrently. This design ensures continuous, reliable operation at 30 frames
per second (fps), with adaptive feedback through a collaborative user interface
to guide optimal signal capture conditions. The network interface includes both
an HTTP server for continuous video streaming and a RESTful API for on-demand
vital sign retrieval. To ensure accurate performance despite the limitations of
low-power devices, we use a hybrid programming model combining Functional
Reactive Programming (FRP) and the Actor Model, allowing event-driven
processing and efficient task parallelization. The system is evaluated under
real-time constraints, demonstrating robustness while minimizing computational
overhead. Our work addresses key challenges in real-time biosignal monitoring,
offering practical solutions for optimizing performance in modern healthcare
and human-computer interaction applications.

</details>


### [38] [Robust and Label-Efficient Deep Waste Detection](https://arxiv.org/abs/2508.18799)
*Hassan Abid,Khan Muhammad,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 该论文通过建立基线和提出基于集成的半监督学习框架，提升了AI在垃圾检测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI在废弃物分拣领域研究落后于商业系统，主要由于数据集有限和依赖传统目标检测器，因此亟需开发更强和更通用的方法来提升实际废弃物检测效果。

Method: 首先在ZeroWaste数据集上基准测试了现有开放词汇目标检测（OVOD）模型，发现经大语言模型（LLM）优化的prompt能显著提升零样本检测准确率。此外，微调现代transformer目标检测器并设立新的性能基线（51.6 mAP），随后提出一种软伪标签方法：融合集成模型预测，通过空间和共识加权获得伪标签，进行半监督训练。

Result: 应用该框架于ZeroWaste-s无标注子集，得到的伪标签使模型性能超越全监督训练，显著提升了垃圾检测效果，并生成了高质量的无标注数据注释。

Conclusion: 本研究通过建立高标准基线、提出鲁棒的伪标注集成方法、公开高质量注释和系统评测了OVOD，推动了废弃物分拣领域的AI研究进步。

Abstract: Effective waste sorting is critical for sustainable recycling, yet AI
research in this domain continues to lag behind commercial systems due to
limited datasets and reliance on legacy object detectors. In this work, we
advance AI-driven waste detection by establishing strong baselines and
introducing an ensemble-based semi-supervised learning framework. We first
benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on
the real-world ZeroWaste dataset, demonstrating that while class-only prompts
perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.
Next, to address domain-specific limitations, we fine-tune modern
transformer-based detectors, achieving a new baseline of 51.6 mAP. We then
propose a soft pseudo-labeling strategy that fuses ensemble predictions using
spatial and consensus-aware weighting, enabling robust semi-supervised
training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations
achieve performance gains that surpass fully supervised training, underscoring
the effectiveness of scalable annotation pipelines. Our work contributes to the
research community by establishing rigorous baselines, introducing a robust
ensemble-based pseudo-labeling pipeline, generating high-quality annotations
for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models
under real-world waste sorting conditions. Our code is available at:
https://github.com/h-abid97/robust-waste-detection.

</details>


### [39] [Embedding Font Impression Word Tags Based on Co-occurrence](https://arxiv.org/abs/2508.18825)
*Yugo Kubota,Seiichi Uchida*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于印象标签嵌入方法，通过利用字体形状与印象标签的关系来提升字体生成与检索的表现。


<details>
  <summary>Details</summary>
Motivation: 不同字体样式会带来不同的感受，现有的通用词嵌入方法（如BERT、CLIP）在印象标签的表示上并不理想。因此，需要一种专门针对字体印象标签的嵌入方法，以增强基于印象的应用（如字体生成、检索）。

Method: 构建一个节点为印象标签、边为标签共现关系的图，利用谱嵌入得到每个印象标签的向量表示。该方法使得频繁共现的标签向量更为接近，有别于标准词嵌入方法。

Result: 通过定性和定量实验，将该方法与BERT和CLIP进行对比，表明新方法在基于印象的字体生成任务中表现更优。

Conclusion: 提出的印象标签嵌入方法有效提升了印象驱动的字体生成与检索任务效果，优于常规通用词嵌入模型。

Abstract: Different font styles (i.e., font shapes) convey distinct impressions,
indicating a close relationship between font shapes and word tags describing
those impressions. This paper proposes a novel embedding method for impression
tags that leverages these shape-impression relationships. For instance, our
method assigns similar vectors to impression tags that frequently co-occur in
order to represent impressions of fonts, whereas standard word embedding
methods (e.g., BERT and CLIP) yield very different vectors. This property is
particularly useful for impression-based font generation and font retrieval.
Technically, we construct a graph whose nodes represent impression tags and
whose edges encode co-occurrence relationships. Then, we apply spectral
embedding to obtain the impression vectors for each tag. We compare our method
with BERT and CLIP in qualitative and quantitative evaluations, demonstrating
that our approach performs better in impression-guided font generation.

</details>


### [40] [Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory](https://arxiv.org/abs/2508.18829)
*Takayuki Ishikawa,Carmelo Bonannella,Bas J. W. Lerink,Marc Rußwurm*

Main category: cs.CV

TL;DR: 本研究利用预训练的遥感基础模型对荷兰树种进行分类，显著提升了分类精度，超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 国家森林清查(NFI)依赖于人工现场调查，成本高、效率低。为了提升更新频率和覆盖范围，亟需运用遥感和机器学习方法，尤其是在样本量有限的情况下，提高树种分辨能力。

Method: 该研究结合了Sentinel-1、Sentinel-2、ERA5和SRTM等多源遥感数据，采用Google Earth Engine提取时序特征，通过微调公开的遥感时序预训练基础模型，对荷兰地区少量带标签的数据进行树种分类，并与传统基于随机森林与手工特征的方法进行了系统对比。

Result: 实验结果显示，微调基础模型后在所有数据集上的树种分类精度均比当前最佳方法高10个百分点，显著优于手工特征和随机森林分类器。

Conclusion: 深度特征在数据有限时效提升显著，展示了AI驱动的遥感方法对传统NFI流程的巨大补充价值，为森林资源监测奠定了自动化与高效化基础。

Abstract: National Forest Inventory (NFI)s serve as the primary source of forest
information, providing crucial tree species distribution data. However,
maintaining these inventories requires labor-intensive on-site campaigns.
Remote sensing approaches, particularly when combined with machine learning,
offer opportunities to update NFIs more frequently and at larger scales. While
the use of Satellite Image Time Series has proven effective for distinguishing
tree species through seasonal canopy reflectance patterns, current approaches
rely primarily on Random Forest classifiers with hand-designed features and
phenology-based metrics. Using deep features from an available pre-trained
remote sensing foundation models offers a complementary strategy. These
pre-trained models leverage unannotated global data and are meant to used for
general-purpose applications and can then be efficiently fine-tuned with
smaller labeled datasets for specific classification tasks. This work
systematically investigates how deep features improve tree species
classification accuracy in the Netherlands with few annotated data. Data-wise,
we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites
data and SRTM data using Google Earth Engine. Our results demonstrate that
fine-tuning a publicly available remote sensing time series foundation model
outperforms the current state-of-the-art in NFI classification in the
Netherlands by a large margin of up to 10% across all datasets. This
demonstrates that classic hand-defined harmonic features are too simple for
this task and highlights the potential of using deep AI features for
data-limited application like NFI classification. By leveraging openly
available satellite data and pre-trained models, this approach significantly
improves classification accuracy compared to traditional methods and can
effectively complement existing forest inventory processes.

</details>


### [41] [Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2](https://arxiv.org/abs/2508.18831)
*Yosuke Yamagishi,Shouhei Hanaoka*

Main category: cs.CV

TL;DR: 本文针对MIDOG 2025比赛Track 2任务，提出了一种基于ConvNeXt V2模型的二分类方法，实现对正常与非典型有丝分裂像的区分。


<details>
  <summary>Details</summary>
Motivation: 在病理图像分析中，准确区分正常与非典型有丝分裂像对肿瘤诊断和研究具有重要意义，但图像异质性、类别不均衡以及形态多样性带来了挑战。

Method: 采用ConvNeXt V2作为基础模型，结合中心裁剪（取60%中心区域）作为预处理，并利用五折交叉验证集成策略。训练时采用混合精度提升效率，并针对不同数据来源、类型进行模型优化。

Result: 模型在MIDOG 2025多样化数据集上取得了稳健的性能表现，证明了预处理和训练策略的有效性。

Conclusion: 现代卷积网络架构结合高效的训练与预处理方法能有效进行有丝分裂像亚型分类，且保持较高的计算效率。

Abstract: This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.

</details>


### [42] [Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression](https://arxiv.org/abs/2508.18834)
*Zizheng Guo,Bochao Zou,Yinuo Jia,Xiangyu Li,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于先验引导的视频级回归方法，用于微表情分析，并在多个基准数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有微表情分析方法多采用固定窗口大小和硬判决，难以充分捕捉微表情复杂的时序动态，对间隔解码的依赖也未能完全解决问题。因此，亟需更有效的方法提升微表情分析的精度和时序把握能力。

Method: 作者提出基于先验引导的视频级回归方法，创新性地提出了可扩展的区间选择策略，充分考虑微表情的时序演变、持续时间和类别分布特性，实现起始、峰值和终止期的精确定位。此外，通过任务参数共享的协同优化框架，在识别与定位任务间共享参数（分类头除外），提升数据利用效率和模型能力。

Result: 在CAS(ME)^3和SAMMLV等多个基准数据集上的实验结果显示，所提方法达到最先进性能，STRS分别达到0.0562和0.2000，优于现有方法。

Conclusion: 新方法在微表情分析中实现了更准确的定位和识别，并实现了业界领先的性能。同时，协同优化和先验引导为有限数据下的模型能力提升提供了新的解决思路。

Abstract: Micro-expressions (MEs) are involuntary, low-intensity, and short-duration
facial expressions that often reveal an individual's genuine thoughts and
emotions. Most existing ME analysis methods rely on window-level classification
with fixed window sizes and hard decisions, which limits their ability to
capture the complex temporal dynamics of MEs. Although recent approaches have
adopted video-level regression frameworks to address some of these challenges,
interval decoding still depends on manually predefined, window-based methods,
leaving the issue only partially mitigated. In this paper, we propose a
prior-guided video-level regression method for ME analysis. We introduce a
scalable interval selection strategy that comprehensively considers the
temporal evolution, duration, and class distribution characteristics of MEs,
enabling precise spotting of the onset, apex, and offset phases. In addition,
we introduce a synergistic optimization framework, in which the spotting and
recognition tasks share parameters except for the classification heads. This
fully exploits complementary information, makes more efficient use of limited
data, and enhances the model's capability. Extensive experiments on multiple
benchmark datasets demonstrate the state-of-the-art performance of our method,
with an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is
available at https://github.com/zizheng-guo/BoostingVRME.

</details>


### [43] [Quantitative Outcome-Oriented Assessment of Microsurgical Anastomosis](https://arxiv.org/abs/2508.18836)
*Luyin Hu,Soheil Gholami,George Dindelegan,Torstein R. Meling,Aude Billard*

Main category: cs.CV

TL;DR: 本论文提出了一种利用图像处理技术的客观量化评估方法，有效提升了显微外科吻合术技能的评估效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 显微外科吻合术需要高超的技巧，目前评估方法多依赖主观判断，存在偏见和低效问题，因此急需更为客观且高效的评估工具。

Method: 研究团队收集了来自不同医院、不同水平参与者的三组数据集，提出了一种基于图像处理的量化评估框架。该框架通过几何建模检测和评分吻合误差，建立客观的评分机制。

Result: 实验结果表明，几何度量指标能够有效复现专家评分，准确检测并评分相关技术误差，提高了评估结果的可靠性和一致性。

Conclusion: 基于图像处理的客观量化方法能提升显微外科吻合技能评估的效率和可靠性，为未来的外科培训和考核提供坚实基础。

Abstract: Microsurgical anastomosis demands exceptional dexterity and visuospatial
skills, underscoring the importance of comprehensive training and precise
outcome assessment. Currently, methods such as the outcome-oriented anastomosis
lapse index are used to evaluate this procedure. However, they often rely on
subjective judgment, which can introduce biases that affect the reliability and
efficiency of the assessment of competence. Leveraging three datasets from
hospitals with participants at various levels, we introduce a quantitative
framework that uses image-processing techniques for objective assessment of
microsurgical anastomoses. The approach uses geometric modeling of errors along
with a detection and scoring mechanism, enhancing the efficiency and
reliability of microsurgical proficiency assessment and advancing training
protocols. The results show that the geometric metrics effectively replicate
expert raters' scoring for the errors considered in this work.

</details>


### [44] [Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization](https://arxiv.org/abs/2508.18859)
*Muhammad Kashif Ali,Eun Woo Im,Dongjin Kim,Tae Hyun Kim,Vivek Gupta,Haonan Luo,Tianrui Li*

Main category: cs.CV

TL;DR: 本文提出了一种通过测试时快速自适应来提升像素级视频稳定的方法，结合了新颖的急动定位模块和有针对性的自适应策略，在多种数据集上实现了优于SOTA的稳定效果。


<details>
  <summary>Details</summary>
Motivation: 现有像素级视频稳定方法由于视频运动和内容的多样性，常用固定参数模型难以做到通用和鲁棒。需要一种适应性更强的解决方案以提升不同视频场景下的稳定性与画质。

Method: 方法在测试时利用输入视频的低层视觉信息，快速对模型进行自适应调整；提出急动定位模块及有针对性的自适应策略，将自适应重点集中在剧烈抖动区段，从而用较少的调整循环获得更佳稳定效果。

Result: 新方法在多个真实世界数据集上进行实验，无论在定性还是定量评价中，对多种现有全帧合成模型都有稳定提升，且对下游应用也有改善。

Conclusion: 该方法兼具现代稳定器的高质量全帧合成效果和经典方法的可控性，显著提升了像素级视频稳定的实用性和性能，推动领域进步。

Abstract: Video stabilization remains a fundamental problem in computer vision,
particularly pixel-level synthesis solutions for video stabilization, which
synthesize full-frame outputs, add to the complexity of this task. These
methods aim to enhance stability while synthesizing full-frame videos, but the
inherent diversity in motion profiles and visual content present in each video
sequence makes robust generalization with fixed parameters difficult. To
address this, we present a novel method that improves pixel-level synthesis
video stabilization methods by rapidly adapting models to each input video at
test time. The proposed approach takes advantage of low-level visual cues
available during inference to improve both the stability and visual quality of
the output. Notably, the proposed rapid adaptation achieves significant
performance gains even with a single adaptation pass. We further propose a jerk
localization module and a targeted adaptation strategy, which focuses the
adaptation on high-jerk segments for maximizing stability with fewer adaptation
steps. The proposed methodology enables modern stabilizers to overcome the
longstanding SOTA approaches while maintaining the full frame nature of the
modern methods, while offering users with control mechanisms akin to classical
approaches. Extensive experiments on diverse real-world datasets demonstrate
the versatility of the proposed method. Our approach consistently improves the
performance of various full-frame synthesis models in both qualitative and
quantitative terms, including results on downstream applications.

</details>


### [45] [Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2508.18886)
*Yuexuan Xia,Benteng Ma,Jiang He,Zhiyong Wang,Qi Dou,Yong Xia*

Main category: cs.CV

TL;DR: 本论文提出DualFairVL，一种多模态提示学习框架，用于在医学诊断中同时提升跨模态对齐和群体公平性，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学影像诊断中，不同群体间的公平性非常重要，且由于成像设备和临床实践的变化，数据分布易发生偏移。现有视觉-语言模型虽然泛化能力强，但在消除偏见时往往单独处理视觉和文本信息，导致公平性不佳和模态间不一致。该论文致力于解决跨模态表示中的残余偏见与公平性差距。

Method: 作者提出DualFairVL：1）采用并行双分支架构，区分敏感属性和目标属性，实现跨模态的解耦与对齐；2）通过线性投影构建近似正交的文本锚点，利用跨注意力机制对视觉与文本特征融合；3）引入超网络进一步分离属性相关信息，并生成实例感知的视觉提示；4）在视觉分支引入基于原型的正则约束，加强敏感特征的分离及与文本锚点的对齐。

Result: 在八个医学影像数据集和四种模态上进行全面实验，DualFairVL在分布内外均获得了最优的公平性与准确性，超越了全量微调和高效参数方法，仅需3.6M可训练参数。

Conclusion: DualFairVL有效提升了医学诊断中的公平性和鲁棒性，有望推动公平医疗实践的发展，并具备很强的参数效率。源码将在论文发表后公开。

Abstract: Ensuring fairness across demographic groups in medical diagnosis is essential
for equitable healthcare, particularly under distribution shifts caused by
variations in imaging equipment and clinical practice. Vision-language models
(VLMs) exhibit strong generalization, and text prompts encode identity
attributes, enabling explicit identification and removal of sensitive
directions. However, existing debiasing approaches typically address vision and
text modalities independently, leaving residual cross-modal misalignment and
fairness gaps. To address this challenge, we propose DualFairVL, a multimodal
prompt-learning framework that jointly debiases and aligns cross-modal
representations. DualFairVL employs a parallel dual-branch architecture that
separates sensitive and target attributes, enabling disentangled yet aligned
representations across modalities. Approximately orthogonal text anchors are
constructed via linear projections, guiding cross-attention mechanisms to
produce fused features. A hypernetwork further disentangles attribute-related
information and generates instance-aware visual prompts, which encode
dual-modal cues for fairness and robustness. Prototype-based regularization is
applied in the visual branch to enforce separation of sensitive features and
strengthen alignment with textual anchors. Extensive experiments on eight
medical imaging datasets across four modalities show that DualFairVL achieves
state-of-the-art fairness and accuracy under both in- and out-of-distribution
settings, outperforming full fine-tuning and parameter-efficient baselines with
only 3.6M trainable parameters. Code will be released upon publication.

</details>


### [46] [DQEN: Dual Query Enhancement Network for DETR-based HOI Detection](https://arxiv.org/abs/2508.18896)
*Zhehao Li,Chong Wang,Yi Chen,Yinghao Lu,Jiangbo Qian,Jiong Wang,Jiafei Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于人体-物体交互检测（HOI）的模型，名为Dual Query Enhancement Network (DQEN)，通过增强object和interaction查询，提升检测精度，在主流数据集上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有DETR-based HOI检测方法中，查询通常随机初始化，导致表达模糊，限制模型性能。而在HOI分类中，人是固定的，物体和交互是变量。因此需要更有效的方法来增强查询的语义能力。

Method: 提出DQEN，包括两大创新：（1）使用object-aware编码器特征来增强object queries，使模型能更专注于有意义的人-物体区域；（2）设计Interaction Semantic Fusion模块，结合CLIP模型生成的HOI候选，通过语义特征提升interaction queries初始表达。同时引入Auxiliary Prediction Unit辅助提升交互特征表达。

Result: 在HICO-Det和V-COCO主流数据集上，所提模型表现出与当前最优方法有竞争力的性能。

Conclusion: 通过增强object和interaction查询表达，DQEN有效提升了DETR-based HOI检测模型的检测准确率，为提升HOI任务提供了新思路。

Abstract: Human-Object Interaction (HOI) detection focuses on localizing human-object
pairs and recognizing their interactions. Recently, the DETR-based framework
has been widely adopted in HOI detection. In DETR-based HOI models, queries
with clear meaning are crucial for accurately detecting HOIs. However, prior
works have typically relied on randomly initialized queries, leading to vague
representations that limit the model's effectiveness. Meanwhile, humans in the
HOI categories are fixed, while objects and their interactions are variable.
Therefore, we propose a Dual Query Enhancement Network (DQEN) to enhance object
and interaction queries. Specifically, object queries are enhanced with
object-aware encoder features, enabling the model to focus more effectively on
humans interacting with objects in an object-aware way. On the other hand, we
design a novel Interaction Semantic Fusion module to exploit the HOI candidates
that are promoted by the CLIP model. Semantic features are extracted to enhance
the initialization of interaction queries, thereby improving the model's
ability to understand interactions. Furthermore, we introduce an Auxiliary
Prediction Unit aimed at improving the representation of interaction features.
Our proposed method achieves competitive performance on both the HICO-Det and
the V-COCO datasets. The source code is available at
https://github.com/lzzhhh1019/DQEN.

</details>


### [47] [Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025](https://arxiv.org/abs/2508.18904)
*Thien-Phuc Tran,Minh-Quang Nguyen,Minh-Triet Tran,Tam V. Nguyen,Trong-Le Do,Duy-Nam Ly,Viet-Tham Huynh,Khanh-Duy Le,Mai-Khiem Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: EVENTA大赛推出了全球首个面向事件级多模态理解的大规模基准，提升了图像分析的语义与情境深度。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述与检索任务多聚焦于表层的人、物、场景识别，但忽略了真正定义事件的情境和语义层面。EVENTA希望填补该领域在事件理解上的空白。

Method: 大赛依托OpenEvents V1数据集，设置了“事件丰富型图像检索与描述”、“基于事件的图像检索”两大赛道，通过集成情境、时间、语义信息来分析图像中的何人、何时、何地、何事、为何等核心要素。比赛分为公开和私有测试阶段进行评估，保障结果公平可复现。

Result: 共有6个国家的45支队伍参赛，最终3支顶尖队伍受邀在ACM Multimedia 2025进行展示。

Conclusion: EVENTA提供了面向语境感知与叙事驱动的多媒体AI新基石，有望推动新闻、媒体分析、文化档案、无障碍等领域应用的发展。

Abstract: The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM
Multimedia 2025, introduces the first large-scale benchmark for event-level
multimodal understanding. Traditional captioning and retrieval tasks largely
focus on surface-level recognition of people, objects, and scenes, often
overlooking the contextual and semantic dimensions that define real-world
events. EVENTA addresses this gap by integrating contextual, temporal, and
semantic information to capture the who, when, where, what, and why behind an
image. Built upon the OpenEvents V1 dataset, the challenge features two tracks:
Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval.
A total of 45 teams from six countries participated, with evaluation conducted
through Public and Private Test phases to ensure fairness and reproducibility.
The top three teams were invited to present their solutions at ACM Multimedia
2025. EVENTA establishes a foundation for context-aware, narrative-driven
multimedia AI, with applications in journalism, media analysis, cultural
archiving, and accessibility. Further details about the challenge are available
at the official homepage: https://ltnghia.github.io/eventa/eventa-2025.

</details>


### [48] [Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories](https://arxiv.org/abs/2508.18939)
*Amartaivan Sanjjamts,Morita Hiroshi*

Main category: cs.CV

TL;DR: 本研究提出了基于真实行人轨迹数据的群体与个体行人区分框架，并建立了一套行为与空间利用度指标体系，为后续人群动力学研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 城市及公共空间的行人活动极为复杂，群体与个体行人在空间利用和行为模式上可能存在显著差异。精确识别与量化这些差异，对于人群动力学、空间设计优化及仿真建模具有重要意义。

Method: 采用基于Transformer的配对分类模型，结合时间分段的轨迹数据，识别群体与个体行人。提出空间指标（如凸包面积、最小外接圆半径、空间密度热力图）与行为指标（如速度变化、运动角度偏差、避让半径、轨迹直线性）对行人进行多维度特征刻画。同时提出行人相遇类型（如单独-单独、单独-群体、群体-群体）的分型方案。

Result: 目前已搭建完成行人区分类及数据结构化处理流程，建立了指标体系和行人相遇类型的基础框架，为后续量化分析及多长度序列的扩展做好准备。

Conclusion: 本文提出的分类与评价流程为后续人群动力学中差异化空间与行为分析提供技术储备，未来将进行完整指标量化分析，并服务于行人仿真和空间设计应用。

Abstract: This study presents an initial framework for distinguishing group and single
pedestrians based on real-world trajectory data, with the aim of analyzing
their differences in space utilization and emergent behavioral patterns. By
segmenting pedestrian trajectories into fixed time bins and applying a
Transformer-based pair classification model, we identify cohesive groups and
isolate single pedestrians over a structured sequence-based filtering process.
To prepare for deeper analysis, we establish a comprehensive metric framework
incorporating both spatial and behavioral dimensions. Spatial utilization
metrics include convex hull area, smallest enclosing circle radius, and
heatmap-based spatial densities to characterize how different pedestrian types
occupy and interact with space. Behavioral metrics such as velocity change,
motion angle deviation, clearance radius, and trajectory straightness are
designed to capture local adaptations and responses during interactions.
Furthermore, we introduce a typology of encounter types-single-to-single,
single-to-group, and group-to-group to categorize and later quantify different
interaction scenarios. Although this version focuses primarily on the
classification pipeline and dataset structuring, it establishes the groundwork
for scalable analysis across different sequence lengths 60, 100, and 200
frames. Future versions will incorporate complete quantitative analysis of the
proposed metrics and their implications for pedestrian simulation and space
design validation in crowd dynamics research.

</details>


### [49] [The point is the mask: scaling coral reef segmentation with weak supervision](https://arxiv.org/abs/2508.18958)
*Matteo Contini,Victor Illien,Sylvain Poulain,Serge Bernard,Julien Barde,Sylvain Bonhommeau,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文提出了一种多尺度弱监督语义分割框架，用于将水下图像中的精细生态信息迁移到无人机航拍图像，实现大范围珊瑚礁分类，仅需极少人工标注。


<details>
  <summary>Details</summary>
Motivation: 无人机航拍图像虽然覆盖范围广，但分辨率有限，难以识别细致的珊瑚类型；而对大范围图像进行像素级注释极为耗时和昂贵。因此亟需可扩展、低成本、高分辨率的珊瑚礁监测方法。

Method: 提出结合分类监督、空间插值、自蒸馏等技术的多尺度弱监督语义分割框架，将水下图像的详细信息转移到无人机图像，实现对大范围区域的精细珊瑚类别分割，同时只需极少人工标注。

Result: 该方法能够在大区域范围内对珊瑚形态类型实现高效分割，并且具有良好的灵活性，能够轻松整合新类别。

Conclusion: 该研究提出了一种低成本、高效且可扩展的方法，结合弱监督深度学习、多尺度遥感与低成本数据采集，为高分辨率珊瑚礁监测提供了新思路。

Abstract: Monitoring coral reefs at large spatial scales remains an open challenge,
essential for assessing ecosystem health and informing conservation efforts.
While drone-based aerial imagery offers broad spatial coverage, its limited
resolution makes it difficult to reliably distinguish fine-scale classes, such
as coral morphotypes. At the same time, obtaining pixel-level annotations over
large spatial extents is costly and labor-intensive, limiting the scalability
of deep learning-based segmentation methods for aerial imagery. We present a
multi-scale weakly supervised semantic segmentation framework that addresses
this challenge by transferring fine-scale ecological information from
underwater imagery to aerial data. Our method enables large-scale coral reef
mapping from drone imagery with minimal manual annotation, combining
classification-based supervision, spatial interpolation and self-distillation
techniques. We demonstrate the efficacy of the approach, enabling large-area
segmentation of coral morphotypes and demonstrating flexibility for integrating
new classes. This study presents a scalable, cost-effective methodology for
high-resolution reef monitoring, combining low-cost data collection, weakly
supervised deep learning and multi-scale remote sensing.

</details>


### [50] [Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers](https://arxiv.org/abs/2508.18959)
*Claudio Affolter,Sidi Wu,Yizi Chen,Lorenz Hurni*

Main category: cs.CV

TL;DR: 本文提出一种结合生成式AI和矢量数据的新方法，实现了风格可控且精确的地图自动生成，提高了地图制作的便捷性和普及性。


<details>
  <summary>Details</summary>
Motivation: 传统地图制作依赖GIS，需要专业知识且耗时，尤其是处理大量重复性任务时效率低下。同时，现有生成式AI在地图生成上的空间和语义可控性较差，难以自动生成高质量地图。

Method: 作者提出将矢量数据融入生成式AI（如扩散模型）中，并通过文本提示词来控制地图风格，实现空间结构和语义布局受控的地图生成。并开发了配套网页版应用，便于用户使用。

Result: 通过与专业制图师的用户研究，结果显示生成地图的准确性强，网页应用易用。用户反馈表明该技术有助于提升地图制作效率，无论是非专业用户还是专业人员都可受益。

Conclusion: 该研究验证了生成式AI结合矢量数据在地图制作上的潜力，并指出了技术改进空间。同时，论文强调制图师角色的转型和AI辅助地图制作新范式的重要意义。

Abstract: Traditional map-making relies heavily on Geographic Information Systems
(GIS), requiring domain expertise and being time-consuming, especially for
repetitive tasks. Recent advances in generative AI (GenAI), particularly image
diffusion models, offer new opportunities for automating and democratizing the
map-making process. However, these models struggle with accurate map creation
due to limited control over spatial composition and semantic layout. To address
this, we integrate vector data to guide map generation in different styles,
specified by the textual prompts. Our model is the first to generate accurate
maps in controlled styles, and we have integrated it into a web application to
improve its usability and accessibility. We conducted a user study with
professional cartographers to assess the fidelity of generated maps, the
usability of the web application, and the implications of ever-emerging GenAI
in map-making. The findings have suggested the potential of our developed
application and, more generally, the GenAI models in helping both non-expert
users and professionals in creating maps more efficiently. We have also
outlined further technical improvements and emphasized the new role of
cartographers to advance the paradigm of AI-assisted map-making.

</details>


### [51] [Enhancing compact convolutional transformers with super attention](https://arxiv.org/abs/2508.18960)
*Simpenzwe Honore Leandre,Natenaile Asmamaw Shiferaw,Dillip Rout*

Main category: cs.CV

TL;DR: 本文提出了一种结合token混合、序列池化和卷积tokenizer的新型视觉模型，在CIFAR100基准任务上实现了当前最优性能，并且推理效率更高。该模型在精度与效率上均超过了SDPA transformer，且无需常用的训练增强技巧。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉模型（如transformer）在固定上下文长度任务中存在效率瓶颈，且通常依赖复杂的训练增强和调度技巧。论文旨在设计一种结构更高效、性能更优且训练更稳定的新模型。

Method: 作者提出了一种融合token mixing、sequence-pooling和卷积tokenizer的方法，构建了优化后的视觉模型架构，并针对固定长度任务进行了设计与实验。

Result: 在CIFAR100数据集上，该模型top 1准确率由36.5%提升至46.29%，top 5准确率从66.33%提升至76.31%，且模型规模仅为对比transformer的60%，执行效率更高，同时训练时无需数据增强、位置编码和学习率调度。

Conclusion: 该模型在不依赖常规增强技术的情况下实现了高准确率和高效率，为视觉任务提供了一种更优的建模思路，对实际应用有较大推动价值。

Abstract: In this paper, we propose a vision model that adopts token mixing,
sequence-pooling, and convolutional tokenizers to achieve state-of-the-art
performance and efficient inference in fixed context-length tasks. In the
CIFAR100 benchmark, our model significantly improves the baseline of the top 1%
and top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,
while being more efficient than the Scaled Dot Product Attention (SDPA)
transformers when the context length is less than the embedding dimension and
only 60% the size. In addition, the architecture demonstrates high training
stability and does not rely on techniques such as data augmentation like mixup,
positional embeddings, or learning rate scheduling. We make our code available
on Github.

</details>


### [52] [USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning](https://arxiv.org/abs/2508.18966)
*Shaojin Wu,Mengqi Huang,Yufeng Cheng,Wenxu Wu,Jiahe Tian,Yiming Luo,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: 该论文提出了USO模型，实现了风格驱动和内容驱动生成的统一优化，兼顾风格相似性和主体一致性，并公布了相关数据集和评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前风格生成（风格相似性）和主体生成（内容一致性）通常被视为互相对立的任务，缺乏统一解决方案。作者尝试打通二者，以促进内容与风格的有效解耦和重组。

Method: 1. 构建大规模三元组数据集，包括内容图片、风格图片及其对应的风格化图片。2. 设计了异质化学习方法，结合风格对齐训练与内容-风格解耦训练以分别优化风格和内容表征。3. 引入风格奖励学习（SRL）机制提升风格匹配能力。4. 公布USO-Bench基准，实现对风格相似性与主体一致性的联合评估。

Result: 大量实验证明，USO在主体一致性与风格相似性两个维度上均优于当前开源模型，达到了SOTA水平。

Conclusion: USO有效整合了风格与内容生成的目标，推动了风格化生成领域的实用进展，并为后续研究提供了标准数据集与评测工具。

Abstract: Existing literature typically treats style-driven and subject-driven
generation as two disjoint tasks: the former prioritizes stylistic similarity,
whereas the latter insists on subject consistency, resulting in an apparent
antagonism. We argue that both objectives can be unified under a single
framework because they ultimately concern the disentanglement and
re-composition of content and style, a long-standing theme in style-driven
research. To this end, we present USO, a Unified Style-Subject Optimized
customization model. First, we construct a large-scale triplet dataset
consisting of content images, style images, and their corresponding stylized
content images. Second, we introduce a disentangled learning scheme that
simultaneously aligns style features and disentangles content from style
through two complementary objectives, style-alignment training and
content-style disentanglement training. Third, we incorporate a style
reward-learning paradigm denoted as SRL to further enhance the model's
performance. Finally, we release USO-Bench, the first benchmark that jointly
evaluates style similarity and subject fidelity across multiple metrics.
Extensive experiments demonstrate that USO achieves state-of-the-art
performance among open-source models along both dimensions of subject
consistency and style similarity. Code and model:
https://github.com/bytedance/USO

</details>


### [53] [Can we make NeRF-based visual localization privacy-preserving?](https://arxiv.org/abs/2508.18971)
*Maxime Pietrantoni,Martin Humenberger,Torsten Sattler,Gabriela Csurka*

Main category: cs.CV

TL;DR: 本文探讨了以NeRF（神经辐射场）为基础的视觉定位（VL）方法所面临的隐私问题，并提出了一种隐私保护的解决方案（ppNeSF），能够在实现有效定位的同时保护场景细节隐私。


<details>
  <summary>Details</summary>
Motivation: 近年来，NeRF因其高质量新视角合成能力在视觉定位领域受到广泛关注，但其模型中无意编码的细节信息可能泄露敏感隐私，尤其是在云端应用时。研究如何在不牺牲定位准确性的前提下保护这些细节隐私，成为亟需解决的问题。

Method: 作者首先提出一种新的协议，用于评估基于NeRF的表示是否具有隐私保护能力，并指出主流NeRF模型即便去除颜色头部，也会因几何信息的保留而被隐私攻击。针对这一问题，作者提出ppNeSF方法——它通过分割监督（而非RGB图像）进行训练，并利用自监督方式获取分割标签，使得网络学到的信息较为粗略，既能模糊关键细节，又能保证3D区分性。

Result: 实验证明：1）传统NeRF模型高度易泄露细节隐私；2）ppNeSF在保证私密性的同时依然能够实现高精度的视觉定位，并在相关数据集上取得了最先进的定位效果。

Conclusion: NeRF原生方法存在明显的隐私风险，提出的ppNeSF方法可以在保障隐私的同时，实现高效、精确的视觉定位，为云端等实际应用场景提供了可行的隐私保护技术方案。

Abstract: Visual localization (VL) is the task of estimating the camera pose in a known
scene. VL methods, a.o., can be distinguished based on how they represent the
scene, e.g., explicitly through a (sparse) point cloud or a collection of
images or implicitly through the weights of a neural network. Recently,
NeRF-based methods have become popular for VL. While NeRFs offer high-quality
novel view synthesis, they inadvertently encode fine scene details, raising
privacy concerns when deployed in cloud-based localization services as
sensitive information could be recovered. In this paper, we tackle this
challenge on two ends. We first propose a new protocol to assess
privacy-preservation of NeRF-based representations. We show that NeRFs trained
with photometric losses store fine-grained details in their geometry
representations, making them vulnerable to privacy attacks, even if the head
that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving
Neural Segmentation Field), a NeRF variant trained with segmentation
supervision instead of RGB images. These segmentation labels are learned in a
self-supervised manner, ensuring they are coarse enough to obscure identifiable
scene details while remaining discriminativeness in 3D. The segmentation space
of ppNeSF can be used for accurate visual localization, yielding
state-of-the-art results.

</details>


### [54] [Enhancing Document VQA Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18984)
*Eric López,Artemis Llabrés,Ernest Valveny*

Main category: cs.CV

TL;DR: 本文探讨了检索增强生成（RAG）方法在文档视觉问答（Document VQA）中的应用，通过选择相关证据片段以减少内存消耗并提升多页文档问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有Document VQA方法存在内存消耗大、难以高效处理多页文档的问题。作者希望通过RAG方法，优化证据选择，从而提升文档问答的效率和效果。

Method: 作者设计了不同的检索变体：基于OCR文本的检索和纯视觉检索，并在多个数据集上（如MP-DocVQA、DUDE和InfographicVQA）系统评估了这些方法对Document VQA的影响。并进行了消融实验以分析各组件贡献。

Result: 基于文本的RAG变体相比拼接全部页面的基线最多提升22.5 ANLS，基于视觉的RAG无需文本提取下提升5.0 ANLS。检索和重排序模块带来主要性能提升，而布局引导切分策略无明显帮助。

Conclusion: 精细的证据选择策略能显著提升多模型、多数据集的Document VQA准确率，证实该方法在实际场景中的应用价值。

Abstract: Document Visual Question Answering (Document VQA) must cope with documents
that span dozens of pages, yet leading systems still concatenate every page or
rely on very large vision-language models, both of which are memory-hungry.
Retrieval-Augmented Generation (RAG) offers an attractive alternative, first
retrieving a concise set of relevant segments before generating answers from
this selected evidence. In this paper, we systematically evaluate the impact of
incorporating RAG into Document VQA through different retrieval variants -
text-based retrieval using OCR tokens and purely visual retrieval without OCR -
across multiple models and benchmarks. Evaluated on the multi-page datasets
MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the
"concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variant
achieves +5.0 ANLS improvement without requiring any text extraction. An
ablation confirms that retrieval and reranking components drive most of the
gain, whereas the layout-guided chunking strategy - proposed in several recent
works to leverage page structure - fails to help on these datasets. Our
experiments demonstrate that careful evidence selection consistently boosts
accuracy across multiple model sizes and multi-page benchmarks, underscoring
its practical value for real-world Document VQA.

</details>


### [55] [Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone](https://arxiv.org/abs/2508.18989)
*Shaivi Malik,Hasnat Md Abdullah,Sriparna Saha,Amit Sheth*

Main category: cs.CV

TL;DR: 本文提出了一个名为GRAS的新基准，用于系统揭示视觉语言模型（VLMs）在性别、种族、年龄和肤色上的偏见，并提出了可量化的GRAS Bias Score。实验显示主流VLMs在该基准下均存在明显偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在实际应用中日益普及，其潜在的人口统计学偏见问题尚未被充分理解和测量，且缺乏覆盖多元人群的权威评测基准。作者希望推动对VLMs偏见的深入研究。

Method: 1）构造GRAS数据集，涵盖性别、种族、年龄及肤色等多维度，并确保样本多样性；2）提出GRAS Bias Score，用于量化模型偏见程度；3）评测五个主流VLMs，并对VLMs在VQA任务中偏见的评测方法提出改进建议。

Result: 五个SOTA VLMs在GRAS基准上均出现显著偏见，最优模型Bias Score仅为2/100，反映出目前模型在公平性上的严重不足。评测还显示，VQA任务提问方式的不同会对偏见评估结果产生影响。

Conclusion: 目前主流VLMs在人口统计学特征上均存在严重偏见，现有评测与训练机制亟需改进。GRAS基准及Bias Score为后续相关研究和模型改进提供了基础工具。所有相关代码和数据公开，便于社区共同推进公平AI的发展。

Abstract: As Vision Language Models (VLMs) become integral to real-world applications,
understanding their demographic biases is critical. We introduce GRAS, a
benchmark for uncovering demographic biases in VLMs across gender, race, age,
and skin tone, offering the most diverse coverage to date. We further propose
the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark
five state-of-the-art VLMs and reveal concerning bias levels, with the least
biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings
also reveal a methodological insight: evaluating bias in VLMs with visual
question answering (VQA) requires considering multiple formulations of a
question. Our code, data, and evaluation results are publicly available.

</details>


### [56] [RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation](https://arxiv.org/abs/2508.19003)
*Siyuan You,Guozheng Xu,Pengwei Zhou,Qiwen Jin,Jian Yao,Li Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于Transformer的端到端屋顶平面分割网络RoofSeg，解决LiDAR点云屋顶分割中现有深度学习方法的三大难题，并引入边缘感知和几何约束机制提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有屋顶平面分割方法在特征提取、边缘判别和几何约束等方面存在不足。特别是大多数深度学习方法不是端到端，边缘点特征区分度低，且未充分利用屋顶的几何特性，导致分割效果和精度的提升有限。

Method: 作者提出RoofSeg网络，采用Transformer编码-解码架构，配合可学习平面查询，从点云中端到端分割屋顶平面。引入了边缘感知掩码模块（EAMM），基于几何先验增强边缘判别力。同时，提出了自适应加权的掩码损失策略以弱化错误分割点的影响，设计了新的平面几何损失以收敛和提升分割效果。

Result: 该方法显著提高了分割准确性，尤其是在屋顶平面边缘区表现更好。新型几何损失和边缘增强机制有效改善了点云分割的精细度和鲁棒性。

Conclusion: RoofSeg是一种有效的基于Transformer的端到端屋顶平面分割方法，通过边缘感知和几何约束提升了LiDAR点云分割的精度与实用性，对于三维建筑建模等实际应用具有广泛意义。

Abstract: Roof plane segmentation is one of the key procedures for reconstructing
three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from
airborne light detection and ranging (LiDAR) point clouds. The majority of
current approaches for roof plane segmentation rely on the manually designed or
learned features followed by some specifically designed geometric clustering
strategies. Because the learned features are more powerful than the manually
designed features, the deep learning-based approaches usually perform better
than the traditional approaches. However, the current deep learning-based
approaches have three unsolved problems. The first is that most of them are not
truly end-to-end, the plane segmentation results may be not optimal. The second
is that the point feature discriminability near the edges is relatively low,
leading to inaccurate planar edges. The third is that the planar geometric
characteristics are not sufficiently considered to constrain the network
training. To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner. In the RoofSeg, we leverage a transformer
encoder-decoder-based framework to hierarchically predict the plane instance
masks with the use of a set of learnable plane queries. To further improve the
segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module
(EAMM) that sufficiently incorporates planar geometric prior of edges to
enhance its discriminability for plane instance mask refinement. In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.

</details>


### [57] [MicroDetect-Net (MDN): Leveraging Deep Learning to Detect Microplastics in Clam Blood, a Step Towards Human Blood Analysis](https://arxiv.org/abs/2508.19021)
*Riju Marwah,Riya Arora,Navneet Yadav,Himank Arora*

Main category: cs.CV

TL;DR: 本论文提出了一种结合荧光染色和深度学习的新模型MicroDetect-Net（MDN），能够高效检测血样中的微塑料。实验表明，MDN模型在分割和识别微塑料方面具有较高的准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着每年超过3.68亿吨塑料的使用，微塑料污染在全球环境和生物体中变得普遍，对人类健康构成潜在威胁。亟需开发高效、准确的方法来检测生物体内的微塑料，从而评估其健康风险。

Method: 本研究提出MDN模型，通过将血样用Nile Red染色后进行荧光显微观察，并结合卷积神经网络（CNN）进行图像分割与微塑料颗粒的定位和计数。实验采用276张用Nile Red染色的血液荧光图像进行训练与测试。

Result: MDN模型在微塑料检测任务上表现优异，整体准确率达到92%，IoU为87.4%，F1分数为92.1%，精准率90.6%，召回率93.7%。

Conclusion: 结合荧光染色和深度学习的MDN模型在血液样本中检测微塑料展现出很高的准确度和鲁棒性，为未来人类微塑料暴露水平的监测和健康风险评估提供了有力工具。

Abstract: With the prevalence of plastics exceeding 368 million tons yearly,
microplastic pollution has grown to an extent where air, water, soil, and
living organisms have all tested positive for microplastic presence. These
particles, which are smaller than 5 millimeters in size, are no less harmful to
humans than to the environment. Toxicity research on microplastics has shown
that exposure may cause liver infection, intestinal injuries, and gut flora
imbalance, leading to numerous potential health hazards. This paper presents a
new model, MicroDetect-Net (MDN), which applies fluorescence microscopy with
Nile Red dye staining and deep learning to scan blood samples for
microplastics. Although clam blood has certain limitations in replicating real
human blood, this study opens avenues for applying the approach to human
samples, which are more consistent for preliminary data collection. The MDN
model integrates dataset preparation, fluorescence imaging, and segmentation
using a convolutional neural network to localize and count microplastic
fragments. The combination of convolutional networks and Nile Red dye for
segmentation produced strong image detection and accuracy. MDN was evaluated on
a dataset of 276 Nile Red-stained fluorescent blood images and achieved an
accuracy of ninety two percent. Robust performance was observed with an
Intersection over Union of 87.4 percent, F1 score of 92.1 percent, Precision of
90.6 percent, and Recall of 93.7 percent. These metrics demonstrate the
effectiveness of MDN in the detection of microplastics.

</details>


### [58] [ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval](https://arxiv.org/abs/2508.19024)
*Yi Pan,Yujia Zhang,Michael Kampffmeyer,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 本文提出了ProPy，一种专为部分相关视频检索（PRVR）任务而设计的CLIP适配模型，通过多粒度事件语义机制大幅提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有PRVR方法多针对单一模态特征开发模型，尚未充分挖掘如CLIP等强大的视觉-语言预训练模型的潜力。因此，亟需将CLIP有效适配至PRVR任务场景。

Method: 提出ProPy模型，在CLIP架构基础上进行系统性适配。具体创新点包括：（1）提出Prompt Pyramid结构，通过多级事件prompt提取多粒度语义；（2）基于金字塔的祖先-后代交互机制，实现事件间的动态语义交互，从而更好地刻画视频中部分相关片段的语义关联。

Result: 在三个公开数据集上取得了当前最优（SOTA）性能，显著超越现有相关模型。

Conclusion: 通过系统性架构创新，ProPy有效挖掘和适配了CLIP的潜力，为部分相关视频检索任务提供了新的高效方案。

Abstract: Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task
that involves retrieving videos based on queries relevant to only specific
segments. While existing works follow the paradigm of developing models to
process unimodal features, powerful pretrained vision-language models like CLIP
remain underexplored in this field. To bridge this gap, we propose ProPy, a
model with systematic architectural adaption of CLIP specifically designed for
PRVR. Drawing insights from the semantic relevance of multi-granularity events,
ProPy introduces two key innovations: (1) A Prompt Pyramid structure that
organizes event prompts to capture semantics at multiple granularity levels,
and (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that
enables dynamic semantic interaction among events. With these designs, ProPy
achieves SOTA performance on three public datasets, outperforming previous
models by significant margins. Code is available at
https://github.com/BUAAPY/ProPy.

</details>


### [59] [GReAT: leveraging geometric artery data to improve wall shear stress assessment](https://arxiv.org/abs/2508.19030)
*Julian Suk,Jolanda J. Wentzel,Patryk Rygiel,Joost Daemen,Daniel Rueckert,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文通过大规模动脉几何模型数据集及自监督预训练，提升了冠状动脉血流动力学生物标志物（如壁面切应力）的评估准确性，尤其在临床小样本情境下表现良好。


<details>
  <summary>Details</summary>
Motivation: 大数据与机器学习能帮助提升心血管等领域的患者诊疗水平，但高质量医学影像标注数据匮乏，严重制约模型表现。如何在少量临床数据下更高效、准确地评估血流动力学生物标志物，是一大瓶颈。

Method: 作者利用8449个3D动脉血管几何模型，采用自监督学习方法，以热核签名（基于Laplacian特征向量的几何特征）为目标进行预训练，学习出数据的有效几何表示。随后在49例临床冠状动脉数据的小样本集上，利用所学表征提升壁面切应力区域分割。

Result: 实验结果显示，即使在仅有49例患者的小样本条件下，通过大规模数据的自监督几何表征预训练，冠状动脉壁面切应力的分割（低、中、高分区）效果有明显提升。

Conclusion: 大规模血管几何模型及自监督学习，可有效提升小样本临床数据下的冠状动脉血流动力学生物标志物评估能力，为此类医学影像AI应用提供了新路径。

Abstract: Leveraging big data for patient care is promising in many medical fields such
as cardiovascular health. For example, hemodynamic biomarkers like wall shear
stress could be assessed from patient-specific medical images via machine
learning algorithms, bypassing the need for time-intensive computational fluid
simulation. However, it is extremely challenging to amass large-enough datasets
to effectively train such models. We could address this data scarcity by means
of self-supervised pre-training and foundations models given large datasets of
geometric artery models. In the context of coronary arteries, leveraging
learned representations to improve hemodynamic biomarker assessment has not yet
been well studied. In this work, we address this gap by investigating whether a
large dataset (8449 shapes) consisting of geometric models of 3D blood vessels
can benefit wall shear stress assessment in coronary artery models from a
small-scale clinical trial (49 patients). We create a self-supervised target
for the 3D blood vessels by computing the heat kernel signature, a quantity
obtained via Laplacian eigenvectors, which captures the very essence of the
shapes. We show how geometric representations learned from this datasets can
boost segmentation of coronary arteries into regions of low, mid and high
(time-averaged) wall shear stress even when trained on limited data.

</details>


### [60] [No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes](https://arxiv.org/abs/2508.19060)
*Blaž Rolih,Matic Fučka,Danijel Skočaj*

Main category: cs.CV

TL;DR: 本文提出了SuperSimpleNet，一种高效且适应性强的表面缺陷检测模型，可适用于无监督、弱监督、混合监督和全监督等多种工业实际标注场景，实现在各类基准数据集上高精度和极快推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的表面缺陷检测方法往往局限于特定标注和监督类型，难以同时兼顾高性能、效率及对多种数据标注的适应性，限制了其在真实工业环境中的应用能力。

Method: SuperSimpleNet基于SimpleNet架构，提出了新颖的合成异常生成、改进的分类头以及优化后的训练策略，使其能兼容所有四种监督场景，并充分利用各类标注数据。

Result: 在四个具有挑战性的基准数据集上，SuperSimpleNet在所有监督类型下均取得领先表现，推理速度快于10毫秒，展现了优异的精度与效率。

Conclusion: SuperSimpleNet实现了多监督统一兼容，同时保持高速度与可靠性，为实际制造业的缺陷检测及学术与工业间的应用落地提供了新的标杆和解决方案。

Abstract: Surface defect detection is a critical task across numerous industries, aimed
at efficiently identifying and localising imperfections or irregularities on
manufactured components. While numerous methods have been proposed, many fail
to meet industrial demands for high performance, efficiency, and adaptability.
Existing approaches are often constrained to specific supervision scenarios and
struggle to adapt to the diverse data annotations encountered in real-world
manufacturing processes, such as unsupervised, weakly supervised, mixed
supervision, and fully supervised settings. To address these challenges, we
propose SuperSimpleNet, a highly efficient and adaptable discriminative model
built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel
synthetic anomaly generation process, an enhanced classification head, and an
improved learning procedure, enabling efficient training in all four
supervision scenarios, making it the first model capable of fully leveraging
all available data annotations. SuperSimpleNet sets a new standard for
performance across all scenarios, as demonstrated by its results on four
challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an
inference time below 10 ms. With its ability to unify diverse supervision
paradigms while maintaining outstanding speed and reliability, SuperSimpleNet
represents a promising step forward in addressing real-world manufacturing
challenges and bridging the gap between academic research and industrial
applications. Code: https://github.com/blaz-r/SuperSimpleNet

</details>


### [61] [Learning Binary Sampling Patterns for Single-Pixel Imaging using Bilevel Optimisation](https://arxiv.org/abs/2508.19068)
*Serban C. Tudosie,Alexander Denker,Zeljko Kereta,Simon Arridge*

Main category: cs.CV

TL;DR: 本论文提出了一种用于单像素成像的新型二值照明模式学习方法，通过双层优化提升重建效果，尤其适用于荧光显微等实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 单像素成像技术利用单个探测器和结构光序列实现图像重建，但如何设计最优、任务相关的二值照明模式以提升重建质量，尤其在极端欠采样情况下，是一大挑战。

Method: 作者提出了一个双层优化框架，使用Straight-Through Estimator处理二值化不可微的问题，在优化过程中引入Total Deep Variation正则项，学习与特定任务相匹配的二值照明模式。该方法在CytoImageNet显微镜数据集上进行了验证。

Result: 实验表明，所提出算法学习到的二值照明模式，相较基线方法在图像重建质量上有明显提升，特别是在高欠采样情景下表现突出。

Conclusion: 方法验证了专为任务设计和优化二值照明模式的有效性，显著提升了单像素荧光显微成像等应用中的重建效果。

Abstract: Single-Pixel Imaging enables reconstructing objects using a single detector
through sequential illuminations with structured light patterns. We propose a
bilevel optimisation method for learning task-specific, binary illumination
patterns, optimised for applications like single-pixel fluorescence microscopy.
We address the non-differentiable nature of binary pattern optimisation using
the Straight-Through Estimator and leveraging a Total Deep Variation
regulariser in the bilevel formulation. We demonstrate our method on the
CytoImageNet microscopy dataset and show that learned patterns achieve superior
reconstruction performance compared to baseline methods, especially in highly
undersampled regimes.

</details>


### [62] [Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents](https://arxiv.org/abs/2508.19162)
*Rafael Sterzinger,Tingyu Lin,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文提出了一种高效的文本行分割方法，用于数字化文档，特别是历史文档，在仅使用极少标注数据的情况下大幅提高了分割准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法在文本行分割上依赖大量标注数据，但历史文档中标注数据昂贵稀缺，且标注过程复杂。因此需要少样本（few-shot）方法，降低数据及标注需求。

Method: 作者采用轻量级UNet++结构，结合专为连接性设计的loss函数（原为神经元形态设计），显式惩罚结构性错误（如行断裂、合并）。训练时仅从每份手稿中抽取三页做标注，进一步用小patch增加样本量。

Result: 在U-DIADS-TL数据集上，识别准确率提升200%，行IoU提升75%；在DIVA-HisDB基线检测任务上F-Measure成绩与冠军持平或超越，而只需三页标注数据，远优于现有方法。

Conclusion: 轻量网络+拓扑感知loss能有效提升少样本文档文本行分割准确率，极大降低人工标注成本，方案公开可复现，易于大规模实际应用。

Abstract: A foundational task for the digital analysis of documents is text line
segmentation. However, automating this process with deep learning models is
challenging because it requires large, annotated datasets that are often
unavailable for historical documents. Additionally, the annotation process is a
labor- and cost-intensive task that requires expert knowledge, which makes
few-shot learning a promising direction for reducing data requirements. In this
work, we demonstrate that small and simple architectures, coupled with a
topology-aware loss function, are more accurate and data-efficient than more
complex alternatives. We pair a lightweight UNet++ with a connectivity-aware
loss, initially developed for neuron morphology, which explicitly penalizes
structural errors like line fragmentation and unintended line merges. To
increase our limited data, we train on small patches extracted from a mere
three annotated pages per manuscript. Our methodology significantly improves
upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%
increase in Recognition Accuracy and a 75% increase in Line Intersection over
Union. Our method also achieves an F-Measure score on par with or even
exceeding that of the competition winner of the DIVA-HisDB baseline detection
task, all while requiring only three annotated pages, exemplifying the efficacy
of our approach. Our implementation is publicly available at:
https://github.com/RafaelSterzinger/acpr_few_shot_hist.

</details>


### [63] [Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding](https://arxiv.org/abs/2508.19165)
*Yuzhen Li,Min Liu,Yuan Bian,Xueping Wang,Zhaoyang Li,Gen Li,Yaonan Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种提升单目3D视觉定界任务中文本与几何信息匹配精度的新方法，显著提升了现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前单目3D视觉定界任务在处理具备精确几何描述的文本时，预训练语言模型对于数值大小敏感，却容易忽略度量单位的变化，导致同一物理长度仅因单位转换即性能大幅下降。这暴露了预训练语言模型对文本3D理解能力的薄弱。

Method: 提出了两项提升模型3D文本理解能力的方法：1）3D文本增强（3DTE）：通过多样化单位的距离描述，提升模型对不同长度单位映射关系的理解；2）文本引导的几何增强（TGE）：将文本特征投影到几何一致空间，以几何一致的文本特征引导模型对几何特征的关注。

Result: 在Mono3DRefer数据集上进行对比和消融实验，所提方法在多个评价场景下表现优异，在“Far”场景下准确率提升11.94%，刷新了最新的性能纪录。

Conclusion: 增强文本与几何特征的一致性对单目3D视觉定界任务至关重要，所提双重增强方法有效提高了系统在实际应用中的准确性。

Abstract: Monocular 3D visual grounding is a novel task that aims to locate 3D objects
in RGB images using text descriptions with explicit geometry information.
Despite the inclusion of geometry details in the text, we observe that the text
embeddings are sensitive to the magnitude of numerical values but largely
ignore the associated measurement units. For example, simply equidistant
mapping the length with unit "meter" to "decimeters" or "centimeters" leads to
severe performance degradation, even though the physical length remains
equivalent. This observation signifies the weak 3D comprehension of pre-trained
language model, which generates misguiding text features to hinder 3D
perception. Therefore, we propose to enhance the 3D perception of model on text
embeddings and geometry features with two simple and effective methods.
Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),
which enhances the comprehension of mapping relationships between different
units by augmenting the diversity of distance descriptors in text queries.
Next, we propose a Text-Guided Geometry Enhancement (TGE) module to further
enhance the 3D-text information by projecting the basic text features into
geometrically consistent space. These 3D-enhanced text features are then
leveraged to precisely guide the attention of geometry features. We evaluate
the proposed method through extensive comparisons and ablation studies on the
Mono3DRefer dataset. Experimental results demonstrate substantial improvements
over previous methods, achieving new state-of-the-art results with a notable
accuracy gain of 11.94\% in the "Far" scenario. Our code will be made publicly
available.

</details>


### [64] [Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions](https://arxiv.org/abs/2508.19167)
*Zhihang Xin,Xitong Hu,Rui Wang*

Main category: cs.CV

TL;DR: 提出了一种新的Weierstrass椭圆函数位置编码（WEF-PE）方法，提升了Vision Transformer的空间感知能力，实现了更优的表现。


<details>
  <summary>Details</summary>
Motivation: 现有ViT的位置编码方式破坏了图像的二维结构，且传统位置编码缺乏几何约束，难以有效利用空间接近性，限制了模型性能。

Method: 提出WEF-PE，利用椭圆函数直接编码二维坐标，利用其双周期和非线性几何特性，更自然地表达空间距离关系，且可直接通过代数加法公式计算相对位置。

Result: WEF-PE在CIFAR-100（从零训练达到63.78%准确率，微调93.28%）及VTAB-1k等基准任务上优于传统方法，并在理论上证明了距离衰减性质和提升了注意力的空间几何归纳偏置。

Conclusion: WEF-PE能有效增强Vision Transformer对空间结构的建模能力，带来更好的性能和更强的几何感知能力，为视觉任务中的位置编码提供了新的解决思路。

Abstract: Vision Transformers have demonstrated remarkable success in computer vision
tasks, yet their reliance on learnable one-dimensional positional embeddings
fundamentally disrupts the inherent two-dimensional spatial structure of images
through patch flattening procedures. Traditional positional encoding approaches
lack geometric constraints and fail to establish monotonic correspondence
between Euclidean spatial distances and sequential index distances, thereby
limiting the model's capacity to leverage spatial proximity priors effectively.
We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data. Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings. Comprehensive experiments demonstrate that
WEF-PE achieves superior performance across diverse scenarios, including
63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,
93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on
VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay
property through rigorous mathematical proof, while attention visualization
reveals enhanced geometric inductive bias and more coherent semantic focus
compared to conventional approaches.The source code implementing the methods
described in this paper is publicly available on GitHub.

</details>


### [65] [SoccerNet 2025 Challenges Results](https://arxiv.org/abs/2508.19182)
*Silvio Giancola,Anthony Cioppa,Marc Gutiérrez-Pérez,Jan Held,Carlos Hinojosa,Victor Joos,Arnaud Leduc,Floriane Magera,Karen Sanchez,Vladimir Somers,Artur Xarles,Antonio Agudo,Alexandre Alahi,Olivier Barnich,Albert Clapés,Christophe De Vleeschouwer,Sergio Escalera,Bernard Ghanem,Thomas B. Moeslund,Marc Van Droogenbroeck,Tomoki Abe,Saad Alotaibi,Faisal Altawijri,Steven Araujo,Xiang Bai,Xiaoyang Bi,Jiawang Cao,Vanyi Chao,Kamil Czarnogórski,Fabian Deuser,Mingyang Du,Tianrui Feng,Patrick Frenzel,Mirco Fuchs,Jorge García,Konrad Habel,Takaya Hashiguchi,Sadao Hirose,Xinting Hu,Yewon Hwang,Ririko Inoue,Riku Itsuji,Kazuto Iwai,Hongwei Ji,Yangguang Ji,Licheng Jiao,Yuto Kageyama,Yuta Kamikawa,Yuuki Kanasugi,Hyungjung Kim,Jinwook Kim,Takuya Kurihara,Bozheng Li,Lingling Li,Xian Li,Youxing Lian,Dingkang Liang,Hongkai Lin,Jiadong Lin,Jian Liu,Liang Liu,Shuaikun Liu,Zhaohong Liu,Yi Lu,Federico Méndez,Huadong Ma,Wenping Ma,Jacek Maksymiuk,Henry Mantilla,Ismail Mathkour,Daniel Matthes,Ayaha Motomochi,Amrulloh Robbani Muhammad,Haruto Nakayama,Joohyung Oh,Yin May Oo,Marcelo Ortega,Norbert Oswald,Rintaro Otsubo,Fabian Perez,Mengshi Qi,Cristian Rey,Abel Reyes-Angulo,Oliver Rose,Hoover Rueda-Chacón,Hideo Saito,Jose Sarmiento,Kanta Sawafuji,Atom Scott,Xi Shen,Pragyan Shrestha,Jae-Young Sim,Long Sun,Yuyang Sun,Tomohiro Suzuki,Licheng Tang,Masato Tonouchi,Ikuma Uchida,Henry O. Velesaca,Tiancheng Wang,Rio Watanabe,Jay Wu,Yongliang Wu,Shunzo Yamagishi,Di Yang,Xu Yang,Yuxin Yang,Hao Ye,Xinyu Ye,Calvin Yeung,Xuanlong Yu,Chao Zhang,Dingyuan Zhang,Kexing Zhang,Zhe Zhao,Xin Zhou,Wenbo Zhu,Julian Ziegler*

Main category: cs.CV

TL;DR: SoccerNet 2025提出了四项面向足球视频理解的计算机视觉挑战，涉及球动作识别、单目深度估计、多视角犯规识别和比赛状态重建，并公布了数据集、基线和评测。在此报告中，展示了各项挑战的成绩，并分析了社区进展。


<details>
  <summary>Details</summary>
Motivation: 推动足球视频理解方向的计算机视觉研究发展，通过持续且开放的挑战平台，促进学术界和工业界的技术交流，提高任务标准化与可复现性。

Method: 提出了四个挑战任务，并为每个任务提供大规模标注数据集、统一评测协议和强力基线。参赛者需针对不同任务开发模型并提交结果，通过排行榜进行评价。

Result: 报告总结了四项挑战的最终结果，介绍了表现最优的方案，并针对整体进展进行了讨论。

Conclusion: SoccerNet挑战为计算机视觉与体育人工智能领域的开放性与可复现性研究做出重要推动，并带动了学术和技术社区的共同进步。

Abstract: The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet
open benchmarking effort, dedicated to advancing computer vision research in
football video understanding. This year's challenges span four vision-based
tasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions
in football broadcasts and assigning actions to teams; (2) Monocular Depth
Estimation, targeting the recovery of scene geometry from single-camera
broadcast clips through relative depth estimation for each pixel; (3)
Multi-View Foul Recognition, requiring the analysis of multiple synchronized
camera views to classify fouls and their severity; and (4) Game State
Reconstruction, aimed at localizing and identifying all players from a
broadcast video to reconstruct the game state on a 2D top-view of the field.
Across all tasks, participants were provided with large-scale annotated
datasets, unified evaluation protocols, and strong baselines as starting
points. This report presents the results of each challenge, highlights the
top-performing solutions, and provides insights into the progress made by the
community. The SoccerNet Challenges continue to serve as a driving force for
reproducible, open research at the intersection of computer vision, artificial
intelligence, and sports. Detailed information about the tasks, challenges, and
leaderboards can be found at https://www.soccer-net.org, with baselines and
development kits available at https://github.com/SoccerNet.

</details>


### [66] [FastMesh:Efficient Artistic Mesh Generation via Component Decoupling](https://arxiv.org/abs/2508.19188)
*Jeonghwan Kim,Yushi Lan,Armando Fortes,Yongwei Chen,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种更高效的三角网格生成框架，通过分离顶点和面来减少重复，大幅提高生成速度，并提升网格质量。


<details>
  <summary>Details</summary>
Motivation: 当前主流三角网格生成方法在标记化时存在顶点复用，导致序列冗长、生成低效。作者希望解决高效表示和生成的问题，减少生成所需计算量。

Method: 首先，框架将顶点和面分开处理，仅对顶点用自回归模型生成，减少了大量重复token。然后，利用双向transformer结合顶点关系，一步生成面（邻接矩阵）。此外，增加了顶点精细化和后处理机制，以提升细节和去除不合理边。

Result: 实验显示，新方法相比最优现有方法，token数量仅为其约23%，速度提升超过8倍，且生成网格质量更高。

Conclusion: 分离顶点与面、自回归与双向建模结合的策略显著提升了生成效率与网格质量，方法实用性强，优于现有主流方法。

Abstract: Recent mesh generation approaches typically tokenize triangle meshes into
sequences of tokens and train autoregressive models to generate these tokens
sequentially. Despite substantial progress, such token sequences inevitably
reuse vertices multiple times to fully represent manifold meshes, as each
vertex is shared by multiple faces. This redundancy leads to excessively long
token sequences and inefficient generation processes. In this paper, we propose
an efficient framework that generates artistic meshes by treating vertices and
faces separately, significantly reducing redundancy. We employ an
autoregressive model solely for vertex generation, decreasing the token count
to approximately 23\% of that required by the most compact existing tokenizer.
Next, we leverage a bidirectional transformer to complete the mesh in a single
step by capturing inter-vertex relationships and constructing the adjacency
matrix that defines the mesh faces. To further improve the generation quality,
we introduce a fidelity enhancer to refine vertex positioning into more natural
arrangements and propose a post-processing framework to remove undesirable edge
connections. Experimental results show that our method achieves more than
8$\times$ faster speed on mesh generation compared to state-of-the-art
approaches, while producing higher mesh quality.

</details>


### [67] [All-in-One Slider for Attribute Manipulation in Diffusion Models](https://arxiv.org/abs/2508.19195)
*Weixin Ye,Hongguang Zhu,Wei Wang,Yahui Liu,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文提出了All-in-One Slider模块，实现了文本到图像（T2I）模型属性的高效、多样、灵活操控，可用于丰富细节内容（如人脸）且支持新属性的零样本编辑，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成模型难以实现对特定属性（如人脸细节）的渐进式、灵活操控；传统滑块方法为“一属性一滑块”，存在参数冗余、扩展性和灵活性不足问题。

Method: 设计了All-in-One Slider模块，将文本嵌入空间分解为稀疏且语义明确的属性方向，训练后可针对不同属性提供连续可解释的调控功能。该模块还支持重新组合属性方向，对未见过的新属性和多属性组合进行零样本操控。可无缝集成到反演框架，实现对真实图像的属性编辑。

Result: 大量实验表明该方法可精准、可扩展地操控各类属性，在细粒度、解释性和可拓展性上优于前人方法。还可在真实图像上实现属性编辑，拓宽实际应用场景。

Conclusion: All-in-One Slider有效提升了属性操控的精度和灵活性，扩展性强，并能直接应用于真实图像，实用价值高，相关代码公开。

Abstract: Text-to-image (T2I) diffusion models have made significant strides in
generating high-quality images. However, progressively manipulating certain
attributes of generated images to meet the desired user expectations remains
challenging, particularly for content with rich details, such as human faces.
Some studies have attempted to address this by training slider modules.
However, they follow a One-for-One manner, where an independent slider is
trained for each attribute, requiring additional training whenever a new
attribute is introduced. This not only results in parameter redundancy
accumulated by sliders but also restricts the flexibility of practical
applications and the scalability of attribute manipulation. To address this
issue, we introduce the All-in-One Slider, a lightweight module that decomposes
the text embedding space into sparse, semantically meaningful attribute
directions. Once trained, it functions as a general-purpose slider, enabling
interpretable and fine-grained continuous control over various attributes.
Moreover, by recombining the learned directions, the All-in-One Slider supports
zero-shot manipulation of unseen attributes (e.g., races and celebrities) and
the composition of multiple attributes. Extensive experiments demonstrate that
our method enables accurate and scalable attribute manipulation, achieving
notable improvements compared to previous methods. Furthermore, our method can
be extended to integrate with the inversion framework to perform attribute
manipulation on real images, broadening its applicability to various real-world
scenarios. The code and trained model will be released at:
https://github.com/ywxsuperstar/KSAE-FaceSteer.

</details>


### [68] [LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding](https://arxiv.org/abs/2508.19204)
*Julian Ost,Andrea Ramazzina,Amogh Joshi,Maximilian Bömer,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，可直接生成具有精确三维几何的驾驶场景，实现高真实感与可控性，并支持新的视角合成。


<details>
  <summary>Details</summary>
Motivation: 当前的神经重建方法能为机器人学习重建大规模户外场景，但受限于静态数据，仅能生成被捕获的数据和视角，缺乏可控性。而扩散模型可控性高却丧失物理几何基础。作者希望弥合两者的优缺点，生成几何精确、可编辑、真实的3D驾驶场景。

Method: 方法结合了代理几何和环境表示的生成，并引入2D图像先验的得分蒸馏，将2D模型的先验知识引入3D生成，实现几何可控和高保真纹理生成，可根据地图或布局进行条件生成。

Result: 该方法实现了可控性强、几何与纹理真实且一致的3D驾驶场景生成，支持根据输入提示词和地图布局定制复杂场景。

Conclusion: 方法有效结合了几何精确性与生成模型的可控能力，为机器人等领域带来更真实、多样和可编辑的大规模三维驾驶数据。

Abstract: Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.

</details>


### [69] [OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation](https://arxiv.org/abs/2508.19209)
*Jianwen Jiang,Weihong Zeng,Zerong Zheng,Jiaqi Yang,Chao Liang,Wang Liao,Han Liang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniHuman-1.5通过多模态LLM生成具备语义指导的人物动画，使动画与语音、图像、文本深度协同，实现高度自然且语义一致的动画表现，在唇动同步、视频质量、动作自然度等方面表现领先。


<details>
  <summary>Details</summary>
Motivation: 当前的视频虚拟人动画模型虽然能够生成流畅的物理姿态，但难以反映出角色的情感、意图等高层语义信息，动画多止于低层次节奏同步，缺乏真正的“神韵”。为此，作者希望生成既物理合理、又语义表达丰富的人物动画。

Method: 本方法核心包括两点：1）利用多模态大语言模型，生成结构化的语义条件，引导动画生成不仅仅受节奏限制，还能体现情绪和上下文；2）提出多模态DiT架构和创新的Pseudo Last Frame设计，高效融合音频、图像、文本等多模态信息，缓解模态冲突问题。

Result: 实验表明，OmniHuman-1.5在唇动同步准确性、视频质量、动作自然度、与文本语义一致性等综合指标上均领先。同时，该方法可良好扩展到多人和非人类角色等复杂场景。

Conclusion: OmniHuman-1.5实现了跨模态、具备高语义一致性和表达力的人物动画生成，推动了虚拟人动画从物理层面向语义层面的跃升，对复杂场景的适配能力强，具有广泛应用前景。

Abstract: Existing video avatar models can produce fluid human animations, yet they
struggle to move beyond mere physical likeness to capture a character's
authentic essence. Their motions typically synchronize with low-level cues like
audio rhythm, lacking a deeper semantic understanding of emotion, intent, or
context. To bridge this gap, \textbf{we propose a framework designed to
generate character animations that are not only physically plausible but also
semantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, is
built upon two key technical contributions. First, we leverage Multimodal Large
Language Models to synthesize a structured textual representation of conditions
that provides high-level semantic guidance. This guidance steers our motion
generator beyond simplistic rhythmic synchronization, enabling the production
of actions that are contextually and emotionally resonant. Second, to ensure
the effective fusion of these multimodal inputs and mitigate inter-modality
conflicts, we introduce a specialized Multimodal DiT architecture with a novel
Pseudo Last Frame design. The synergy of these components allows our model to
accurately interpret the joint semantics of audio, images, and text, thereby
generating motions that are deeply coherent with the character, scene, and
linguistic content. Extensive experiments demonstrate that our model achieves
leading performance across a comprehensive set of metrics, including lip-sync
accuracy, video quality, motion naturalness and semantic consistency with
textual prompts. Furthermore, our approach shows remarkable extensibility to
complex scenarios, such as those involving multi-person and non-human subjects.
Homepage: \href{https://omnihuman-lab.github.io/v1_5/}

</details>


### [70] [Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth](https://arxiv.org/abs/2508.19232)
*Kaveh Safavigerdini,Ramakrishna Surya,Jaired Collins,Prasad Calyam,Filiz Bunyak,Matthew R. Maschmann,Kannappan Palaniappan*

Main category: cs.CV

TL;DR: 本文提出了一种名为VFTrack的实时纳米颗粒追踪框架，实现了对扫描电镜（SEM）图像序列中碳纳米管（CNT）颗粒的自动检测和追踪，显著提升了CNT动态生长表征的自动化与高效性。


<details>
  <summary>Details</summary>
Motivation: 目前对CNT生长的动态表征受限于实验测量手段。传统的ex situ技术只能提供静态分析，而in situ技术往往需要人工初始化，且缺乏对单颗粒运动轨迹的连续分解，因此迫切需要新的自动化粒子追踪方法以实现对纳米材料生长过程的精准动态监测。

Method: 本文提出了VFTrack，一种结合手工和深度特征检测器及匹配器的粒子追踪框架。利用13,540条人工标注轨迹，系统评估了多种特征检测与匹配器组合，最终确定ALIKED检测器与LightGlue匹配器效果最佳（F1分数0.78，α分数0.89）。VFTrack可以将运动矢量分解为轴向生长、横向漂移和振荡，用于区域生长速率计算和CNT形貌重构。

Result: VFTrack实现了对CNT微柱生长过程的实时运动追踪和分解监测，大幅提升了实验自动化能力。同时，评测结果显示ALIKED+LightGlue组合具备最高的准确性与鲁棒性。

Conclusion: VFTrack有效推动了CNT等纳米材料的自动化表征，为物理建模与实验观测之间架起桥梁，为CNT合成的实时优化奠定基础，有望加速纳米材料领域的发展。

Abstract: Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yet
the characterization of their dynamic growth is limited by the experimental
challenges in nanoscale motion measurement using scanning electron microscopy
(SEM) imaging. Existing ex situ methods offer only static analysis, while in
situ techniques often require manual initialization and lack continuous
per-particle trajectory decomposition. We present Visual Feature Tracking
(VFTrack) an in-situ real-time particle tracking framework that automatically
detects and tracks individual CNT particles in SEM image sequences. VFTrack
integrates handcrafted or deep feature detectors and matchers within a particle
tracking framework to enable kinematic analysis of CNT micropillar growth. A
systematic using 13,540 manually annotated trajectories identifies the ALIKED
detector with LightGlue matcher as an optimal combination (F1-score of 0.78,
$\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth,
lateral drift, and oscillations, facilitate the calculation of heterogeneous
regional growth rates and the reconstruction of evolving CNT pillar
morphologies. This work enables advancement in automated nano-material
characterization, bridging the gap between physics-based models and
experimental observation to enable real-time optimization of CNT synthesis.

</details>


### [71] [Autoregressive Universal Video Segmentation Model](https://arxiv.org/abs/2508.19242)
*Miran Heo,Sukjun Hwang,Min-Hung Chen,Yu-Chiang Frank Wang,Albert Gu,Seon Joo Kim,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 本文提出了一种名为AUSM的自回归通用分割模型，统一了解决有提示和无提示的视频分割任务，并在多个基准数据集上超过了以往方法且训练速度大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割模型往往只针对有提示或无提示两类任务，实际应用中常需要无提示分割，但这方面方案分散和低效，缺乏通用统一的方法。

Method: 作者将流视频分割任务重构为类似于语言建模的序列掩膜预测任务，提出基于最新时空状态空间模型的AUSM，并对模型结构做了剪裁和优化，支持跨帧并行训练，能够处理任意长度的视频序列。

Result: AUSM在DAVIS17、YouTube-VOS、MOSE、YouTube-VIS和OVIS等主流数据集上效果优于同类通用分割方法，且16帧序列训练速度提升最高达2.5倍。

Conclusion: AUSM为视频分割领域提供了统一、高效的新范式，兼容有提示和无提示两类任务，训练过程显著加速，具备实际应用潜力。

Abstract: Recent video foundation models such as SAM2 excel at prompted video
segmentation by treating masks as a general-purpose primitive. However, many
real-world settings require unprompted segmentation that aims to detect and
track all objects in a video without external cues, leaving today's landscape
fragmented across task-specific models and pipelines. We recast streaming video
segmentation as sequential mask prediction, analogous to language modeling, and
introduce the Autoregressive Universal Segmentation Model (AUSM), a single
architecture that unifies both prompted and unprompted video segmentation.
Built on recent state-space models, AUSM maintains a fixed-size spatial state
and scales to video streams of arbitrary length. Furthermore, all components of
AUSM are designed for parallel training across frames, yielding substantial
speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS
2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior
universal streaming video segmentation methods and achieves up to 2.5x faster
training on 16-frame sequences.

</details>


### [72] [Style4D-Bench: A Benchmark Suite for 4D Stylization](https://arxiv.org/abs/2508.19243)
*Beiqi Chen,Shuai Shao,Haitang Feng,Jianhuang Lai,Jianlou Si,Guangcong Wang*

Main category: cs.CV

TL;DR: 本论文提出了Style4D-Bench，这是首个专为4D风格化任务设计的基准套件，并提供了Style4D作为强力基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前4D风格化（动态3D场景的风格转移）缺乏统一的评测标准和公开数据集， hindering新方法的评估与研发。因此，作者希望通过设计一个系统性的基准和高效基线模型来推动该领域进步。

Method: 1. 搭建Style4D-Bench，包含评估协议（空间、时间、多视图一致性，感知与量化指标）、高分辨率4D动态场景数据集。2. 设计Style4D基线模型，核心包括4D高斯Splatting场景表示、基于每个高斯MLP的风格表达机制、对比一致性和结构保持的风格迁移模块。

Result: 在Style4D-Bench上，以Style4D模型为基线进行了广泛实验，结果显示其在空间细节、时间稳定性和多视图一致性方面均达到或超过当前最新水平。

Conclusion: Style4D-Bench为4D风格化领域建立了标准化评测体系和高质量数据集，同时Style4D提供了性能优越的基线方法，有望推动相关研究发展。

Abstract: We introduce Style4D-Bench, the first benchmark suite specifically designed
for 4D stylization, with the goal of standardizing evaluation and facilitating
progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive
evaluation protocol measuring spatial fidelity, temporal coherence, and
multi-view consistency through both perceptual and quantitative metrics, 2) a
strong baseline that make an initial attempt for 4D stylization, and 3) a
curated collection of high-resolution dynamic 4D scenes with diverse motions
and complex backgrounds. To establish a strong baseline, we present Style4D, a
novel framework built upon 4D Gaussian Splatting. It consists of three key
components: a basic 4DGS scene representation to capture reliable geometry, a
Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for
temporally and spatially aware appearance control, and a Holistic
Geometry-Preserved Style Transfer module designed to enhance spatio-temporal
consistency via contrastive coherence learning and structural content
preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D
achieves state-of-the-art performance in 4D stylization, producing fine-grained
stylistic details with stable temporal dynamics and consistent multi-view
rendering. We expect Style4D-Bench to become a valuable resource for
benchmarking and advancing research in stylized rendering of dynamic 3D scenes.
Project page: https://becky-catherine.github.io/Style4D . Code:
https://github.com/Becky-catherine/Style4D-Bench .

</details>


### [73] [Articulate3D: Zero-Shot Text-Driven 3D Object Posing](https://arxiv.org/abs/2508.19244)
*Oishi Deb,Anjun Hu,Ashkan Khakzar,Philip Torr,Christian Rupprecht*

Main category: cs.CV

TL;DR: 提出了一种无需训练即可通过文本控制3D资产姿态的方法Articulate3D，并通过输入图片和文本指令来操控3D物体姿态，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前结合视觉与语言模型，直接用自然语言控制3D物体姿态仍非常具有挑战性；如何更自然、灵活地操控3D对象姿态，是计算机视觉与图形领域的重要问题。

Method: 1. 将任务分为两步：先修改一强大图像生成器，根据输入图片和文本指令生产目标图片；2. 设计自注意力重连机制（RSActrl），实现图像生成模型中结构与姿态的解耦，从而不同姿态下能保持结构一致性；3. 利用关键点关联输入与目标图片，实现多视角姿态优化（不是依赖可微渲染）。

Result: Articulate3D能在多种3D对象和任意文本提示下，实现姿态操控且保持原有结构，在定量实验和用户调查中表现出优越性，其中用户偏好超过85%。

Conclusion: Articulate3D为训练自由、文本控制3D姿态提供了有效解决方案，在多场景和对象下均优于现有方法，有望推动3D资产智能编辑的发展。

Abstract: We propose a training-free method, Articulate3D, to pose a 3D asset through
language control. Despite advances in vision and language models, this task
remains surprisingly challenging. To achieve this goal, we decompose the
problem into two steps. We modify a powerful image-generator to create target
images conditioned on the input image and a text instruction. We then align the
mesh to the target images through a multi-view pose optimisation step. In
detail, we introduce a self-attention rewiring mechanism (RSActrl) that
decouples the source structure from pose within an image generative model,
allowing it to maintain a consistent structure across varying poses. We
observed that differentiable rendering is an unreliable signal for articulation
optimisation; instead, we use keypoints to establish correspondences between
input and target images. The effectiveness of Articulate3D is demonstrated
across a diverse range of 3D objects and free-form text prompts, successfully
manipulating poses while maintaining the original identity of the mesh.
Quantitative evaluations and a comparative user study, in which our method was
preferred over 85\% of the time, confirm its superiority over existing
approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/

</details>


### [74] [VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space](https://arxiv.org/abs/2508.19247)
*Lin Li,Zehuan Huang,Haoran Feng,Gengxiong Zhuang,Rui Chen,Chunchao Guo,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为VoxHammer的创新性3D局部编辑方法，能够在三维潜空间实现精确且一致的区域编辑，显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D局部编辑方法往往通过渲染多视图图像再重建3D模型，难以准确保护未编辑区域并保持整体一致性。为克服这些挑战，论文受结构化3D生成模型启发，探索更精准且保真的编辑机制。

Method: VoxHammer无需训练，直接在3D潜空间进行编辑：首先对给定的3D模型预测其反演轨迹、获得反演潜变量和关键值token；编辑时，将需保留区域的去噪特征替换为反演潜变量和缓存的key-value token，从而在局部编辑中精确保留上下文特征。论文还构建了Edit3D-Bench数据集，对保护区域的一致性进行评测。

Result: 实验显示，VoxHammer在保留区域的3D一致性和整体编辑质量上显著优于现有方法。

Conclusion: VoxHammer为高质量3D局部编辑、生成配对数据奠定基础，有望推动上下文感知的3D生成领域发展。

Abstract: 3D local editing of specified regions is crucial for game industry and robot
interaction. Recent methods typically edit rendered multi-view images and then
reconstruct 3D models, but they face challenges in precisely preserving
unedited regions and overall coherence. Inspired by structured 3D generative
models, we propose VoxHammer, a novel training-free approach that performs
precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer
first predicts its inversion trajectory and obtains its inverted latents and
key-value tokens at each timestep. Subsequently, in the denoising and editing
phase, we replace the denoising features of preserved regions with the
corresponding inverted latents and cached key-value tokens. By retaining these
contextual features, this approach ensures consistent reconstruction of
preserved areas and coherent integration of edited parts. To evaluate the
consistency of preserved regions, we constructed Edit3D-Bench, a
human-annotated dataset comprising hundreds of samples, each with carefully
labeled 3D editing regions. Experiments demonstrate that VoxHammer
significantly outperforms existing methods in terms of both 3D consistency of
preserved regions and overall quality. Our method holds promise for
synthesizing high-quality edited paired data, thereby laying the data
foundation for in-context 3D generation. See our project page at
https://huanngzh.github.io/VoxHammer-Page/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 本文提出了一种基于复数意义空间中语义吸引子的AGI理论框架，实现对当前基于统计预测的语言模型的替代。


<details>
  <summary>Details</summary>
Motivation: 当前transformer等语言模型依赖于下一个token的概率预测，难以实现真正的语义理解，尤其是像讽刺、歧义等复杂语言现象。作者期望构建一种能主动塑造意义、提升表达深度的AGI新架构。

Method: 文中引入了以复数空间中的张量递归变换为核心的语义框架，利用虚数单位i进行循环变换，以模型中的“语义吸引子”作为指导意义稳定与澄清的目的因子，通过梯度流、张量形变及迭代矩阵动力学实现语义转化。

Result: 该理论模型可对讽刺、同音异义、歧义等复杂语义进行数学建模，并实现意义的收敛与深化，比传统统计模拟更具有哲学内涵。

Conclusion: 作者认为意义的真实生成依赖于通向语义一致性的递归收敛过程，需要全新的认知架构，不只是语言模拟或预测。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [76] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 本文提出了一个名为KAIROS的新基准，用于研究大型语言模型（LLMs）在多智能体系统（MAS）下如何建立信任、抵抗错误信息及融合同伴意见，并评估了多种提升集体智能的策略。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注于LLMs的从众偏差，但对它们在复杂群体互动环境中，如何基于过往印象建立信任、抵御假消息和同伴影响、融合同伴意见的机制仍缺乏系统性分析。本研究旨在弥补这一空白，推动集体智慧中的关键社交机制理解与优化。

Method: 作者提出KAIROS基准，通过模拟包含不同可靠性同伴的知识竞赛（如专家-新手、噪声拥挤、敌对同伴等），让LLMs接收历史互动和当前同伴反馈，测试及分析信任、同伴行为与自信对决策的影响。同时，对比评估了提示工程（prompting）、有监督微调、强化学习中的群体相对策略优化（GRPO）等多种提升策略。

Result: 实验显示，结合多智能体信息和基于结果奖励、无约束推理的GRPO方法取得了最佳整体表现。但是，这一方法相比基础模型，降低了系统对社会影响的鲁棒性。

Conclusion: KAIROS为分析LLMs在多智能体互动下的复杂社交行为提供了新的工具和洞见。尽管GRPO增强了集体决策能力，但需要权衡社会影响的鲁棒性。该基准及方法有望推动进一步的集体智能与社交AI研究。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [77] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本文介绍了针对非拉丁文字主导的多语言网站可访问性的挑战，并提出了新数据集LangCrUX和自动化评测工具Kizuki。


<details>
  <summary>Details</summary>
Motivation: 网络内容日益多语言化，许多网站将英语与本地语言混合使用。然而，现有辅助技术对非拉丁文字支持有限，给视力障碍用户带来极大不便。缺乏针对多语言网站的可访问性大规模数据和系统研究，制约了解决这一挑战的进展。

Method: 作者构建了LangCrUX数据集，包含12种主要为非拉丁文字的语言、共12万个受欢迎网站。基于该数据集，系统分析了多语言网页的可访问性，并考察网站可访问性提示（如语言声明）对于多语言内容的反映。作者还提出了Kizuki，一个以语言感知为核心的自动可访问性测试扩展工具，用于检测和改进语言不一致下的可访问性提示。

Result: 系统性分析发现，网页的可访问性提示普遍忽视实际的语言多样性，与页面真实内容不符，导致屏幕阅读器效果不佳，限制了网络的可访问性。

Conclusion: 多语言网页的可访问性提示严重不足，亟需改进。通过数据集和语言感知工具，可推动辅助技术更好地服务于多语言、多文字环境下的无障碍访问。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [78] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本论文提出PLAST方法，通过精确微调部分层提升大规模视觉-语言模型（LVLMs）的多语言能力，在保持参数高效的同时提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLMs在视觉与人类语言理解方面表现出色，但在多语言能力上存在不平衡问题。作者希望深入理解LVLMs的多语言机制，并提升其多语言表现。

Method: 作者分析LVLMs的工作机制，发现模型多语言理解与浅层的语言特定神经元激活有关。基于此，提出PLAST方法：首先监测神经元激活找出与多语言理解相关的层，然后只对这些层进行基于问句-翻译对的精确微调，从而实现多语言对齐。

Result: 实验证明，PLAST方法在MM-Bench和MMMB等多语言评测集上，有效提升了LVLMs多语言能力，并且微调参数仅占原始参数的14%。

Conclusion: PLAST不仅提升了LVLMs的多语言能力，还兼顾了参数效率，且对低资源和复杂视觉推理任务有良好的泛化能力，突显浅层语言特定信息的重要性。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [79] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 本论文提出了一种名为backprompting的方法，用于生成类似实际生产环境的标注数据，帮助提升大型语言模型（LLMs）健康建议防护机制的效果。并通过稀疏人工参与的聚类法进行辅助标注，提升检测器鲁棒性，取得了优于现有解决方案的性能。


<details>
  <summary>Details</summary>
Motivation: 企业在广泛应用LLM的过程中，面临其输出潜在风险，尤其是难以获得高质量真实输出的标注数据，这对守护栏（guardrails）技术的研发构成重大挑战，尤其是在如健康建议等敏感领域。

Method: 作者提出backprompting方法，自动生成接近生产环境的带标注数据，然后结合人工参与的聚类技术，对数据进行辅助标注，并将生成的数据与已有数据集融合，用来训练检测器。

Result: 使用此方法开发的检测器相较于其他解决方案，以及参数数量远超的GPT-4o，准确率提升最高可达3.73%。

Conclusion: 通过自动化样本生成及高效标注流程，可以有效提升LLM守护栏检测器在实际应用中的表现，并降低对大规模人工标注和模型参数的依赖。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [80] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 提出了Integral Transformer，一种新的自注意力机制，通过积分采样的方法降低注意力噪声，在多个知识和推理任务上优于传统以及近期方法。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力容易赋予不重要的符号（如特殊符号、标点）过高权重，产生噪声且影响模型表现；近期方法虽引入负注意力缓解此问题，但可能丢弃有用信息。需要方法在降噪和信息保留间找到平衡。

Method: 设计Integral Transformer，将注意力分数视作分布并对采样信号进行积分，既减少对无信息token的关注，又保留特殊token对性能的关键贡献。

Result: 在多个主流知识及推理类语言任务中，Integral Transformer超过了 vanilla、Cog Attention 和 Differential Attention。此外，分析显示底层采用vanilla自注意力，上层用Integral Transformer可更好平衡注意力分布并减少秩塌陷。

Conclusion: Integral Transformer在有效降低注意力噪声的同时，避免有用信息丢失，理论与实证结果显示其优于现有自注意力变种，未来Transformer模型结构可借鉴其层间动态配置。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [81] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的自洽性选择方法——Latent Self-Consistency（LSC），在LLM推理任务中同时提升了短和长答案的一致性，并且几乎不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的自洽性方法（如SC、USC、WUCS）在提升长答案一致性时往往牺牲了短答案的准确度，且仍存在输出不一致的问题，需要一种兼顾所有类型答案、计算高效的方法。

Method: LSC方法通过学习到的token嵌入，对生成的候选答案进行语义级一致性筛选；具体实现是在推理时微增前向生成步骤，无需更改原有模型结构。

Result: LSC在6个短答案和5个长答案推理基准（如MATH, MMLU, TruthfulQA）上均优于SC、USC和WUCS方法，既提升了一致性，又维持低计算开销。此外，LSC还能提供校准良好的置信度估计。

Conclusion: LSC是一种通用、高效、易部署（几乎无额外成本）、在短长答案场景下都表现优异的一致性选择方法。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [82] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文质疑了人工智能中常用的通过分布外（OOD）数据集评估模型泛化能力的方法，发现这些评估与真实应用中的失效模式并不总是一致。研究指出，不同的OOD数据集对QA模型鲁棒性的评估质量差异大，甚至可能不如简单的分布内评估。


<details>
  <summary>Details</summary>
Motivation: 当前普遍采用OOD数据评估AI模型泛化能力，但默认其能反映模型在真实环境中的失效情况。本研究质疑这一假设，并希望通过具体的QA模型失效案例（如依赖虚假特征或预测捷径），验证现有评估方法的有效性。

Method: 作者对现有QA模型在不同OOD数据集和分布内评估下的表现进行了对比分析，重点考察模型对虚假特征和预测捷径的鲁棒性，并分析了不同数据集在训练和评估时的表现联系。

Result: 发现不同的OOD数据集估算模型对捷径依赖的鲁棒性质量有巨大差异，有些甚至还不如分布内评估。同时发现训练和评估用数据集的质量在某些情况下关联不大。

Conclusion: 基于OOD的泛化评估存在局限，不能完全反映模型的真实鲁棒性。论文提出了更健壮的泛化评估方法论和建议，强调在QA及更广泛领域中，需谨慎采用OOD评估并结合其它方法以全面衡量模型泛化能力。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [83] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在重排序任务中的语义理解和可解释性能力，并分析了不同训练方法对其影响。


<details>
  <summary>Details</summary>
Motivation: LLMs尽管在与人类价值观的对齐及语义理解上表现提升，但以牺牲透明性为代价，尤其在新系统和数据有限场景下准确重排序十分困难。用户需要模型提供可解释的推理以做出明智决策，但现有方法似乎并未真正学到准确的语义理解，更多只是追求评价指标最优化，这导致对模型可靠性的质疑。

Method: 作者比较了不同LLMs训练方法对重排序语义理解的影响，并使用了一个较小、来自环境与地球科学领域的排名数据集做实证分析。同时分析了模型输出的可解释推理文本，看其重排序决策能否被合理解释。

Result: 结果发现，不同训练方法产生的LLMs在解释能力和重排序表现上存在差异；部分训练方法能够产出更具可解释性的推理文本，而一些方法仅获得了抽象知识并未真正理解语义关系。

Conclusion: 研究强调了训练方法对提升LLMs可解释性和语义理解能力的重要性，并指出当前LLMs在可解释重排序任务上仍有改进空间，须警惕模型仅以优化评价指标为目标而非真实的语义理解。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [84] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 本文关注波兰语中由历史和政治因素导致的性别偏见问题，提出采用IPIS数据集对大语言模型进行微调，以促进波兰语生成任务中的性别包容性。


<details>
  <summary>Details</summary>
Motivation: 波兰语存在男性词语泛指男性、女性或混合性别群体的不公平现象，这使得用波兰语训练的大语言模型表现出男性偏向，强化了性别不平等。为了减少由语言系统带来的社会偏见，有必要提升模型的性别包容性。

Method: 作者基于理论语言学框架，采用IPIS数据集（含有人类手工编写的性别包容性校对和波兰语-英语翻译指令）对多语言模型（如Llama-8B、Mistral-7B、Mistral-Nemo）及波兰语专用模型（Bielik和PLLuM）进行系统化的微调，并设计了明确性别包容性的系统提示词。

Result: 经过IPIS数据集微调，模型在波兰语文本生成时表现出更平衡的性别包容性，系统性缓解了原有的性别倾向性。

Conclusion: 本研究提出的方法为波兰语大语言模型实现性别包容性提供了系统性解决方案，有助于减少模型在实际应用中的性别偏见。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [85] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文提出利用多重假设检验方法检测大语言模型（LLM）生成文本中的幻觉（hallucination），并通过实验验证了该方法相较于现有方法的优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其产生的幻觉问题引起了关注。幻觉导致模型输出自信但错误的信息，影响其可靠性。因此，如何有效检测并应对幻觉成为亟需解决的问题。

Method: 作者将幻觉检测问题建模为假设检验问题，并借鉴机器学习中分布外检测的思想，创新性地引入多重检验方法用于识别幻觉内容。

Result: 通过大量实验，作者证明所提出的方法在检测性能和稳健性上优于当前主流的幻觉检测方法。

Conclusion: 多重假设检验方法可以有效提升幻觉检测的准确率和鲁棒性，对提升LLM的实用性和安全性具有重要意义。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [86] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 该论文提出两种改进的自动化翻译评测指标，通过加入额外的翻译信息提升评测与人工判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 目前自动机器翻译评测方法通常只考虑源句和单一译文，未能像人工评测那样结合多种备选翻译进行综合判断，由此影响自动评测指标的表现。

Method: 提出了COMET-polycand和COMET-polyic两种新型评测指标。COMET-polycand利用同源句的其他备选译文进行对比评估；COMET-polyic借鉴检索式in-context learning方法，结合了相似源句对应译文及其人工评分，引导质量评价。

Result: 在COMET-polycand中，通过加入一个额外译文，段级Kendall's tau-b相关系数由0.079提升至0.118，更多备选译文可进一步提升。在COMET-polyic中，结合检索样例后相关系数由0.079升至0.116。

Conclusion: 引入更多参考或相关翻译信息能显著提升自动翻译评测指标对人工评价的拟合能力，所研发模型已公开发布。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [87] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文提出了一种自评估的视觉隐喻生成框架，能提升生成图像与输入隐喻文本的一致性，并通过结构化提示和轻量化强化学习实现优势表现。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成需要将语言理解与视觉呈现结合，现有方法难以有效对齐文本隐喻的源-目标-意义映射，且缺乏优质的自动评测手段。

Method: 作者提出一个融合自我评估的新框架，包含两种创新方法：一是训练自由、利用源-目标-意义（S-T-M）结构化分解生成提示的管线；二是基于自评奖励机制、无需大规模重训练的小规模训练增强管线。框架结合了新提出的隐喻分解分数与意义对齐度（MA）指标。

Result: 在测试集中，训练自由的管线在分解、CLIP和MA等指标上优于GPT-4o、Imagen等强大闭源模型，训练增强管线表现略落后。用户调研显示GPT-4o综合最受欢迎，但作者管线在抽象隐喻任务和开源方法中占优。

Conclusion: 结构化提示和轻量强化学习能高效提升视觉隐喻对齐效果，但与人类偏好还存在美学和采样相关的差距，未来应进一步优化采样策略和美学表现。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [88] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）究竟在模拟什么，以及它们与人类语言能力的关联，作者主张LLM主要建模的是训练语料，而非人类认知本身。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型（如transformer架构的模型）模拟语言能力的本质存在争议：它们是否模拟了人类认知，或者只是学习了大量语料的模式？澄清这一问题对于理解AI与人类智能的异同至关重要。

Method: 作者提出与认知科学的理论对比，指出人类语言能力依赖超线性（supralinear）计算格式，而transformer架构仅能实现线性计算格式。分析侧重于transformer结构的计算不变性，并结合Liu等人（2022）关于“捷径自动机（shortcut automata）”的假设，讨论LLM的实际运行机制。

Result: 通过理论分析和与认知科学的比较，作者认为transformer在计算能力上与人类大脑存在根本差异，且更可能是在建模其训练语料而非人类认知机制。捷径自动机的概念进一步支持了这一观点，即transformer倾向采用语料中的模式和“捷径”来生成语言。

Conclusion: 作者强调，尽管LLM工作的原理不同于人类思维，但这并不意味着其能力微不足道（非贬义）。语言不仅是表达思想的工具，还是制造新话语的“机器”；虽然人类和LLM最终学会运用语言，但过程和实现机制存在本质差异。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [89] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一种新型神经机器翻译系统NOOV，能在缺乏足够医学领域平行语料的情况下，将英文电子健康记录（EHR）高效翻译为西班牙语。


<details>
  <summary>Details</summary>
Motivation: 医学领域特别是EHR的英文到西班牙语翻译非常重要，但由于缺乏平行语料和大量生僻词，该任务很具挑战。

Method: NOOV系统融合了通过平行语料自动学习获得的双语词典和从大型生物医学知识库提取的短语查找表，有效解决了神经机器翻译中的生僻词（OOV）和词语重复问题，从而优化医学短语翻译。

Result: 评估结果显示，NOOV能够显著提升EHR翻译的准确性和流畅性。

Conclusion: NOOV系统在缺乏充足领域平行语料的环境下，提升了EHR翻译质量，对医学行业的机器翻译应用具有实际价值和推广前景。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [90] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 该论文系统分析了后训练量化（PTQ）对大语言模型知识能力的影响，提出任务分层的扩展定律，并给出针对性量化建议。


<details>
  <summary>Details</summary>
Motivation: 虽然PTQ是缩减大语言模型规模和部署成本的有效方法，但其对不同知识能力（如记忆和应用）的精确影响尚不清楚。现有定律也忽略了一些PTQ及任务相关的重要参数。

Method: 作者通过实证研究，按照任务类型详细分析模型在记忆和利用两大知识能力上的量化敏感性，建立统一的定量框架，考虑了模型规模、有效比特宽度、校准集大小和group size等因素。

Result: 发现知识记忆能力对于比特宽度、校准集规模和模型规模的变化更加敏感，而知识利用能力表现更为稳健。

Conclusion: 结果帮助深入理解PTQ细致影响，并为设计知识保持更好的量化策略提供了理论依据和实践指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [91] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种提升大语言模型（LLMs）复杂推理能力的新方法，通过在推理步骤间主动生成“洞见”（insight）引导模型推理过程，并设计了自动化的例子收集与过滤流程，实验验证了其在数学任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学等复杂推理任务上表现不佳，主要原因是其训练数据缺乏人类在推理过程中使用的内在思考流程与关键洞见。人为的推理更为谨慎，但很少表达这些隐含的思路，导致模型训练数据中缺乏关键的推理桥梁。

Method: 文章提出了一种名为“Thinking Before You Speak（TBYS）”的推理框架。在每两个连续推理步骤之间，主动插入由模型生成的“洞见”，用以回顾当前状态并启动下一步推理。同时，作者设计了自动化的流程，用于收集和筛选带有洞见的上下文示例，减轻了人工标注和微调的难度。

Result: 在多个具有挑战性的数学数据集测试中，TBYS方法均提升了推理效果，实验结果支持所提方法的有效性。

Conclusion: 主动生成并插入洞见以引导推理，可有效提升LLMs在复杂推理任务（如数学题）上的表现，且自动化流程减少了人工干预成本，具有实用推广价值。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [92] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 本文提出了一种称为CoDe（Collaborative Decoding）的解码新方法，通过智能整合带外部知识与不带外部知识的输出概率，提升大模型生成文本的真实性（faithfulness）与表达性（expressiveness），减少幻觉（hallucination）现象。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型即使有能力结合外部知识，也难以在确保表达真实同时保证流畅表达，两者之间经常需要权衡。为解决该模型输出要么缺乏知识支撑、要么表达生硬冗长的问题，作者希望打破“真实性与表达性”的二元对立。

Method: 提出CoDe（Collaborative Decoding），动态融合带知识和不带知识的输出概率，依据分布差异与模型置信度有选择地激活模型潜在表达；并设计了知识感知重排序机制，有效规避对模型内在知识的过度依赖，实现合理利用外部知识。

Result: 实验证明，CoDe作为即插即用模块，能广泛提升不同LLM在多项评测中的结果，在增强事实准确性的同时不损害输出的表达能力，验证了方法的有效性和通用性。

Conclusion: CoDe有效平衡了大模型输出的真实性与表达性，减少了幻觉现象，对大模型外部知识整合具有实际意义，且框架兼容性和推广性较强。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [93] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了Emotion Omni模型和数据生成管线，实现了在数据有限和无需大规模训练的情况下，理解和生成具备情感响应的语音助手系统。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型虽然支持语音交互，但大多不能充分理解用户语音中的情感及副语言信息，导致用户体验下降，也需要大量数据与算力训练。如何高效地在有限数据下实现具有同理心的语音生成模型是一个挑战。

Method: 作者提出了一种新的模型架构Emotion Omni，能够分析用户语音输入中的情感，并生成有同理心的语音响应。此外，基于开源TTS框架开发数据生成管道，合成了20万条带情感色彩的对话数据，用来支持模型训练。

Result: 通过Emotion Omni模型与自制情感数据集，实现了更加具备情感理解和回应能力的语音助手，验证了方法在小数据规模下的有效性。相关演示已开放。

Conclusion: Emotion Omni为在有限数据情况下训练具有情感理解力的语音助手提供了新途径，显著降低了训练成本，并提升了人机交互体验。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [94] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种通过平衡样本难度来优化多模态链式思维（MCoT）提示示例选择的方法，从而显著提升多模态大模型的推理能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有MCoT方法常依赖随机或手工选择示例，忽略了模型感知难度和数据本身复杂度，导致提示效果不稳定、性能不佳。因此需要一种有理论支撑且能自适应模型能力的示例选择框架。

Method: 作者提出将提示选择视作课程设计问题，结合模型自身在主动学习框架下的预测分歧（模型感知难度）和样本的本质复杂度这两个信号，采用难度平衡抽样，生成既符合模型当前水平又覆盖多样性的高效提示集。

Result: 在5个有挑战性的基准数据集和多个流行的多模态大模型上进行大量实验证明，该方法相比随机采样大幅提升了模型推理性能，并显著降低了性能波动。

Conclusion: 采用难度平衡的课程式提示设计，为提升多模态模型的推理能力和稳定性提供了有效、鲁棒的解决方案。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [95] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 该论文发现当前医用视觉语言模型（Med-VLMs）在面对同义且不同表达的医学视觉问答时，回答极度不稳定，并提出了一套改进方法大幅提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学领域对于诊断问答的一致性要求极高，但现有视觉语言模型在问题表述变化下表现出严重的不一致性，影响其在实际临床中的可用性。因此，作者希望提升Med-VLMs的问答一致性和鲁棒性。

Method: 作者新建了RoMed数据集，包含14.4万道不同层次改写的医学视觉问答，以测试模型鲁棒性。在此基础上提出了一种名为CCL（Consistency and Contrastive Learning）的训练框架，包括1）基于医学知识的一致性学习，使模型学会依据医学本质做出稳定回答，2）偏差感知对比学习，有效缓解模型对数据表层特征和偏见的依赖。

Result: 在RoMed数据集上，现有SOTA模型如LLaVA-Med表现大幅下降（回忆率下降约40%），证明模型脆弱。改用CCL训练后，不仅在RoMed测试集上的回答一致性提升了50%，还在三个主流VQA基准上获得了SOTA表现。

Conclusion: 现有Med-VLMs在医学场景存在严重一致性问题，所提出的RoMed基准和CCL方法显著提升了模型鲁棒性和问答一致性，对于医学AI应用具有重要推动作用。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [96] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本论文提出了一种名为Attention2Probability的注意力驱动术语概率估计方法，用于提升语音识别和翻译系统在特定领域术语输出的准确性，并公开了相关新数据集。实验显示该方法显著优于VectorDB，术语召回率和速度均有提升。


<details>
  <summary>Details</summary>
Motivation: 随着语音大模型在通用领域的识别和翻译能力提升，如何精确生成专业术语或新词成为难点。传统方法在保证识别准确性的同时，往往对专业术语的召回不足，急需有效方法弥补该短板。

Method: 提出Attention2Probability方法：通过分析语音-术语的cross-attention权重，将其转化为术语存在概率，并引入课程学习策略提升检索精度。此外，作者构建并公开了一个包含术语的语音数据集用于训练和测试。

Result: 在实验中，Attention2Probability召回率高于VectorDB模型，中文术语最高召回率92.57%，英文86.83%，单次查询延迟仅8.71ms。该方法用于SLM系统术语干预，术语准确率提升6-17%。

Conclusion: Attention2Probability在术语召回与速度上优于现有方法，有效改善了SLMs对领域术语的识别和翻译能力。同时，揭示了现有SLMs在专业术语处理上的局限，为后续研究提供了基础与数据支撑。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [97] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为自适应原创性过滤（AOF）的新型提示框架，有效提升多语种谜语生成任务中新颖性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成多语种谜语时，容易复用记忆内容或只是简单换句话说，缺乏新意，难以兼顾文化语境与创造性表达。

Method: 提出Adaptive Originality Filtering（AOF）框架，通过基于余弦相似度的语义过滤，剔除冗余生成内容，同时约束词汇新颖性和跨语言一致性。

Result: AOF增强的GPT-4o模型在日语谜语生成任务中，Self-BLEU为0.177、Distinct-2为0.915，较其他方法和语言对展现出更高的词汇多样性和更少重复。

Conclusion: 语义拒绝机制无需特定微调即可引导LLMs生成更具文化底蕴、创新的多语种谜语，有效提升生成质量。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [98] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 提出了一种新的解释性优先的MGT检测框架EMMM，在保障高准确率和低延迟的同时，更加关注对非专业用户可解释。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）广泛应用于客服，恶意用户可利用MGT进行大规模用户冒充，现有检测方法在实际对话中可靠性和可解释性不足，尤其对非专业客服操作员支持有限。

Method: 提出了解释-再检测的EMMM框架，强调首先生成易懂解释，再进行MGT检测，关注延迟、准确性和面向非专家的易用性。进行了与现有先进模型的对比实验，并组织人工评测。

Result: EMMM输出的解释70%获评估者青睐，准确率与先进模型持平，且输出延迟低（1秒内），适用实际场景。

Conclusion: EMMM能够实现对非专业用户友好的可解释MGT检测，兼具高准确率与低延迟，为可信AI在客服场景部署提供新思路。

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [99] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 本论文提出了DIVER框架，利用大语言模型生成同时具备高质量和多样性的广告标题，在广告效果和多样性之间取得了平衡，并在真实工业环境中取得提升。


<details>
  <summary>Details</summary>
Motivation: 现有广告标题生成方法过于关注质量或点击率，忽视了标题多样性，导致输出结果同质化，难以满足不同用户群体需求。

Method: 提出DIVER框架，基于大语言模型，联合优化标题质量与多样性：1）构建语义和风格敏感的数据生成流程，自动生成高质量训练对；2）使用多阶段多目标优化、监督微调和强化学习方法，实现一遍推理即可输出高质量、多样化广告标题。

Result: 在真实工业数据集上，DIVER在标题质量和多样性上取得良好平衡，并在数亿级内容平台上线应用后，实现广告主价值提升4.0%、CTR提升1.4%。

Conclusion: DIVER框架有效提升了广告标题的质量与多样性，具备实际商业价值，可推广应用于大规模在线广告平台。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [100] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文引入了一个全新的多模态、多场景情感原因三元组抽取（MECTEC）数据集MECAD，并提出了一种新模型M3HG，通过多模态异构图显式建模情感和因果上下文，实现了更优异的信息融合和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的MECTEC研究主要受限于稀缺且单一场景的数据集，导致模型泛化能力和效果受限。同时，主流方法未能显式建模情感及因果上下文，忽略了不同层级语义信息融合，性能有待提升。

Method: 作者提出了MECAD数据集，收集并整理了来自56部电视剧共989段对话，涵盖多种场景，极大丰富了训练与评估资源。同时，提出了M3HG模型，利用多模态异构图显式建模情感与因果上下文，并在话语内与话语间层面高效融合上下文语义信息。

Result: 实验结果显示，M3HG模型在所提出的数据集上相较于当前主流方法表现更佳，验证了其在情感原因三元组抽取任务中的有效性。

Conclusion: MECAD数据集的发布和M3HG模型的提出，有效推动了多模态对话情感原因三元组抽取领域的研究，提升了情感及因果建模和信息融合能力。代码和数据集均已开源，有望促进后续相关研究。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [101] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了一种针对故事性文本的长上下文问答新方法ChronoRAG，相较于传统RAG方法，能更好地抓住事件时序和上下文流，实验上取得了显著更好的效果。


<details>
  <summary>Details</summary>
Motivation: 针对叙事性文本的QA任务中，答案常需在长文本中重建事件时间线，但现有RAG索引方法只检索碎片内容，无法捕捉段落间的时序与上下文关系。作者希望克服这些在叙事文本问答中存在的局限。

Method: 提出ChronoRAG框架，优化散乱段落为结构化连贯的内容，并通过建模和维护段落的时间顺序，保证叙事流畅性。具体通过对检索到的文本进行整理和顺序重建，以更好地支持长文本理解和问答。

Result: 在NarrativeQA数据集上实验，ChronoRAG在事实识别和复杂时序关系理解两方面均显著优于现有方法。

Conclusion: 实验表明，在叙事文本QA中，推理时间顺序十分关键，ChronoRAG能更好地处理这类任务，提升了整体理解和问答性能。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [102] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为ThinkDial的开源框架，实现了类似OpenAI gpt-oss系列对推理能力离散可控的特性，可以根据需求在不同推理模式间切换，在节省计算资源的同时尽量保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理任务上表现强大，但其计算资源消耗难以控制，这限制了实际应用。主流的推理模式控制多由闭源系统提供，开源社区缺乏类似能力。

Method: 提出了ThinkDial框架，通过端到端的训练方法在整个模型训练流程中融入预算-模式控制，包括带推理可控能力的有监督微调及两阶段预算感知强化学习，用适应性奖励实现推理能力与输出长度的平衡。

Result: ThinkDial能在三种推理模式下（高、中、低）根据设定减少token数并控制性能损失。实验证明模型输出长度显著下降，且在性能阈值内保持良好任务表现，对分布外任务也具备较强泛化能力。

Conclusion: ThinkDial为开源社区带来可控推理能力，为大模型实际部署提供了资源-性能兼顾的新方案，提升了模型在不同需求下的适应性和灵活性。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [103] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则和强化学习（Rule-Based RL）的新型语法纠错方法，并在中文数据集上取得了业界领先表现，尤其在召回率方面提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前利用大模型进行语法纠错的研究主要依赖有监督微调，使模型仅关注句子生成纠错，未能充分利用大模型强大的推理能力。因此，需要一种新方法，更好地发挥大模型的能力。

Method: 提出基于规则和强化学习的训练框架，通过定义规则结合强化学习训练语言大模型，提升其纠错的可控性和可靠性。

Result: 在中文语法纠错任务上，新方法取得了最先进的性能指标，特别是在召回率上有显著提升。

Conclusion: 使用规则结合强化学习来引导大模型，能够更有效地进行语法纠错，为该领域提供了一种更可控和可靠的新范式。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [104] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了对话分析中一个新的关键任务——主题检测，通过大规模语言模型（LLMs）提升自动化水平，并作为DSTC12竞赛的一个公开赛道进行探讨。该任务旨在自动识别和归类对话中的主题，实现更灵活和个性化的对话摘要。文章还分享了数据集、评测方法以及参与队伍的经验。相关数据和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 对话分析尤其在客户服务、销售等领域，人工分析大型对话数据既耗时又费力。传统意图识别方法局限性明显，难以满足对灵活、多样、个性化主题识别的需求，因此需要新的自动主题检测方法。

Method: 将可控的会话主题检测作为一项竞赛任务，问题被设定为对话话语的联合聚类与主题标注，其特色是可根据用户偏好数据控制主题聚类的粒度。介绍了用到的数据集、自动与人工评测指标，并讨论了参赛方法。

Result: 总结了各参赛队伍的提交方案与表现，分析了不同方法在本任务下的优劣和经验。所有竞赛材料包括数据和代码均已在GitHub公开发布。

Conclusion: 可控的主题检测比传统意图识别更能满足实际需求，能有效减轻人工分析负担。这一公开赛道推动了新方法的发展，为后续研究提供了资源与经验。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [105] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出了一种名为LaTeXTrans的多智能体系统，专门解决LaTeX文档在机器翻译中的结构保真与内容一致性难题，实验结果优于主流翻译系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统虽在通用领域取得显著进展，但面对包含公式、表格、交叉引用等结构复杂的LaTeX文档时，难以兼顾语义、结构与可编译性，因此亟需新的翻译方法。

Method: 提出LaTeXTrans系统，包括六个专用代理：解析器将LaTeX文档拆分并作格式处理；翻译器、验证器、摘要器、术语提取器协作保证上下文相关、术语一致的翻译与自我纠错；生成器将翻译结果重构为结构良好的LaTeX文档。

Result: 实验显示，LaTeXTrans在翻译准确性和结构保真性上均超越主流机器翻译系统。

Conclusion: LaTeXTrans在LaTeX文档翻译场景下有效解决了结构与内容双重挑战，具备较高实用价值，可望推广应用于专业领域文档的自动翻译。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [106] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 本文提出了一种结合AMR语义和新闻传播特征的自监督虚假信息检测框架，无需大量标注数据也能优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前虚假信息检测方法难以捕捉长距离依赖、复杂语义关系和社交传播动态，且依赖大量标注数据，限制了实际部署和泛化能力。

Method: 提出自监督检测框架，利用AMR捕捉复杂语义关系，并结合社交传播图特征。创新性地提出基于LLM的图对比损失(LGCL)，通过生成负锚点提升特征可区分性。采用多视角图掩码自编码器学习传播特征，将语义与传播特征有效融合。

Result: 大量实验显示，该自监督方法即使在标注样本有限的情况下，也显著优于现有最先进方法，且泛化性更好。

Conclusion: 本文方法突破数据依赖困境，能高效准确地检测虚假新闻，对实际应用具有较强推广价值。

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [107] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出一种程序辅助的数据生成框架，自动合成大规模、高质量、复杂且多样性的数学问题与解答数据，用于提升大语言模型的数学推理能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有提升大模型数学推理的数据获取方式存在扩展性差、成本高、可信度不足等问题，亟需新方法批量生成高质量训练数据。

Method: 提出一个程序辅助合成框架，将数学知识系统与领域工具结合，自动生成可执行程序；程序产出转换为自然语言问答对，通过双向校验机制确保答案与程序一致性及正确性。最终生成了1230万个高质量问题-解答三元组。

Result: 用该框架生成的数据微调模型后，在多个数学推理基准（如GSM8K等）上取得了当前最新的领先性能。

Conclusion: 程序辅助合成框架可大规模、低成本地生产高质量训练数据，显著提升大模型的数学推理能力，有望推广到更多领域。

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [108] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出了一种简单高效的微调方法——ConfTuner，通过改进损失函数显著提升大语言模型（LLM）表达不确定性的准确性，从而提升其在科学、法律、医疗等高风险领域的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高风险领域（如科学、法律和医疗）被广泛应用，但它们常常会以极高自信输出错误答案，即“过度自信”。已有的置信度校准方法效果有限，难以泛化，亟需更有效的校准方案以增强模型的可靠性和信任度。

Method: 作者提出ConfTuner，一种基于新型损失函数（tokenized Brier分数）的微调方法，无需真实置信度或代理置信度即可高效校准模型自信表达。该损失函数理论上被证明为合适的评分规则，能激励模型诚实反映其正确概率。ConfTuner操作简单，附加开销极小，并能泛化到如GPT-4o等黑盒模型。

Result: ConfTuner在多种推理任务上显著提升了大语言模型的置信度校准效果。实验显示，校准后的模型在自我纠错和多模型级联应用中有更优表现，表明其改进不仅限于表层的分数提升，还能带来实际应用层面的收益。

Conclusion: ConfTuner为大语言模型提供了一种有效的置信度校准方式，无须额外置信度标签或复杂工程，便能提升模型可靠性，有助于打造更值得信赖的高风险领域LLM系统。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [109] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种基于进化算法的新型自动提示（autoprompting）方法ReflectivePrompt，在提示工程领域取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 自动提示能够为大模型自动选择最优提示，提升模型性能，但现有方法在提示选择的全面性和效率上仍有提升空间，因此需要更精确、全面的自动提示技术。

Method: ReflectivePrompt方法基于进化算法，创新性地引入了短期与长期反思操作，在交叉和精英变异前进行修正，以优化提示的质量，并积累和动态更新演化过程中的知识。

Result: 在33个数据集的分类和文本生成任务测试中，ReflectivePrompt相较现有最强基线EvoPrompt，在平均指标上有明显提升（如在BBH数据集提升28%）。

Conclusion: ReflectivePrompt在基于进化算法的自动提示领域表现突出，是当前最有效的自动提示方案之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [110] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）辅助内容分析的新方法（LACA），以提升计算教育研究（CER）的规模与严谨性，尤其是在处理大规模文本数据时。此方法能够帮助资源有限的研究者进行更具普适性的研究，提高研究质量和可复现性。


<details>
  <summary>Details</summary>
Motivation: 许多计算教育研究者受限于同事、资源或能力，难以开展足够严谨与普适性的研究，特别是在需要处理大量质性资料时。这亟需新的研究方法能在减轻研究者负担的同时扩展数据分析能力。

Method: 作者提出一种名为 LLM-assisted content analysis (LACA) 的方法，将大语言模型与内容分析结合，用于高效分析和归纳大量文本数据，并以计算教育领域的数据集为例，展示方法的可复现和严谨实现。

Result: 通过具体案例，作者证明了 LACA 方法能在不显著增加研究者负担情况下，拓展数据分析范围，实现规模更大、结果更具普适性的质性研究。

Conclusion: LACA 方法为计算教育研究领域提供了一条可行途径，使研究者能够产出更一般化和更高质量的研究成果，对提升学科整体的实践和研究质量具有重要推动作用。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [111] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 本文利用自然语言处理自动分析了六个欧洲国家议会演讲语料，发现所有议会都存在一致的情感极化现象，即议员在提到对立群体成员时表达更多负面情感。


<details>
  <summary>Details</summary>
Motivation: 近年来，政治领域对立情绪变得非常普遍，政治群体之间的负面态度和敌意日益加剧（情感极化）。但在欧洲多国议会层面对这一现象的大规模、量化自动分析研究尚不多见。作者希望通过自动化、大样本分析来厘清欧洲议会中的情感极化现象及其机制。

Method: 作者收集了六个欧洲国家议会的全部演讲文本，运用自然语言处理（NLP）技术自动识别文本中议员对自己和对方群体成员的情感倾向（积极/消极）。通过定量比较议员在谈及对立群体和本群体成员时的消极情感强度，考察情感极化的普遍性及相关机制。还分析了议员活跃度与情感极化的关系，以及负面互动的互惠性。

Result: 所有六国议会均展现出稳定明显的情感极化特征，即议员对对立群体成员表达更强烈的负面情感。议员参与活跃度越高，消极情感表达也往往越高，但活跃和不活跃者之间在极化程度上无显著区别。此外，各国议员之间的消极互动呈现互惠性，即双方互有负面回应。

Conclusion: 情感极化是欧洲议会普遍存在的现象，并且其形成与群体间负面情感的回馈机制有关。议员活跃度虽与负面情感表达量相关，但并不会提升极化水平。该研究展示了自动化文本分析在政治极化研究中的巨大潜力。

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [112] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体框架，自动生成用于RAG（检索增强生成）系统评估的合成问答数据集，重点在于提升语义多样性与隐私保护效果。实验结果显示新方法优于现有基线，实现更高多样性与有效隐私屏蔽。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估主要关注性能指标，忽视了评估数据集的多样性和数据隐私保护，这影响了评估的真实性和可信度。在实际场景中，保护敏感信息尤为重要，因此有必要提升评估数据集的设计质量。

Method: 作者设计了三个协作代理：1）多样性代理通过聚类最大化主题覆盖与语义多样性；2）隐私代理能检测并屏蔽多领域敏感信息；3）问答整理代理负责合成符合隐私和多样性规范的QA对，用于RAG评测。最终生成自动化、可控的高质量数据集。

Result: 实验结果表明，该方法生成的数据集在多样性指标上优于主流基线，并能在多个领域实现稳健的隐私信息屏蔽能力，有效提升评测数据集的现实可靠性和安全性。

Conclusion: 本文方法为RAG系统评估提供了一种更实用、符合伦理且兼顾隐私的方案，为未来AI法规与合规要求下的评估体系搭建了基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [113] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种使神经网络发展“AI母语”的框架，实现模型内部原生、可解释的符号化推理方法，在多个AI任务中取得了良好的准确率与推理可验证性。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络方法虽然准确，但推理过程不透明，难以解释。本研究旨在开发一种能够原生支持可解释推理和组合式符号链条的模型结构，为神经网络带来更高的可解释性和直观性。

Method: 该方法在神经网络内部嵌入符号化表达和组合链式推理，利用门控诱导机制引导模型关注关键环节。通过引入互补的训练目标提升符号纯度和决策稀疏性，并采用顺序专精策略，先进行广义符号能力训练，再精化直观判断。

Result: 实验显示，该方法在多个AI任务上的准确率具有竞争力，并能生成可验证的推理链条，兼顾模型性能与透明度。

Conclusion: AI母语框架能够统一实现神经网络的可解释性、直观推理能力和符号推理，对推动神经网络可解释性研究具有重要意义。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [114] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的自动提示（autoprompting）方法DistillPrompt，通过多阶段整合任务信息，有效提升了文本分类和生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和提示工程研究的迅速发展，如何自动化并优化提示选择成为热门课题。

Method: DistillPrompt方法基于大语言模型，通过多阶段整合训练数据中的任务特定信息，结合蒸馏、压缩和聚合等操作，更全面地探索提示空间，无需梯度更新。

Result: 在多个文本分类和生成任务数据集以及t-lite-instruct-0.1模型上实验，DistillPrompt在关键指标上平均性能优于现有方法（如比Grips高20.12%）。

Conclusion: DistillPrompt成为最有效的无梯度自动提示方法之一，显著提升了自动提示效果，为领域发展提供了新思路。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [115] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: 本文提出了MovieCORE数据集，聚焦于电影内容的深度认知型视频问答任务，并提出方法提升现有模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答数据集多停留在对表层内容的理解，缺乏对深层次认知（如推理、观影体验等）的评估。作者希望推动AI在更高认知层面上理解复杂的电影内容。

Method: 1. 采用多大语言模型（LLMs）作为思维代理，协同生成、筛选高质量深度问题与答案；2. 设计认知测试指标评估问题质量（深度、启发性和复杂性）；3. 提出系统性评测方案评估VQA模型在深认知任务的性能；4. 提出Agentic Choice Enhancement (ACE)模块，对现有VLM进行推理能力强化。

Result: 构建了高质量的MovieCORE数据集；ACE模块能使VLM后训练推理能力提升最多25%；实现了对现有模型在面对高难度电影认知问答时的能力与局限的系统评判。

Conclusion: MovieCORE为AI系统电影理解带来新挑战，ACE模块显著提升模型认知推理表现，为未来深层视频理解及模型研究提供了新工具及洞见。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [116] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: 本文提出了HiPlan，一种为大语言模型（LLM）智能体提供分层规划的框架，能够显著提升其在复杂、长周期任务中的决策与执行表现。


<details>
  <summary>Details</summary>
Motivation: LLM智能体虽然在决策任务上表现卓越，但在处理涉及复杂和长远规划的场景时常遇到困难，主要由于缺乏宏观指引和持续监督，导致易迷失方向且对实时变化环境反应不足。作者试图通过引入分层规划机制，来提升LLM智能体在此类任务中的规划和执行能力。

Method: 提出HiPlan分层规划框架：1）离线阶段，从专家演示中构建milestone库，通过检索语义相似任务与里程碑，实现结构化经验复用；2）执行阶段，动态适配历史milestone对应的轨迹片段，生成与任务目标一致的逐步提示，动态修正偏差并应对环境变化。

Result: 在两个具有挑战性的基准测试中，HiPlan明显优于强力基线方法。消融实验进一步验证了分层组件的互补效应。

Conclusion: HiPlan通过全局-局部的自适应引导，为LLM智能体提供了有效的复杂任务分解与实施策略，显著提升了其在复杂和长周期任务下的决策与执行能力。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [117] ["Where does it hurt?" - Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 本文首次研究医生在医患对话中的意图轨迹，基于Aci-bench数据集，建立了细粒度意图分类体系并构建专家标注的数据集，对现有模型进行了评测，并对改进诊断系统结构提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 以往研究很少关注医生在医患对话过程中意图的动态变化与结构，作者希望通过深入分析医生的意图轨迹，提升医学意图识别和自动诊断系统的设计效果。

Method: 1. 结合医生专家建立基于SOAP框架的意图细分体系。
2. 组织专家在Aci-bench数据集上标注5000余轮医患对话。
3. 用该数据集评估主流生成式和编码器模型的医学意图分类能力。
4. 分析意图轨迹和意图过滤对对话摘要的影响。

Result: 所选模型能高精度理解医疗对话结构，但难以准确识别SOAP类别间的转变；首次揭示了医疗对话中的常见意图轨迹；意图过滤能显著提升医疗对话摘要效果。

Conclusion: 构建大规模高质量的医患对话意图标注数据集能推动医学意图识别和对话系统发展，其发现为差异性诊断系统和自动摘要等任务提供有力支持。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [118] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLM）通过上下文学习（ICL）和参数高效微调（PEFT）处理极低资源语言（特别是少见文字书写语言）的能力，发现对极低资源语言，带有语言对齐的零样本ICL效果最好，而微调针对极端低资源的罕见文字效果有限。


<details>
  <summary>Details</summary>
Motivation: 极低资源语言（尤其用罕见文字书写的语言）在大语言模型中的支持度很低，主要因为缺乏训练数据。因此，本文旨在分析LLM是否可以通过上下文学习习得这些语言，以及对比不同方法的有效性。

Method: 作者对20种低资源语言进行了系统性评估，实验方法包括：仅依靠上下文学习（ICL）、带/不带辅助对齐信号，以及参数高效微调（PEFT），并比较其在三种主流多语言LLM上的表现。

Result: 实验显示：当目标语言及其文字在LLM中极度稀缺时，PEFT的效果受限；而带有语言对齐信号的零样本ICL在极低资源语言上表现突出；对于在LLM中相对略有代表性的语言，少样本ICL和PEFT更有优势。

Conclusion: 本文提出对极低资源语言采用方法的实用建议，强调对于罕见文字的低资源语言不建议直接微调LLM，而应优先采用带有语言对齐的零样本上下文学习方法。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [119] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 本研究提出了一种结合检索与生成（RAG）的框架，用于艺术品溯源研究，显著提升了在复杂、多语言艺术档案中检索和理解信息的能力。


<details>
  <summary>Details</summary>
Motivation: 艺术品溯源研究是验证艺术品真实性、支持法律主张和理解历史背景的重要环节，但受限于分散且多语言的档案数据，现有检索方式门槛高，难以支持探索性查询。

Method: 采用RAG框架，综合语义检索与上下文总结，以自然语言和多语言查询替代对精确元数据的依赖。基于Getty Provenance Index German Sales中1万条记录进行检索和总结实验。

Result: 实验证明RAG方法能高效检索和总结艺术拍卖档案，提升了大规模艺术市场档案的可导航性和检索效率。

Conclusion: 该方法为历史学家和文化遗产专业人员提供了实用、可扩展的工具，有助于开展历史敏感性强的艺术品溯源研究。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [120] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本论文提出了一种结合词汇与语义两种方法的透明、可复现的计算话语分析框架，强调方法学的透明性与可解释性，并通过开源代码实现。


<details>
  <summary>Details</summary>
Motivation: 当前话语定量分析常用的工具如MAXQDA和NVivo存在“黑箱”问题，影响方法的透明度和与研究目标的契合度，因此需要一种更为开放、可控的研究流程。

Method: 作者基于历史政治话语案例，构建了以Python为核心的自定义处理流程，集成NLTK、spaCy、Sentence Transformers进行数据预处理、词形还原和文本嵌入，并应用BERTopic（结合UMAP、HDBSCAN和c-TF-IDF）进行主题建模，通过参数优化实现话题高一致性与覆盖率；同时结合精确词汇检索与语义聚类。

Result: 经实际案例验证，混合方法能够相互补充，提升话题分析的精度与解释力，并支持流程的可追溯与可复现。相关代码和材料已开源。

Conclusion: 作者主张采用多层次、透明的定量话语分析方法，以提升学术界对方法论透明性、研究者自主性与话语研究三角验证（triangulation）的重视。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [121] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 本文系统分析了大型视觉语言模型在视觉问答任务中对自身知识边界的感知能力，并探索了提升信心感知的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大型视觉语言模型（LVLMs）在视觉问答任务中表现出色，但存在幻觉现象，其自身知识边界感知能力有限。一个可靠的模型应当知道自己知道什么，不知道什么，因此有必要研究并提升这类模型的信心感知能力。

Method: 作者从三类信心信号（概率型、答案一致性型、语言表述型）出发，评估三种LVLMs在三个视觉问答数据集上的表现，并借鉴大型语言模型（LLMs）的信心校准方法，提出三种有效的新方法以改进感知能力。作者还比较了LVLMs和其文本-only LLMs的表现差异。

Result: 实验显示LVLMs在信心感知上有一定能力，但距离理想状态仍有较大提升空间。其中，概率型和一致性型信号比语言表述型信号更可靠，而后者容易导致过度自信。引入信心校准方法后，LVLMs的感知能力得到提升。此外，LVLMs联合视觉和文本输入后，问答表现略有下降，却因信心降低而展现出更好的知识边界感知。

Conclusion: LVLMs对知识边界有一定感知，但现有信心信号依然不足，过度依赖语言表述会导致过度自信。通过适当的信心校准方法可以有效提升LVLMs的知识边界感知，对未来模型的可靠性建设有重要意义。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [122] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 该论文提出了一套新的科学推理评测基准（SciReas及其子集SciReas-Pro），并通过系统分析知识获取与推理能力在科学任务中的作用，对现有模型在科学推理任务中的表现进行了深入评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学问题求解方面面临知识深度与复杂推理的双重挑战，缺乏全面性的科学推理评估基准，同时缺少对知识与推理角色的系统区分。

Method: 作者提出了SciReas（多样化科学推理任务集合）和SciReas-Pro（高复杂度子集）两套测评基准，以及KRUX探查框架，用于分析知识与推理在科学任务中的不同作用，并进行综合实验分析。

Result: （1）LLM在科学推理中的关键瓶颈是难以从模型参数中检索与任务相关的知识；（2）在加强推理能力的基础上，引入外部知识可持续提升模型表现；（3）强化语言化推理过程有助于模型激活与任务相关的知识。

Conclusion: 系统性评估揭示了知识检索与推理机制对科学任务表现的深层影响，并为后续科学推理模型的改进提供数据基准与分析工具，同时发布了一个8B参数的科学推理基线模型（SciLit01）。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [123] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice提出了一种创新的多说话人长文本语音合成模型，利用next-token diffusion技术实现高效的音频压缩和保真度，有效提升了长段语音生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前的语音合成模型在处理长时段、多说话人的语音时遇到数据压缩与计算效率瓶颈，且难以保持音频保真和支持多说话人自然对话。

Method: VibeVoice采用了next-token diffusion方法，能够自回归地生成音频潜在向量。此外，作者设计了一种新型连续语音分词器，与主流的Encodec模型相比压缩效率提高了80倍，同时保持了音频质量。

Result: VibeVoice能够高效合成最长达90分钟（64K上下文窗口）的多说话人语音（支持最多4位说话人），并在多项指标上超越了其他开源或专有对话语音模型。

Conclusion: VibeVoice有效提升了多说话人长文本语音合成的效率和质量，在长语音上下文与多说话人的情境下表现突出，为自然对话及内容创作提供了更强的技术支持。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [124] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文调查了通俗语言摘要（PLS）可读性评估方式，发现传统可读性指标与人工评判相关性低，而语言模型能够更好地模拟人类判断，建议采用语言模型进行PL可读性评价。


<details>
  <summary>Details</summary>
Motivation: 目前PLS领域通常采用传统可读性指标（如FKGL）来衡量摘要的易读性，但这些指标与人工评判的一致性尚未明确，缺乏对其有效性的验证。

Method: 作者对PLS领域8种常用可读性评价指标进行分析，并将其结果与人工可读性评判进行相关性对比，同时引入大型语言模型进行同样的评估，并扩展分析到PLS数据集，考察对摘要背景知识要求等更深维度的可读性。

Result: 结果显示，绝大多数传统可读性指标与人工判断相关性较低，最常用的FKGL同样表现不佳，而表现最佳的语言模型与人工评判的皮尔逊相关系数达到了0.56，显著优于传统指标。语言模型对于背景知识等深层次可读性因素有更好的捕捉能力。

Conclusion: 作者建议采用语言模型作为PLS可读性的新评估标准，替代目前通用但效果有限的传统可读性指标，并发布了分析代码和数据以支持后续研究。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [125] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种创新的人机交互范式——生成式界面（Generative Interfaces），让大模型不仅通过对话回答用户，还能主动生成适应任务的交互界面，提升多轮、信息密集和探索性任务中的交互效率。实验显示生成式界面在用户体验上显著优于传统聊天式界面。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多以线性的对话方式辅助用户，这种模式在多轮复杂任务场景下效率不足，难以满足高效、灵活的交互需求。为突破现有限制，需要探索新的交互范式，提升模型对复杂任务的支持能力。

Method: 作者提出生成式界面范式，利用结构化界面表达和多次迭代优化，将用户请求转化为具体、可交互的任务界面。并设计了评测体系，从功能、交互及情感等多维度，对比评估生成式界面与传统聊天界面的优劣。

Result: 实验结果显示，生成式界面在各类任务、交互模式和问题类型中均优于传统聊天界面。超过70%的用户更偏好生成式界面，显示出明显的用户体验提升。

Conclusion: 生成式界面不仅提升了多轮和复杂任务的交互效率，还明显提高了用户满意度。该范式为人机交互提供了新方向，预示了未来人机协作界面的发展趋势。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [126] [Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning](https://arxiv.org/abs/2508.18397)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 本论文针对离线强化学习在自动驾驶训练中的数据不均衡问题，系统比较了多种数据筛选和加权方法，显著提升了安全和规划能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习能够借助大量实际驾驶数据训练自动驾驶策略，但常规均匀采样容易导致模型忽略少见但极其重要的“长尾”事件，进而影响安全性。本研究旨在解决数据极度不平衡带来的策略脆弱和不安全问题。

Method: 作者系统地研究了六种不同的关键性权重方案（分为启发式、基于不确定性、基于行为三类），并在时序和场景两个尺度上进行比较。使用先进的注意力机制架构和目标导向的守恒Q学习（CQL）方法，在Waymax高保真模拟器中测试七种Agent，评估不同数据筛选方法的实际效果。

Result: 所有非均匀采样方法均显著优于常规基线，尤其是基于不确定性的数据筛选使碰撞率下降近三倍（从16.0%降至5.5%）。此外，时步级权重在安全反应上表现更佳，而场景级权重则提升了长时规划能力。

Conclusion: 智能、非均匀筛选数据是离线强化学习构建安全可靠自动驾驶策略的关键。本文为该领域提供了全面的实践框架和重要经验。

Abstract: Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.

</details>


### [127] [Maintenance automation: methods for robotics manipulation planning and execution](https://arxiv.org/abs/2508.18399)
*Christian Friedrich,Ralf Gulde,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 本文提出了一套完整的机器人系统，实现了在环境存在不确定性时对装配和拆卸任务的自动化操作，并进行了实际应用测试。


<details>
  <summary>Details</summary>
Motivation: 复杂任务自动化需要机器人具备计划、控制和执行能力，尤其是在实际维护等场景中常常受到环境不确定性的影响。因此，开发能够应对这些挑战的完整自动化系统具有重要意义。

Method: 该系统以借助CAD与RGBD数据的规划方法为基础，能够将符号化的计划解释并转化为机器人可执行的指令，从而实现智能认知与控制。

Result: 系统在真实应用环境中进行了实验性评估，展示了其在实际操作中的有效性和应用潜力。

Conclusion: 本研究为将理论研究成果应用到实际机器人解决方案中迈出了关键一步，验证了系统在自动化运维领域中的可行性。

Abstract: Automating complex tasks using robotic systems requires skills for planning,
control and execution. This paper proposes a complete robotic system for
maintenance automation, which can automate disassembly and assembly operations
under environmental uncertainties (e.g. deviations between prior plan
information). The cognition of the robotic system is based on a planning
approach (using CAD and RGBD data) and includes a method to interpret a
symbolic plan and transform it to a set of executable robot instructions. The
complete system is experimentally evaluated using real-world applications. This
work shows the first step to transfer these theoretical results into a
practical robotic solution.

</details>


### [128] [Efficient task and path planning for maintenance automation using a robot system](https://arxiv.org/abs/2508.18400)
*Christian Friedrich,Akos Csiszar,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 本文提出了一种结合CAD离线数据与RGBD视觉系统在线数据的概率滤波方法，用于提升维护自动化中机器人自主规划与路径规划的能力，特别关注低计算复杂度和环境不确定性处理。


<details>
  <summary>Details</summary>
Motivation: 工厂自动化需要高效、智能的维护自动化解决方案，传统方法难以应对复杂环境和大规模任务，因此探索自主机器人系统以提升工业智能化水平。

Method: 结合CAD离线数据和RGBD在线视觉数据，通过概率滤波减少数据不确定性，并提出基于符号描述和新型采样方法的拆卸空间规划方法；路径规划采用可调节探索步长的全球领先算法以缩短规划时间，所有方法均通过实验验证。

Result: 所提方法能有效融合多源数据，提升任务规划与路径规划的自主性和实时性；实验结果显示该系统能在维护自动化任务中实现更高效的规划和执行。

Conclusion: 结合离线CAD和在线RGBD数据的概率滤波方法及创新型任务和路径规划技术，可显著提升维护自动化中机器人系统的自主性、适应性和效率，为未来智能工厂的发展提供了有效解决方案。

Abstract: The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.

</details>


### [129] [PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing](https://arxiv.org/abs/2508.18443)
*Ruohan Zhang,Uksang Yoo,Yichen Li,Arpit Argawal,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的新型软体机器人传感方法，使用嵌入式相机实现高分辨率本体感知和触觉感知，并通过仿真-到现实的传感优化管道，有效提升了软体机器人的感知能力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人由于其柔顺和灵活性，越来越多应用于工业和人机交互等场合，但实际部署时缺乏先进的传感手段实现触觉反馈与本体感受，成为限制其发展的关键问题。

Method: 提出PneuGelSight软体气动机械手，将相机嵌入机器人本体实现高分辨率触觉与本体感知。同时，开发了完整的仿真管道，能够准确模拟其光学和动态特性，实现仿真到现实（sim-to-real）的知识迁移，无需实际数据即可优化传感器性能。

Result: 实验证明，该方法可在无现实数据参与下，将仿真中优化的传感能力直接迁移到实际软体机器人上，提升其感知与反馈的准确性和鲁棒性。

Conclusion: PneuGelSight结合仿真到现实的方法，为软体机器人提供了一种新型、易于实施且可靠的传感技术，有望推动配备更强感知能力的先进软体机器人的发展。

Abstract: Soft pneumatic robot manipulators are popular in industrial and
human-interactive applications due to their compliance and flexibility.
However, deploying them in real-world scenarios requires advanced sensing for
tactile feedback and proprioception. Our work presents a novel vision-based
approach for sensorizing soft robots. We demonstrate our approach on
PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution
proprioception and tactile sensing via an embedded camera. To optimize the
sensor's performance, we introduce a comprehensive pipeline that accurately
simulates its optical and dynamic properties, facilitating a zero-shot
knowledge transition from simulation to real-world applications. PneuGelSight
and our sim-to-real pipeline provide a novel, easily implementable, and robust
sensing methodology for soft robots, paving the way for the development of more
advanced soft robots with enhanced sensory capabilities.

</details>


### [130] [Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models](https://arxiv.org/abs/2508.18460)
*Tianze Liu,Md Abu Bakr Siddique,Hongyu An*

Main category: cs.RO

TL;DR: 该论文提出通过模仿动物的联想学习机制，赋予神经形态机器人在线空间任务自适应能力，从而减少对大数据和高算力的依赖，提升自主性，适用于如行星探测等对体积、重量、功耗有严格要求的场景。


<details>
  <summary>Details</summary>
Motivation: 传统AI大量依赖大数据和神经网络，导致功耗高、适应性差，难以应用在对体积、重量和功耗（SWaP）要求严格的场合，如行星探测等。动物自然界中的联想学习能力能有效应对变化环境，具有较强适应性，激发研究者借鉴其机制用于机器人自主导航。

Method: 论文借鉴啮齿动物联想学习和空间细胞（如位置细胞和网格细胞）机制，设计神经形态机器人，并在开放场地迷宫中开展实验，实现机器人对环境的实时感知、记忆和学习，进而导航和自主优化行为。

Result: 实验显示，结合动物空间细胞模型和机器人联想学习机制，机器人可在动态环境中实现在线学习与适应，顺利完成空间任务，并展现出类生物的自主性。

Conclusion: 本文通过引入生物灵感的联想学习模型并结合神经形态计算，显著提升了机器人在动态环境下的自适应与自主能力，为低功耗、自主导航机器人在复杂未知环境（如行星探索）中的应用奠定基础。

Abstract: Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.

</details>


### [131] [SignLoc: Robust Localization using Navigation Signs and Public Maps](https://arxiv.org/abs/2508.18606)
*Nicky Zimmerman,Joel Loo,Ayush Agrawal,David Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种名为SignLoc的全球定位方法，利用导航标识（如方向牌）实现机器人在公开地图上的定位，无需事先构建传感器地图。实验结果显示，该方法能在大型复杂环境中，机器人仅通过观测一至两个标识即可实现可靠定位。


<details>
  <summary>Details</summary>
Motivation: 尽管导航标识和地图广泛用于人类环境中的寻路，但机器人大多未加利用，普遍依赖传统的传感器建图方法，有局限性。因此，研究如何直接利用现有地图（如楼层图和OpenStreetMap）中的导航标识来进行机器人定位，具有实际意义。

Method: SignLoc方法首先从输入地图中提取导航图，然后通过概率观测模型将感知到的导航标识的方向和位置线索与导航图进行匹配，在蒙特卡洛框架内实现鲁棒的拓扑-语义定位。该方法不依赖于机器人自身的传感器地图构建。

Result: 在大学校园、购物中心和医院等大规模多样环境下，SignLoc可以在机器人仅看到一到两个导航标识后，即可实现稳健的全球定位。

Conclusion: SignLoc展示了基于导航标识的机器人定位的可行性和高效性，为无需事前建图的机器人自主导航提供了新的技术途径。

Abstract: Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs--without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.

</details>


### [132] [Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning](https://arxiv.org/abs/2508.18627)
*Ziyuan Jiao,Yida Niu,Zeyu Zhang,Yangyang Wu,Yao Su,Yixin Zhu,Hangxin Liu,Song-Chun Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种顺序移动操作规划（SMMP）框架，通过将环境结构抽象为运动学模型，并与机器人运动学结合，构建统一的增强配置空间（A-Space），实现智能多步骤任务执行，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 实际复杂环境中的机器人操作需要应对多步（长时域）、须全身协调的任务，且常涉及可动结构对象。现有方法往往将导航与操作任务分开建模，难以应对联合约束，限制了任务复杂性的提升。

Method: 通过将环境及对象的结构抽象为运动学模型并与机器人运动学结合，构建统一的A-Space。采用三级规划：任务规划器生成符号动作序列、基于优化的运动规划器在A-Space中产生连续轨迹、中间步骤精炼行动目标确保长时域可行性。

Result: 仿真实验证明A-Space的规划方法比基线方法任务成功率高84.6%；实际机器人验证可流畅完成包含7种刚性与关节物体、17个场景、最长14步的复杂移动操作任务。

Conclusion: 场景运动学建模为规划实体而非任务约束，极大提高了移动操作任务的可扩展性和通用性，为复杂机器人操作提供了有效通用解。

Abstract: We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.

</details>


### [133] [Engineering Automotive Digital Twins on Standardized Architectures: A Case Study](https://arxiv.org/abs/2508.18662)
*Stefan Ramdhan,Winnie Trandinh,Istvan David,Vera Pantelic,Mark Lawford*

Main category: cs.RO

TL;DR: 本文评估了ISO 23247数字孪生参考架构在汽车行业的适用性，基于1/10缩比自动驾驶车辆自适应巡航控制数字孪生的案例，分析其优劣并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 汽车行业对基于数字孪生的新一代智能服务（如远程控制、车队协调）的需求日益增长；然而，缺乏专门针对汽车DT的软件架构指导成为工程实现的瓶颈。目前仅有ISO 23247标准可供参考，但它并非为汽车系统特别设计。

Method: 本文以开发一个1/10比例自动驾驶车的自适应巡航控制（ACC）数字孪生为案例，应用ISO 23247参考架构，并通过实证分析其在汽车DT应用中的优缺点。

Result: 通过案例研究，作者发现ISO 23247架构对于汽车DT初步开发具有一些优势，但也存在局限性。例如，部分模块未针对汽车专用需求进行优化，存在适配上的不足。

Conclusion: 该研究表明ISO 23247虽能作为汽车DT架构的起点，但需进一步针对汽车行业特定场景完善和调整。作者提出了面向未来研究、实践应用和标准改进的建议。

Abstract: Digital twin (DT) technology has become of interest in the automotive
industry. There is a growing need for smarter services that utilize the unique
capabilities of DTs, ranging from computer-aided remote control to cloud-based
fleet coordination. Developing such services starts with the software
architecture. However, the scarcity of DT architectural guidelines poses a
challenge for engineering automotive DTs. Currently, the only DT architectural
standard is the one defined in ISO 23247. Though not developed for automotive
systems, it is one of the few feasible starting points for automotive DTs. In
this work, we investigate the suitability of the ISO 23247 reference
architecture for developing automotive DTs. Through the case study of
developing an Adaptive Cruise Control DT for a 1/10\textsuperscript{th}-scale
autonomous vehicle, we identify some strengths and limitations of the reference
architecture and begin distilling future directions for researchers,
practitioners, and standard developers.

</details>


### [134] [Deep Sensorimotor Control by Imitating Predictive Models of Human Motion](https://arxiv.org/abs/2508.18691)
*Himanshu Gaurav Singh,Pieter Abbeel,Jitendra Malik,Antonio Loquercio*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，通过模仿人类运动的预测模型，训练机器人感知-运动策略，实现更高效的机器人学习，并且无需复杂的运动重定向或生成对抗损失。实验结果显示，该方法能在不同机器人和任务上取得远超基线的表现。


<details>
  <summary>Details</summary>
Motivation: 当前人机之间的体现差距正在缩小，人类与环境交互的数据资源丰富，但如何高效应用这些数据来提升机器人学习效果仍面临挑战。现有方法在使用大规模人类数据时，通常受限于运动重定向和对抗损失等技术，其通用性和扩展性有限。该论文旨在寻找一种方法，可以更直接、有效地利用人类运动数据训练机器人。

Method: 论文提出从人类交互数据中对人体关键点的运动进行建模，然后利用这些预测模型实现对机器人末端执行器关键点运动的零样本迁移。机器人策略被训练为追踪这些由人类数据预测的运动轨迹，并结合稀疏任务奖励进行优化。整个流程不依赖传统的梯度运动重定向或对抗损失，直接利用预测模型进行模仿。

Result: 实验证明，该方法能跨不同机器人和任务应用，且在性能上远超现有的基线方法。此外，追踪基于人类运动的预测模型还能替代对稠密奖励和课程设计的需求，简化了机器人操作任务的奖励结构。

Conclusion: 所提出的方法能高效地将人类运动数据应用于机器人学习，跨任务和跨硬件具有良好通用性，并简化了训练流程，为机器人操作领域提供了新的视角和工具。

Abstract: As the embodiment gap between a robot and a human narrows, new opportunities
arise to leverage datasets of humans interacting with their surroundings for
robot learning. We propose a novel technique for training sensorimotor policies
with reinforcement learning by imitating predictive models of human motions.
Our key insight is that the motion of keypoints on human-inspired robot
end-effectors closely mirrors the motion of corresponding human body keypoints.
This enables us to use a model trained to predict future motion on human data
\emph{zero-shot} on robot data. We train sensorimotor policies to track the
predictions of such a model, conditioned on a history of past robot states,
while optimizing a relatively sparse task reward. This approach entirely
bypasses gradient-based kinematic retargeting and adversarial losses, which
limit existing methods from fully leveraging the scale and diversity of modern
human-scene interaction datasets. Empirically, we find that our approach can
work across robots and tasks, outperforming existing baselines by a large
margin. In addition, we find that tracking a human motion model can substitute
for carefully designed dense rewards and curricula in manipulation tasks. Code,
data and qualitative results available at
https://jirl-upenn.github.io/track_reward/.

</details>


### [135] [AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot](https://arxiv.org/abs/2508.18694)
*Jaehwan Jeong,Tuan-Anh Vu,Mohammad Jony,Shahab Ahmad,Md. Mukhlesur Rahman,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 现有精准农业数据集多基于静态或受控环境，难以反映真实农田动态变化，模型泛化能力有限。本文提出AgriChrono数据集与智能采集平台，支持长期、多模态、同步采集实地农田数据，并验证其对3D重建模型的挑战性和促进泛化能力提升的价值。


<details>
  <summary>Details</summary>
Motivation: 现有精准农业领域数据集普遍缺乏反映真实田间环境动态变化的能力；由于多在实验室或温室等受控场景采集，且传感器单一，时间跨度短，导致模型对现实复杂环境适应性弱。因此，亟需动态、真实、多模态的实际环境数据支持模型训练和测试，提高其对真实应用场景的泛化能力。

Method: 提出AgriChrono平台，通过集成多种传感器（包括RGB、深度、激光雷达、IMU等），能远程、时间同步高效采集真实田间长期、多阶段作物生长和环境变化数据。基于此数据集，对多种主流3D重建模型进行评测，分析其在动态环境下的表现及挑战。

Result: 建立了真实田间、多模态、可长期采集的AgriChrono数据集，并详细Benchmark了多种3D重建模型的表现，结果显示在真实农田动态环境下，3D重建任务难度较高，现有模型仍有较大提升空间。

Conclusion: AgriChrono平台和数据集为精准农业及农业机器人领域提供了反映动态真实田间环境的新型研究资源，可以促进模型在现实应用中的泛化能力提升。相关代码和数据已公开，有助于社区后续研究发展。

Abstract: Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono

</details>


### [136] [Enhancing Video-Based Robot Failure Detection Using Task Knowledge](https://arxiv.org/abs/2508.18705)
*Santosh Thoduka,Sebastian Houben,Juergen Gall,Paul G. Plöger*

Main category: cs.RO

TL;DR: 本文提出了一种结合空间和时间知识的视频化机器人故障检测方法，通过机器人执行的动作和任务相关物体信息辅助检测，实验结果表明显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人失败检测方法在实际应用多样性场景下表现不足，需要更健壮、泛化性更强的检测方法以提升机器人任务执行的安全性和可靠性。

Method: 提出了基于视频的故障检测方法，利用机器人动作与任务相关物体的时空信息进行联合分析。方法在三个数据集上进行了测试，并结合可变帧率的数据增强方法进一步提升了性能。

Result: 在ARMBench数据集上，F1分数从77.9提升至80.0，无需额外计算资源，通过测试时的数据增强进一步提升至81.4。

Conclusion: 结果显示时空信息对于失败检测具有重要作用，未来应继续探索合适的启发式策略以提升检测效率与实用性。代码与标注已开放。

Abstract: Robust robotic task execution hinges on the reliable detection of execution
failures in order to trigger safe operation modes, recovery strategies, or task
replanning. However, many failure detection methods struggle to provide
meaningful performance when applied to a variety of real-world scenarios. In
this paper, we propose a video-based failure detection approach that uses
spatio-temporal knowledge in the form of the actions the robot performs and
task-relevant objects within the field of view. Both pieces of information are
available in most robotic scenarios and can thus be readily obtained. We
demonstrate the effectiveness of our approach on three datasets that we amend,
in part, with additional annotations of the aforementioned task-relevant
knowledge. In light of the results, we also propose a data augmentation method
that improves performance by applying variable frame rates to different parts
of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the
ARMBench dataset without additional computational expense and an additional
increase to 81.4 with test-time augmentation. The results emphasize the
importance of spatio-temporal information during failure detection and suggest
further investigation of suitable heuristics in future implementations. Code
and annotations are available.

</details>


### [137] [HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation](https://arxiv.org/abs/2508.18802)
*Li Sun,Jiefeng Wu,Feng Chen,Ruizhe Liu,Yanchao Yang*

Main category: cs.RO

TL;DR: 本文提出了一种新的场景表示学习方法HyperTASR，能根据具体任务和执行阶段动态调整视觉特征表示，在机器人操作任务中显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作的表示学习方法多为任务无关，无法像人类一样根据当前任务动态调整感知，导致对任务相关特征的捕捉不足，影响政策学习效果。

Method: 作者提出了HyperTASR，这是一种基于超网络(hypernetwork)的结构，可根据任务目标与当前执行阶段动态生成表征变换参数，实现场景表示随着任务进展自适应演化，且和现有政策学习框架兼容。方法上通过分离任务上下文与状态依赖两条处理路径，提升学习效率和表征质量。

Result: 在仿真和真实环境的实验中，HyperTASR在不同表征范式下均有显著性能提升。消融实验与注意力可视化结果表明该方法能选择性关注任务相关信息，效果更接近人类的感知适应性。

Conclusion: HyperTASR为机器人操作任务中的视觉表征和政策学习提供了更高效、适应性更强的解决方案，其动态表征机制可广泛应用于多种相关领域，并推动人机感知系统的进一步发展。

Abstract: Effective policy learning for robotic manipulation requires scene
representations that selectively capture task-relevant environmental features.
Current approaches typically employ task-agnostic representation extraction,
failing to emulate the dynamic perceptual adaptation observed in human
cognition. We present HyperTASR, a hypernetwork-driven framework that modulates
scene representations based on both task objectives and the execution phase.
Our architecture dynamically generates representation transformation parameters
conditioned on task specifications and progression state, enabling
representations to evolve contextually throughout task execution. This approach
maintains architectural compatibility with existing policy learning frameworks
while fundamentally reconfiguring how visual features are processed. Unlike
methods that simply concatenate or fuse task embeddings with task-agnostic
representations, HyperTASR establishes computational separation between
task-contextual and state-dependent processing paths, enhancing learning
efficiency and representational quality. Comprehensive evaluations in both
simulation and real-world environments demonstrate substantial performance
improvements across different representation paradigms. Through ablation
studies and attention visualization, we confirm that our approach selectively
prioritizes task-relevant scene information, closely mirroring human adaptive
perception during manipulation tasks. The project website is at
\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\_projectpage}.

</details>


### [138] [Learning Real-World Acrobatic Flight from Human Preferences](https://arxiv.org/abs/2508.18817)
*Colin Merk,Ismail Geles,Jiaxu Xing,Angel Romero,Giorgia Ramponi,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 该论文提出了一种改进的偏好基强化学习方法，用于训练无人机执行杂技飞行动作，无需手工奖励设计，并在模拟及真实环境中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 杂技飞行等复杂控制任务难以通过手工奖励精准定义目标，而且这些任务通常涉及主观和风格化的要求。偏好基强化学习能够根据人的偏好直接引导策略学习，但现有方法在学习稳定性和奖励建模上仍有提升空间。

Method: 作者基于Preference PPO，提出了Reward Ensemble under Confidence (REC)方法，改进了奖励学习的目标函数，更好地建模人类偏好，提高学习的稳定性。此外，作者在模拟环境中预训练策略，并迁移到真实无人机上进行验证。还验证了该方法在MuJoCo等连续控制任务中的适用性。

Result: 新方法在杂技飞行任务中达到88.4%的专家奖励表现，标准Preference PPO仅为55.2%。手工奖励与人类偏好的一致率只有60.7%。方法不仅适用于无人机，还在MuJoCo环境中验证了可行性。

Conclusion: 改进的PbRL方法能更好贴合人类复杂主观需求，在实际无人机和模拟环境下均有卓越表现，优于依赖手工奖励的传统方法。

Abstract: Preference-based reinforcement learning (PbRL) enables agents to learn
control policies without requiring manually designed reward functions, making
it well-suited for tasks where objectives are difficult to formalize or
inherently subjective. Acrobatic flight poses a particularly challenging
problem due to its complex dynamics, rapid movements, and the importance of
precise execution. In this work, we explore the use of PbRL for agile drone
control, focusing on the execution of dynamic maneuvers such as powerloops.
Building on Preference-based Proximal Policy Optimization (Preference PPO), we
propose Reward Ensemble under Confidence (REC), an extension to the reward
learning objective that improves preference modeling and learning stability.
Our method achieves 88.4% of the shaped reward performance, compared to 55.2%
with standard Preference PPO. We train policies in simulation and successfully
transfer them to real-world drones, demonstrating multiple acrobatic maneuvers
where human preferences emphasize stylistic qualities of motion. Furthermore,
we demonstrate the applicability of our probabilistic reward model in a
representative MuJoCo environment for continuous control. Finally, we highlight
the limitations of manually designed rewards, observing only 60.7% agreement
with human preferences. These results underscore the effectiveness of PbRL in
capturing complex, human-centered objectives across both physical and simulated
domains.

</details>


### [139] [AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy](https://arxiv.org/abs/2508.18820)
*Christian Henkel,Marco Lampacrescia,Michaela Klauck,Matteo Morelli*

Main category: cs.RO

TL;DR: 本文提出了一种新的正式验证方法来在设计阶段验证自主机器人的系统属性，通过扩展SCXML模型和开发AS2FM工具，实现利用统计模型检测（SMC）将复杂自主系统转换为JANI标准格式，并以此进行高效验证。


<details>
  <summary>Details</summary>
Motivation: 由于自主机器人难以预见的运行环境和复杂多变的系统行为，使得在设计阶段确保其安全性和功能正确性变得非常具有挑战。作者希望通过引入正式化验证方法，实现对复杂系统属性的可靠检验，提升系统设计的鲁棒性。

Method: 作者扩展了SCXML，用以全面建模包含ROS 2和行为树（BT）特性的系统组件。随后开发AS2FM工具，将完整模型自动转换为JANI格式，并利用现成的SMC工具对模型进行属性验证。其方法还通过案例研究和性能评估，考察了工具在实际应用中的有效性和可扩展性。

Result: 通过在ROS 2环境下的机器人操作案例中应用该方法，快速检测出实际系统中的问题，使用普通硬件的模型验证耗时不到一秒。通过与现有方法对比，AS2FM在系统功能支持方面更为全面，且验证时间随模型规模呈线性增长，而非指数增长。

Conclusion: 新提出的AS2FM工具和建模方法有效支持了对现实复杂自主机器人系统的全面、高效验证，为自主系统的安全与可靠设计提供了实用方法，特别是在系统规模扩展时其效率优势明显。

Abstract: Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.

</details>


### [140] [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](https://arxiv.org/abs/2508.18937)
*Wang Jiayin,Wei Yanran,Jiang Lei,Guo Xiaoyu,Zheng Ayong,Zhao Weidong,Li Zhongkui*

Main category: cs.RO

TL;DR: 本文提出了一种名为VisionSafeEnhanced Visual Predictive Control (VPC)的新方法，用于机器人辅助手术中内窥镜的自主安全控制，实现高鲁棒性和视野安全。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术中的内窥镜自主控制有望提升手术安全和医生体验，但通常受限于图像可见性和各种不确定因素（如参数误差、噪声等），因此亟需更鲁棒且能适应不确定性的控制方法。

Method: 作者提出了一种基于高斯过程回归（GPR）进行不确定性量化的控制框架，通过概率安全约束与不确定性自适应的路线优化设计，实现手术摄像头稳健自主控制。采用安全控制屏障函数（CBF）对不确定性进行适配，结合机会约束，动态分配控制资源，确保手术区域视野始终安全。

Result: 该方法在商用手术机器人平台（微创医疗图迈机器人）上通过多靶点淋巴结切除任务仿真和实验证明，相较于其他基线方法，该框架能实现>99.9%的视野可见性，并提升了跟踪精度。

Conclusion: VisionSafeEnhanced VPC方法有效提升了机器人手术中内窥镜控制的鲁棒性和安全性，在复杂不确定环境下依然保障了高安全、低干预的摄像头轨迹优化，在实际和仿真中均表现突出。

Abstract: Autonomous control of the laparoscope in robot-assisted Minimally Invasive
Surgery (MIS) has received considerable research interest due to its potential
to improve surgical safety. Despite progress in pixel-level Image-Based Visual
Servoing (IBVS) control, the requirement of continuous visibility and the
existence of complex disturbances, such as parameterization error, measurement
noise, and uncertainties of payloads, could degrade the surgeon's visual
experience and compromise procedural safety. To address these limitations, this
paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and
uncertainty-adaptive framework for autonomous laparoscope control that
guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian
Process Regression (GPR) is utilized to perform hybrid (deterministic +
stochastic) quantification of operational uncertainties including residual
model uncertainties, stochastic uncertainties, and external disturbances. Based
on uncertainty quantification, a novel safety aware trajectory optimization
framework with probabilistic guarantees is proposed, where a
uncertainty-adaptive safety Control Barrier Function (CBF) condition is given
based on uncertainty propagation, and chance constraints are simultaneously
formulated based on probabilistic approximation. This uncertainty aware
formulation enables adaptive control effort allocation, minimizing unnecessary
camera motion while maintaining robustness. The proposed method is validated
through comparative simulations and experiments on a commercial surgical robot
platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph
node dissection. Compared with baseline methods, the framework maintains
near-perfect target visibility (>99.9%), reduces tracking e

</details>


### [141] [Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm](https://arxiv.org/abs/2508.18967)
*Hichem Cheriet,Khellat Kihel Badra,Chouraqui Samira*

Main category: cs.RO

TL;DR: 本文提出了一种新的无人机路径规划算法TIG，可在静态和动态环境中高效、安全地规划路径，实验表现优越。


<details>
  <summary>Details</summary>
Motivation: 无人机在军事、快递、搜救等任务中的高效安全导航具有重要意义，现有路径规划方法在效率和安全性方面存在不足。

Method: 提出TIG算法，利用椭圆切线交点方法生成可行路径，每遇到威胁点生成两条子路径，通过启发式规则选择最优路径，迭代直到到达目标。结合无人机动力学约束，使用基于二次Bezier曲线的平滑技术优化路径。

Result: 实验显示，TIG算法在静态环境下，相较A*、PRM、RRT*等主流算法，能以更短时间（起始0.01秒）、更少转角生成更短路径；在未知或部分已知动态环境中，TIG在实时避障与路径优化上也优于APF和Dynamic APPATT等算法。

Conclusion: TIG算法能有效提升无人机的路径规划效率和安全性，在多种环境下均优于传统规划方法，具有推广应用价值。

Abstract: Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical
for various applications, including combat support, package delivery and Search
and Rescue Operations. This paper introduces the Tangent Intersection Guidance
(TIG) algorithm, an advanced approach for UAV path planning in both static and
dynamic environments. The algorithm uses the elliptic tangent intersection
method to generate feasible paths. It generates two sub-paths for each threat,
selects the optimal route based on a heuristic rule, and iteratively refines
the path until the target is reached. Considering the UAV kinematic and dynamic
constraints, a modified smoothing technique based on quadratic B\'ezier curves
is adopted to generate a smooth and efficient route. Experimental results show
that the TIG algorithm can generate the shortest path in less time, starting
from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent
Graph, and Static APPATT algorithms in static environments. Furthermore, in
completely unknown and partially known environments, TIG demonstrates efficient
real-time path planning capabilities for collision avoidance, outperforming APF
and Dynamic APPATT algorithms.

</details>


### [142] [HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots](https://arxiv.org/abs/2508.19002)
*Shipeng Lyu,Fangyuan Wang,Weiwei Lin,Luhao Zhu,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 本文提出了一种新的双层闭环框架HuBE，用于在不同类人机器人之间实现高保真、适应性强的人类动作生成，并通过新数据集与增强策略提升行为的可迁移性和自然性。


<details>
  <summary>Details</summary>
Motivation: 当前类人机器人要实现既与人类行为高度相似又适应环境的动作生成面临诸多挑战，尤其在不同硬件平台（cross-embodiment）之间的泛化能力有限。此外，当前方法存在运动生成与实际执行之间结构失配的问题。

Method: 提出HuBE双层闭环架构，融合机器人状态、目标姿态和上下文情境，实现高质量的人类行为生成；构建了带有细粒度情境标注的HPose数据集，并设计了基于骨骼缩放的数据增强方法以增强不同机器人之间的运动兼容性。

Result: 在多个商业化类人机器人平台上进行了全面测试，结果显示HuBE在动作相似性、行为适宜性和计算效率等方面，均优于现有主流方法。

Conclusion: HuBE为类人机器人之间可迁移、拟人行为的稳定执行奠定了基础，让不同硬件平台上的机器人都能表现出高度类似且合适的人类行为。

Abstract: Achieving both behavioral similarity and appropriateness in human-like motion
generation for humanoid robot remains an open challenge, further compounded by
the lack of cross-embodiment adaptability. To address this problem, we propose
HuBE, a bi-level closed-loop framework that integrates robot state, goal poses,
and contextual situations to generate human-like behaviors, ensuring both
behavioral similarity and appropriateness, and eliminating structural
mismatches between motion generation and execution. To support this framework,
we construct HPose, a context-enriched dataset featuring fine-grained
situational annotations. Furthermore, we introduce a bone scaling-based data
augmentation strategy that ensures millimeter-level compatibility across
heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial
platforms demonstrate that HuBE significantly improves motion similarity,
behavioral appropriateness, and computational efficiency over state-of-the-art
baselines, establishing a solid foundation for transferable and human-like
behavior execution across diverse humanoid robots.

</details>


### [143] [An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](https://arxiv.org/abs/2508.19074)
*ZhenDong Chen,ZhanShang Nie,ShiXing Wan,JunYi Li,YongTian Cheng,Shuai Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种集成反馈机制和验证流程的大模型机器人编程框架，有效提升了生成代码的正确性，显著增强了轻量级大语言模型在机器人控制任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前，利用大语言模型（LLM）根据自然语言用户指令为机器人生成控制程序存在部分类似代码错误频发且正确性难以保障的问题，尤其在使用轻量级LLM时更为严重。因此，需要一种机制来提升代码生成的可靠性和准确性，以更好地推广LLM在机器人领域的应用。

Method: 作者提出了自然-机器人语言翻译框架（NRTrans），核心在于设计了一种机器人技能语言（RSL），用以抽象和过渡自然语言任务与具体的机器人技能控制代码。此外，研发了RSL的编译器与调试器，能对由大模型生成的RSL程序进行验证与反馈，促使LLM根据错误反馈不断调整输出，直到通过验证。最终保证只有通过验证的程序才会被下发至机器人执行。

Result: 实验表明，NRTrans在多种大语言模型和任务测试下均优于现有方法，显著提高了代码正确率，特别是在轻量级LLM场景下也能实现高成功率。

Conclusion: NRTrans为基于大模型的机器人应用提供了程序正确性保障，为轻量级LLM赋能机器人任务提供了有效的途径，对提升未来机器人智能与安全具备重要意义。

Abstract: The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

</details>


### [144] [DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning](https://arxiv.org/abs/2508.19114)
*Alkesh K. Srivastava,Jared Michael Levin,Alexander Derrico,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了DELIVER，一个面向多机器人协作拾取和投递任务的全流程系统，能够通过自然语言指令协同管理多个机器人，高效、无碰撞地完成任务。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，多机器人协作完成物品的搬运任务，常涉及复杂的协调、路径规划及人机交互问题，尤其是通过自然语言进行灵活指挥的能力极为缺乏。为解决上述挑战，作者开发了DELIVER，实现了可扩展、通用且易于部署的多机器人自然语言任务管理与执行框架。

Method: DELIVER整合了自然语言理解（通过LLaMA3解释指令并提取任务地点）、空间划分（应用Voronoi图分割作业区）、接力点规划（优化机器人交接位置）以及状态机行为控制。该方案在MultiTRAIL仿真平台、ROS2 Gazebo仿真及TurtleBot3真实机器人上进行了验证。

Result: 实验表明，DELIVER系统能够在不同规模的机器人团队中维持一致的任务消耗，并相比单机器人方案降低高达55%的单体工作量。同时，即使团队规模扩大，实际参与中继的机器人数量仍然很少，展示了出色的可扩展性和高效利用能力。

Conclusion: DELIVER具备模块化、可扩展性及对自然语言交互的强大支持，为多机器人系统的集成和人机协作前沿提供了新进展。

Abstract: We present DELIVER (Directed Execution of Language-instructed Item Via
Engineered Relay), a fully integrated framework for cooperative multi-robot
pickup and delivery driven by natural language commands. DELIVER unifies
natural language understanding, spatial decomposition, relay planning, and
motion execution to enable scalable, collision-free coordination in real-world
settings. Given a spoken or written instruction, a lightweight instance of
LLaMA3 interprets the command to extract pickup and delivery locations. The
environment is partitioned using a Voronoi tessellation to define
robot-specific operating regions. Robots then compute optimal relay points
along shared boundaries and coordinate handoffs. A finite-state machine governs
each robot's behavior, enabling robust execution. We implement DELIVER on the
MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo
simulations and real-world hardware using TurtleBot3 robots. Empirical results
show that DELIVER maintains consistent mission cost across varying team sizes
while reducing per-agent workload by up to 55% compared to a single-agent
system. Moreover, the number of active relay agents remains low even as team
size increases, demonstrating the system's scalability and efficient agent
utilization. These findings underscore DELIVER's modular and extensible
architecture for language-guided multi-robot coordination, advancing the
frontiers of cyber-physical system integration.

</details>


### [145] [ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments](https://arxiv.org/abs/2508.19131)
*Shreya Gummadi,Mateus V. Gasparino,Gianluca Capezzuto,Marcelo Becker,Girish Chowdhary*

Main category: cs.RO

TL;DR: 该论文提出了一种基于大语言模型视觉推理能力的实时地形可通行性预测新方法ZeST，实现了无需将机器人暴露于危险环境中即可生成可通行性地图。该方法具备零样本能力，降低了数据采集风险，加速了导航系统开发，且实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人及自主导航系统发展受限于对地形可通行性精准预测，传统数据集获取方法需要将机器人投放到危险环境中，有设备和安全风险。

Method: 作者提出ZeST方法，利用大语言模型的视觉推理能力，实时生成地形可通行性地图，无需机器人实地采集数据，直接实现零样本推理。

Result: 在室内受控和室外非结构化环境下的导航实验表明，ZeST方法对比多种主流方法，能够更安全、更成功地完成终点导航任务。

Conclusion: ZeST不仅提供一种安全、高效、可扩展的生成地形可通过性图方法，还推动了导航系统的发展，实验验证了其实用价值和相较于现有方法的优势。

Abstract: The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.

</details>


### [146] [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](https://arxiv.org/abs/2508.19150)
*Juan Carlos Saborío,Marc Vinci,Oscar Lima,Sebastian Stock,Lennart Niecksch,Martin Günther,Alexander Sung,Joachim Hertzberg,Martin Atzmüller*

Main category: cs.RO

TL;DR: 本论文提出了一种能更好处理不确定性和传感器噪声的机器人助理意图识别和规划框架，并在实体机器人上取得了成功的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人助理系统要么过度依赖于对明确提示的识别，限制了自主性，要么过度简化问题（如假设信息几乎完美），未能充分应对在意图识别中固有的不确定性和感知误差。

Method: 提出并实现了一个集成多规划器与实时传感器数据的框架，以意图识别POMDP为核心，能够在不确定性下进行协作规划与行动。

Result: 该框架已在实际的物理机器人上进行了测试，并取得了有前景的表现，显示出其鲁棒性和有效性。

Conclusion: 框架验证了在面对意图识别的不确定性及感知噪声时的优越性，为增强机器人助理的自主性和协作能力提供了实证支持。

Abstract: Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.

</details>


### [147] [QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning](https://arxiv.org/abs/2508.19153)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: 该论文提出QuadKAN，一种结合本体感受与视觉输入、基于样条参数化和Kolmogorov-Arnold网络（KANs）的四足机器人视觉导航运动控制方法，并在多种复杂地形上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人视觉引导控制方法在鲁棒性、能效与动作平滑性上存在不足，简单地依赖视觉或本体感受信息无法同时兼顾多样场景下的稳定行走。因而需要一种能高效融合两种感知方式、并具备更好动作解释性与控制效果的新方法。

Method: 作者提出QuadKAN：采用样条编码器处理本体感受输入，样条融合头整合视觉与本体感受信号，利用Kolmogorov-Arnold网络进行状态到动作的映射，配合多模态延迟随机化（MMDR）、使用PPO算法端到端训练。通过样条结构对步态的分段平滑特性对齐，提高采样效率，减少动作抖动与能耗，并提升动作解释性。

Result: 在多种地形（平坦/不平坦、有静态/动态障碍场景）下测试，QuadKAN的回报、行进距离明显高于SOTA基线，碰撞率更低，显示出更强的鲁棒性和适应性。

Conclusion: 基于样条参数化与KANs的新型跨模态策略为视觉引导四足运动控制提供了一种简洁、高效且易解释的方法，在实际机器人复杂场景控制中具有明显优势。

Abstract: We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.

</details>


### [148] [Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform](https://arxiv.org/abs/2508.19164)
*Morokot Sakal,George Nehma,Camilo Riano-Rios,Madhur Tiwari*

Main category: cs.RO

TL;DR: 本文提出了一种包括反应轮健康状态估算功能的自适应卫星姿态控制系统的硬件在环（HIL）测试方案。


<details>
  <summary>Details</summary>
Motivation: 此前的仿真和软件在环测试表明有必要进一步用实际的动量交换设备进行实验，以验证控制器的有效性。目标是朝着建立一个完善的航天器姿态控制算法测试验证框架迈进。

Method: 构建了包含无刷直流电动机及其驱动器（通过CAN总线通信）、用于执行控制与自适应算法的嵌入式计算机，以及可产生虚拟传感器数据和估算姿态状态的卫星模拟器的硬件在环测试平台。提出了方法以人为方式模拟反应轮故障，并总结了相关实用经验与教训。

Result: 通过HIL测试验证了该系统和方法的可行性，发现并收集了反应轮人工故障诱导下控制系统的表现、相关问题及应对措施。

Conclusion: 此项工作为卫星姿态控制算法的全面测试和验证提供了有力工具，并为今后更加复杂的HIL测试奠定了基础。

Abstract: We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite
attitude control system with reaction wheel health estimation capabilities.
Previous simulations and Software-in-the-Loop testing have prompted further
experiments to explore the validity of the controller with real momentum
exchange devices in the loop. This work is a step toward a comprehensive
testing framework for validation of spacecraft attitude control algorithms. The
proposed HIL testbed includes brushless DC motors and drivers that communicate
using a CAN bus, an embedded computer that executes control and adaptation
laws, and a satellite simulator that produces simulated sensor data, estimated
attitude states, and responds to actions of the external actuators. We propose
methods to artificially induce failures on the reaction wheels, and present
related issues and lessons learned.

</details>


### [149] [Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic](https://arxiv.org/abs/2508.19168)
*Liding Zhang,Kejia Chen,Kuanqi Cai,Yu Zhang,Yixuan Dang,Yansong Wu,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种新的路径规划算法DIT*，它通过方向成本启发式优化路径发现过程，使搜索速度更快，且已在高维空间和实际任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的路径规划算法依赖启发式信息，但难以兼顾启发式的准确性与计算效率。本研究希望通过优化搜索方向，提升算法性能。

Method: 作者提出了方向引导树（Direction Informed Trees, DIT*）的采样算法，并将边表示为广义向量，用相似度指标筛选临近节点和估算方向成本，将方向成本用于边的评估，从而引导搜索过程。

Result: DIT*在R^4到R^16等高维环境中的收敛速度快于其他单次查询采样算法，并在多种实际任务中演示了其有效性。

Conclusion: DIT*以方向启发为核心，能高效快速地完成高维空间下的路径规划任务，在实际环境中表现优异。

Abstract: Optimal path planning requires finding a series of feasible states from the
starting point to the goal to optimize objectives. Popular path planning
algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to
guide the search. Effective heuristics are accurate and computationally
efficient, but achieving both can be challenging due to their conflicting
nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based
planner that focuses on optimizing the search direction for each edge,
resulting in goal bias during exploration. We define edges as generalized
vectors and integrate similarity indexes to establish a directional filter that
selects the nearest neighbors and estimates direction costs. The estimated
direction cost heuristics are utilized in edge evaluation. This strategy allows
the exploration to share directional information efficiently. DIT* convergence
faster than existing single-query, sampling-based planners on tested problems
in R^4 to R^16 and has been demonstrated in real-world environments with
various planning tasks. A video showcasing our experimental results is
available at: https://youtu.be/2SX6QT2NOek

</details>


### [150] [From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](https://arxiv.org/abs/2508.19172)
*Luca Grillotti,Lisa Coiffard,Oscar Pang,Maxence Faldor,Antoine Cully*

Main category: cs.RO

TL;DR: 本文提出了URSA算法，实现机器人在真实世界中自主发现并习得多样化技能。与以往方法相比，URSA无需人工精细定义技能空间，并在实机和仿真中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法在真实机器人硬件上的应用受限于安全性和数据效率，且通常需手动设置技能空间和调优启发式规则，限制了实际应用。本文旨在解决这些限制，实现自动、无监督、多样化技能的自主学习。

Method: 提出了Unsupervised Real-world Skill Acquisition（URSA）方法，对现有QDAC算法进行扩展，使机器人能直接在实物环境中，通过启发式或完全无监督方式，探索并习得多样性运动技能。算法同时允许对技能空间和启发式的自动调整。

Result: URSA在Unitree A1四足机器人上，在仿真和现实世界中均能自主发现多样化的运动技能。实验结果还表明，在损伤适应等任务上，所习得的技能库可直接迁移并优于传统基线，在9个仿真和5个现实损伤场景下分别有5次和3次表现最佳。

Conclusion: URSA提出了一种通用且适用于真实机器人环境的技能发现框架，能在较少人工干预下持续探索与学习新技能，显著推动了机器人系统的自主性和适应性。

Abstract: Autonomous skill discovery aims to enable robots to acquire diverse behaviors
without explicit supervision. Learning such behaviors directly on physical
hardware remains challenging due to safety and data efficiency constraints.
Existing methods, including Quality-Diversity Actor-Critic (QDAC), require
manually defined skill spaces and carefully tuned heuristics, limiting
real-world applicability. We propose Unsupervised Real-world Skill Acquisition
(URSA), an extension of QDAC that enables robots to autonomously discover and
master diverse, high-performing skills directly in the real world. We
demonstrate that URSA successfully discovers diverse locomotion skills on a
Unitree A1 quadruped in both simulation and the real world. Our approach
supports both heuristic-driven skill discovery and fully unsupervised settings.
We also show that the learned skill repertoire can be reused for downstream
tasks such as real-world damage adaptation, where URSA outperforms all
baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.
Our results establish a new framework for real-world robot learning that
enables continuous skill discovery with limited human intervention,
representing a significant step toward more autonomous and adaptable robotic
systems. Demonstration videos are available at
http://adaptive-intelligent-robotics.github.io/URSA .

</details>


### [151] [Real-Time Model Checking for Closed-Loop Robot Reactive Planning](https://arxiv.org/abs/2508.19186)
*Christopher Chandler,Bernd Porr,Giulia Lafratta,Alice Miller*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的模型检测应用，实现了自主机器人在真实环境下的实时多步规划与避障。通过小规模、定制的模型检测算法，在低功耗设备上无需预计算即可动态生成规划，并在典型场景下取得了优良表现，对提高本地障碍规避与规划效率具有意义。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人通常依赖反应式规划方法，无法进行有效的多步障碍避让；而现有多步规划方法常需预计算或高性能设备。作者希望设计一种受生物体知识与注意力机制启发、可实时运行且对硬件要求低的多步规划方法。

Method: 作者开发了一种小型化的模型检测算法，通过临时生成串联控制系统应对环境扰动，并提出了对2D LiDAR数据的创新离散化方法以增强对局部环境变化的敏感性。核心算法利用前向深度优先搜索实现多步规划，并在特定的典型场景下进行实验验证。

Result: 实验和非正式证明表明，该模型检测方法能够高效生成多步局部避障规划，显著优于只能一步规划的传统反应式代理，在类似困境和开放场景下表现出色。

Conclusion: 实验证明了模型检测可以推动安全、可靠且可解释的多步规划在自主车辆等场景中的应用。该方法为开发更安全和高效的自主系统提供了有益的案例参考。

Abstract: We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.

</details>


### [152] [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](https://arxiv.org/abs/2508.19191)
*Yue Wang,Wenjie Deng,Haotian Xue,Di Cui,Yiqi Chen,Mingchuan Zhou,Haochao Ying,Jian Wu*

Main category: cs.RO

TL;DR: 本文提出了一个基于模仿学习的自主眼内异物环操作系统AutoRing，针对现有手术机器人高难度、依赖人工操作的问题，实现了无需深度传感的自主操作，并在未校准的显微镜条件下成功验证。


<details>
  <summary>Details</summary>
Motivation: 现有眼内异物移除手术依赖人工远程操作，存在学习曲线陡峭及精度难以保证等问题。为了提升自动化水平与操作精度，需要解决运动缩放和远心点变化产生的运动学不确定性。

Method: 提出了AutoRing框架，通过集成动态远心点（RCM）校准，克服了手术器械变动导致的坐标系不一致问题，并设计了RCM-ACT架构，将动作分块的变换器与实时运动学重新校准结合。该系统通过专家在仿生眼模型上的示范进行端到端模仿学习，仅使用立体视觉数据和器械运动学信息。

Result: AutoRing系统能够自主完成环的抓取与定位任务，无需深度传感器。实验验证显示，在未校准的显微镜下也可实现端到端自主操作。

Conclusion: 本文框架为智能眼科手术系统的开发提供了可行技术路径，有望实现复杂的眼内自动微创操作，提升手术安全性与效率。

Abstract: Intraocular foreign body removal demands millimeter-level precision in
confined intraocular spaces, yet existing robotic systems predominantly rely on
manual teleoperation with steep learning curves. To address the challenges of
autonomous manipulation (particularly kinematic uncertainties from variable
motion scaling and variation of the Remote Center of Motion (RCM) point), we
propose AutoRing, an imitation learning framework for autonomous intraocular
foreign body ring manipulation. Our approach integrates dynamic RCM calibration
to resolve coordinate-system inconsistencies caused by intraocular instrument
variation and introduces the RCM-ACT architecture, which combines
action-chunking transformers with real-time kinematic realignment. Trained
solely on stereo visual data and instrument kinematics from expert
demonstrations in a biomimetic eye model, AutoRing successfully completes ring
grasping and positioning tasks without explicit depth sensing. Experimental
validation demonstrates end-to-end autonomy under uncalibrated microscopy
conditions. The results provide a viable framework for developing intelligent
eye-surgical systems capable of complex intraocular procedures.

</details>


### [153] [Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation](https://arxiv.org/abs/2508.19199)
*Alex LaGrassa,Zixuan Huang,Dmitry Berenson,Oliver Kroemer*

Main category: cs.RO

TL;DR: 本文提出了一种自动生成任务相关、空间自适应动力学模型的新方法，能在保持较优任务表现的同时显著提升高维空间（如可变形物体）规划的计算效率。


<details>
  <summary>Details</summary>
Motivation: 高维空间（例如可变形物体）的高效规划需要动力学模型既要表达能力强（不足以会影响任务表现），又要计算开销低。但将所有区域都用高分辨率建模计算量极大；本研究希望只在任务要求的关键区域用高分辨率，从而兼顾效率与表达力。

Method: 提出一种基于扩散模型的生成器，输入为描述规划起点和目标的点云，输出各区域所需动力学分辨率。数据收集采用两阶段流程：先用预测性动力学作先验优化各区域分辨率，再用闭环性能做直接优化。

Result: 在树状物体操作任务上，所提方法仅牺牲很少任务表现，即可实现规划速度翻倍，对比全分辨率模型显著提高了效率。

Conclusion: 该方法展现了可通过历史经验数据和任务上下文自适应生成高效且足够表达的动力学模型，有助于未来在新任务中实现快速、低计算成本但高效能的规划。

Abstract: Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.

</details>


### [154] [MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2508.19236)
*Hao Shi,Bin Xie,Yingfei Liu,Lin Sun,Fengrong Liu,Tiancai Wang,Erjin Zhou,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的Cognition-Memory-Action框架MemoryVLA，用于提升机器人在长时序操作任务中的表现。该方法模拟人类工作记忆和长时记忆机制，显著提升了机器人对非马尔可夫性的感知与应对能力，在多个基准任务中远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主流视觉语言动作（VLA）模型在处理需要时间依赖的长时序机器人操作任务中表现不佳，主要因为它们忽略了对时序信息的建模，而人类能够依赖复杂的记忆系统有效应对这类任务。本文动机是借鉴认知神经科学中人类记忆系统的结构，设计更强大的机器人操作模型。

Method: MemoryVLA框架由三个部分组成，模拟了人类认知与记忆：首先通过预训练的视觉语言模型编码观测信息形成工作记忆（短时）；然后建立感知-认知记忆库，用于存储整合的低级细节和高级语义信息（长时）；决策时工作记忆动态检索、融合和更新记忆库中的相关内容；最终通过记忆条件化的扩散模型生成时序相关的动作序列。

Result: 在150多个模拟和实际机器人任务中进行验证：在SimplerEnv-Bridge、Fractal、LIBERO-5等套件上取得71.9%、72.7%、96.5%的成功率，均明显超越最先进的CogACT和pi-0基线（如在Bridge中提升14.6个百分点）；在12个实际任务中总体成功率达84.0%，长时序任务相较于基线提升26个百分点。

Conclusion: MemoryVLA通过类人认知-记忆机制建模，大幅提高了机器人的长时序操作能力，显著优于现有模型。展示了将认知科学机制用于机器人智能控制的巨大潜力。

Abstract: Temporal context is essential for robotic manipulation because such tasks are
inherently non-Markovian, yet mainstream VLA models typically overlook it and
struggle with long-horizon, temporally dependent tasks. Cognitive science
suggests that humans rely on working memory to buffer short-lived
representations for immediate control, while the hippocampal system preserves
verbatim episodic details and semantic gist of past experience for long-term
memory. Inspired by these mechanisms, we propose MemoryVLA, a
Cognition-Memory-Action framework for long-horizon robotic manipulation. A
pretrained VLM encodes the observation into perceptual and cognitive tokens
that form working memory, while a Perceptual-Cognitive Memory Bank stores
low-level details and high-level semantics consolidated from it. Working memory
retrieves decision-relevant entries from the bank, adaptively fuses them with
current tokens, and updates the bank by merging redundancies. Using these
tokens, a memory-conditioned diffusion action expert yields temporally aware
action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks
across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it
achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming
state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on
Bridge. On 12 real-world tasks spanning general skills and long-horizon
temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon
tasks showing a +26 improvement over state-of-the-art baseline. Project Page:
https://shihao1895.github.io/MemoryVLA

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [155] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: 该论文提出了一种通过深度学习进行协作式缺血性脑卒中灶区分割的新方法，能跨医疗中心提升分割效果，为实际医疗应用提供了更泛化的解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前卒中病灶的分割高度依赖人工并受多中心数据差异影响，现有方法通用性不足且训练样本有限，难以推广到各类临床场景，亟需能实现中心间知识共享、具备强泛化能力的自动分割方法。

Method: 作者基于联邦学习（FedAvg）框架，在14个模拟医疗中心的2031例卒中病例数据上，训练不依赖单一中心知识的深层特征分割模型，实现跨中心知识共享，从而提升模型在不同中心和人群上的泛化能力。

Result: FedAvg模型在所有中心的平均DSC为0.71±0.24，AVD为5.29±22.74，ALD为2.16±3.60，LF1为0.70±0.26，在分割精度上超过了集中训练和其他联邦学习方法，对类别和外部中心的泛化效果也显著优良。

Conclusion: 基于联邦学习的卒中病灶分割方法能有效提升模型的泛化能力和多中心适用性，为实际医疗应用中的自动化卒中影像分析提供了有力的技术支持。

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


### [156] [Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas](https://arxiv.org/abs/2508.18509)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: eess.IV

TL;DR: 本文研究了SalUn机器卸载模型在医学图像分类中的应用，并评估其在多个医学影像数据集上的表现。结果显示，SalUn在卸载敏感数据同时，能保持模型性能接近完全重训练，具备很高实用价值。


<details>
  <summary>Details</summary>
Motivation: 在现有的医疗图像分类模型中，如何安全高效地移除特定敏感数据，确保隐私合规且不损失模型表现，是重要但尚未深入解决的问题。

Method: 作者在PathMNIST、OrganAMNIST和BloodMNIST三个医学图像数据集上，对SalUn机器卸载算法进行实验，并分析数据增强对卸载质量的影响。

Result: SalUn模型在卸载敏感数据后，依然能保持接近完全重训练模型的性能。数据增强对卸载效果也有一定影响。

Conclusion: SalUn为医学影像模型的数据卸载提供了一种高效、实际可行的解决方案，便于医疗领域中敏感数据的合规移除与应用。

Abstract: Machine unlearning aims to remove private or sensitive data from a
pre-trained model while preserving the model's robustness. Despite recent
advances, this technique has not been explored in medical image classification.
This work evaluates the SalUn unlearning model by conducting experiments on the
PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of
data augmentation on the quality of unlearning. Results show that SalUn
achieves performance close to full retraining, indicating an efficient solution
for use in medical applications.

</details>


### [157] [A Deep Learning Application for Psoriasis Detection](https://arxiv.org/abs/2508.18528)
*Anna Milani,Fábio S. da Silva,Elloá B. Guedes,Ricardo Rios*

Main category: eess.IV

TL;DR: 本文比较了三种卷积神经网络（ResNet50、Inception v3和VGG19）在银屑病皮肤病变图像分类任务中的表现，发现Inception v3表现最佳。


<details>
  <summary>Details</summary>
Motivation: 准确诊断银屑病对患者治疗和管理非常重要；探索深度学习在医学图像自动分类上的效果。

Method: 收集带银屑病病变的皮肤图像，利用ResNet50、Inception v3、VGG19三种CNN模型进行训练和验证，并应用某些评估指标调整技术对模型评估。

Result: Inception v3模型在准确率和F1-Score方面均表现突出，F1-Score达97.5% ± 0.2。

Conclusion: Inception v3是辅助银屑病诊断的有效工具，优于其它比较模型。

Abstract: In this paper a comparative study of the performance of three Convolutional
Neural Network models, ResNet50, Inception v3 and VGG19 for classification of
skin images with lesions affected by psoriasis is presented. The images used
for training and validation of the models were obtained from specialized
platforms. Some techniques were used to adjust the evaluation metrics of the
neural networks. The results found suggest the model Inception v3 as a valuable
tool for supporting the diagnosis of psoriasis. This is due to its satisfactory
performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

</details>


### [158] [A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework](https://arxiv.org/abs/2508.18790)
*Yuhui Tao,Yizhe Zhang,Qiang Chen*

Main category: eess.IV

TL;DR: 该论文提出一种用于黄斑水肿区域分割的新方法，结合视网膜层结构信息和测试时自适应策略，在弱监督条件下提升了分割准确性和鲁棒性，有效缩小了与全监督方法之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有黄斑水肿区域（EA）分割依赖专家标注的高质量像素级数据集，收集成本高昂；弱监督异常检测方法虽有潜力，但性能仍不及全监督方法。亟需提升弱监督模型分割表现，并降低高标注成本。

Method: 提出结合视网膜层结构信息与测试时自适应（TTA）策略的对抗性分割框架。方法包括引入结构引导的后处理步骤，将原本密集预测任务转化为判定EA轮廓与视网膜层的交点问题，从而更贴合EA区域的形状先验，并通过TTA策略缓解训练与测试集间表现差异。

Result: 在两个公开数据集上，通过引入层结构指引和TTA方法，显著提升了弱监督分割模型的准确度和鲁棒性，性能接近全监督模型。

Conclusion: 结合层结构信息与测试时自适应方法的弱监督EA分割框架，有效提升分割效果并缩小与全监督方法的差距，具有实际应用潜力。

Abstract: The development of artificial intelligence models for macular edema (ME)
analy-sis always relies on expert-annotated pixel-level image datasets which
are expen-sive to collect prospectively. While anomaly-detection-based
weakly-supervised methods have shown promise in edema area (EA) segmentation
task, their per-formance still lags behind fully-supervised approaches. In this
paper, we leverage the strong correlation between EA and retinal layers in
spectral-domain optical coherence tomography (SD-OCT) images, along with the
update characteristics of weakly-supervised learning, to enhance an
off-the-shelf adversarial framework for EA segmentation with a novel
layer-structure-guided post-processing step and a test-time-adaptation (TTA)
strategy. By incorporating additional retinal lay-er information, our framework
reframes the dense EA prediction task as one of confirming intersection points
between the EA contour and retinal layers, result-ing in predictions that
better align with the shape prior of EA. Besides, the TTA framework further
helps address discrepancies in the manifestations and presen-tations of EA
between training and test sets. Extensive experiments on two pub-licly
available datasets demonstrate that these two proposed ingredients can im-prove
the accuracy and robustness of EA segmentation, bridging the gap between
weakly-supervised and fully-supervised models.

</details>


### [159] [Understanding Benefits and Pitfalls of Current Methods for the Segmentation of Undersampled MRI Data](https://arxiv.org/abs/2508.18975)
*Jan Nikolas Morshuis,Matthias Hein,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: 本文提出并实现了首个用于欠采样MRI数据分割的统一基准，系统比较了7种主流方法，并发现包含数据一致性的简单两阶段方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然加速MRI采集技术已取得进展，但现有多种直接在加速数据上分割的算法未进行系统对比，存在评估标准不统一、缺乏公开数据等问题，实际应用中最优的分割策略尚不明确。

Method: 作者统一收集并整理了两组带有人工标注真实分割的多线圈MRI原始数据，比较了7种当前主流的加速MRI分割方法，特别关注将重建与分割联合建模的一阶段方法与重建和分割分开的两阶段方法；系统对各方法进行实验和评估。

Result: 实验结果显示，包含数据一致性处理的简单两阶段模型，在分割表现上明显优于针对该任务特别开发的复杂一阶段方法。

Conclusion: 对于加速MRI的分割任务，把重建与分割分离、强调数据一致性原则的两阶段方法更有效；本工作为该领域今后的研究建立了基准，并为选择高效分割策略提供了指导。

Abstract: MR imaging is a valuable diagnostic tool allowing to non-invasively visualize
patient anatomy and pathology with high soft-tissue contrast. However, MRI
acquisition is typically time-consuming, leading to patient discomfort and
increased costs to the healthcare system. Recent years have seen substantial
research effort into the development of methods that allow for accelerated MRI
acquisition while still obtaining a reconstruction that appears similar to the
fully-sampled MR image. However, for many applications a perfectly
reconstructed MR image may not be necessary, particularly, when the primary
goal is a downstream task such as segmentation. This has led to growing
interest in methods that aim to perform segmentation directly on accelerated
MRI data. Despite recent advances, existing methods have largely been developed
in isolation, without direct comparison to one another, often using separate or
private datasets, and lacking unified evaluation standards. To date, no
high-quality, comprehensive comparison of these methods exists, and the optimal
strategy for segmenting accelerated MR data remains unknown. This paper
provides the first unified benchmark for the segmentation of undersampled MRI
data comparing 7 approaches. A particular focus is placed on comparing
\textit{one-stage approaches}, that combine reconstruction and segmentation
into a unified model, with \textit{two-stage approaches}, that utilize
established MRI reconstruction methods followed by a segmentation network. We
test these methods on two MRI datasets that include multi-coil k-space data as
well as a human-annotated segmentation ground-truth. We find that simple
two-stage methods that consider data-consistency lead to the best segmentation
scores, surpassing complex specialized methods that are developed specifically
for this task.

</details>


### [160] [RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration](https://arxiv.org/abs/2508.19154)
*Yan Chen,Yi Wen,Wei Li,Junchao Liu,Yong Guo,Jie Hu,Xinghao Chen*

Main category: eess.IV

TL;DR: 本文提出了RDDM，一种直接从传感器RAW数据恢复照片级真实图像的端到端扩散模型，突破了传统sRGB域扩散方法的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的sRGB域扩散模型在照片级图像恢复上达到了不错效果，但在高保真与真实感之间存在困境。此外，sRGB输入本身有信息损失，并且在许多场景下原始RAW图像是可获取的，所以基于sRGB的方法性能受限。

Method: RDDM直接在RAW域上进行图像恢复，替代传统的ISP与IR两阶段流程。其创新性方案包括：(1)提出RAW域VAE用于学习最优的潜在表示；(2)设计可微的Post Tone Processing（PTP）模块，实现RAW与sRGB空间联合优化；(3)建立可扩展的退化合成流程，将sRGB数据转为RAW的低高质量对以支持大规模训练；(4)提出可配置的多拜耳（CMB）LoRA模块，适应不同的RAW拜耳模式。

Result: 大量实验表明，RDDM优于现有最优的sRGB扩散方法，在恢复图像时具有更高的保真度与更少伪影。

Conclusion: RDDM突破了现有扩散模型受限于sRGB域的局限，在RAW域下实现了更高质量的图像恢复，有望推动边缘设备及相关图像恢复场景的研究与应用。

Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion
model that restores photo-realistic images directly from the sensor RAW data.
While recent sRGB-domain diffusion methods achieve impressive results, they are
caught in a dilemma between high fidelity and realistic generation. As these
models process lossy sRGB inputs and neglect the accessibility of the sensor
RAW images in many scenarios, e.g., in image and video capturing in edge
devices, resulting in sub-optimal performance. RDDM bypasses this limitation by
directly restoring images in the RAW domain, replacing the conventional
two-stage image signal processing (ISP) + IR pipeline. However, a simple
adaptation of pre-trained diffusion models to the RAW domain confronts the
out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE
(RVAE) learning optimal latent representations, (2) a differentiable Post Tone
Processing (PTP) module enabling joint RAW and sRGB space optimization. To
compensate for the deficiency in the dataset, we develop a scalable degradation
pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for
large-scale training. Furthermore, we devise a configurable multi-bayer (CMB)
LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive
experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion
methods, yielding higher fidelity results with fewer artifacts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [161] [EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding](https://arxiv.org/abs/2508.18785)
*Luqing Luo,Wenjin Gui,Yunfei Liu,Ziyue Zhang,Yunxi Zhang,Fengxiang Wang,Zonghao Guo,Zizhi Ma,Xinzhu Liu,Hanxiang He,Jinhai Li,Xin Qiu,Wupeng Xie,Yangang Sun*

Main category: eess.SP

TL;DR: 本文提出了EMind，一种面向电磁信号的基础模型，实现了跨任务高效泛化。作者还构建了最大且统一的电磁信号数据集，并创新性地设计了长度自适应的多信号打包方法和硬件感知训练策略，有效提升了模型在多源异构信号上的表现。实验显示EMind在多个下游任务中均有优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前电磁信号在动态频谱管理、智能交通、自动驾驶等领域有广泛应用，但因其异质性强、噪声大、时频结构复杂等特点，通用模型无法直接迁移应用。此外，电磁通信与感知任务种类繁多，现有方法泛化和迁移效率有限，且缺少高质量大规模数据集，限制了多任务学习框架的发展。

Method: 作者首先构建了第一个涵盖多种信号类型和任务的最大标准化电磁信号数据集。在模型设计上，提出了长度自适应的多信号打包方法，并采用硬件感知训练策略，提升多源异构信号的数据利用率和学习效率。通过大规模预训练，EMind能够捕捉电磁信号的物理特性，达到更好的泛化能力。

Result: EMind在多个下游电磁信号处理任务上都展现出强大的性能和出色的泛化能力，明显优于以往的各类任务专用模型，实现了从单任务向统一电磁智能框架的转变。

Conclusion: EMind作为一款电磁领域的基础模型，通过创新数据集、独特的数据处理和训练策略，为丰富复杂的电磁信号建模和多任务泛化提供了新的思路和有效方案。代码已开源，将推动电磁信号智能研究进展。

Abstract: Deep understanding of electromagnetic signals is fundamental to dynamic
spectrum management, intelligent transportation, autonomous driving and
unmanned vehicle perception. The field faces challenges because electromagnetic
signals differ greatly from text and images, showing high heterogeneity, strong
background noise and complex joint time frequency structure, which prevents
existing general models from direct use. Electromagnetic communication and
sensing tasks are diverse, current methods lack cross task generalization and
transfer efficiency, and the scarcity of large high quality datasets blocks the
creation of a truly general multitask learning framework. To overcome these
issue, we introduce EMind, an electromagnetic signals foundation model that
bridges large scale pretraining and the unique nature of this modality. We
build the first unified and largest standardized electromagnetic signal dataset
covering multiple signal types and tasks. By exploiting the physical properties
of electromagnetic signals, we devise a length adaptive multi-signal packing
method and a hardware-aware training strategy that enable efficient use and
representation learning from heterogeneous multi-source signals. Experiments
show that EMind achieves strong performance and broad generalization across
many downstream tasks, moving decisively from task specific models to a unified
framework for electromagnetic intelligence. The code is available at:
https://github.com/GabrielleTse/EMind.

</details>
