<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 27]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 本论文提出了一套结合运动学与语言解析的自动标注流程，发布了细粒度动作理解基准KPM-Bench，并引入了解决幻觉问题的新算法MoPE及相关指标，有效提升了运动型视频字幕的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成模型在描述复杂细节动作和避免生成错误内容（幻觉）方面存在明显短板，特别是在关注人体细致动作的视频中，这一问题尤为突出。作者希望通过精确刻画动作细节来提升生成字幕的质量和可靠性。

Method: 作者设计了一条自动化标注流程，将运动学计算与语言解析结合，实现了对复杂动作的细致分解与描述。基于该流程构建了KPM-Bench数据集，提供细粒度视频-字幕对、与动作理解相关的问答对和针对幻觉问题的评测试集。同时，提出MoPE算法可从字幕文本中提取动作属性，用来独立精确评估和缓解幻觉问题。最后将MoPE融入GRPO后训练框架，优化模型表现。

Result: KPM-Bench为细致动作理解任务提供了丰富的数据资源。MoPE算法实现了与主流大模型无关的幻觉自动评估与监控，将其应用于训练后，显著减少了模型输出中关于动作的幻觉现象，提高了字幕的准确性。

Conclusion: 文章提出的方法与评测体系，可系统性解决运动型视频字幕中的细粒度动作描述及幻觉问题，为该领域研究提供了新的资源与工具，对提升视频字幕生成的可靠性具有重要意义。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了一个大规模“野外”3D手部动作与文本配对数据集（3D-HIW），以及基于大语言模型的高保真手部动作生成系统CLUTCH，通过创新的动作编码机制和几何精细化方法，实现了文本与手部动画的高质量对齐与生成。


<details>
  <summary>Details</summary>
Motivation: 现有手部动作建模多依赖昂贵且受限的实验室数据，且模型难以实现自然场景下手势动画与文本的高质量对应，制约了真实应用与大规模推广。

Method: 1）构建3D-HIW数据集，结合视觉-语言模型与3D手追踪器，对大量头戴视角动作视频进行自动注释；2）提出CLUTCH系统，采用创新的SHIFT（分部模态VQ-VAE）对手部动作进行有效编码，并引入几何精细化阶段，通过重建损失提升动画质量和对齐度。

Result: 在文本到动作、动作到文本等任务上，CLUTCH系统均取得了当前最优性能，首创“野外”手部动作建模基准，展现出更强泛化和重建能力。

Conclusion: 提出的3D-HIW数据集和CLUTCH系统推动了自然场景下大规模手部动作建模与生成，有望促进手势理解、多模态人机交互等领域的研究与应用。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 提出了PRISM，一种用于结肠镜导航的自监督单目深度与位姿估计方法，结合结构边缘检测和光照分离，在多数据集上取得了SOTA效果，并分析了训练策略。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查存在盲区、复发漏检及不完全检查等风险，改进单目深度与位姿估计有助于提升导航精度，但受限于低纹理、复杂光照、形变和缺乏真实标注数据等挑战。

Method: 提出PRISM框架，充分利用自监督学习，引入基于深度网络的边缘检测器获取结构边缘，采用内在分解模块进行光照与反射分离，使模型能够更好利用阴影信息提升深度估计能力。

Result: 在多个真实和合成数据集上测试，PRISM取得了当前最优表现。消融实验还分析了训练数据选择问题。

Conclusion: 研究表明：(1) 用真实数据进行自监督训练优于用仿真数据进行有监督训练，现实域特性胜于绝对真值；(2) 视频采样帧率对任务效果极为关键，需针对数据集优化采样策略。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新型神经网络（LGD-Net），可利用H&E切片直接评价HER2表达水平，避免常规IHC染色步骤，精度高效率好。


<details>
  <summary>Details</summary>
Motivation: IHC染色作为HER2表达评估标准，成本高、步骤繁琐，并非所有地区都能获得。迫切需要一种仅用常规H&E切片就能准确评价HER2的方法，降低资源消耗和等待时间。现有基于H&E的虚拟IHC方法依赖像素级重建，既算力消耗大，也易引入伪影影响诊断。

Method: 提出Latent-Guided Dual-Stream Network（LGD-Net），用跨模态特征“幻觉”，不做显式像素级转换，而是将H&E切片建模到IHC分子层潜在空间，通过教师IHC编码器引导训练。训练中引入与核分布和膜染色强度相关的辅助正则项，利用领域知识增强模型表现。

Result: 在公开BCI数据集上，LGD-Net表现大幅优于传统方法，达到了当前最优的HER2评分效果，推理阶段仅需H&E切片即可高效预测。

Conclusion: LGD-Net可在不需要IHC染色的情况下，实现对HER2表达的精确自动评估，兼顾准确率和效率，有潜力推广至资源有限地区，为乳腺癌精准诊疗提供支持。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 该论文提出了一种无需额外训练即可对遥感影像进行零样本文本引导分割的方法，结合了对比型和生成型视觉语言模型与SAM模型，方法简洁高效，达到了当前最优的开放词汇语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本文本引导遥感影像分割方法大多仍依赖可训练模块，限制了泛化能力与实际应用价值。本文着力探索基于已存在的大模型，完全无需额外训练的可行性。

Method: 方法结合了对比型（如CLIP）和生成型（如GPT-5、Qwen-VL）VLMs与SAM模型：对比路线利用CLIP筛选SAM生成的掩膜提议实现零训练开放词汇分割，生成路线则用大模型生成点击提示词驱动SAM进行分割，同时支持训练免疫与轻量调优两种模式。

Result: 在19个遥感数据集上，涵盖开放词汇、指代与推理等任务，所提方法展现出强大分割能力，部分方案达到了最新最优性能。

Conclusion: 本文方法无需额外训练或仅少量轻量调优，就能实现高效、泛化性强的多类型遥感影像分割，降低了实际应用门槛。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: 论文提出了一种无需复杂追踪模块的视频分割新方法VidEoMT，依靠ViT主干和轻量的查询传播机制，在大幅简化模型结构与加快速度的同时，实现了与传统方法相当甚至更优的精度。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割多依赖帧级分割器与复杂追踪模块结合，结构复杂、计算开销大。而近期研究显示，如果ViT容量和预训练数据充分，仅用ViT也能获得优异分割效果，这启发作者探索能否完全去除追踪模块。

Method: VidEoMT完全基于编码器结构，通过引入轻量的查询传播机制实现跨帧信息流动：每一帧的查询部分由前一帧继承，与一组不依赖时间的学习查询融合，从而兼顾了时序信息的延续性和对新内容的适应性。模型基于ViT-L主干，无需任何专用追踪子模块。

Result: VidEoMT在准确率上具备当前主流追踪方法的性能，但模型结构更简单，运行速度提升5--10倍，最高可达160帧每秒。

Conclusion: 通过查询传播和融合，VidEoMT实现了高效且高精度的在线视频分割，无需复杂追踪模块，开创了简单、高效的新范式。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文首次提出了视频查询性能预测（VQPP）基准，填补了视频内容检索领域内QPP研究的空白，并验证了多种性能预测方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 过去的查询性能预测（QPP）研究主要服务于文本和图像检索，对内容为主的视频检索领域极为缺乏。视频检索应用场景广泛，亟需专门的QPP研究来评估和提升检索系统性能。

Method: 作者构建了首个面向视频检索的QPP基准VQPP，包含两个文本到视频检索数据集和两套CBVR系统，共有5.6万条查询和5.1万个视频，并提供标准的训练、验证、测试划分。文中对多种检索前和检索后指标做了系统评估。

Result: 实验显示，检索前性能预测器表现已相当优越，使得在检索前即能实现相关应用。此外，最佳预测器还被作为奖励模型，结合直接偏好优化方法训练大语言模型用于查询重写任务。

Conclusion: VQPP为视频检索领域的查询性能预测研究树立了新基准，为后续相关研究和实际应用提供了基础资源和实验平台，并带来了新的探索方向。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文批判分析了Liu和Szirányi提出的手势识别方法，指出其评估协议存在严重数据泄漏问题，导致评测结果失真，强调了在相关研究中必须采用独立受试者划分方案。


<details>
  <summary>Details</summary>
Motivation: 作者注意到相关手势识别研究中常用的帧级随机划分训练集和测试集的方法可能导致同一受试者的数据同时出现于训练和测试集，从而引发评测失真。鉴于手势识别应用（如无人机与人交互）需要对未知个体具备泛化能力，当前做法存隐患。

Method: 对Liu和Szirányi的评估协议进行系统性分析，包括审查其混淆矩阵、学习曲线和数据集构建方式，验证数据划分方法是否导致数据泄漏及评测失真。

Result: 作者发现其高准确率主要源自训练集和测试集之间样本混杂，即评测过程中模型可能已见过测试数据中的受试者，未能有效反映对新个体的泛化能力。

Conclusion: 论文强调，基于视觉的手势识别研究需采用受试者独立的数据划分，否则实际评测结果将高估模型性能，影响其在实际应用中的可靠性。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 本文提出了一套端到端的长视频理解系统，有效解决了高冗余视频序列分析的效率与信息提取难题，在多个基准任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以高效处理时长几十分钟的长视频，主要面临两大挑战：(1) 在有限内存下高效纳入更多帧；(2) 在海量输入中提取有判别力的信息。

Method: 提出了一种基于信息密度自适应采样（AVS）机制和基于自编码器的时空视频压缩器（SVC），这两者与多模态大语言模型（MLLM）深度结合，形成了端到端的视频理解框架。AVS自适应采样关键帧，SVC实现高压缩率但信息不丢失。

Result: 该系统在长视频理解与标准视频理解基准测试上均取得了优异成绩，尤其在复杂长序列处理上展现出强大优势。

Conclusion: 该方法在高效处理长时长、多冗余的视频理解任务中效果显著，对不同长度的视频都表现出极佳的泛化性和信息提取能力。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 本文分析了当前主流视觉-语言模型（VLMs）在细粒度图像分类任务中的表现和影响因素，发现其在该领域性能落后，并揭示了视觉编码器、预训练方式等对细粒度任务的重要作用。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务上取得了显著进步，但在需要精细视觉认知的传统图像分类基准测试中表现不足，因此需要深入探讨其原因并提升细粒度能力。

Method: 作者对多个近期VLMs进行了系统性细粒度分类基准测试，并通过消融实验，分析了基础LLM、视觉编码器、预训练阶段（尤其是语言模型权重是否解冻）的影响。

Result: 结果显示：一、更强的LLM对所有任务分数均有提升；二、更优秀的视觉编码器能显著提升细粒度分类表现；三、预训练阶段（尤其解冻语言权重）对细粒度表现极为关键。

Conclusion: 本文的实验揭示了影响VLMs细粒度能力的关键因素，为提升其视觉认知和视觉特定能力提供了理论和方法依据。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出用极其稀疏的多模态传感（如雷达或激光雷达）数据重建深度图，为扩散模型驱动的新视角合成提供更鲁棒的几何条件，从而提升单张图片的新视角视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的单图新视角合成方法，在实际应用中受限于单目估深的局限（如低纹理、恶劣天气、遮挡），导致生成结果的一致性和质量下降。解决上述瓶颈，是提升生成模型实际应用的关键。

Method: 提出一种多模态深度重建框架，利用极其稀疏的雷达或激光雷达数据，通过局部高斯过程在角域上建模，实现高效推断同时量化稀疏区域的不确定性。重建得到的深度及其不确定性可无缝替换现有扩散渲染流程中的单目深度输入，无需更改生成模型本身。

Result: 在真实多模态驾驶场景实验中，将传统视觉深度替换为所提稀疏多模态深度显著提升了生成视频中的几何一致性与视觉质量。

Conclusion: 稀疏多模态深度显著强化了扩散式视角合成的鲁棒性和最终观感，即便传感极度稀疏也能带来明显实用价值，强调了可靠几何先验和多模态感知的重要性。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: 本文提出了一种无需位置嵌入和[CLS] token的新型小型Vision Transformer架构ZACH-ViT，在空间先验信息不强的情况下，比如医学影像中，实现了更好的泛化能力与紧凑推理。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer高度依赖位置编码和[CLS] token，这些空间先验对医学影像等空间布局信息弱或不一致的应用场景造成泛化困难。作者希望跳出固定空间先验框架，寻求对空间顺序不敏感的变换器结构。

Method: 提出ZACH-ViT：彻底移除位置嵌入和[CLS]聚合token，实现patch表示的全局平均池化来获得输出。同时，采用自适应残差投影来稳定紧凑模型的训练。模型参数严格受限（0.25M），并进行多数据集严苛少样本测试。

Result: ZACH-ViT在BloodMNIST等空间先验弱的数据集上取得最突出表现，并在PathMNIST上与TransMIL基本持平。对于空间先验强的数据集（如OCTMNIST、OrganAMNIST），其优势减弱。少样本（每类50个样本）实验下性能与更大、更复杂的模型相比亦有竞争力。

Conclusion: 模型结构与数据属性对齐，比一味追求通用基准胜利更为关键。ZACH-ViT在极小模型规模和无预训练条件下，性能优良且推理速度快，适合资源受限的临床部署。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET提出了一种更高效和高效的多层表征对齐方法，大幅提升了视觉-语言-动作模型在实际任务中的空间理解和执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型缺乏对三维空间的理解，仅在单一层或简单多层上对齐表征，未充分利用深层特征信息，且易引发梯度干扰。

Method: ROCKET采用残差导向的多层表征对齐框架，通过共享映射器对多个2D VLA模型层与3D视觉基础模型层进行对齐，并提出层不变映射和稀疏激活方案，理论和实验证明该共享映射器有效减少梯度冲突。

Result: ROCKET在LIBERO任务上仅用4%计算量就实现98.5%的SOTA成功率，并在LIBERO-Plus、RoboTwin及多个VLA模型上表现优越。

Conclusion: ROCKET极大提高了VLA模型的三维理解与任务执行表现，同时兼具高效性与通用性，是VLA模型表征对齐领域的重要进展。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 本文提出了一种基于记忆机制的图像质量评价新框架（MQAF），可在有无参考图像的情况下通过存储畸变模式进行高效评估，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的全参考图像质量评价（FR-IQA）方法在参考图像质量不佳时性能受限，而真实应用中往往缺乏理想参考，因此需要降低对高质量参考图像的依赖。

Method: 受到人类视觉记忆机制启发，提出了一个基于记忆驱动的质量感知框架（MQAF），通过建立畸变模式记忆库并根据参考图像有无动态切换评估模式。参考图像可用时，结合存储的畸变模式和参考，对失真图像加权比对，无参考时仅依赖记忆库进行质量推断。

Result: 在多个数据集上的实验表明，该方法在全参考和无参考任务中均优于现有最先进的方法，具有更强泛化和适应性。

Conclusion: MQAF创新性地结合记忆机制，实现了双模图像质量评价，扩大了算法适用范围，并提升了评估准确率，具有较强实际应用潜力。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 本文提出了MUOT_3M首个大规模、伪多模态的水下目标跟踪（UOT）数据集及MUTrack多模态跟踪方法，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人和生态监测重要，但目前缺乏大规模、多模态且有多样性的公开数据集，现有基准数据集规模小、仅含RGB，在复杂水下环境中鲁棒性较差。

Method: 1) 构建MUOT_3M数据集：包含3030段视频、300万帧，标注丰富，具有RGB、增强RGB、深度和语言四种模态，由海洋生物学家参与验证。2) 提出MUTrack方法：结合视觉、几何校准与视觉-语言融合，采用基于SAM的多模态知识蒸馏，将多模态信息迁移至单模态模型，提高实际部署效率。

Result: 在五个主流水下目标跟踪基准数据集上，MUTrack比最强SOTA方法分别高出8.40%的AUC和7.80%的精度，且运行速度达到24 FPS。

Conclusion: MUOT_3M和MUTrack为水下目标跟踪领域提供了大规模多模态训练与实际可落地的基础，推动了可扩展性与鲁棒性的提升。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大模型（LLM）的情感视觉定制任务（L-AVC），并提出了一种高效精确的图像情感编辑方法。


<details>
  <summary>Details</summary>
Motivation: 过去的视觉定制主要关注客观信号与图像编辑的对齐，忽视了主观情感内容，同时缺乏通用的情感视觉定制基础模型。为解决这一空白，作者提出将情感信息作为视觉定制的核心考量。

Method: 提出了L-AVC任务与EPEM方法。其中EPEM方法包含两个核心模块：高效情感转换模块（EIC），实现编辑前后语义中的情感转化对齐；精确非情感信息保留模块（PER），确保与情感无关的内容不被误改。

Result: 在作者构建的L-AVC数据集上，所提出方法在情感编辑的效率和精确度上均显著优于多种当前主流方法，体现了情感信息在L-AVC中的重要性。

Conclusion: 情感内容对视觉定制任务至关重要，所提出的EPEM方法能够高效、精确地操控图像情感，为多模态情感视觉生成和编辑任务提供了有力支撑。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 该论文提出了更深入的视频安全理解任务（DeepSVU），不仅检测并定位威胁，还分析威胁产生的原因，并设计了UPRM方法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于安全的视频理解主要关注于威胁的检测和定位，缺乏对威胁原因的有效生成和评估能力。因此，作者希望填补这一空白。

Method: 提出了一种新的任务范式——DeepSVU，以及创新的UPRM方法。UPRM方法包含两个核心模块：UPE Block用于增强对粗到细物理世界信息（如人行为、物体交互、背景）的建模，PTR正则器用于自适应权衡不同因素。

Result: 在自建的DeepSVU数据集（UCF-C/CUVA指令集）上，UPRM方法整体优于多种先进的视频大模型（Video-LLMs）和非视觉语言模型方法。

Conclusion: 结果表明，粗到细的物理世界信息在DeepSVU任务中具有重要性，UPRM能够有效刻画这些信息并提升安全视频理解的能力。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出了一个无需额外训练和数据的插件模块UAOR，通过不确定性量化动态重注入关键观测信息，显著提升VLA模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型通常需依赖额外观测数据（如深度图、点云）或辅助模块（如物体检测器），来提升机器人操控能力，但这会带来额外的数据采集、训练成本。作者希望在不增加训练与数据负担的情况下，提升VLA模型的智能与泛化能力。

Method: 提出了一种不确定性感知的观测重注入机制UAOR：通过当前语言模型层的动作熵（表示不确定性）作为触发条件，将关键观测信息重新注入到下一层FFN中，具体通过attention机制进行检索。这一模块免训练、插拔式，可以在推理阶段动态提高模型对观测的关注。

Result: 在模拟和真实任务中，UAOR能够显著、持续性提升不同VLA模型的表现，且对计算与系统架构影响最小。

Conclusion: UAOR可无缝集成进现有VLA模型，既无需额外观测输入与辅助模块，也无需再训练，是一种高效灵活的增强方法，具有广泛的应用前景。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出了一种无训练、能同时控制Key与Value通道的扩散模型编辑方法DCAG，在图像编辑强度上实现了更精细的调节和更高保真度。


<details>
  <summary>Details</summary>
Motivation: 目前基于DiT架构的扩散模型图像编辑方法，主要依赖于修改注意力层Key通道以调整编辑强度，但完全忽略Value通道，导致控制能力受限，缺乏更精细调节。

Method: 作者首先发现DiT多模态注意力层的Key和Value投影都呈现明显的bias-delta结构。进而提出Dual-Channel Attention Guidance (DCAG)，无需训练即可同时操纵Key（决定关注位置）和Value（决定聚合内容）通道，通过调整($δ_k, δ_v$)两个维度参数，实现二维空间更细致的编辑与保真度权衡。理论分析表明Key以softmax带来粗粒度控制，而Value以线性加权实现细粒度控制。

Result: 在PIE-Bench（700图，10编辑类别）上，DCAG在所有保真度指标上均优于传统仅Key调控方法，尤其在局部编辑任务如对象删除和添加上表现突出，LPIPS指标分别降低了4.9%和3.2%。

Conclusion: DCAG方法证明了联合调控Key与Value通道在不需训练的前提下能大幅提升扩散模型的图像编辑能力，特别是在精细化和保真度需求高的任务中优势明显，有望成为扩散图像编辑领域的重要框架。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出一种名为DiST的新型FSAR方法，利用大语言模型分解并融合空间和时间知识来提升动作识别效果，在五个主流数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有FSAR方法通常只利用动作类别名作为辅助信息，语义过于粗糙，无法为捕捉新颖动作的时空概念提供充分的背景知识。因此，作者希望引入更丰富的分解属性知识，提高模型的泛化能力。

Method: 方法分为两个阶段：（1）分解阶段：将原始动作名拆分为丰富的空间与时间属性描述，补充常识知识；（2）融合阶段：提出空间知识补偿器（SKC）自适应聚合重要patch，获得物体级原型；时间知识补偿器（TKC）利用时间属性建模帧间时序关系，获得帧级原型，两者提升空间细节与时间模式捕获能力。

Result: 在五个主流的few-shot动作识别数据集上进行实验，DiST方法均取得了当前最新最优的结果，显著优于现有方法。

Conclusion: DiST通过引入由大语言模型生成的时空属性知识，有效提升了FSAR模型对动作的细粒度理解和泛化能力，实验证明方法有效且具有潜力。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: 该论文提出了CityGuard，一种面向隐私保护的城市级分布式摄像头身份检索系统，融合了自适应度量学习、空间信息注入和差分隐私技术，实现了在严苛隐私约束下的高效人像再识别。


<details>
  <summary>Details</summary>
Motivation: 城市级分布式摄像头下的人像再识别面临视角、遮挡、域偏移等变化，同时还需遵循数据保护法规，无法共享原始图像，现有方法部分忽视了这些综合挑战。

Method: 提出了一套由三部分组成的新框架：（1）离散自适应度量学习器，根据特征分布自适应调整分类边界，加强同类紧致性；（2）空间条件自注意力，将粗粒度几何（如GPS、楼层平面图）注入图结构自注意力机制，实现跨视角一致性对齐；（3）差分隐私嵌入与高效检索索引结合，提高安全性与效率。

Result: 在Market-1501等多个公开数据集及规模化检索实验中，CityGuard相较强基线方法在检索准确率和查询效率上均有显著提升，展现了其实用价值。

Conclusion: CityGuard框架兼顾隐私保护和实用性，在实际城市安防人像再识别应用中具备推广前景，并为后续隐私保护下的多摄像头识别提供了新思路。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: 本文提出了一种新的T2M（文本到动作）生成框架TCA-T2M，注重跨序列的时间一致性，通过创新的空间VQ-VAE和变换器方法，在多个数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有T2M方法多忽略了同一动作跨不同实例间的时间一致性，导致生成结果存在语义错位和物理不合理。本文致力于解决这一时间一致性问题，以生成更真实连贯的动作序列。

Method: 1）提出了注重时间一致性的空间VQ-VAE（TCaS-VQ-VAE），用于跨序列时间对齐；2）引入了基于mask的动作Transformer，实现文本控制的动作生成；3）通过动力学约束块减轻离散化带来的物理不合理。

Result: 在HumanML3D和KIT-ML等主流T2M基准上，TCA-T2M都实现了当前最优的性能。

Conclusion: 注重时间一致性对于提升T2M生成动作的连贯性和物理合理性至关重要，TCA-T2M展示了这一点，有助于推动T2M领域的发展。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 该论文提出了3DMedAgent，这是一种无需3D特定微调也能使用2D多模态大模型（MLLMs）进行3D CT分析的统一智能体，显著提升了从图像感知到临床理解的系统能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D CT分析方法要么局限于任务专用模型，要么是端到端模型，无法系统地积累和利用感知证据支持下游推理。同时，当前多模态大语言模型对3D医学数据的理解力受限，难以满足临床实际需求。

Method: 3DMedAgent统一协调视觉和文本工具，通过MLLM主体将复杂的3D CT任务分解为可控的子任务，实现从整体到局部、从体积到切片、从图像到结构化文本等多层次推理，并通过长期结构化记忆汇聚工具输出支撑多步推理。论文还引入了新的DeepChestVQA基准用于评估3D胸部成像中的端到端理解能力。

Result: 在40多项任务的实验中，3DMedAgent在表现上稳定超过通用、医学和现有3D特定MLLMs，证明了其强大的3D分析及推理能力。

Conclusion: 3DMedAgent为3D医学影像分析提供了一条无需特定3D微调，可扩展、通用且高效的技术路径，对未来3D临床辅助系统的开发具有重要推动作用。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出了一种无需完全依赖昂贵且不一致的BEV标注，降低标注和训练成本的BEV语义分割新方法，通过自监督预训练和数据减半微调，效果优于全监督方案。


<details>
  <summary>Details</summary>
Motivation: 多摄像头生成BEV语义图依赖于高成本且常常带有噪声的BEV标注，限制了自动驾驶感知系统的规模与效率，因此希望减少标注依赖，提高训练效率。

Method: 分为两阶段：1）自监督预训练阶段，利用现有的Mask2Former生成伪标签，将BEVFormer预测结果可微分地投影回图像平面，用多视角伪标签进行自监督训练，并采用时序损失促进帧间一致；2）微调时，只用50%已有标注数据做监督训练。

Result: 提出的方法在nuScenes上mIoU比完全监督多达2.5个百分点，同时只用一半数据，训练时间也缩短2/3。

Conclusion: 该方案降低了对昂贵标注的依赖，证明了通过可微投影与伪标签结合能提取可迁移BEV特征，为自动驾驶感知探索了可扩展的减标注新途径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 本文提出一种结合多源遥感数据与机器学习的方法，实现了欧洲植被地区10米高分辨率的土壤水分估算，突破了现有卫星产品分辨率过低的限制。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤水分产品空间分辨率过粗，难以满足精准农业等田块尺度应用的需求。提升估算分辨率对农业、水资源和气候监测具有重要意义。

Method: 将Sentinel-1雷达、Sentinel-2光学影像与ERA-5再分析数据相结合，利用机器学习在113个ISMN实测站点数据上进行训练与交叉验证，并对比了不同特征（传统光谱特征与Prithvi基础模型嵌入）和时序参数配置的表现。

Result: 最佳方法组合（Sentinel-2当前影像+Sentinel-1降轨+10天ERA5回溯）R^2达0.518。Prithvi模型嵌入特征对提升精度作用甚微（R^2=0.515），传统手工特征依然表现优异。

Conclusion: 结合领域知识的光谱特征与树模型在稀疏监督下依然高效，符合实际大范围、田块级土壤水分监测的需求，基础大模型特征在该小样本场景下未体现优势。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: 本文提出了DohaScript，这是一个针对手写天城文（Devanagari）的高质量大规模数据集，旨在改善现有数据集在规模、内容和多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管天城文有数亿使用者，但现有的手写数据集规模有限，仅覆盖离散字符或短词，且缺乏写作者多样性，不适合现代手写分析和识别研究。

Method: 研究者从531名独立写作者处收集数据，每位参与者都誊写相同的6首传统印地语对句，从而构建了一个各方面受控的大型书写风格平行语料库。同时，数据集配备了不可识别的人口统计元数据、客观锐度与分辨率标准的质量筛选、以及页面难度注释，用于分层基准测试。

Result: 基准实验结果显示，数据集在质量分层和对新写作者的泛化能力方面均表现出色，验证了其作为可靠基准的实用性与价值。

Conclusion: DohaScript旨在为资源稀缺情境下，连续手写天城文文本的识别与风格分析等研究领域提供标准化、可复现的基准，推动相关技术发展。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: 提出了一种名为PrediT的训练无关型加速框架，显著加速了Diffusion Transformers生成任务，几乎不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers在高保真图像和视频生成上有突出表现，但其迭代去噪过程导致高计算耗时。目前训练无关的加速方法依赖特征缓存，存在潜在特征漂移和质量下降问题。

Method: 提出PrediT框架，将特征预测转化为线性多步问题，利用历史信息预测未来输出，并在剧烈变化区域引入校正器避免累积误差，还通过动态步幅调整机制根据特征变化率自适应调节预测步长。

Result: 大量实验表明，PrediT在多种DiT模型上实现了最高5.54倍延迟下降，且画质几乎无损。

Conclusion: PrediT无需额外训练即可大幅加速DiT生成过程，并且能够很好地权衡推理速度与生成质量，具有广泛应用前景。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: 提出了OODBench新基准，用于自动化评估视觉-语言模型（VLMs）在分布外（OOD）数据上的表现，结果显示现有模型表现明显下降，并介绍了新的评测指标。


<details>
  <summary>Details</summary>
Motivation: 现实应用中AI系统获得的数据往往不是独立同分布（IID），处理OOD（分布外）对象能力不足会带来安全问题。现有研究缺乏系统性OOD评测基准，难以衡量VLM的实际泛化能力。

Method: 提出自动化的OODBench基准，包含4万组实例级分布外类别对，无需大量人工参与，并提出基于由易到难问题序列的自动化评测指标，系统性考查VLMs在不同难度下处理OOD数据的能力。

Result: 实验证明，即使原始图片类别常见，现有VLMs在OODBench上的性能依然大幅下降。新的评测指标能够全面评估不同难度下的OOD影响。

Conclusion: OODBench为评测VLM对分布外数据的鲁棒性和泛化能力提供了有效工具，发现现有模型在OOD任务上表现有明显不足，为未来OOD数据获取与模型评估研究指明方向。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: 本文比较了Vision Transformers（ViTs）、卷积神经网络（CNNs）和人类在视觉图形感知任务上的表现，发现ViTs虽然整体性能优秀，但在人类感知一致性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: ViTs在各类图像任务中表现突出，但其在基础视觉判断及图形感知能力尚未被系统评估。此前CNNs已被研究过，作者希望探究ViTs在该领域的表现，并与人类标准及CNNs进行对比。

Method: 作者借鉴Cleveland和McGill的基础视觉感知实验设计，针对不同的可视编码方式，设置一系列受控的图形感知任务，并将ViTs、CNNs及人类参与者的表现进行对比分析和基准测试。

Result: ViTs在一般视觉任务中表现强劲，但在与人类感知模式保持一致方面，其在可视化领域的图形感知能力有限，明显弱于CNNs和人类。

Conclusion: ViTs在图形感知与可视化任务中的人类一致性不足，这提醒业界在将ViTs应用于可视化系统或感知建模时，需要考虑其感知差异并采取相应改进措施。

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diff2DGS的双阶段方法，有效提升了机器人手术实时重建中的遮挡区域质量和3D深度准确性。该方法在外观和几何指标上均超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景重建方法虽可实现实时密集重建，但在遮挡区域质量和深度准确性方面仍有不足，且相关数据集缺乏真实3D基准，难以系统性评估。

Method: 提出两阶段框架Diff2DGS：第一阶段，通过基于扩散模型的视频模块，结合时间先验，对被手术器械遮挡的组织区域进行高空间-时间一致性的修复；第二阶段，将二维高斯溅射与可学习形变模型结合，捕捉组织动态形变和解剖结构。以及在SCARED数据集上拓展了以往评价方法，不仅关注外观，还评测了定量深度准确度。

Result: Diff2DGS在EndoNeRF和StereoMIS上分别达到38.02 dB和34.40 dB的PSNR，外观和几何表现在现有方法中最优。实验表明，仅优化图像质量不能确保最佳三维重建精度。

Conclusion: Diff2DGS兼顾外观和几何，显著提升了手术场景遮挡重建和三维深度质量，为机器人手术中的实时高精度重建提供了有效解决方案。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [31] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对短视频广告内容审核的新框架BLM-Guard，能更精细和高效地检测多模态虚假或误导性内容，在实际场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台广告形式多样化，内容中出现了图像、语音、字幕等多模态的误导与欺骗，因此需要比传统社区审核更精细、更具政策导向的判断。

Method: 提出BLM-Guard框架，结合Chain-of-Thought推理、自定义政策规则和基于critic的增强学习。通过设计规则驱动的数据合成管道减少标注成本，并利用多任务架构同时建模模态内和模态间的信息误差。

Result: BLM-Guard在真实短视频广告数据上的准确性、一致性和泛化能力均超过主流基线方法。

Conclusion: BLM-Guard能高效、有针对性地实现多模态广告的违规内容检测，并具备良好的扩展性和实际应用价值。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [32] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: 论文提出了一个新基准CapNav，专为考察视觉-语言模型（VLM）在具备不同物理能力限制的机器人/人类导航中的表现，发现在能力受限场景下VLM表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实中的导航任务受制于执行体的物理能力限制，如不同的机器人因尺寸、运动方式的不同，其可达空间和操作方式也各异。许多已有的视觉-语言导航研究未充分考虑这些实际约束，导致模型与真实应用场景存在差距。为此，研究者需要能反映能力约束的基准。

Method: 作者提出CapNav基准，设定了五种具代表性的机器人/人类代理，详细描述其物理尺寸、运动能力及环境交互能力。基准包含45个实际室内场景、473个导航任务和2365个QA对，用于系统性测试VLM是否能根据不同代理的能力进行合理导航。

Result: 对13种最新VLM进行评测，发现随能力约束加严，导航性能显著下降。即使是最先进的模型，在需考虑空间维度才能避障的任务上依然表现不佳。

Conclusion: 目前VLM在面临强物理/空间能力约束时仍有较大提升空间。作者建议今后VLM研究需更重视能力感知的导航推理，推动模型在具身空间推理能力上的进步。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [33] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: 提出Distortion-aware Motion Calibrator (DMC)，作为后处理模块，提升文本驱动人体动作生成的物理真实感和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动作人体运动生成虽有进展，但往往生成的动作存在物理不合理（如脚悬浮）问题，且增进物理真实感的同时保持与文本描述的语义一致仍具挑战。

Method: 提出DMC模块，采用自监督、数据驱动方法。给定人工扭曲的动作和文本描述，DMC学习生成既符合物理规律又保持语义一致的动作。该方法作为独立后处理模块，可兼容多种文本到动作的方法并强化其生成动作的物理和语义表现。

Result: 在T2M、T2M-GPT等基准上，DMC显著降低FID分数（分别降幅42.74%与13.20%），R-Precision达到最高，对优秀基线模型MoMask可使动作穿透现象减少33%，脚部浮空等问题得到明显修正，更接近真实动作。

Conclusion: DMC能作为任意文本转动作模型的后处理模块，以融合文本语义和物理可行性，实现更高质量的人体动作生成，对提升文本到动作领域的实际可用性具有推广价值。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [34] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 本文首次研究了离散图像分词器（tokenizer）的对抗攻击脆弱性，并提出有效攻击方法与无监督对抗训练防御手段，显著提升了多模态模型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉编码器的对抗鲁棒性已有研究，但离散分词器在多模态系统中的安全性尚未受到关注；而分词器作为基础组件，其脆弱性可能直接影响多模态模型的性能与安全。

Method: 1）设计对分词器特征进行扰动的泛用型对抗攻击，适用于分类、检索、描述等多模态任务；2）借鉴鲁棒CLIP训练思路，仅对分词器进行无监督对抗微调，其他模块保持冻结。

Result: 所提攻击方式具备高效、无任务依赖等优点，可大幅影响不同任务表现。提出的无监督对抗训练方法显著增强了分词器与下游多模态模型对已有及新型对抗样本的防御能力，并能利用无标注数据。

Conclusion: 分词器鲁棒性对多模态基础模型安全具有重大影响。文中方法提升了鲁棒性和通用性，为后续安全、可靠的多模态系统开发奠定了基础。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [35] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一种针对多实例生成的新框架，解决了以往方法在复杂文本描述中的细粒度语义理解难题，通过新的特征提取与融合机制提升了生成质量，并引入了数据集和基准测试用于细粒度评估，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前多实例生成在空间布局和属性绑定上虽有进展，但细粒度语义理解和对复杂文本描述的精准控制仍有明显不足。

Method: 提出DEIG框架，其中包括实例细节提取器（IDE），将文本编码器输出转为更紧凑的、实例感知的表征；细节融合模块（DFM）利用基于实例的掩码注意力，防止属性在实例间泄露。还新建细粒度标注数据集和区域级评测基准。

Result: 在多个数据集上，DEIG在空间一致性、语义准确性及组合泛化能力等方面均优于现有方法，可灵活集成于主流扩散模型管线。

Conclusion: DEIG显著提升了多实例生成的细粒度可控性和文本与图像的精准对齐水平，为后续相关研究和应用提供了新的重要工具。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [36] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: 本文提出了LOTS框架，通过结合全局草图引导与多个局部草图-文本对，实现更高质量且结构一致的时尚图像生成。


<details>
  <summary>Details</summary>
Motivation: 时尚设计早期常用草图和文本描述表达创意，如何有效结合两者以提升图像生成质量仍具挑战，尤其是在保持草图结构的同时，利用文本的丰富细节信息。

Method: LOTS框架分为两个阶段：第一，Multi-level Conditioning Stage将每个局部特征独立编码到共享潜在空间，兼顾全局结构；第二，Diffusion Pair Guidance阶段在扩散模型去噪过程中，利用基于注意力的方式融合全局和局部引导。此外，作者还构建了Sketchy数据集，包含专业和非专业多对草图-文本样本。

Result: 实验结果显示，LOTS在全局结构一致性和局部细节多样性上优于现有方法，并能适应专业与非专业草图场景。

Conclusion: LOTS框架能充分结合草图和文本描述，提升时尚图像生成的结构保真与细节丰富性。新构建的Sketchy数据集对该领域具有推动作用，相关工具与数据已公开。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [37] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出了Luminance-GS++，一种针对于多视角成像中复杂照明和色彩变化场景的3D高斯分布（3DGS）框架，用于实现高质量的新视图合成，在低光、过曝等多种挑战下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实环境中高质量图像采集受限于复杂的照明变化和相机成像流程的局限，尤其在多视角捕获下，传感器响应和ISP配置的不一致导致光照和色彩的不一致性，从而影响3D新视图合成方法（如NeRF和3DGS）的重建与渲染质量。

Method: 作者提出在3DGS框架中，融合全局自适应亮度调整与像素级残差校正，实现精确的色彩纠正。同时设计了无监督目标函数，联合约束亮度校正以及多视角的几何和光度一致性。

Result: 所提方法在复杂光照和色彩变化（如低光、过曝等）环境下，对比现有方法取得了最先进的重建和渲染效果，显著提升了图像的一致性和质量。

Conclusion: Luminance-GS++能有效应对多视角场景中的复杂光照与色彩挑战，不改变3DGS原有表达的基础上提升了重建质量及实时渲染性能，优于现有同类方法。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [38] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出了一种新的G-LoG双参数过滤方法，用于医学图像中的拓扑特征提取，证明其稳定性并在MedMNIST数据集上进行实验，结果优于单参数方法，并能以简单MLP模型获得与复杂深度学习模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑数据分析中，基于单一参数的过滤方法难以充分挖掘医学图像中的复杂拓扑和几何特征，且对噪声敏感。作者希望开发一种更强鲁棒性、适用于多参数持久性模块的过滤方案，用于提升医学图像分析中的表现。

Method: 作者通过利用高斯-拉普拉斯（LoG）算子增强医学图像边界，将体积图像建模为有界函数，提出G-LoG双参数过滤方案，从而生成更适合多参数持久化模块的特征。同时理论证明该过滤生成的模块，在有界函数的最大范数意义下具备稳定性。

Result: 在MedMNIST数据集上的实验表明，G-LoG双参数过滤在拓扑特征提取、区分能力等方面显著优于单参数过滤。进一步，基于这些拓扑特征训练的简单MLP模型，其性能已可与自动化复杂深度学习模型媲美。

Conclusion: 所提出的G-LoG双参数过滤在医学图像领域展现出强大的特征表达与实际应用优势，为多参数持久性模块在真实场景的拓扑数据分析指明了新方向。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [39] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 本文提出了一种自感知目标检测框架，使检测器能够识别自身输入是否受降质干扰，而不仅仅输出预测结果。方法通过结构化特征空间来感知各种图像降质，对异常情况做出预警。


<details>
  <summary>Details</summary>
Motivation: 当前目标检测器在图像清晰时性能优秀，但面对模糊、噪声、压缩或恶劣天气等图像降质时会悄无声息地失效，这对于安全关键应用是不可接受的。因此，开发能够自动检测输入是否异常的自感知目标检测器刻不容缓。

Method: 作者提出了一种基于降质流形的自感知框架，在检测主干网络后增设轻量级嵌入头，通过多层对比学习训练。该嵌入将具有相同降质类型和程度的图像拉近，将降质不同的图像推远，并用干净图像的嵌入均值作为操作基准点，利用几何偏离衡量降质程度，无需降质标签或密度建模。

Result: 在合成干扰基准、跨数据集零样本迁移与自然天气分布变化下，所提方法表现出强大的干净-降质区分能力，对多种检测架构具备一致性和通用性，并能在语义分布变化时保持鲁棒的泛化性能。

Conclusion: 降质感知的特征表征几何为检测器提供了实践有效且与架构无关的自感知基础，有助于提升在非理想环境下的检测可靠性。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [40] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 本文展示了一类能够在隐空间学习等变运算的神经网络架构，在处理超出训练分布的对称变换对象识别任务上优于传统与手工设计等变网络。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在训练样本少见的对称变换（如极端姿势、尺度、位置变化等）下表现不佳。已有的等变网络需先验获知所有变换类型，但这在实际中受限。需要一种方法能无需先验知识而适应新变换。

Method: 使用一种在隐空间中从数据例子中学习等变运算符的神经网络架构，并在含旋转、平移噪声的MNIST数据集上进行实验，对比传统与等变网络。

Result: 这种架构有效地提升了对训练分布外的对象识别能力，克服了传统神经网络和先验等变网络的局限。

Conclusion: 该架构有望提高神经网络的泛化和鲁棒性，但在推广到复杂真实数据集上还需解决诸多挑战。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [41] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 本文提出了一种以人体运动为中心的视频生成模型，实现了对XR环境更精细的用户动作控制，提升了虚拟互动体验。


<details>
  <summary>Details</summary>
Motivation: 目前的XR视频生成模型只能接受粗粒度的控制信号（如文本或键盘输入），无法充分发挥用户在沉浸式交互中的身体动作，因此亟需能基于真实动作细节控制生成内容的新模型。

Method: 作者提出了一种结合头部及手部关节动作的条件控制机制，对现有扩散变换器模型的控制策略进行评测，并引入3D头手控制机制，用以训练双向视频扩散模型教师网络，并将其知识蒸馏至因果型交互系统，最终实现可生成用户视角虚拟环境。

Result: 实验通过真人测试，结果显示该系统不但提升了用户完成任务的表现，还显著增强了用户对虚拟操作的可控性感知，优于现有基线方法。

Conclusion: 基于头部和手部动作追踪的条件控制视频生成模型能有效提高XR环境的人体交互体验，为下一代更自然沉浸的虚拟体验奠定了基础。

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 本文提出了首个可用于实时VR头显的空间感知型对话体态生成方法，能根据用户位置和语音输入生成自然、空间对齐的虚拟人动作，并实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人对话体态生成仅关注语音驱动的手势，缺乏对用户空间位置和对话者动态的感知，难以实现自然“转身”“注视”与动作互动，对VR、远程临场等应用存在限制。

Method: 提出基于因果Transformer-VAE和用户轨迹条件流匹配模型的方法，采用插入的潜在token实现流式推理；引入新型注视得分机制和无分类引导，可数据驱动学习空间对齐、支持定制化目光接触强度，全部端到端实现。

Result: 在Embody 3D数据集上达到最优的动作自然度，帧率超300FPS，较非因果基线提升3倍，细腻还原自然对话中的空间动态；在真实VR系统中验证，并可流式部署。

Conclusion: 本文方法填补了空间感知体态生成的空白，实现了虚拟人在实时对话场景中的空间交互和自然动作，适用于VR、数字人等实时系统，有较大应用前景。

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出了一种新的视频问答方法MemStream，通过扩展token预算与引入自适应选择和检索专家机制，实现了对连续视频流的细粒度理解和高效记忆管理，显著提升了主流长视频VQA基准的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的流式视频理解方法由于每帧token数受限，导致细节信息丢失且检索时偏向后续帧，不能很好地处理密集视频流。作者希望解决这一问题，以提升视频问答的准确率。

Method: 1. 扩大每帧的token预算，增强时空细粒度理解能力。2. 设计自适应token选择策略，减少冗余并保留局部时空信息。3. 引入无训练检索混合专家策略，结合外部模型提高相关帧的检索能力。

Result: 所提方法MemStream在多个长视频VQA基准测试中相较于ReKV+Qwen2.5-VL-7B有明显性能提升：CG-Bench提升8.0%，LVBench提升8.5%，VideoMME（Long）提升2.4%。

Conclusion: MemStream能高效扩展视频记忆容量并提升视频理解、检索相关性，在流式视频问答中能有效突破现有方法的局限，推动了长视频AI理解能力的发展。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: 本文提出了QueryPlot，一个结合地质文本语料库与地质图数据的语义检索与空间映射系统，通过NLP方法辅助矿产远景区预测。


<details>
  <summary>Details</summary>
Motivation: 矿产远景区预测需要融合异构的地质知识（如文字模型与空间数据），传统流程人工、依赖经验，效率和客观性受限，需要自动化和智能化的方法。

Method: 1. 整理120多种矿床类型的描述性模型。2. 将地质图多边形数据结构化为文本。3. 用预训练嵌入模型将用户查询与区域描述编码，计算语义相似度，排序并空间可视化。4. 支持组合式查询及多准则的远景分析。5. 在钨矽卡岩矿床案例中验证方法有效性。6. 将相似度分数作为特征，提升监督学习表现。7. 实现为网端交互式查询与可下载GIS数据。

Result: 1. 嵌入式检索在回召已知矿床方面表现优异。2. 预测出的远景区与专家定义相符。3. 相似度特征能提升分类模型性能。4. 系统支持互动式可视化和数据导出。5. 代码和数据集已开源。

Conclusion: QueryPlot能有效结合文本和空间数据，自动化、智能化地辅助矿产远景区预测，提升效率和准确性。该方法为后续相关研究提供了新工具和开源资源。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [45] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: 本文提出将“神经同步”作为衡量大模型（LLM）社会性的一个代理指标，并通过实验证明该同步性与模型社会表现密切相关，揭示了大模型内在互动动态与人类社会互动的惊人相似性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）已展示出类人的行为能力，但它们是否能被视为像人类一样拥有“社会心智”仍有争议。神经科学表明，人类在社会互动时大脑会产生同步活动，本文希望借此类比，探索LLM间是否存在类似的同步现象，并以此为依据考察其社会性。

Method: 作者设计了一系列模拟LLM之间社会互动的实验，采用神经同步（即LLMs内部表征活动的相似性与时序匹配度）作为新颖分析指标，来测定在对话等互动过程中的同步特征，并与其社会表现进行相关性分析。

Result: 实验证明，LLM间的神经同步显著反映了它们在互动中的社会参与程度与时间协同性；此外，这种神经同步性与LLM在社会任务中的表现有很高的相关性。

Conclusion: 神经同步能够作为新视角揭示和量化LLM的“社会心智”特性，且其内在交互机制与人类社会互动存在惊人相似。该方法为后续理解和提升大模型社会能力提供了理论与实验基础。

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [46] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 大型语言模型在预测人类眼动和阅读时间表现上更出色，但对低层次信息的敏感度降低。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型相对于小型模型在模拟人类阅读与词预测任务上的表现差异，特别是其与人类实际反应的一致性。

Method: 对比不同规模的语言模型在预测眼动、阅读时间数据，以及填空(cloze)数据中的下一个词和概率分布的准确性，观察其对词共现与语义对齐的敏感度。

Result: 大型模型对人类响应分配的概率质量更高，更擅长预测语义相关的词。同时，对词汇共现的敏感性降低，对低层次识别信息不如小模型敏感。

Conclusion: 大型语言模型强大的记忆和语义处理能力提升了它们的预测质量，但在处理与词识别相关的细节信息时能力有所下降。

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [47] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 论文分析了“Steering vectors”在控制语言模型行为时的不可靠性和产生这种不可靠性的原因，并提出了更健壮方法的研发方向。


<details>
  <summary>Details</summary>
Motivation: 虽然steering vectors能有效地调整模型行为，但实际应用中表现出不一致与不可靠性。作者想探究导致可靠性变化的内在原因及训练数据的作用，为未来改进方法提供理论依据。

Method: 论文通过实验分析：1）训练集中激活差的余弦相似度与steering效果的关系；2）数据集内正负样本在steering方向上的分离与可调控性的关系；3）不同prompt变体训练得到的steering vector的方向性与效果一致性。

Result: 1）训练激活差余弦相似度高时，steering更可靠；2）正负样本分离度高的数据集更易可靠steering；3）不同prompt产生的steering vector虽方向不同但性能类似；4）当目标行为的潜在表征无法被线性方向良好逼近时，steering易失败。

Conclusion: 提出通过数据分析判断steering可靠性，同时指出steering方法对模型潜在表征的适配性有限，建议研发能处理非线性表征的更健壮控制方法。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [48] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了一种利用语言模型生成的语义丰富软标签，以提升神经主题模型质量的方法，在主题一致性、纯度及文档检索方面均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有神经主题模型大多依赖于重建文档词袋表示，忽略了上下文信息，且在数据稀疏时效果较差。

Method: 利用语言模型，通过专门的提示词生成下一个词的概率分布，并投影到预定义词表上，得到语义丰富的软标签。训练主题模型重构这些由语言模型隐藏状态得到的软标签，增强主题与语料库语义结构的一致性。

Result: 在三组数据集上的实验表明，该方法在主题一致性和纯度方面对比现有方法有明显提升。同时提出新的检索评估指标，显示该方法在识别语义相似文档上明显优于竞争方法。

Conclusion: 结合语言模型提供的上下文信息和软标签信号后，神经主题模型能学得更高质量、与语料真实主题结构更贴合的主题，且在文档检索等应用任务中表现更佳。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [49] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 本文提出了CondMedQA，一个针对条件性生物医学问答的基准，并提出了Condition-Gated Reasoning(CGR)框架，用于更好地处理具体病人条件下的问题。CGR在准确性和稳健性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学问答系统假定医学知识是统一适用的，忽略了现实临床推理中几乎所有决策都要考虑病人具体的条件（如合并症、禁忌症）。同时，当前问答基准与方法未能有效评估和处理这种条件性推理能力。

Method: 作者构建了CondMedQA基准，包含多跳、答案依赖病人条件的问题；并提出CGR方法，通过构建具备病人条件感知的知识图，并基于具体查询条件动态激活或剪枝推理路径，实现条件自适应推理。

Result: 实验表明，CGR能够更可靠地选择符合具体条件的答案，同时在标准生物医学问答基准上达到或超过最先进方法的性能。

Conclusion: 在医学领域，显式地建模和处理条件性是实现稳健推理的关键。提出的CondMedQA与CGR为条件性推理任务提供了新的实验平台与方法基础。

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [50] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文系统性地比较了不同指令优化方法在基于表格的事实验证任务上的表现，发现指令优化能显著提升LLM的推理和验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在推理任务中的表现不断进步，但如何进一步通过轻量、无模型依赖的方式（如指令优化）来提升其针对表格事实验证任务的性能，尚缺乏系统性研究。

Method: 作者基于DSPy优化框架，使用四种主流的prompt技巧（直接预测、链式思维CoT、结合SQL工具的ReAct、结合Python执行的CodeAct），比较三种指令优化方法（COPRO、MiPROv2、SIMBA）在四个基准数据集和三类语言模型上的表现。

Result: 实验显示，指令优化在各prompt和模型下都提升了验证准确率。其中，MiPROv2优化的CoT在多场景下效果最稳定；SIMBA在ReAct agent及大模型场景下提升最大，还能引导模型采取更直接的推理路径，提升数字比较能力，减少不必要的工具调用。CoT对小模型表现尤好，而大模型的ReAct优化精细度要求更高。

Conclusion: 指令优化对于提升LLM在表格事实核查任务中的推理准确性非常有效，不同prompt和优化器组合效果不同；合理选择和调优指令优化方法对提升实际性能至关重要。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [51] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: 本论文提出了CUICurate，一种基于图检索与大语言模型集成的自动UMLS概念集构建工具，以提升临床命名实体在下游任务中的表达能力，显著减少人工劳动。


<details>
  <summary>Details</summary>
Motivation: 传统的临床命名实体识别工具只能将文本映射到单个UMLS概念（CUI），但许多应用需要一组相关概念（如同义词、子类和超类）。而现有工具对于自动化构建和管理这些概念集的支持不足，往往需要耗时且不一致的人工整理。这成为临床NLP应用的瓶颈。

Method: 该研究构建了UMLS知识图并通过嵌入实现语义检索。针对每个目标概念，先用知识图检索候选CUI集合，再利用两个大语言模型（GPT-5和GPT-5-mini）进行自动过滤和分类。最终将自动生成的概念集和人工基准进行了比较评估。

Result: CUICurate框架产生的概念集比人工方法更大、更全，并能保持与人工方法相当的精准度。GPT-5-mini在候选筛选中召回率更高，GPT-5分类结果与临床专家一致性更好。工具输出稳定且计算开销低。

Conclusion: CUICurate大幅简化和标准化了UMLS概念集的构建过程，为临床NLP管道的表型构建及分析提供了高效、可扩展的支持。框架结合图检索与大模型推理，可适配不同的临床分析需求。

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [52] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 本文针对金融领域的检索增强生成（RAG）系统在长篇法规问答应用中的一个常见失效模式展开分析，提出并验证了更精细化的页面级检索单元，有效提升了答案召回率和信息检索精度。


<details>
  <summary>Details</summary>
Motivation: 金融法规文件篇幅长、结构复杂，现有RAG系统常在检索到正确文档后无法定位含答案的具体页面或片段，造成依据不充分的生成结果。此类文档内检索失败在高风险金融问答应用中尤为关键，但文献较少系统性探索这个细致层次的检索问题。

Method: 作者在FinanceBench问答集上，系统比较了稠密、稀疏、混合及层次化等多种检索策略，并提出页面级打分模型，将页面作为文档和片段之间的中间层检索单元。该模型通过在金融文件领域特定微调的双编码器，提升页面级语义相关性判定能力，同时结合oracle分析为检索极限提供参考。

Result: 实验证明，文档层检索进步虽带动页面召回提升，但页面及片段层仍有改进空间。引入微调页面级打分器显著提高了页面和chunk（片段）召回率，缩小了RAG生成系统对答案证据覆盖的不足。

Conclusion: 在金融法规问答场景中，精细化的页面级检索单元有效弥补了传统RAG系统的上下文缺失，有助于提高答案的精准性与可信度。建议未来进一步优化更细粒度的信息检索机制，提升高风险领域RAG问答系统的实用性。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [53] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 本文探讨了在AI系统逐步发展为能够自主行动、调用工具的复合体后，评估已从机器学习生命周期的最终环节，转变为核心控制机制。作者批评现有静态基准和一次性评分方式对复杂AI系统的局限性，并主张评价应成为建立信任、支持系统迭代和治理的科学过程。


<details>
  <summary>Details</summary>
Motivation: 传统的模型评价方式，侧重于静态基准和聚合评分，难以准确反映现代、动态且不可预测的AI agent系统的实际行为和可靠性。随着AI 系统不断进化，如何科学地评估其在变化、规模化场景下的行为以及能否被信任，成为亟需探讨的新课题。

Method: 本文主要通过理论分析和实例讨论，揭示当前主流评估方法的局限性，比如静默失败、误导性分数等问题，从而引发对评估本质和作用的再思考。并未着重提出新指标，而是聚焦于厘清AI时代、特别是agent系统中评估的正确定位和方法论变革。

Result: 文章指出，现有面向静态模型的评价体系易隐藏agent系统中的失败和漏洞，高评分不能反映系统真实表现。通过分析评估流程本身的失效模式，作者强调了传统方法在大规模、变化环境下的失能。

Conclusion: 论文结论强调，AI agent系统的评估不应再是展示性能的数据剧场，而应转型为保障信任、支持持续改进和治理的测量学科。这种转型对于实现AI系统可控、可托管和符合伦理治理至关重要。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [54] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 本文研究了当对话式AI被认为具有党派偏见时，其说服公众纠正错误观点的有效性下降。


<details>
  <summary>Details</summary>
Motivation: 对话式AI被视为纠正公众误区和抵制错误信息的工具，但其效果可能受限于用户对其政治中立性的看法。随着大型语言模型（LLMs）被卷入党派冲突，其被视为具有特定意识形态倾向。因此，有必要检验针对其公信力的攻击是否降低其说服力。

Method: 作者在美国进行了一项预注册的问卷实验（N=2144），设置三轮ChatGPT对话，试图纠正参与者的一项经济政策误区。在实验组中，给出一句“该AI对您的政党存在偏见”的提示，并与对照组（没有该提示）进行比较。

Result: 结果显示，提醒AI存在党派偏见的信息使说服率降低了28%。对话分析发现，接收到偏见警告的参与者更可能反驳AI的观点、参与度下降。

Conclusion: 对话式AI的说服效果受用户对其政治中立性的看法影响较大。如果用户认为AI有党派偏见，其说服力会大幅下降。

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [55] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 本文提出了一种对大型语言模型（LLMs）在专业领域进行高效微调的新方法，通过对抗式问题生成生成紧凑且有挑战性的训练样本，有效提升领域任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽然经过大规模通用语料预训练，但在专业领域适应性不佳，且高质量的领域数据稀缺。常用的合成数据（如释义、知识抽取）方法存在样本冗余与推理能力不足等问题，亟需更高效的领域适应方法。

Method: 提出一种对抗式问题生成框架：将待适应模型与专家模型在参考文档基础上进行输出比对，通过反馈驱动的迭代流程自动揭示和应对模型理解不足的领域难题，生成高质量、有针对性的紧凑问题集用于微调。

Result: 在法律领域基准LegalBench的专项任务上评估表明，该方法使用更少的合成样本即可获得更高的准确率，优于现有合成数据方法。

Conclusion: 针对专业领域LLM微调难点，本方法有效提升了用样本效率，增强了模型推理理解能力，为领域微调提供了新的高效解决思路。

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [56] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了一种基于频率分析的注意力特征检测大模型幻觉的方法，通过分析生成过程中的注意力变化，利用高频成分检测幻觉，并在多个基准任务上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在生成文本时容易出现幻觉，之前的检测方法大多使用粗略的注意力汇总，无法捕捉细粒度注意力不稳定，亟需更精细、有效的检测手段。

Method: 受信号处理启发，将注意力分布建模为离散信号，提取其中高频（局部快速变化）成分作为特征，用于监测和检测生成中易产生幻觉的token，设计了轻量级幻觉检测器。

Result: 在RAGTruth和HalluRAG等基准数据集上，提出的方法相比基于验证、内部表示和传统注意力的方法，在各类型大模型和任务上都取得了更好的检测性能。

Conclusion: 频率化注意力分析可以精准揭示幻觉token的不稳定特征，所提出的高频注意力特征检测方法有效提升了大模型幻觉检测能力，方法高效且可推广到不同任务和模型中。

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [57] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 本文提出利用无损压缩作为一种简单且与模型无关的方法，能够区分由大型语言模型（LLM）生成的文本与人类文本，并用其来衡量语言的统计结构规律。结果显示，LLM文本普遍比人类文本结构规律性更高、更易压缩。该方法在多个语境下有效，但在较小规模的互动环境中区分效果减弱。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大型语言模型以概率采样生成文本的过程如何影响语言的结构性统计规律。缺少一种简单、无需依赖模型内部信息的方法，以直接从文本表面区分机器和人的文本及其结构特点。

Method: 作者提出用无损压缩率作为衡量文本结构规律的度量指标，通过分析不同情境下（如人机续写、知识基础环境和完全虚构的社交互动环境），LLM与人类或其他机器的文本的压缩行为进行比较。

Result: 在受控及中介性生成环境中，LLM生成的语言展现出更高度结构规律、压缩率更高。该效应在不同模型、任务和领域下都较为一致，但在小规模式交互环境下，这一差别有所减弱。

Conclusion: 无损压缩为区分LLM与人类文本提供了简单且健壮的结构化视角，可直接基于文本本身进行分析，对理解和监测生成系统如何改变语言及通信复杂性具有实际价值。

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [58] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: 本论文介绍了FENCE，一个用于金融领域多模态越狱检测的中英双语数据集，并展示了其在检测大模型越狱行为中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）和视觉语言模型（VLMs）在金融等敏感领域存在被‘越狱’攻击的风险，尤其VLMs因同时处理文本和图像暴露更多攻击面。然而，现有针对金融领域VLM越狱检测的数据集和工具稀缺，限制了检测技术的发展。

Method: 作者构建了FENCE数据集，涵盖中英双语和金融相关的真实场景问题，结合图像诱导的攻击样例。通过该数据集对商用及开源VLM进行越狱实验，并用FENCE训练基线检测器评估检测效果。

Result: 实验发现，无论是商用还是开源VLM，均存在被越狱的风险。GPT-4o等大模型被攻破的概率可量化，开源模型暴露更严重。用FENCE训练的基线检测器在测试集上准确率高达99%，在外部基准上也表现良好。

Conclusion: FENCE数据集提升了金融领域多模态越狱检测的研究基础，可助力开发更加安全和可靠的AI系统，尤其适用于敏感行业。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [59] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 本文提出一种结合大型语言模型嵌入和语言学特征的点击诱饵检测方法，提升了检测准确率并增强了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题削弱了网上信息质量并损害用户信任，因此需要更有效且可解释的方法来自动检测点击诱饵。

Method: 方法将基于transformer的大型语言模型嵌入与15种显式语言学特征结合，包括使用自然语言处理技术、传统向量化器、词嵌入（Word2Vec,GloVe）、LLM以及树模型分类器（如XGBoost），通过特征增强提升效果。

Result: 最优模型为结合15种显式特征的XGBoost+嵌入，F1值达91%，显著优于TF-IDF、传统词嵌入、仅特征或仅提示等基线方法。

Conclusion: 结合嵌入和人工挑选的语言学特征不仅提高了点击诱饵检测准确率，还能根据重要语言线索解释模型决策，有助于透明、可复现的研究推进。

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [60] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 本论文提出了一种新的用于Masked Diffusion Models（MDMs）的采样方法—Info-Gain Sampler，能够提升MDMs在多任务下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有MDMs采样器通常使用贪心策略，即优先解码最有把握的位置，但这种方法忽略了当前决策对后续解码步骤和整体不确定性的影响，限制了生成质量提升。

Method: 通过分析当前贪心采样方法的缺陷，作者提出了Info-Gain Sampler，一种能够在解码过程中兼顾即时不确定性和未来信息增益的采样框架。其核心思想是结合当前解码决策对剩余未知Token概率及不确定性的全局影响，充分利用MDMs的非因果特性。

Result: 在推理、代码生成、创意写作和图像生成任务下，Info-Gain Sampler在多种架构中均超越现有采样器。例如，推理任务平均准确率提升3.6%，创意写作中获胜率达63.1%。推理任务中的累计不确定性从78.4下降到48.6，显著领先于最佳基线方法。

Conclusion: Info-Gain Sampler显著提升了MDMs模型的生成性能，尤其在减少不确定性和提升准确率方面效果突出，适用于多种下游任务，展示了其广泛的应用潜力和实用价值。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [61] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本研究提出了一种基于信息理论的句子加工存储成本度量方法，并用神经语言模型对其有效性进行了实证分析，显示其优于传统符号语法方法。


<details>
  <summary>Details</summary>
Motivation: 传统的句子加工负载度量主要依赖于符号语法，对语法预测赋予离散且同质的成本，缺乏连续性和理论通用性。为了更精确且理论中立地衡量工作记忆负载，本文试图通过信息理论和预训练神经语言模型提出更为细致和现实的度量方式。

Method: 提出以条件互信息为核心的处理存储成本度量，将前文对未来语境的不确定性感知量化。该方法依托神经语言模型，依次进行了三项英文实证分析：(1) 检验中心嵌套和关系从句的处理不对称性；(2) 与句法注释语料库中基于语法的存储成本进行相关性比较；(3) 在两个大规模自然语料阅读时间数据集上，与传统信息基预测模型对比预测力。

Result: 1）新方法准确复现了中心嵌套结构与关系从句的经典加工不对称性；2）新方法与语法基的存储成本度量结果高度相关；3）新方法在预测自然语料阅读时长方面优于传统基线模型。

Conclusion: 基于信息理论的连续、理论中立的句子加工存储成本度量方法，能够超越传统方法，更好地解释并预测语言加工中的认知负载，为未来语言理解与认知研究方法提供了新思路。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [62] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 本文提出了一种在大型语言模型推理中，针对推理过程中的低置信度Token进行干预的新解码方法CCD（Confidence-Driven Contrastive Decoding），显著提升推理准确率的同时减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 以往对在推理时增加算力的研究，多假设推理过程计算量越大越好，但事实上推理的不确定性主要集中在部分低置信度Token，这些Token带来大部分推理错误和多余输出。因而需要一种能够精准定位并优化这些关键Token的方法。

Method: 提出了CCD方法，在解码过程中实时检测低置信度Token，仅在这些Token处进行对比干预。具体做法是用最小占位符替换高置信度Token形成一个对比参考，并在低置信度Token处用预测概率分布减去这个参考分布，实现有针对性的优化。整个流程无需额外训练，只通过推理时的调节实现。

Result: 实验表明，CCD方法在数学推理等基准测试上显著提升了准确率，并有效减少了输出长度，同时KV-cache开销极小，提升效率。

Conclusion: CCD作为一种无需训练的解码优化方法，通过针对低置信度的Token精准干预，提升了大型语言模型的推理可靠性，并减少了计算冗余。

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [63] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: 本论文提出并验证了一款名为ELIA的交互式网页应用，通过AI自动生成自然语言解释，使LLM的机制解释结果更加易懂并适合非专业用户。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）机制解释工具使用门槛较高，主要被专家使用，普通用户难以理解，因此需要开发更加友好和易用的工具来提升解释性与普及性。

Method: 集成归因分析、函数向量分析和回路追踪三种主流解释方法，并创新性地利用视觉-语言模型自动为复杂的分析可视化生成自然语言解释，同时通过用户研究验证有效性。

Result: 混合型用户研究表明，用户更喜欢交互式、可探索界面，AI驱动的自然语言解释有效帮助了非专家理解LLM机制，用户的理解能力与其LLM经验无显著相关性，显示新系统降低了理解门槛。

Conclusion: 优良设计与AI解释相结合，能够极大简化LLM机制解释的复杂性，提升了结果的可访问性和普适性，尤其对非专家用户帮助突出。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [64] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 研究建立了第一个罗马尼亚语心理健康文本语料库（PsihoRo），为NLP心理健康研究填补了空白。


<details>
  <summary>Details</summary>
Motivation: 目前罗马尼亚语缺乏可公开获取的心理健康文本数据，限制了相关NLP研究的发展。与英文等语种相比，罗马尼亚语在心理健康自动检测领域资源极为有限。

Method: 通过设计包含6个开放式问题和PHQ-9/GAD-7量表的问卷，收集了205名受访者的文本数据，形成PsihoRo语料库。随后采用统计分析、罗马尼亚版LIWC文本分析、情感检测和主题建模等技术对语料库进行了特征分析。

Result: 建立了包含205份问卷文本的PsihoRo语料库，并初步展示了该语料库在心理健康和情感分析方面的价值和特征。

Conclusion: PsihoRo语料库填补了罗马尼亚语心理健康文本资源的空白，为相关NLP研究尤其是心理健康自动检测在罗马尼亚语人群中的应用打下基础。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [65] [Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning](https://arxiv.org/abs/2602.18326)
*Tao Wu,Adam Kapelner*

Main category: cs.CL

TL;DR: 本文提出并比较了三种自动选择用于高中生词汇教学的优质上下文例句的深度学习方法，并引入了新的评估指标。结果显示，结合人为特征及监督学习的嵌入模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 高效获取和筛选适用的词汇教学语境对高中词汇学习非常关键，但手工筛选代价高昂，因此需要自动化、智能化的方法来替代。

Method: 作者对比了三种模型：(1) 基于MPNet的自监督相似度策略，(2) 基于指令感知并微调Qwen3嵌入和非线性回归头的监督方法，(3) 在模型(2)基础上加入手工特征。论文还提出了Retention Competency Curve这一新型指标，用于评价丢弃优质语境比例与优劣比的折中。

Result: 第三种方法效果最优，仅丢弃70%的好语境时，可达到440的优劣语境比，显著优于其他方法。

Conclusion: 现代深度神经网络嵌入模型在人工监督指导下，可以低成本大规模生成质量极高的词汇教学上下文，极大促进词汇教学自动化。

Abstract: We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.

</details>


### [66] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个专为印度司法系统设计的AI框架，通过结构化处理和解释法律上诉案的判决，显著提升了判决预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 印度法庭案件积压严重，尤其是上诉案件。现有人工智能方法在判决预测和解释上存在准确性和可解释性不足。作者希望提出一个系统，既能预测结果，又能为法律专业人士提供清晰可用的解释。

Method: Vichara框架针对上诉案件英文文档，将其分解为多个关键决策点，应用结构化表示（参考IRAC框架本地化改编），并使用多种大语言模型（GPT-4o mini, Llama-3.1-8B, Mistral-7B, Qwen2.5-7B）在两个数据集PredEx和ILDC_expert上进行实验。

Result: Vichara在两套数据集上均超越现有判决预测基线。GPT-4o mini表现最佳（F1分别达81.5、80.3）。在人类评估中，Vichara生成的解释也在清晰性、关联性和实用性方面表现突出。

Conclusion: Vichara显著提升了印度法律案件判决预测的准确性与可解释性，对法律从业者有重要辅助意义。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [67] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 本文提出了一种用于主观、连续性知识表征的双尺度验证框架，结合点评和对比两种人工标注方式，并将其应用于政治立场预测任务，实现对现有主流人类评价标准的补充。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识（如政治立场）往往是连续、主观并富有争议的，传统的成对验证方法（pairwise validation）难以充分应对这种主观性和复杂性，因此需要新的、兼具可扩展性和可靠性的方法。

Method: 作者通过结合点评标注（pointwise annotation）和对比标注（pairwise annotation）的方式，对政治辩论中的论点进行标注和分析。使用22种语言模型，针对英国政论节目“Question Time”中的30场辩论、23228条论点，生成大规模政治立场预测知识库，并用双尺度人工评估验证模型输出。

Result: 在人类与模型的立场点评一致性上，Krippendorff's α为0.578，显示一定主观性；而在人类与模型的排序一致性上，最优模型达到了0.86，说明模型在排序判断上与人相当一致。

Conclusion: （1）提出了一套适用于主观连续性知识的可扩展且可靠的验证方法；（2）构建并验证了结构化政治立场知识库，有益于基于图的推理与增强式检索生成；（3）证明了能够基于模型输出提取有序结构，推动了对主观现实领域的知识表征。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [68] [SPQ: An Ensemble Technique for Large Language Model Compression](https://arxiv.org/abs/2602.18420)
*Jiamin Yao,Eren Gultepe*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型压缩技术SPQ，将SVD、剪枝和量化三种技术结合，实现内存占用大幅降低且精度优良。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量庞大，部署和推理时受限于存储和计算资源，因此需要高效压缩技术以便在有限设备上部署并兼顾精度与吞吐率。

Method: SPQ融合了三种技术：基于SVD的注意力投影低秩分解减少线性变换参数；基于激活重要性的剪枝去除MLP中的冗余神经元；对所有线性层进行8位后训练量化，实现层级互补压缩。

Result: 在LLaMA-2-7B模型上，SPQ实现内存最高75%压缩（6.86GB），优于单独使用SVD、剪枝或量化的效果，并在WikiText-2等基准上保持甚至提升困惑度和下游任务准确率。相较GPTQ和SparseGPT等主流方案，SPQ在相同比例压缩下表现更优并提升速度（最高1.9倍推理加速）。

Conclusion: SPQ通过多维度协同压缩，有效减少LLM部署所需内存且保持良好精度，为实际应用提供可行方案。

Abstract: This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/

</details>


### [69] [RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering](https://arxiv.org/abs/2602.18425)
*Deniz Qian,Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文提出了一种多轮检索框架RVR，通过“检索-验证-再检索”过程，实现对开放性多答案问题的更全面答案覆盖，实验结果显示比现有方法显著优越。


<details>
  <summary>Details</summary>
Motivation: 针对多答案检索任务中，传统检索方法难以涵盖所有有效答案的问题，提出更加全面地覆盖多种可能答案的需求。

Method: 提出RVR框架：1）用检索器基于原始查询检索候选文档；2）用验证器筛选高质量文档子集；3）将已验证文档与原查询拼接，进一步挖掘未覆盖答案，多轮嵌套进行。该框架兼容现有检索器，并对其进行微调以适应推理过程。

Result: 在多答案检索数据集QAMPARI上，RVR模型的完整召回率相比基线方法至少提高10%（相对）和3%（绝对）。在两个域外数据集（QUEST与WebQuestionsSP）上，不同检索器基础上也有一致性提升。

Conclusion: RVR方法能更全面地检索多答案问题的资料，验证-反馈式机制和检索器适应性微调有助于召回大幅提升，在多数据集的泛化表现优越，很有前景。

Abstract: Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.

</details>


### [70] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: 本文提出了VIRAASAT，一个用于印度文化的多跳知识问答数据集，并通过新方法提升大模型在文化推理任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在涉及印度等具有丰富社会文化背景的任务时表现不佳，且现有文化基准数据集无法全面评价模型的文化推理能力。为了解决多样本覆盖和推理链不充分的问题，作者希望构建一个自动化、涵盖更丰富文化内容的数据集，并研究提升模型文化推理能力的新方法。

Method: 作者提出了半自动化的VIRAASAT多跳问答数据集，依托包含700余个专家梳理文化实体的知识图谱，覆盖印度28个邦和8个联邦属地的13类文化属性。随后，提出并训练了一种新的SCoM（Symbolic Chain-of-Manipulation）范式，使模型能在内部模拟知识图谱的推理操作，提升链式推理效果。

Result: VIRAASAT数据集生成了3200多个需要链式推理的高质量多跳文化问题。实验表明，SCoM方法在监督微调下，相较于标准的链式思维(CoT)方法，正确率提高了最多20%。

Conclusion: VIRAASAT数据集和SCoM方法显著提升了大模型对特定文化背景下复杂多跳推理的能力，为构建具备文化认知和推理能力的AI模型奠定了基础。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [71] [Nested Training for Mutual Adaptation in Human-AI Teaming](https://arxiv.org/abs/2602.17737)
*Upasana Biswas,Durgesh Kalwar,Subbarao Kambhampati,Sarath Sreedharan*

Main category: cs.RO

TL;DR: 本论文提出了一种新的人机协作训练方法，通过引入分层I-POMDP和嵌套训练，使机器人能够与未见过的自适应人类合作，实现更高的任务表现和适应性。


<details>
  <summary>Details</summary>
Motivation: 人类与AI团队协作时，会相互适应，但现有训练仅用静态伙伴代理人，无法有效模拟人类的自适应行为，导致机器人难以适应新的和自适应的人类伙伴。为提升泛化能力，需要机器人接触更多自适应行为。

Method: 将人机团队建模为基于I-POMDP的交互过程，将人类的自适应建模为状态的一部分。提出递进嵌套训练，训练时每一层的智能体都面对更低层的自适应但静止伙伴，从而实现对自适应行为的暴露，又不产生仅对训练伙伴有效的隐式协调策略。方法在Overcooked游戏多回合必需合作任务中，与多种已有基线做对比，并测试与未见过的自适应伙伴配对下的效果。

Result: 实验结果显示，本文方法的机器人不仅与新自适应伙伴配合时任务完成度更高，而且在团队交互中展现出更强的适应能力，超过对比基线。

Conclusion: 引入分层自适应建模和嵌套训练方案，能有效提升机器人在人—机团队中的泛化能力，使其能更好地适应不同、新颖的合作者，改善团队总体表现。

Abstract: Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.

</details>


### [72] [Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton](https://arxiv.org/abs/2602.17794)
*Neethan Ratnakumar,Mariya Huzaifa Tohfafarosh,Saanya Jauhri,Xianlian Zhou*

Main category: cs.RO

TL;DR: 本研究评估了一种用于髋膝外骨骼的强化学习神经网络控制器，以辅助下蹲运动，并证实其能有效降低生理负荷。


<details>
  <summary>Details</summary>
Motivation: 下蹲是对下肢要求很高的动作，在需要反复下蹲的工业场合尤为消耗体力，因此开发能够智能和个性化帮助下蹲动作的外骨骼有助于减少操作者的体力消耗和疲劳。

Method: 设计了可模块化的髋-膝外骨骼，并用基于强化学习的神经网络控制器，在物理仿真环境中训练其根据历史关节角度与速度，实时生成髋膝助力扭矩。五名健康成年志愿者在三种条件下完成节奏引导的三分钟下蹲：无外骨骼（No-Exo）、外骨骼零扭矩（Zero-Torque）、外骨骼主动助力（Assistance）；通过间接量热法和心率监测评估生理负荷，同时采集运动学数据。

Result: 强化学习控制器可根据个人运动学和时序自适应地生成助力曲线。在主动助力下，相较于无助力条件，净代谢率降低约10%，心率也略有下降；但辅助时下蹲深度有减小，髋膝屈曲角度变小。

Conclusion: 该强化学习神经网络控制器能够有效降低反复下蹲的生理负荷，证明其初步有效性，为未来更优硬件和控制方法优化提供动力。

Abstract: Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.

</details>


### [73] [Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array](https://arxiv.org/abs/2602.17818)
*Zachary Turcotte,François Grondin*

Main category: cs.RO

TL;DR: 本文提出通过物理优化阶段改进语音增强性能，具体做法是动态调整麦克风阵列几何结构，应对工业环境噪声。


<details>
  <summary>Details</summary>
Motivation: 现有语音增强技术（如DSP、深度学习等）在高噪声环境下效果有限，严重影响在工业场景（如制造工厂）的部署。需要创新的方法提升在复杂噪声下的增强能力。

Method: 将16麦克风阵列安装在具有7自由度的机械臂上，麦克风分为4组，其中一组靠近末端执行器。通过调整机械臂关节，将麦克风靠近说话人，结合声源定位、计算机视觉、逆运动学、MVDR波束形成及深度学习时频掩蔽，实现系统动态重构阵列。

Result: 实验证明，所提方法在不同输入信噪比条件下，相比传统配置，获得了更高的信号失真比（SI-SDR）和更低的词错误率（WER）。

Conclusion: 动态几何优化的麦克风阵列结合多种技术后优于传统方法，能有效提升工业高噪声环境下的语音增强效果。

Abstract: Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.

</details>


### [74] [Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066](https://arxiv.org/abs/2602.17822)
*Daniel Hartmann,Kristýna Hamříková,Aleš Vysocký,Vendula Laciok,Aleš Bernatík*

Main category: cs.RO

TL;DR: 本文对比分析了ISO 10218:2011与ISO 10218:2025工业机器人安全标准，揭示了新标准对功能安全与网络安全的扩展及其综合安全框架。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人和协作机器人在制造业的广泛应用，尤其是网络化趋势加强，现有安全标准已无法涵盖数字和网络安全的新需求，迫切需要修订和完善。

Method: 通过比较ISO 10218:2011与ISO 10218:2025两个版本的结构、术语、技术要求及附录，系统梳理标准演变和扩展内容，并分析其对机器人及协作应用分类、技术规范的变化。

Result: 新版标准在功能安全与网络安全方面有显著扩充，引入新的机器人与协作应用分类，将ISO/TS 15066技术规范纳入，形成机械、功能和数字三位一体的安全要求体系。

Conclusion: ISO 10218:2025建立了更为全面的现代机器人系统设计和运行的安全框架，更好地适应人机协作和网络化工厂环境的发展趋势。

Abstract: Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.

</details>


### [75] [WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection](https://arxiv.org/abs/2602.17908)
*Mingzhang Zhu,Alvin Zhu,Jose Victor S. H. Ramos,Beom Jun Kim,Yike Shi,Yufeng Wu,Ruochen Hou,Quanyou Wang,Eric Song,Tony Fan,Yuchen Cui,Dennis W. Hong*

Main category: cs.RO

TL;DR: 提出了WHED可穿戴手部外骨骼系统，用于高质量自然抓取示范采集，并验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 高精度灵巧操作的自动学习受限于难以采集高保真多指人手示范，因为存在视觉遮挡和复杂手部运动等难题。

Method: 设计了可穿戴、长时间使用的手部外骨骼WHED系统，支持大拇指自由动作和与机器人手一致的映射，集成机械连接指界面、改造的手部感知系统和自供电传感模块，并构建了多模态同步采集和回放的数据管线。

Result: 在各种典型抓取与操作任务中，完成了自然抓取动作的演示，验证了数据采集与机器人重放的一致性和系统的可行性。

Conclusion: WHED系统能有效采集复杂自然的人类多指抓取操作，为灵巧操作学习提供高质量数据基础。

Abstract: Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.

</details>


### [76] [Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation](https://arxiv.org/abs/2602.17921)
*Kei Ikemura,Yifei Dong,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 本文提出了一个协同优化框架，可同时优化机器人的末端执行器结构和操控策略，从而提升对易变形和脆弱物体的操作表现。


<details>
  <summary>Details</summary>
Motivation: 变形和脆弱物体的操作因复杂的接触动力学及对物体完整性的高要求而极富挑战。目前的方法通常仅优化执行器设计或控制策略，限制了性能的提升。

Method: 作者提出了首个能够协同优化执行器形态和操控策略的框架。具体方法包括：1. 潜在微分同胚形状参数化，实现可表达且易优化的执行器几何形状设计；2. 基于应力的双层协同设计流程，将结构与控制优化紧密结合；3. 优享到点云的策略蒸馏，实现零样本现实部署。

Result: 方法在包括抓取果冻、推动果冻、舀鱼片等复杂食物操作任务上进行了仿真和实际测试，显示出较好的操作效果。

Conclusion: 这项工作证明了协同设计执行器结构和操控策略对于提升机器人操作脆弱和变形物体的能力的有效性和实用价值。

Abstract: Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.

</details>


### [77] [Homotopic information gain for sparse active target tracking](https://arxiv.org/abs/2602.17926)
*Jennifer Wakulicz,Ki Myung Brian Lee,Teresa Vidal-Calleja,Robert Fitch*

Main category: cs.RO

TL;DR: 本文提出了一种针对移动机器人主动目标跟踪问题的新规划方法，通过最大化目标的同伦类信息获取，提升目标未来轨迹的预测精度。


<details>
  <summary>Details</summary>
Motivation: 在主动目标跟踪中，传统基于信息增益的轨迹规划方法在面对多模态运动模型时，信息增益难以合理定义与计算。为解决这一问题，作者提出考虑目标运动的同伦类信息，而非仅低层次的度量增益，从而提升多模态情况下的跟踪效果。

Method: 提出并定义了“同伦信息增益”，即针对目标高层次运动的预期信息量，并证明其为度量信息增益的下界。方法通过规划使感知动作获得最大同伦信息，从而优化数据收集和轨迹估计。

Result: 在真实及仿真行人数据集上的实验表明，该方法比传统度量信息方法用更少观测即可获得更高准确度的轨迹预测。

Conclusion: 最大化同伦信息增益的主动跟踪方法在多模态目标运动场景下具有更优的观测效率与估计准确度，优于常规度量信息方法，适用于实际端的移动机器人目标跟踪。

Abstract: The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.

</details>


### [78] [Quasi-Periodic Gaussian Process Predictive Iterative Learning Control](https://arxiv.org/abs/2602.18014)
*Unnati Nigam,Radhendushka Srivastava,Faezeh Marzbanrad,Michael Burke*

Main category: cs.RO

TL;DR: 本文提出将准周期高斯过程（QPGP）引入预测迭代学习控制（ILC）框架，用于高效、准确地建模和预测机器人重复任务中的扰动和漂移，实现更快、更稳健的学习和收敛。


<details>
  <summary>Details</summary>
Motivation: 传统ILC虽然能通过利用历史误差信息改善机器人重复任务表现，但环境变化及机器人本身磨损会导致性能随时间下降。此外，现有基于高斯过程的ILC方法在迭代次数增加时计算量急剧增加，限制了其实用性。

Method: 作者通过采用一种基于结构方程的QPGP建模方法，将其嵌入预测型ILC控制框架中，使得推断计算复杂度从$mathcal{O}(i^2p^3)$下降到$mathcal{O}(p^3)$，极大提升大规模应用的效率。该方法能在控制回路中持续、无信息丢失地估计参数，并预测下次迭代中的误差轮廓。最后，在自动驾驶、机械臂和现实机器人任务上进行实验验证。

Result: 在三类任务实验中，该方法相较于标准ILC和常规GP-ILC，都能更快收敛，对扰动的稳健性更强，同时显著降低了计算资源消耗。

Conclusion: 所提框架能高效预测和补偿复杂扰动下的机器人任务误差，提升ILC性能与实用性，适用于多种实际重复性任务，具有较高的应用价值。

Abstract: Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.

</details>


### [79] [EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots](https://arxiv.org/abs/2602.18071)
*Boyuan An,Zhexiong Wang,Yipeng Wang,Jiaqi Li,Sihang Li,Jing Zhang,Chen Feng*

Main category: cs.RO

TL;DR: 本论文提出了一种只用机器人自身体感摄像头来控制其在复杂环境下移动并重排多个物体的新方法EgoPush。该方法通过设计对象相对空间关系的隐空间，并用强化学习结合视觉关键点进行训练。实验证明，EgoPush在仿真和实际中均大幅优于传统端到端方法，并可实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的多物体重排机器人方法往往依赖全局位置估计（如外部定位系统），然而现实环境下这些估计易受遮挡、动态变化影响，难以应用。人类能用自身视角感知完成复杂重排启发了作者。

Method: EgoPush方法通过设计编码物体相对关系的对象中心隐空间，使用强化学习让教师模型借助关键点信息学习动作和隐状态，再通过知识蒸馏让视觉学生模型模仿。为缩小教师-学生感知差异，限制教师只用可视信息，并利用时序分解与局部奖励解决长时序任务。

Result: 模拟实验显示，EgoPush在任务成功率上显著优于端到端RL基线，消融实验验证方法各组成部分有效性。此外，该方法支持零样本仿真到现实迁移，在实际移动平台实验中表现良好。

Conclusion: EgoPush无需全局状态估计，仅基于自身体感和视觉完成复杂物体重排任务，在仿真和真实世界有效，验证了感知驱动的策略学习在动态场景下的适用性。

Abstract: Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.

</details>


### [80] [Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning](https://arxiv.org/abs/2602.18097)
*Aarati Andrea Noronha,Jean Oh*

Main category: cs.RO

TL;DR: 本文提出让自动驾驶汽车与骑行者安全高效互动的框架，将Hamilton-Jacobi可达性分析与深度Q学习结合，兼顾安全和导航效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车需要在保证骑行者安全的前提下，实现高效通行，但当前方法在安全性保障与导航效率之间存在权衡，因此亟需解决。

Method: 方法融合Hamilton-Jacobi-Bellman方程求值函数作为安全度量，将该安全指标嵌入强化学习奖励，同时建模骑行者对车辆的潜在行为反应，通过扰动输入反映人类舒适性与适应行为。

Result: 该方法在仿真环境中进行了评估，并与人类驾驶行为及现有先进方法进行了对比，展示了其有效性。

Conclusion: 提出的框架兼顾了对骑行者的安全保障和自动驾驶车辆的时效性导航，在与现有方法和人类驾驶表现对比中显示出潜在优越性。

Abstract: In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.

</details>


### [81] [GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation](https://arxiv.org/abs/2602.18164)
*Jonas Frey,Turcan Tuna,Frank Fu,Katharine Patterson,Tianao Xu,Maurice Fallon,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: GrandTour数据集首次提供了覆盖多种复杂真实环境的四足机器人多模态数据，为状态估计、SLAM及感知算法的研究和评估奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 以往缺乏覆盖大规模、复杂真实场景的四足机器人公开多模态数据集，限制了相关算法的开发与公平评测。

Method: 作者使用ANYmal-D四足机器人，搭载多模态传感器（旋转激光雷达、多路RGB相机、本体感知与立体深度相机）在多种具有挑战性的室内外环境下采集数据，并引入高精度GNSS和全站仪作为轨迹基准。

Result: 数据集涵盖多种自然和城市环境，具备较大规模、丰富的数据类型，并以多种格式公开。

Conclusion: GrandTour数据集为腿式机器人领域的状态估计、感知及多传感器融合等算法研究和评测提供了宝贵资源，为学界和工业界带来推动。

Abstract: Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.

</details>


### [82] [Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark](https://arxiv.org/abs/2602.18174)
*Hyoseok Ju,Bokeon Suh,Giseop Kim*

Main category: cs.RO

TL;DR: 提出了首个专为评估尺度一致性的大规模室内视觉SLAM基准数据集ScaleMaster，并系统分析当前先进深度单目SLAM的尺度问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度单目视觉SLAM系统在小场景内表现良好，但在大规模室内环境下尺度一致性差，主流数据集和评测手段无法暴露这些关键问题。

Method: 构建并公开了ScaleMaster数据集，涵盖多层、多回路、低纹理等复杂室内场景，通过引入地图对地图的多种直接评估指标（如Chamfer距离），系统定量和定性分析现有主流深度单目SLAM系统的尺度一致性问题。

Result: 实验表明，当前主流深度单目SLAM系统在传统基准和小场景下表现优秀，但在ScaleMaster等现实复杂环境中，存在明显的尺度漂移和尺度歧义问题，导致性能大幅下降。

Conclusion: ScaleMaster揭示了深度单目SLAM系统在复杂大场景下的关键瓶颈，期望为后续研究开发具备强尺度一致性和可靠性的视觉SLAM系统提供基础数据和评价标准。

Abstract: Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.

</details>


### [83] [Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator](https://arxiv.org/abs/2602.18212)
*Rui Chen,Domenico Chiaradia,Daniele Leonardis,Antonio Frisoli*

Main category: cs.RO

TL;DR: 本论文提出并验证了一种新型轻便两自由度软体肩部外骨骼系统，通过优化和集成新型气动执行器，显著提升了响应速度，减少重量，同时有效降低了肩部相关肌群的工作负担。


<details>
  <summary>Details</summary>
Motivation: 当前的软体肩部外骨骼在实现多自由度复杂动作时面临扭矩输出和动态响应之间的权衡，尤其是在小型化和可穿戴性方面缺乏高效的解决方案。

Method: 1. 设计并开发了一种体积优化的纺锤形倾角气动执行器（SSAA），其在减少体积的同时保持较高的输出扭矩并提升响应速度。
2. 基于SSAA开发了曲线外展执行器（CAA）和水平内收执行器（HAA），并集成入双自由度纺织基肩部外骨骼。
3. 通过对10名健康志愿者的人体实验（肌电EMG）评估不同配置下对肩部相关肌群活动的辅助效果。

Result: 新型SSAA相比传统圆柱执行器体积减少35.7%，但保持94.2%的输出扭矩，动态响应速度提升35.2%。外骨骼在肩外展和屈曲任务中最大可减少目标肌群59%-63.7%的EMG活动。双执行器对屈曲增益有限，仅对胸大肌表现出统计学优势。

Conclusion: 本研究证实通过体积优化的气动结构可实现轻便且高效的多自由度软体肩部外骨骼，有效减少健康人群相关肌群负荷。结果为后续针对多自由度外骨骼的结构优化与应用提供了实验依据与设计指导。

Abstract: Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.
  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.

</details>


### [84] [SimVLA: A Simple VLA Baseline for Robotic Manipulation](https://arxiv.org/abs/2602.18224)
*Yuankai Luo,Woping Chen,Tong Liang,Baiqiao Wang,Zhenguo Li*

Main category: cs.RO

TL;DR: 提出了一个精简的 VLA（视觉-语言-动作）模型基线 SimVLA，在保持小模型规模和标准化训练的前提下，实现了超越当前大模型的性能，为以后相关研究提供了清晰、易复现的参考。


<details>
  <summary>Details</summary>
Motivation: 现有 VLA 模型的性能提升常因架构或训练差异难以归因，影响了领域内的创新与公正比较，因此亟需一个结构简单、训练严格标准化且易复现的基线。

Method: 严格分离感知与控制模块，采用标准视觉-语言 backbone 和轻量级动作 head，并标准化训练细节，构建小型（0.5B 参数）但高效的基线模型 SimVLA。

Result: SimVLA 无需机器人预训练，在标准仿真基准上性能优于多参数量级（数十亿参数）的现有模型，在真实机器人实验中表现与已有高性能模型持平。

Conclusion: SimVLA 提供了一个简单、强大、可重复的研究基线，有助于明确未来架构创新带来的性能提升归因，推动领域可靠进步。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA

</details>


### [85] [RoEL: Robust Event-based 3D Line Reconstruction](https://arxiv.org/abs/2602.18258)
*Gwangtak Bae,Jaeho Shin,Seunggu Kang,Junho Kim,Ayoung Kim,Young Min Kim*

Main category: cs.RO

TL;DR: 本文提出了一种基于事件相机的3D线特征提取与定位优化方法，大幅提升了事件相机在实际场景中的操控与映射表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机对物体边界和纹理变化的响应主要以线为主，这些线为场景提供了鲁棒的空间结构信息。但因线的稀疏性和估计误差，相关方法对结果影响较大，且现有少数方法多依赖其他传感器弥补事件相机的局限。

Method: 该方法通过在事件数据不同时间片上多视角挖掘，稳定地提取线迹轨迹，并用几何代价函数优化3D线地图和相机位姿，消除投影畸变和深度歧义。这一代价函数同样适用于点云或2D图像等任意含线特征的观测数据。

Result: 实验证明该方法在多数据集上显著提升了基于事件数据的地图构建和位姿优化精度，并能灵活应用于多模态场景。

Conclusion: 提出的基于线的事件相机感知和映射框架具有鲁棒性与高效性，有利于事件相机系统在实际环境中的部署和应用。

Abstract: Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/

</details>


### [86] [Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments](https://arxiv.org/abs/2602.18260)
*Magnus Norén,Marios-Nektarios Stamatopoulos,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文提出了一种面向四足机器人团队的角色自适应领航—跟随编队规划与控制框架，实现了在复杂环境中的灵活协作导航。该方法通过动态分配角色和部分目标规划，有效保障了编队稳定性和避障能力。


<details>
  <summary>Details</summary>
Motivation: 传统编队方法多采用固定领队或刚性角色，导致在障碍密集和动态变化场景下灵活性不足，难以实现高效安全的协同导航。本文旨在解决四足机器人团队在复杂环境下的灵活编队和避障问题。

Method: 方法结合了动态角色分配、部分目标规划、虚拟弹簧阻尼系统以及全新的自适应避障层，利用Fast Marching Square（FM2）算法规划全球和局部路径，并加入动态远景参考生成器以提升灵活避障能力。

Result: 通过大量仿真和实际机器人实验，验证了该方法的有效性。实验结果显示，多机器人能够顺畅协作，灵活切换角色，并在复杂、非结构化环境中保持稳健的编队移动。

Conclusion: 提出的编队规划与控制框架能够显著提升四足机器人团队在复杂环境下的协作能力，体现了良好的灵活性、自适应性和可靠性，具有良好的实际应用前景。

Abstract: This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.

</details>


### [87] [Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty](https://arxiv.org/abs/2602.18312)
*Zhaoming Xie,Kevin Karol,Jessica Hodgins*

Main category: cs.RO

TL;DR: 论文提出用action Jacobian惩罚项来约束强化学习中动作的非自然高频变化，并配合新结构Linear Policy Net（LPN），有效提升动作平滑性，减少调参成本和训练计算负担，实现了更加自然流畅的控制输出，验证于模拟人物和真实机器人的动态技能模仿任务。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习控制策略常出现非自然的高频控制信号，这不符合人类或真实机器人的物理能力，现有的抑制方法需要大量调参，影响应用便利性和泛化能力。

Method: 提出使用action Jacobian惩罚项，通过自动微分方式直接约束动作对状态的变化带来的高频分量，消除非自然的高频动作信号；并设计了Linear Policy Net（LPN）作为低计算成本的策略网络架构，使Jacobian惩罚项在训练中高效可用，无需特定任务调参。

Result: LPN结合Jacobian惩罚项能稳定学到平滑且高效的控制策略，实验涵盖多种动态仿动作任务（如后空翻、跑酷等），并迁移到带机械臂的四足机器人上，取得了良好性能和泛化表现。

Conclusion: 所提方法能自动消除强化学习中不自然的高频动作信号，提高动作平滑性，无需任务相关的参数调试，并降低训练和推理时的计算负担，适用于模拟和实际平台的复杂动态动作任务。

Abstract: Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.

</details>


### [88] [Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams](https://arxiv.org/abs/2602.18330)
*Mohsen Jafarpour,Ayberk Yüksek,Shahab Eshghi,Stanislav Gorb,Edoardo Milana*

Main category: cs.RO

TL;DR: 采用螺旋结构的拍弦（snapping beam）与肌腱驱动机制，提出了一种可控且高效的软体机器人驱动方案，具备往复和非往复运动能力，实现高效推进。


<details>
  <summary>Details</summary>
Motivation: 软体机器人对灵活、高效、可编程的驱动方式需求强烈。传统弹性体作为驱动材料存在变形受限、效率不高等问题，本研究通过非线性失稳（snapping）现象寻求突破。

Method: 设计了基于螺旋结构的metabeam，并结合肌腱驱动，采用PLA材料通过熔融沉积建模（FDM）3D打印成型。通过改变边界条件，实验分析其非线性响应和力学特性，并集成到仿生游动机器人，实现双模式驱动。

Result: 不同边界条件下，结构的临界力和稳定性可调；螺旋结构实现了刚性材料的大变形；集成机器人实验中，非往复驱动模式实现了每0.4秒约32mm的高效推进。

Conclusion: 基于几何设计的snapping结构能为软体机器人提供可控、高效的驱动模式，具备结构简单、可调节、可编程等优势，有望推动软体机器人应用发展。

Abstract: Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.

</details>


### [89] [Downwash-aware Configuration Optimization for Modular Aerial Systems](https://arxiv.org/abs/2602.18344)
*Mengguang Li,Heinz Koeppl*

Main category: cs.RO

TL;DR: 本文提出了一种面向任务的同质模块化空中系统组装框架，考虑了气动力干扰与下洗效应，能够自动生成并筛选最佳组装方案，在仿真和实物实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 以往的模块化无人机组装多简化局限于二维平面，并常常忽略了模块间下洗等气动力学干扰，实际应用中可能导致性能受损，因此需要一个能兼顾实际约束和优化控制输入的组装方案生成方法。

Method: 首先大规模枚举非同构的连接拓扑结构，然后通过非线性规划检验组装可行性，并在执行器极限与下洗约束条件下，选择所需控制输入最小的组装方案。

Result: 所提框架在基于物理的仿真环境和真实机器人实验中得到了评估和演示，显示出能够有效考虑约束并优化性能。

Conclusion: 该框架能系统且高效地为同质模块化空中系统自动生成和筛选优化后的装配方案，有效避免气动力干扰，提高组装性能和适应性，具有实际应用价值。

Abstract: This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.

</details>


### [90] [Zero-shot Interactive Perception](https://arxiv.org/abs/2602.18374)
*Venkatesh Sripada,Frank Guerin,Amir Ghalamzan*

Main category: cs.RO

TL;DR: 提出了一种零样本交互感知（ZS-IP）框架，结合多策略操作和视觉语言模型，提升机器人在复杂场景下推、抓操作的效率和智能性。


<details>
  <summary>Details</summary>
Motivation: 复杂半可见场景中，遮挡和语义歧义让机器人难以感知和操作，需要主动交互手段获取隐藏信息，提升对复杂任务的适应能力。

Method: 1）提出Zero-Shot IP（ZS-IP）框架，结合推和抓两种交互策略并由视觉语言模型引导；2）创新性引入推行动作相关的新视觉增强特征pushlines，配合传统关键点；3）内置带记忆的语义查询机制；4）将各部分与机器人控制执行模块结合，能根据VLM输出自主选择推、拉、抓等动作。

Result: 在7自由度Franka Panda机械臂上评测，ZS-IP在多种遮挡及任务复杂场景下推的操作效果显著优于被动或基于视角的感知方法（如MOKA），且能保持非目标物体的完整性。

Conclusion: ZS-IP框架有效提升了机器人在复杂、部分可观测场景中的主动感知与推抓能力，推动交互感知领域的进步，展示了pushlines增强和语义记忆查询的实际价值。

Abstract: Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.

</details>


### [91] [Ori-Sense: origami capacitive sensing for soft robotic applications](https://arxiv.org/abs/2602.18379)
*Hugo de Souza Oliveira,Xin Li,Mohsen Jafarpour,Edoardo Milana*

Main category: cs.RO

TL;DR: 本研究提出了Ori-Sense，这是一款受Kresling折纸结构启发的柔性电容式传感器，可以实现软体机器人自我感知的反应。通过单体硅胶成型嵌入导电TPU电极，Ori-Sense能够将扭转变形转化为电容变化，高灵敏度监测姿态变化。


<details>
  <summary>Details</summary>
Motivation: 软体机器人对环境和自身状态感知能力有限，尤其是在关节和扭转运动的检测方面缺乏高柔顺性和集成性的传感器。本研究致力于开发结构柔顺、可集成于软体系统、可检测扭转载荷的传感器方案。

Method: 设计受Kresling折纸启发的传感器结构，使用可溶核心成型制备单体硅胶传感器并嵌入导电TPU电极，形成柔性电容器。结合力学实验与有限元仿真，分析结构响应和电信号变化。

Result: 机械测试显示传感器刚度极低，扭力低于0.01 N·mm（轴向-15~15mm范围），30°扭转和受压下扭力最高为0.03 N·mm。FEA仿真验证了沿折痕集中应力。电性测试展现最大高达30%的电容变化，灵敏度最高为0.0067 pF/deg（轴向5mm变形时）。

Conclusion: Ori-Sense传感器具备高柔顺性、低机械阻抗和优异的扭转感知灵敏度，可为软体机器人提供可靠的本体感知能力，且制备方法利于集成与扩展应用。

Abstract: This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.

</details>


### [92] [Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO](https://arxiv.org/abs/2602.18386)
*Mohamed Elgouhary,Amr S. El-Wakeel*

Main category: cs.RO

TL;DR: 本文提出利用强化学习方法（PPO）在线联合优化Pure Pursuit（PP）算法中的lookahead距离和转向增益，实现了无需依赖地图的参数自适应，显著提升了自动驾驶赛车的轨迹跟踪与速度表现。


<details>
  <summary>Details</summary>
Motivation: 传统Pure Pursuit在自动驾驶赛车中的性能极度依赖lookahead距离和steering gain的参数设定，常见的基于速度的参数调度方案泛化性不足，难以适配不同赛道和速度环境，导致性能波动大。

Method: 采用PPO强化学习算法，策略网络每步观测控制车辆的速度、曲率等状态特征，联合输出lookahead和steering gain两个参数，直接驱动PP方法。模型在F1TENTH Gym仿真环境训练，并迁移到ROS 2实际车辆系统，无需每张新地图重新调参。

Result: 无论是在仿真还是真实车辆测试中，联合RL-PP控制器在单圈成绩、轨迹跟踪精度、转向平滑性等指标上均优于传统固定lookahead PP、基于速度自适应PP和仅lookahead RL方案，也超过在评估设置下的运动学MPC算法。

Conclusion: 利用RL指导的参数调优可稳定地改善基于几何模型的经典轨迹跟踪控制，提升自动驾驶赛车在不同环境下的泛化性和表现。

Abstract: Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.

</details>


### [93] [How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf](https://arxiv.org/abs/2602.18397)
*Wenqi Jiang,Jason Clemons,Karu Sankaralingam,Christos Kozyrakis*

Main category: cs.RO

TL;DR: 本文提出了VLA-Perf性能分析模型，系统研究了视觉-语言-动作（VLA）模型的推理性能，揭示了影响VLA模型在实时应用中表现的关键因素，旨在为未来VLA模型和推理系统的设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在多种具身人工智能任务中表现优异，但由于其架构和推理系统组合空间巨大，导致实际部署时的实时推理性能尚未被系统研究。因此，作者提出要系统性地分析影响VLA模型实时推理的关键因素，以推动其在真实机器人等实时场景中的应用。

Method: 作者提出了VLA-Perf分析框架，可对任意VLA模型与推理系统的组合进行性能分析。基于此框架，作者系统考察了模型扩展性、架构选择、长上下文视频输入、异步推理、双系统模型管道等对推理性能的影响，并评估了本地端、边缘服务器和云端部署下，硬件与网络状况对端到端延迟的综合影响。

Result: 通过VLA-Perf，作者首次系统性地描绘了VLA模型推理性能的全景，量化了模型与部署相关设计因素对性能的影响，总结出15项对未来VLA模型和推理系统设计有指导意义的经验结论。

Conclusion: 本研究为VLA模型与推理系统的实时推理性能分析提供了方法和数据支持，对模型设计和系统部署提供了切实可行的建议，为推动其在真实世界的广泛应用打下基础。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.

</details>


### [94] [Snapping Actuators with Asymmetric and Sequenced Motion](https://arxiv.org/abs/2602.18421)
*Xin Li,Ye Jin,Mohsen Jafarpour,Hugo de Souza Oliveira,Edoardo Milana*

Main category: cs.RO

TL;DR: 本研究提出一种偏心穹顶形的快弹式致动器，通过结构形变实现可控的非对称运动。将其应用于气动四足机器人，实现了单一气压输入下的协调运动。


<details>
  <summary>Details</summary>
Motivation: 传统的软体机器人致动方式通常响应较慢、效率较低。作者希望利用快弹不稳定性实现快速且高效的运动控制，拓展软体机器人的应用空间。

Method: 设计并制造了偏心穹顶形快弹致动器，通过有限元仿真与实验验证其非对称变形与气压特性。进一步将四个致动器组合于气动网络，实现四足机器人的波浪式协调运动。

Result: 致动器展现出明显的非对称变形，四足机器人在单一气源下实现了频率相关的高效运动，最快速度可达72.78 mm/s（7.5 Hz）。

Conclusion: 非对称快弹致动机制为软体机器人提供了高效的物理控制新路径，对实现完全无线、高效的软体机器人系统具有重要意义。

Abstract: Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.

</details>
