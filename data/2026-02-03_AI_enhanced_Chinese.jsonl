{"id": "2602.00222", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00222", "abs": "https://arxiv.org/abs/2602.00222", "authors": ["Guoxin Lian", "Shuo Wang", "Yucheng Wang", "Yongcai Wang", "Maiyue Chen", "Kaihui Wang", "Bo Zhang", "Zhizhong Su", "Deying Li", "Zhaoxin Fan"], "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u5730\u56fe\u5b66\u4e60\u65b0\u65b9\u6cd5MapDream\uff0c\u901a\u8fc7\u751f\u6210\u4e09\u901a\u9053\u9e1f\u77b0\u56feBEV\u6620\u5c04\uff0c\u63d0\u5347\u5bfc\u822a\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u4e0e\u5bfc\u822a\u7b56\u7565\u65e0\u5173\u3001\u624b\u5de5\u6784\u5efa\u7684\u5730\u56fe\uff0c\u96be\u4ee5\u5145\u5206\u8868\u8fbe\u4efb\u52a1\u76f8\u5173\u73af\u5883\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5730\u56fe\u4f5c\u4e3a\u4e00\u79cd\u53ef\u88ab\u5bfc\u822a\u76ee\u6807\u76f4\u63a5\u5851\u9020\u7684\u5b66\u4e60\u6027\u8868\u8fbe\u3002", "method": "\u63d0\u51faMapDream\u6846\u67b6\uff0c\u5c06\u5730\u56fe\u6784\u5efa\u95ee\u9898\u8868\u8ff0\u4e3a\u81ea\u56de\u5f52\u9e1f\u77b0\u89c6\u56fe\uff08BEV\uff09\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002\u6846\u67b6\u4e2d\uff0c\u5730\u56fe\u751f\u6210\u4e0e\u52a8\u4f5c\u9884\u6d4b\u540c\u6b65\u5b66\u4e60\uff0c\u901a\u8fc7\u6709\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\uff0c\u6700\u7ec8\u83b7\u5f97\u53ea\u4fdd\u7559\u5173\u952e\u5bfc\u822a\u4fe1\u606f\u7684\u7d27\u51d1\u4e09\u901a\u9053BEV\u5730\u56fe\uff0c\u5e76\u8054\u52a8\u5bfc\u822a\u51b3\u7b56\u3002", "result": "\u5728R2R-CE\u4e0eRxR-CE\u8fd9\u4e24\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5355\u76ee\u6a21\u578b\u4e0b\u7684\u6700\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u57fa\u4e8e\u4efb\u52a1\u9a71\u52a8\u751f\u6210\u5f0f\u5730\u56fe\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u76f4\u63a5\u9762\u5411\u5bfc\u822a\u4efb\u52a1\u7684\u5730\u56fe\u751f\u6210\u53ef\u9ad8\u6548\u63d0\u53d6\u73af\u5883\u5fc5\u8981\u4fe1\u606f\uff0c\u7aef\u5230\u7aef\u4f18\u5316\u7684MapDream\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u4efb\u52a1\u9a71\u52a8\u5730\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00401", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00401", "abs": "https://arxiv.org/abs/2602.00401", "authors": ["Jean Pierre Sleiman", "He Li", "Alphonsus Adu-Bredu", "Robin Deits", "Arun Kumar", "Kevin Bergamin", "Mohak Bhardwaj", "Scott Biddlestone", "Nicola Burger", "Matthew A. Estrada", "Francesco Iacobelli", "Twan Koolen", "Alexander Lambert", "Erica Lin", "M. Eva Mungai", "Zach Nobles", "Shane Rozen-Levy", "Yuyao Shi", "Jiashun Wang", "Jakob Welner", "Fangzhou Yu", "Mike Zhang", "Alfred Rizzi", "Jessica Hodgins", "Sylvain Bertrand", "Yeuhi Abe", "Scott Kuindersma", "Farbod Farshidian"], "title": "ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control", "comment": null, "summary": "Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ZEST\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u591a\u6837\u6765\u6e90\u7684\u52a8\u6001\u4eba\u7c7b\u52a8\u4f5c\u5728\u4e0d\u540c\u673a\u5668\u4eba\u4e0a\u7684\u96f6\u6837\u672c\u6280\u80fd\u8fc1\u79fb\uff0c\u65e0\u9700\u590d\u6742\u7684\u8c03\u53c2\u4e0e\u6807\u6ce8\uff0c\u76f4\u63a5\u5b9e\u73b0\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u7684\u9c81\u68d2\u90e8\u7f72\u3002", "motivation": "\u5728\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u5982\u4eba\u7c7b\u822c\u7684\u7075\u5de7\u3001\u5145\u6ee1\u63a5\u89e6\u7684\u5168\u8eab\u63a7\u5236\u975e\u5e38\u56f0\u96be\uff0c\u901a\u5e38\u9700\u8981\u7e41\u7410\u7684\u6280\u80fd\u5de5\u7a0b\u548c\u6613\u788e\u7684\u63a7\u5236\u5668\u8c03\u53c2\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u8fc1\u79fb\u590d\u6742\u52a8\u4f5c\u3001\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u65b9\u6cd5\u3002", "method": "ZEST\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff0c\u4ece\u9ad8\u4fdd\u771f\u52a8\u6355\u3001\u5355\u76ee\u89c6\u9891\u4ee5\u53ca\u52a8\u753b\u7b49\u591a\u6e90\u52a8\u4f5c\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u81ea\u52a8\u8bfe\u7a0b\u8bbe\u8ba1\u805a\u7126\u96be\u5ea6\u8f83\u9ad8\u7684\u7247\u6bb5\uff0c\u5e76\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u3002\u8bad\u7ec3\u4e2d\u8fd8\u5f15\u5165\u4e86\u5173\u8282\u7ea7\u589e\u76ca\u53c2\u6570\u6311\u9009\u548c\u7cbe\u7ec6\u7684\u6267\u884c\u5668\u5efa\u6a21\u3002\u6240\u6709\u8bad\u7ec3\u5747\u5728\u5e26\u6709\u9002\u5ea6\u9886\u57df\u968f\u673a\u5316\u7684\u4eff\u771f\u73af\u5883\u4e2d\u5b8c\u6210\u3002", "result": "ZEST\u80fd\u4ece\u52a8\u6355\u5b66\u4e60\u5404\u79cd\u52a8\u6001\u3001\u591a\u63a5\u89e6\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u5c06\u590d\u6742\u821e\u8e48\u3001\u573a\u666f\u4ea4\u4e92\u6280\u80fd\u76f4\u63a5\u8fc1\u79fb\u5230Atlas\u3001Unitree G1\u7b49\u4e0d\u540c\u673a\u5668\u4eba\uff0c\u5728Spot\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u4e5f\u80fd\u8fc1\u79fb\u6742\u6280\u52a8\u4f5c\u5982\u540e\u7a7a\u7ffb\u3002ZEST\u65e0\u9700\u63a5\u89e6\u6807\u7b7e\u3001\u53c2\u8003\u7a97\u53e3\u6216\u4e30\u5bcc\u5956\u52b1\u8bbe\u8ba1\uff0c\u5c55\u73b0\u4e86\u8de8\u5e73\u53f0\u4e0e\u8de8\u5f62\u6001\u7684\u663e\u8457\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "ZEST\u5b9e\u73b0\u4e86\u4ece\u591a\u79cd\u751f\u7269\u52a8\u4f5c\u5230\u5404\u7c7b\u673a\u5668\u4eba\uff08\u5305\u62ec\u4e0d\u540c\u5f62\u6001\uff09\u7684\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6280\u80fd\u8fc1\u79fb\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u4e0e\u5de5\u7a0b\u4ee3\u4ef7\uff0c\u4e3a\u751f\u7269\u8fd0\u52a8\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u63a5\u53e3\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00480", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00480", "abs": "https://arxiv.org/abs/2602.00480", "authors": ["Mohini Priya Kolluri", "Ammar Waheed", "Zohaib Hasnain"], "title": "FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control", "comment": null, "summary": "Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm \"flows\" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u901a\u4fe1\u7684\u5206\u5e03\u5f0f\u7fa4\u4f53\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u6d41\u4f53\u52a8\u529b\u5b66\uff0c\u4f7f\u5927\u89c4\u6a21\u673a\u5668\u4eba\u7fa4\u80fd\u591f\u9ad8\u6548\u534f\u540c\u3002", "motivation": "\u4f20\u7edf\u5927\u578b\u673a\u5668\u4eba\u7fa4\u9700\u8981\u901a\u8fc7\u901a\u4fe1\u5b9e\u73b0\u534f\u4f5c\uff0c\u4f46\u8fd9\u6837\u4f1a\u5f15\u5165\u5ef6\u8fdf\u3001\u5e26\u5bbd\u53d7\u9650\u548c\u6613\u53d7\u6545\u969c\u5f71\u54cd\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5f00\u53d1\u975e\u901a\u4fe1\u4f9d\u8d56\u7684\u7fa4\u4f53\u534f\u8c03\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u8fd0\u52a8\u539f\u7406\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5206\u5e03\u5f0f\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u5355\u4e2a\u673a\u5668\u4eba\u7684\u72b6\u6001\u4e0e\u6d41\u4f53\u5355\u5143\u5c5e\u6027\u5efa\u7acb\u6620\u5c04\uff0c\u4f7f\u6574\u4e2a\u7fa4\u4f53\u50cf\u6d41\u4f53\u4e00\u6837\u5728\u7a7a\u95f4\u4e2d\u6d41\u52a8\uff0c\u65e0\u9700\u663e\u5f0f\u901a\u4fe1\u3002\u901a\u8fc7\u5bf9\u90e8\u5206\u5b50\u7fa4\u8d4b\u4e88\u6d41\u4f53\u7279\u6027\uff0c\u673a\u5668\u4eba\u7fa4\u80fd\u81ea\u4e3b\u6f14\u5316\u5e76\u4fdd\u6301\u6574\u4f53\u7ed3\u6784\u3002\u8be5\u65b9\u6848\u5728\u7ea6\u5343\u7ea7\u65e0\u4eba\u673a\u7fa4\u7684\u4eff\u771f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0eCFD\uff08\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff09\u89e3\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u7fa4\u4f53\u884c\u4e3a\u4ea7\u751f\u7684\u901f\u5ea6\u573a\u3001\u5bc6\u5ea6\u573a\u548c\u538b\u529b\u573a\u4e0eCFD\u6a21\u62df\u6709\u8f83\u597d\u4e00\u81f4\u6027\uff08\u901f\u5ea6\u5f52\u4e00\u5316RMSE\u4e3a0.15-0.9\uff0c\u5bc6\u5ea6\u4e3a0.61-0.98\uff0c\u538b\u529b\u4e3a0-0.937\uff09\u3002\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u5927\u7fa4\u4f53\u673a\u5668\u4eba\u7684\u6d41\u4f53\u5f0f\u5b8f\u89c2\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u5b9e\u53ef\u4ee5\u5c06\u673a\u5668\u4eba\u7fa4\u89c6\u4e3a\u8fde\u7eed\u4f53\u8fdb\u884c\u5206\u6790\u548c\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u7fa4\u4f53\u7ed3\u6784\u548c\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u6446\u8131\u901a\u4fe1\u4f9d\u8d56\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u534f\u540c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00500", "abs": "https://arxiv.org/abs/2602.00500", "authors": ["Jianyi Zhou", "Yujie Wei", "Ruichen Zhen", "Bo Zhao", "Xiaobo Xia", "Rui Shao", "Xiu Su", "Shuo Yang"], "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning", "comment": null, "summary": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5INFUSE\uff0c\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u57fa\u7840\u6a21\u578b\uff0c\u5373\u4f7f\u7528\u6237\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7528\u7eaf\u51c0\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u540e\u95e8\u4e5f\u80fd\u6301\u7eed\u751f\u6548\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002", "motivation": "VLA\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5177\u8eabAI\u7cfb\u7edf\uff0c\u4f46\u5176\u5728\u5b89\u5168\u6027\u4e0a\u3001\u7279\u522b\u662f\u9762\u5bf9\u540e\u95e8\u653b\u51fb\u65f6\u4fdd\u62a4\u4e0d\u8db3\u3002\u73b0\u6709\u540e\u95e8\u65b9\u6848\u6613\u88ab\u4e0b\u6e38\u5fae\u8c03\u6e05\u9664\uff0c\u96be\u4ee5\u5a01\u80c1\u771f\u5b9e\u90e8\u7f72\u6a21\u578b\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u591f\u5728\u7528\u6237\u4fa7\u4efb\u610f\u5fae\u8c03\u540e\u4f9d\u7136\u6709\u6548\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002", "method": "INFUSE\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u5404\u6a21\u5757\u5728\u4e0d\u540c\u5fae\u8c03\u60c5\u666f\u4e0b\u7684\u53c2\u6570\u654f\u611f\u6027\uff0c\u8bc6\u522b\u5fae\u8c03\u4e0d\u654f\u611f\uff08\u5373\u53c2\u6570\u53d8\u52a8\u5c0f\uff09\u7684\u6a21\u5757\uff0c\u5e76\u4ec5\u5411\u8fd9\u4e9b\u6a21\u5757\u690d\u5165\u540e\u95e8\uff0c\u540c\u65f6\u51bb\u7ed3\u5176\u4ed6\u6a21\u5757\u3002\u8fd9\u6837\uff0c\u5373\u4f7f\u7528\u6237\u540e\u7eed\u8fdb\u884c\u5145\u5206\u5fae\u8c03\uff0c\u540e\u95e8\u4f9d\u7136\u6709\u6548\u3002", "result": "\u5728\u591a\u79cdVLA\u67b6\u6784\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0cINFUSE\u5373\u4f7f\u5728\u7528\u6237\u5fae\u8c03\u540e\uff0c\u540e\u95e8\u653b\u51fb\u6210\u529f\u7387\u5206\u522b\u8fbe91.0%\u548c79.8%\uff0c\u8fdc\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5BadVLA\uff0838.8%\u548c36.6%\uff09\uff0c\u4e14\u5bf9\u6b63\u5e38\u4efb\u52a1\u6027\u80fd\u65e0\u660e\u663e\u5f71\u54cd\u3002", "conclusion": "INFUSE\u63ed\u793a\u4e86\u4e00\u4e2a\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u2014\u2014\u53ea\u8981\u5728\u5206\u53d1\u524d\u690d\u5165\u540e\u95e8\uff0cVLA\u57fa\u7840\u6a21\u578b\u5373\u4f7f\u7ecf\u8fc7\u5927\u91cf\u4e0b\u6e38\u5fae\u8c03\uff0c\u4f9d\u7136\u53ef\u80fd\u88ab\u6fc0\u6d3b\uff0c\u547c\u5401\u793e\u533a\u5173\u6ce8\u548c\u9632\u8303\u6b64\u7c7b\u6301\u4e45\u540e\u95e8\u5a01\u80c1\u3002"}}
{"id": "2602.00095", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00095", "abs": "https://arxiv.org/abs/2602.00095", "authors": ["Weiyu Sun", "Liangliang Chen", "Yongnuo Cai", "Huiru Xie", "Yi Zeng", "Ying Zhang"], "title": "EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b1300\u4f59\u4efd\u5927\u5b66\u7406\u5de5\u79d1\u624b\u5199\u89e3\u9898\u7684\u771f\u5b9e\u6570\u636e\u96c6EDU-CIRCUIT-HW\uff0c\u7528\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u624b\u5199\u5185\u5bb9\u8bc6\u522b\u53ca\u81ea\u52a8\u6279\u6539\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002\u901a\u8fc7\u5bf9\u73b0\u6709\u4e3b\u6d41MLLMs\u6a21\u578b\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u624b\u5199\u5377\u9762\u65b9\u9762\u4ecd\u5b58\u5728\u5927\u89c4\u6a21\u3001\u9690\u853d\u7684\u5931\u8d25\uff0c\u73b0\u9636\u6bb5\u5c1a\u4e0d\u9002\u5408\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u76f4\u63a5\u5e94\u7528\u3002\u8fdb\u4e00\u6b65\uff0c\u4f5c\u8005\u63a2\u7d22\u4e86\u5229\u7528\u9519\u8bef\u68c0\u6d4b\u548c\u81ea\u52a8\u7ea0\u9519\u673a\u5236\u663e\u8457\u63d0\u5347\u81ea\u52a8\u6279\u6539\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684MLLMs\u5728\u6559\u80b2\u9886\u57df\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u51c6\u786e\u89e3\u8bfb\u5305\u542b\u6570\u5b66\u516c\u5f0f\u3001\u56fe\u8868\u3001\u6587\u672c\u7b49\u591a\u6a21\u6001\u624b\u5199\u903b\u8f91\u4ecd\u6781\u5177\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u771f\u5b9e\u4e14\u4e13\u4e1a\u7684\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u5f0f\u3002\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4fa7\u91cd\u4e0b\u6e38\u7ed3\u679c\uff0c\u672a\u80fd\u5168\u9762\u53cd\u6620MLLMs\u5bf9\u590d\u6742\u624b\u5199\u89e3\u9898\u7684\u5b9e\u9645\u7406\u89e3\u6c34\u5e73\u3002", "method": "\u4f5c\u8005\u521b\u5efa\u5e76\u516c\u5f00\u4e86EDU-CIRCUIT-HW\u771f\u5b9e\u624b\u5199\u89e3\u9898\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e13\u5bb6\u9010\u5b57\u8f6c\u5199\u4e0e\u6279\u6539\u62a5\u544a\uff0c\u5206\u522b\u4ece\u4e0a\u6e38\u624b\u5199\u8bc6\u522b\u7cbe\u5ea6\u548c\u4e0b\u6e38\u81ea\u52a8\u6279\u6539\u8868\u73b0\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u4e3b\u6d41MLLMs\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u57fa\u4e8e\u9519\u8bef\u6a21\u5f0f\u68c0\u6d4b\u548c\u7ea0\u6b63\u7684\u589e\u5f3a\u65b9\u6848\uff0c\u4ec5\u9700\u6781\u5c11\u4eba\u5de5\u53c2\u4e0e\u5373\u53ef\u663e\u8457\u63d0\u5347\u81ea\u52a8\u6279\u6539\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524dMLLMs\u5728\u624b\u5199\u5185\u5bb9\u7406\u89e3\u548c\u81ea\u52a8\u6279\u6539\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u591a\u9690\u853d\u5931\u8bef\uff0c\u96be\u4ee5\u80dc\u4efb\u9ad8\u98ce\u9669\u6559\u80b2\u81ea\u52a8\u5316\u573a\u666f\u3002\u91c7\u7528\u57fa\u4e8e\u9519\u8bef\u68c0\u6d4b\u7684\u5c11\u91cf\u4eba\u5de5\u4fee\u6b63\u7b56\u7565\u540e\uff0c\u7cfb\u7edf\u9c81\u68d2\u6027\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u76ee\u524dMLLMs\u5728\u590d\u6742\u624b\u5199\u89e3\u9898\u81ea\u52a8\u7406\u89e3\u4e0e\u6279\u6539\u8fd8\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u6570\u636e\u4e0e\u8bc4\u6d4b\u57fa\u51c6\u7684\u5efa\u8bbe\u5c24\u4e3a\u91cd\u8981\u3002\u901a\u8fc7\u5f15\u5165\u81ea\u52a8\u8bc6\u522b\u5931\u8bef\u548c\u9ad8\u6548\u4eba\u5de5\u6821\u6b63\u673a\u5236\uff0c\u53ef\u5927\u5927\u589e\u5f3a\u57fa\u4e8eAI\u7684\u6559\u80b2\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2602.00007", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00007", "abs": "https://arxiv.org/abs/2602.00007", "authors": ["MinGyu Jeon", "SuWan Cho", "JaeYoung Shu"], "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u65b9\u6cd5PPoGA\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u673a\u5236\uff08\u7279\u522b\u662f\u8ba1\u5212\u81ea\u6211\u7ea0\u9519\u4e0e\u91cd\u6784\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u89e3\u7b54\u4e2d\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u9047\u5230\u521d\u59cb\u63a8\u7406\u8ba1\u5212\u5931\u8bef\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u81ea\u6211\u4fee\u6b63\u548c\u91cd\u6784\uff0c\u7c7b\u4f3c\u4e8e\u8ba4\u77e5\u529f\u80fd\u56fa\u7740\uff0c\u5bfc\u81f4\u95ee\u9898\u4e0d\u80fd\u6709\u6548\u89e3\u51b3\u3002\u8be5\u95ee\u9898\u9650\u5236\u4e86AI\u63a8\u7406\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "PPoGA\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba1\u5212-\u6267\u884c\u5668\u7ed3\u6784\uff0c\u5c06\u9ad8\u5c42\u7b56\u7565\u4e0e\u5e95\u5c42\u6267\u884c\u5206\u79bb\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u6027\u5904\u7406\u673a\u5236\u63d0\u524d\u9884\u5224\u53ef\u80fd\u7ed3\u679c\u3002\u6838\u5fc3\u5728\u4e8e\u5f15\u5165\u4e86\u4e00\u5957\u81ea\u6211\u7ea0\u9519\u673a\u5236\uff0c\u4e0d\u4ec5\u6821\u6b63\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u5c40\u90e8\u8def\u5f84\u9519\u8bef\uff0c\u4e5f\u80fd\u5728\u6574\u4f53\u8ba1\u5212\u65e0\u6548\u65f6\u68c0\u6d4b\u3001\u5e9f\u5f03\u5e76\u91cd\u6784\u6574\u4e2a\u63a8\u7406\u65b9\u6848\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3KGQA\u6570\u636e\u96c6\uff08GrailQA\u3001CWQ\u3001WebQSP\uff09\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cPPoGA\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u8ba1\u5212\u81ea\u6211\u7ea0\u9519\u548c\u4efb\u52a1\u91cd\u6784\u7b49\u5143\u8ba4\u77e5\u80fd\u529b\u5bf9\u4e8e\u589e\u5f3aAI\u7cfb\u7edf\u63a8\u7406\u7075\u6d3b\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5065\u58ee\u3001\u667a\u80fd\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.00514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00514", "abs": "https://arxiv.org/abs/2602.00514", "authors": ["Yaohua Liu", "Binkai Ou", "Zicheng Qiu", "Ce Hao", "Yemin Wang", "Hengjun Zhang"], "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation", "comment": null, "summary": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u89c6\u89c9-\u89e6\u89c9\u673a\u68b0\u624bLVTG\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u73af\u5883\u4e0b\u80fd\u591f\u7a33\u5065\u5730\u64cd\u4f5c\u5927\u578b\u7269\u4f53\uff0c\u5e76\u663e\u8457\u63d0\u5347\u64cd\u4f5c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u5728\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e0b\u6293\u53d6\u5927\u7269\u4f53\u65f6\uff0c\u5e38\u56e0\u611f\u77e5\u8303\u56f4\u7a84\u3001\u53ef\u9760\u6027\u5dee\u3001\u6210\u672c\u9ad8\u53d7\u5230\u9650\u5236\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u673a\u68b0\u624b\u7684\u5b9e\u9645\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578bLVTG\u624b\u722a\uff0c\u5177\u5907\u5927\u5f00\u5408\u89d2\u5ea6\u3001\u5e7f\u89e6\u89c9\u611f\u77e5\u533a\u3001\u8010\u78e8\u76ae\u80a4\u548c\u6a21\u5757\u5316\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u4e0e\u89e6\u89c9\u4f20\u611f\u5668\u3002\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff08\u7c7bCLIP\u65b9\u6cd5\uff09\u5c06\u89e6\u89c9\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u6784\u5efa\u5171\u4eab\u611f\u77e5\u7a7a\u95f4\uff0c\u96c6\u6210\u4e8eAction Chunking Transformer\uff08ACT\uff09\u7b56\u7565\u4ee5\u63d0\u5347\u64cd\u4f5c\u8868\u73b0\u3002", "result": "LVTG\u673a\u68b0\u624b\u5728\u64cd\u4f5c\u5927\u578b\u548c\u91cd\u7269\u65f6\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u66f4\u5f3a\u7684\u611f\u5e94\u80fd\u529b\u3002\u5bf9\u6bd4\u539fACT\u65b9\u6cd5\uff0cLVTG\u5728\u591a\u9879\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6210\u529f\u7387\u3002", "conclusion": "LVTG\u673a\u68b0\u624b\u7ed3\u5408\u9ad8\u8010\u4e45\u7ed3\u6784\u3001\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u89c6\u89c9-\u89e6\u89c9\u878d\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u548c\u64cd\u4f5c\u8868\u73b0\uff0c\u5177\u5907\u5b9e\u9645\u63a8\u5e7f\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00096", "abs": "https://arxiv.org/abs/2602.00096", "authors": ["Zhengqing Gao", "Ziwen Li", "Xin Wang", "Jiaxin Huang", "Zhenyang Ren", "Mingkai Shao", "Hanlue Zhang", "Tianyu Huang", "Yongkang Cheng", "Yandong Guo", "Runqi Lin", "Yuanyuan Wang", "Tongliang Liu", "Kun Zhang", "Mingming Gong"], "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video", "comment": null, "summary": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f62\u7684\u4e16\u754c\u5efa\u6a21\u4e0e\u4eff\u771f\u6846\u67b6\uff0c\u53ea\u9700\u591a\u89c6\u89d2\u89c6\u9891\u548c\u901a\u7528\u7d20\u6750\uff0c\u5373\u53ef\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u8bad\u7ec3\u6570\u636e\uff0c\u7528\u4e8e\u589e\u5f3a\u5177\u8eab\u667a\u80fd\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u5b58\u5728\u7269\u7406\u4e0e\u89c6\u89c9\u843d\u5dee\u8f83\u5927\u3001\u4f9d\u8d56\u6602\u8d35\u4f20\u611f\u5668\u548c\u7cbe\u786e\u6807\u5b9a\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u5177\u8eab\u667a\u80fd\u8bad\u7ec3\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u4e14\u66f4\u771f\u5b9e\u7684\u4eff\u771f\u73af\u5883\u6784\u5efa\u7b56\u7565\u3002", "method": "\u5229\u75283D Gaussian Splatting\u6280\u672f\u6839\u636e\u591a\u89c6\u89d2\u89c6\u9891\u91cd\u5efa\u771f\u5b9e\u73af\u5883\u7684\u7167\u7247\u7ea7\u4e09\u7ef4\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5f0f\u6a21\u578b\u6062\u590d\u7269\u7406\u5c5e\u6027\uff0c\u901a\u8fc7\u7cbe\u51c6\u6807\u5b9a\u5bf9\u9f50\u573a\u666f\u4e0e\u73b0\u5b9e\u5c3a\u5bf8\uff0c\u5c06\u5176\u96c6\u6210\u8fdb\u4eff\u771f\u5e73\u53f0\uff0c\u5b9e\u73b0\u53ef\u7f16\u8f91\u3001\u5177\u7269\u7406\u57fa\u7840\u7684\u4e16\u754c\u6a21\u578b\u3002", "result": "\u57fa\u4e8e\u8be5\u6846\u67b6\u751f\u6210\u7684\u4eff\u771f\u6570\u636e\u8bad\u7ec3\u51fa\u6765\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u91cd\u5efa\u9a71\u52a8\u7684\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\u4e3a\u5177\u8eab\u667a\u80fd\u8bad\u7ec3\u5e26\u6765\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u662f\u5b9e\u73b0\u5927\u89c4\u6a21\u9ad8\u6548\u5177\u8eab\u667a\u80fd\u5e94\u7528\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.00009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00009", "abs": "https://arxiv.org/abs/2602.00009", "authors": ["Samuel Thio", "Matthew Lewis", "Spiros Denaxas", "Richard JB Dobson"], "title": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA", "comment": "26 pages, 5 figures, 2 tables", "summary": "Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u4e34\u5e8a\u68c0\u7d22\u7cfb\u7edfMediGRAF\uff0c\u5c06\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\u7ed3\u5408\uff0c\u63d0\u5347\u4e86EHR\u73af\u5883\u4e0b\u7684\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\uff0c\u517c\u987e\u5b89\u5168\u6027\u548c\u5168\u9762\u6027\u3002", "motivation": "\u5f53\u524dEHR\u7cfb\u7edf\u4fe1\u606f\u5e9e\u5927\uff0c\u5bb9\u6613\u8ba9\u4e34\u5e8a\u533b\u751f\u5ffd\u7565\u5173\u952e\u4fe1\u606f\u3002LLM\u867d\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u8131\u79bb\u548c\u5e7b\u89c9\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u4e5f\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4fe1\u606f\u6574\u5408\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faMediGRAF\uff0c\u4e00\u79cd\u6df7\u5408\u578b\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u7cfb\u7edf\uff0c\u7ed3\u5408Neo4j\u7684Text2Cypher\uff08\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u904d\u5386\uff09\u548c\u5411\u91cf\u5d4c\u5165\uff08\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u68c0\u7d22\uff09\uff0c\u652f\u6301\u5bf9\u60a3\u8005\u5b8c\u6574\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u3002\u901a\u8fc7MIMIC-IV\u6570\u636e\u96c6\u768410\u4f8b\u60a3\u8005\u8fdb\u884c\u8282\u70b9\u548c\u5173\u7cfb\u751f\u6210\u5b9e\u9a8c\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u5ea6\u67e5\u8be2\u4efb\u52a1\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5bf9\u4e8e\u4e8b\u5b9e\u6027\u67e5\u8be2\u7cfb\u7edf\u8fbe\u5230100%\u53ec\u56de\u7387\uff0c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u4e13\u5bb6\u8d28\u91cf\u8bc4\u5206\u5e73\u5747\u4e3a4.25/5\uff0c\u4e14\u65e0\u5b89\u5168\u8fdd\u89c4\u53d1\u751f\u3002", "conclusion": "\u6df7\u5408\u56fe\u8c31\u68c0\u7d22\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u4e34\u5e8a\u4fe1\u606f\u68c0\u7d22\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\uff0c\u662f\u4f20\u7edfLLM\u5e94\u7528\u7684\u66f4\u4f18\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.00551", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00551", "abs": "https://arxiv.org/abs/2602.00551", "authors": ["Daoxuan Zhang", "Ping Chen", "Xiaobo Xia", "Xiu Su", "Ruichen Zhen", "Jianqiang Xiao", "Shuo Yang"], "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation", "comment": "15 pages, 8 figures", "summary": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{https://github.com/4amGodvzx/apex}{GitHub}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u4f53\u7cfb\u7ed3\u6784APEX\uff0c\u4f7f\u65e0\u4eba\u673a\u53ef\u9ad8\u6548\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u4e0e\u8bed\u8a00\u6307\u4ee4\u7684\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u3002APEX\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u5f0f\u63d0\u5347\u7a7a\u95f4\u8868\u8fbe\u8bb0\u5fc6\u3001\u51b3\u7b56\u4e0e\u76ee\u6807\u8bc6\u522b\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u660e\u663e\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u76ee\u6807\u5bfc\u822a\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u95f4\u8868\u5f81\u8bb0\u5fc6\u3001\u52a8\u4f5c\u51b3\u7b56\u53ef\u9760\u6027\u548c\u63a2\u7d22\u6548\u7387\u65b9\u9762\u5b58\u5728\u77ed\u677f\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\u4ee5\u63d0\u5347\u4fe1\u606f\u83b7\u53d6\u548c\u4efb\u52a1\u8fbe\u6210\u7387\u3002", "method": "\u63d0\u51faAPEX\u4f53\u7cfb\u7ed3\u6784\uff0c\u5305\u542b\uff1a1) \u52a8\u6001\u65f6\u7a7a-\u8bed\u4e49\u5730\u56fe\u8bb0\u5fc6\u6a21\u5757\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u9ad8\u5206\u8fa8\u73873D\u5730\u56fe\u6784\u5efa\u5e76\u652f\u6301\u89e3\u91ca\u6027\u8bb0\u5fc6\uff1b2) \u52a8\u4f5c\u51b3\u7b56\u6a21\u5757\uff0c\u501f\u52a9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7cbe\u7ec6\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff1b3) \u76ee\u6807\u951a\u5b9a\u6a21\u5757\uff0c\u91c7\u7528\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5b9e\u73b0\u901a\u7528\u76ee\u6807\u8bc6\u522b\u3002\u6574\u4f53\u67b6\u6784\u4e3a\u5206\u5c42\u3001\u5f02\u6b65\u5e76\u884c\u8bbe\u8ba1\uff0c\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3001\u63d0\u5347\u63a2\u7d22\u79ef\u6781\u6027\u3002", "result": "\u5728UAV-ON\u7b49\u5177\u6709\u6311\u6218\u6027\u57fa\u51c6\u4e0a\uff0cAPEX\u8f83\u524d\u6cbf\u65b9\u6cd5\u63d0\u53474.2%\u4efb\u52a1\u6210\u529f\u7387\uff08SR\uff09\u548c2.8%\u8def\u5f84\u6548\u7387\uff08SPL\uff09\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u63a2\u7d22\u6548\u7387\u548c\u5c42\u6b21\u5f02\u6b65\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "APEX\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u590d\u6742\u73af\u5883\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u7684\u6548\u7387\uff0c\u5176\u6a21\u5757\u5316\u5f02\u6b65\u5206\u5c42\u8bbe\u8ba1\u4e3a\u540e\u7eed\u76f8\u5173\u9886\u57df\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.00104", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00104", "abs": "https://arxiv.org/abs/2602.00104", "authors": ["Zhuohong Chen", "Zhengxian Wu", "Zirui Liao", "Shenao Jiang", "Hangrui Xu", "Yang Chen", "Chaokui Su", "Xiaoyu Liu", "Haoqian Wang"], "title": "R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation", "comment": null, "summary": "Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR3G\u7684\u6a21\u5757\u5316\u63a8\u7406-\u68c0\u7d22-\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347VQA\u4e2d\u7684\u56fe\u50cf\u8bc1\u636e\u83b7\u53d6\u4e0e\u63a8\u7406\u6548\u679c\uff0c\u5728\u591a\u4e2a\u57fa\u7ebf\u548c\u573a\u666f\u4e0b\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "motivation": "\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5f80\u5f80\u7f3a\u4e4f\u5173\u952e\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u9700\u901a\u8fc7\u68c0\u7d22\u8865\u5145\uff0c\u4f46\u5982\u4f55\u9009\u53d6\u548c\u6709\u6548\u96c6\u6210\u9002\u5408\u7684\u56fe\u50cf\u8bc1\u636e\uff0c\u4f9d\u7136\u5b58\u5728\u6311\u6218\u3002", "method": "R3G \u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u751f\u6210\u7b80\u5355\u7684\u63a8\u7406\u8ba1\u5212\uff0c\u660e\u786e\u6240\u9700\u7684\u89c6\u89c9\u7ebf\u7d22\uff1b2\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u65b9\u6cd5\uff0c\u5148\u7c97\u68c0\u7d22\u76f8\u5173\u56fe\u7247\uff0c\u518d\u901a\u8fc7\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\u9009\u51fa\u6700\u5408\u9002\u7684\u8bc1\u636e\u56fe\u50cf\uff1b3\uff09\u5c06\u68c0\u7d22\u5230\u7684\u56fe\u50cf\u6574\u5408\u8fdb\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728MRAG-Bench\u57fa\u51c6\u4e0a\uff0cR3G\u6846\u67b6\u5728\u516d\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u548c\u4e5d\u4e2a\u5b50\u573a\u666f\u4e2d\u5747\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u603b\u4f53\u8868\u73b0\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u8003\u8651\u4fe1\u606f\u5145\u8db3\u5ea6\u7684\u91cd\u6392\u5e8f\u4e0e\u63a8\u7406\u6b65\u9aa4\u5177\u6709\u4e92\u8865\u6027\uff0c\u80fd\u591f\u534f\u540c\u63d0\u5347\u56fe\u50cf\u9009\u62e9\u548c\u63a8\u7406\u6548\u679c\u3002", "conclusion": "R3G\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86VQA\u4efb\u52a1\u4e2d\u56fe\u50cf\u68c0\u7d22\u4e0e\u63a8\u7406\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86SOTA\uff0c\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316\u63a8\u7406\u4e0e\u5145\u8db3\u6027\u611f\u77e5\u91cd\u6392\u5e8f\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.00015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00015", "abs": "https://arxiv.org/abs/2602.00015", "authors": ["Xun Xu"], "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \\textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \\textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aG-MemLLM\u7684\u65b0\u578b\u5b58\u50a8\u589e\u5f3a\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u63a7\u66f4\u65b0\u7684\u65b9\u5f0f\u5f3a\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u53ca\u957f\u671f\u4e8b\u5b9e\u4e00\u81f4\u6027\u96be\u4ee5\u7ef4\u6301\uff0c\u5c24\u5176\u5728\u591a\u8df3\u63a8\u7406\u65f6\u6613\u51fa\u73b0\u201c\u4e0a\u4e0b\u6587\u8870\u51cf\u201d\u53ca\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\uff0c\u5f71\u54cd\u6574\u4f53\u63a8\u7406\u51c6\u786e\u6027\u3002", "method": "G-MemLLM\u5c06\u4e00\u4e2a\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9aa8\u5e72\u4e0e\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u8bb0\u5fc6\u5e93\uff08Latent Memory Bank\uff09\u7ed3\u5408\uff0c\u5f15\u5165\u7c7b\u4f3cGRU\u7ed3\u6784\u7684\u95e8\u63a7\u66f4\u65b0\u673a\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u8bb0\u5fc6\u69fd\u7684\u9009\u62e9\u6027\u66f4\u65b0\u3001\u4fdd\u7559\u6216\u8986\u76d6\uff0c\u907f\u514d\u77e5\u8bc6\u6e10\u9690\u95ee\u9898\u3002\u6a21\u578b\u5728\u4e0d\u540c\u89c4\u6a21\uff08\u5982GPT-2\u548cLlama 3.1-8B\uff09\u4e0a\u4e8eHotpotQA\u548cZero-Shot Relation Extraction\uff08ZsRE\uff09\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "G-MemLLM\u5728\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u4f8b\u5982\uff0cLlama 3.1-8B\u6a21\u578b\u5728ZsRE\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u534713.3%\uff0cGPT-2\u5728HotpotQA\u4e0a\u7684Answer F1\u63d0\u53478.56\u70b9\uff0cLlama 3.1-8B\u5728Supporting Fact F1\u4e0a\u63d0\u53476.89\u70b9\u3002", "conclusion": "G-MemLLM\u80fd\u591f\u7a33\u5b9a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u63a8\u7406\u548c\u590d\u6742\u4fe1\u606f\u7ef4\u6301\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u4e0a\u83b7\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u663e\u793a\u4e86\u5b58\u50a8\u589e\u5f3a\u548c\u95e8\u63a7\u673a\u5236\u5728\u5b9e\u9645NLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00557", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00557", "abs": "https://arxiv.org/abs/2602.00557", "authors": ["Weisheng Dai", "Kai Lan", "Jianyi Zhou", "Bo Zhao", "Xiu Su", "Junwen Tong", "Weili Guan", "Shuo Yang"], "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConLA\u7684\u65b0\u65b9\u6cd5\uff0c\u4ece\u65e0\u6807\u6ce8\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\uff0c\u4ee5\u5bf9\u6bd4\u6027\u65b9\u6cd5\u5206\u79bb\u8fd0\u52a8\u52a8\u6001\u4e0e\u89c6\u89c9\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u9996\u6b21\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4f20\u7edf\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u89c6-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u673a\u5668\u4eba\u4eba\u5de5\u6570\u636e\uff0c\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u66f4\u4e30\u5bcc\u6613\u5f97\uff0c\u4f46\u7f3a\u4e4f\u52a8\u4f5c\u76d1\u7763\u4fe1\u606f\uff0c\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u6377\u5f84\u5b66\u4e60\uff0c\u96be\u4ee5\u83b7\u5f97\u53ef\u8fc1\u79fb\u8868\u5f81\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u9ad8\u6548\u5229\u7528\u4eba\u7c7b\u89c6\u9891\uff0c\u63d0\u53d6\u7eaf\u51c0\u3001\u53ef\u8fc1\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\u6210\u4e3a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ConLA\uff08Contrastive disentanglement for Learning from Action\uff09\uff0c\u5f15\u5165\u5bf9\u6bd4\u5f0f\u89e3\u8026\u673a\u5236\uff0c\u7ed3\u5408\u52a8\u4f5c\u5148\u9a8c\u4e0e\u65f6\u5e8f\u4fe1\u606f\uff0c\u5c06\u8fd0\u52a8\u52a8\u6001\u4ece\u89c6\u89c9\u5185\u5bb9\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u51cf\u5c11\u5bf9\u8868\u9762\u89c6\u89c9\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u5e76\u5bf9\u6f5c\u5728\u52a8\u4f5c\u8868\u5f81\u8fdb\u884c\u53bb\u76f8\u5173\u3002\u6574\u4e2a\u8fc7\u7a0b\u5b8c\u5168\u57fa\u4e8e\u65e0\u6807\u6ce8\u4eba\u7c7b\u89c6\u9891\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eConLA\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4ec5\u7528\u4eba\u7c7b\u89c6\u9891\u9884\u8bad\u7ec3\u5373\u53ef\u8d85\u8fc7\u8fc7\u53bb\u4f9d\u8d56\u673a\u5668\u4eba\u771f\u5b9e\u6f14\u793a\u8f68\u8ff9\u7684\u9884\u8bad\u7ec3\u8868\u73b0\u3002", "conclusion": "ConLA\u80fd\u591f\u901a\u8fc7\u4eba\u7c7b\u89c6\u9891\u9ad8\u6548\u5b66\u4e60\u673a\u5668\u4eba\u53ef\u7528\u7684\u52a8\u4f5c\u8868\u5f81\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5f00\u542f\u65b0\u8def\u5f84\uff0c\u7a81\u7834\u4e86\u6570\u636e\u83b7\u53d6\u548c\u6cdb\u5316\u80fd\u529b\u7684\u53cc\u91cd\u74f6\u9888\u3002"}}
{"id": "2602.00105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00105", "abs": "https://arxiv.org/abs/2602.00105", "authors": ["Wing Chan", "Richard Allen"], "title": "HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models", "comment": "14 pages, 5 figures, for code and data, see https://github.com/sourceful-official/hype-edit-1-benchmark", "summary": "Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHYPE-EDIT-1\u57fa\u51c6\uff0c\u7528\u4e8e\u771f\u5b9e\u7684\u56fe\u7247\u7f16\u8f91\u6a21\u578b\u8bc4\u4f30\uff0c\u7efc\u5408\u8003\u8651\u6a21\u578b\u6027\u80fd\u4e0e\u4eba\u5de5\u5ba1\u6838\u7684\u65f6\u95f4\u548c\u6210\u672c\u3002\u7ed3\u679c\u663e\u793a\u5355\u6b21\u6210\u529f\u7387\u548c\u5b9e\u9645\u7f16\u8f91\u6210\u672c\u5dee\u5f02\u663e\u8457\uff0c\u4f4e\u5b9a\u4ef7\u7684\u6a21\u578b\u5728\u8003\u8651\u91cd\u8bd5\u548c\u4eba\u5de5\u5ba1\u6838\u540e\u672a\u5fc5\u4fbf\u5b9c\u3002", "motivation": "\u5f53\u524d\u516c\u5f00\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u8bc4\u6d4b\u901a\u5e38\u5c55\u793a\u6700\u4f18\u6848\u4f8b\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u591a\u6b21\u91cd\u8bd5\u548c\u4eba\u5de5\u5ba1\u6838\u73af\u8282\uff0c\u96be\u4ee5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5728\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e0b\u7684\u6027\u80fd\u548c\u6210\u672c\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u66f4\u8d34\u8fd1\u5b9e\u9645\u5de5\u4f5c\u573a\u666f\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86HYPE-EDIT-1\u57fa\u51c6\uff0c\u5305\u62ec100\u4e2a\u57fa\u4e8e\u53c2\u8003\u56fe\u7247\u7684\u8425\u9500/\u8bbe\u8ba1\u7f16\u8f91\u4efb\u52a1\uff0c\u7528\u4e8c\u5143\u901a\u8fc7/\u672a\u901a\u8fc7\u8fdb\u884c\u5224\u522b\u3002\u6bcf\u4e2a\u4efb\u52a1\u751f\u621010\u4e2a\u72ec\u7acb\u8f93\u51fa\uff0c\u7528\u4e8e\u4f30\u7b97\u5355\u6b21\u901a\u8fc7\u7387\u300110\u6b21\u5185\u6709\u6210\u529f\u7684\u6982\u7387\u3001\u5728\u91cd\u8bd5\u4e0a\u9650\u4e0b\u5e73\u5747\u91cd\u8bd5\u6b21\u6570\uff0c\u4ee5\u53ca\u7ed3\u5408\u6a21\u578b\u8c03\u7528\u8d39\u7528\u548c\u4eba\u5de5\u5ba1\u6838\u65f6\u957f\u5f97\u5230\u7684\u5b9e\u9645\u6709\u6548\u7f16\u8f91\u6210\u672c\u3002", "result": "\u8bc4\u6d4b\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u5355\u6b21\u901a\u8fc7\u7387\u4e3a34%-83%\uff0c\u6bcf\u6b21\u6210\u529f\u7684\u5b9e\u9645\u6210\u672c\u57280.66\u81f31.42\u7f8e\u5143\u4e4b\u95f4\u3002\u6709\u4e9b\u6a21\u578b\u867d\u7136\u5355\u5f20\u56fe\u7247\u6536\u8d39\u4f4e\uff0c\u4f46\u56e0\u4e3a\u9700\u8981\u591a\u6b21\u91cd\u8bd5\u548c\u8f83\u591a\u4eba\u5de5\u5ba1\u6838\uff0c\u7efc\u5408\u540e\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "\u5355\u9760\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u6807\u4ef7\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u7f16\u8f91\u6210\u672c\uff0c\u5b9e\u9645\u90e8\u7f72\u9700\u5c06\u91cd\u8bd5\u548c\u4eba\u5de5\u5ba1\u6838\u7eb3\u5165\u8003\u91cf\u3002HYPE-EDIT-1\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u66f4\u5b9e\u9645\u7684\u6a21\u578b\u8bc4\u6d4b\u57fa\u51c6\u548c\u5de5\u5177\u3002"}}
{"id": "2602.00016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00016", "abs": "https://arxiv.org/abs/2602.00016", "authors": ["Jiongchi Yu", "Yuhan Ma", "Xiaoyu Zhang", "Junjie Wang", "Qiang Hu", "Chao Shen", "Xiaofei Xie"], "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", "comment": "28 pages", "summary": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PTCBENCH\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u4eba\u683c\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u67d0\u4e9b\u5916\u90e8\u60c5\u5883\u4f1a\u663e\u8457\u6539\u53d8LLM\u7684\u4eba\u683c\u7279\u8d28\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5177\u6709\u4eba\u683c\u7684\u60c5\u611f\u4ee3\u7406\u4e0eAI\u7cfb\u7edf\uff0c\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u4eba\u683c\u7279\u8d28\u52a8\u6001\u4e14\u4f9d\u8d56\u60c5\u5883\u7684\u5fc3\u7406\u5b66\u5171\u8bc6\u3002\u8fd9\u79cd\u5ffd\u89c6\u5f71\u54cd\u4e86\u7528\u6237\u5bf9AI\u771f\u5b9e\u53ef\u9760\u4eba\u683c\u7684\u4fe1\u4efb\u611f\u3002", "method": "\u4f5c\u8005\u63d0\u51faPTCBENCH\u57fa\u51c6\uff0c\u901a\u8fc7\u8bbe\u5b9a12\u79cd\u4e0d\u540c\u7684\u5916\u90e8\u6761\u4ef6\uff08\u5305\u62ec\u5730\u70b9\u548c\u4eba\u751f\u4e8b\u4ef6\u7b49\u60c5\u5883\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u5e76\u91cf\u5316LLM\u7684\u4eba\u683c\u4e00\u81f4\u6027\u3002\u8bc4\u4f30\u65b9\u6cd5\u91c7\u7528\u4e86NEO\u4e94\u56e0\u7d20\u4eba\u683c\u91cf\u8868\uff0c\u5e76\u6536\u96c6\u5206\u6790\u4e8639,240\u6761\u4eba\u683c\u6570\u636e\u8bb0\u5f55\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u5916\u90e8\u60c5\u5883\uff08\u5982\u2018\u5931\u4e1a\u2019\uff09\u80fd\u5f15\u53d1LLM\u663e\u8457\u7684\u4eba\u683c\u53d8\u5316\uff0c\u751a\u81f3\u5f71\u54cd\u5176\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PTCBENCH\u4e3a\u8bc4\u4ef7AI\u5728\u4eba\u683c\u7ef4\u5ea6\u4e0b\u7684\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u52a0\u5065\u58ee\u548c\u5fc3\u7406\u5b66\u4e00\u81f4\u7684AI\u7cfb\u7edf\uff0c\u5e76\u4e3aAI\u4eba\u683c\u8bc4\u6d4b\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.00566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00566", "abs": "https://arxiv.org/abs/2602.00566", "authors": ["Nan Song", "Junzhe Jiang", "Jingyu Li", "Xiatian Zhu", "Li Zhang"], "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning", "comment": "Accepted at NeurIPS 2025", "summary": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8fd0\u52a8\u4efb\u52a1\u6846\u67b6UniMotion\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u6a21\u62df\u3001\u9884\u6d4b\u4e0e\u89c4\u5212\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u5171\u4eab\u3001\u534f\u540c\u4f18\u5316\u4e0e\u6cdb\u5316\u63d0\u5347\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u8fd0\u52a8\u6a21\u62df\u3001\u9884\u6d4b\u548c\u89c4\u5212\u662f\u6838\u5fc3\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u91c7\u7528\u4e13\u95e8\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5404\u4efb\u52a1\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7cfb\u7edf\u89c4\u6a21\u6269\u5c55\u3002\u6b64\u5916\uff0c\u5206\u5272\u7684\u7b56\u7565\u8fd8\u5ffd\u7565\u4e86\u4efb\u52a1\u95f4\u672c\u53ef\u4e92\u5229\u7684\u4fe1\u606f\u5171\u4eab\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8edecoder-only Transformer\u7ed3\u6784\u7684UniMotion\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u4ea4\u4e92\u6a21\u5f0f\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u652f\u6301\u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\uff0c\u540c\u65f6\u53ef\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u4f18\u5316\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8054\u5408\u8bad\u7ec3\u6709\u52a9\u4e8e\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u548c\u591a\u4efb\u52a1\u96c6\u6210\uff0c\u901a\u8fc7\u8fdb\u4e00\u6b65\u5fae\u8c03\uff0cUniMotion\u5728\u591a\u9879\u8fd0\u52a8\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u65b0\u6700\u4f18\u6548\u679c\u3002", "conclusion": "UniMotion\u4f5c\u4e3a\u7edf\u4e00\u5316\u65b9\u6848\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u591a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3001\u6cdb\u5316\u80fd\u529b\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u8fd0\u52a8\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00107", "abs": "https://arxiv.org/abs/2602.00107", "authors": ["Yuan Gao", "Xinyu Guo", "Wenjing Xie", "Zifan Wang", "Hongwen Yu", "Gongyang Li", "Shugong Xu"], "title": "Efficient UAV trajectory prediction: A multi-modal deep diffusion framework", "comment": "in Chinese language", "summary": "To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6fc0\u5149\u96f7\u8fbe\uff08LiDAR\uff09\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4fe1\u606f\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u8f68\u8ff9\u9884\u6d4b\u6df1\u5ea6\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u7a7a\u7ecf\u6d4e\u4e0b\u7684\u975e\u6cd5\u65e0\u4eba\u673a\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u53d1\u5c55\uff0c\u5bf9\u975e\u6cd5\u65e0\u4eba\u673a\u7684\u7ba1\u7406\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u5355\u4e00\u4f20\u611f\u5668\u53d7\u9650\u4e8e\u73af\u5883\u6216\u4fe1\u606f\u7ef4\u5ea6\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u65e0\u4eba\u673a\u8f68\u8ff9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7ed3\u5408\u4e0d\u540c\u4f20\u611f\u5668\u7684\u4e92\u8865\u4fe1\u606f\u4ee5\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u878d\u5408\u6846\u67b6\uff0c\u5305\u62ec\u9488\u5bf9LiDAR\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u72ec\u7acb\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u4ee5\u53ca\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u4fe1\u606f\u878d\u5408\u3002\u4f7f\u7528\u7ed3\u6784\u76f8\u540c\u4f46\u72ec\u7acb\u7684\u7279\u5f81\u7f16\u7801\u5668\u63d0\u53d6\u6bcf\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u7ecf\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7279\u5f81\u4e92\u8865\u548c\u8bed\u4e49\u5bf9\u9f50\u3002\u91c7\u7528CVPR 2024 MMAUD\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e8640%\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u635f\u5931\u51fd\u6570\u548c\u540e\u5904\u7406\u7b56\u7565\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u573a\u666f\u4e0b\u975e\u6cd5\u65e0\u4eba\u673a\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00017", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00017", "abs": "https://arxiv.org/abs/2602.00017", "authors": ["Benyamin Tabarsi", "Wenbo Li", "Tahreem Yasir", "Aryan Santhosh Kumar", "Laura Widman", "Dongkuan Xu", "Tiffany Barnes"], "title": "SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations", "comment": null, "summary": "The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u5bf9\u8bdd\u751f\u6210\u6846\u67b6SafeTalkCoach\uff0c\u7528\u4e8e\u6a21\u62df\u6027\u5065\u5eb7\u9886\u57df\u4e2d\u7236\u6bcd\u4e0e\u5b50\u5973\u7684\u5bf9\u8bdd\uff0c\u5e76\u751f\u6210\u76f8\u5173\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301AI\u548c\u5065\u5eb7\u6c9f\u901a\u7814\u7a76\u3002", "motivation": "\u6709\u6548\u7684\u4eb2\u5b50\u6027\u5065\u5eb7\u6c9f\u901a\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u5b9e\u4e2d\u76f8\u5173\u5bf9\u8bdd\u6570\u636e\u56e0\u79c1\u5bc6\u548c\u654f\u611f\u6027\u96be\u4ee5\u6536\u96c6\u3002\u76ee\u524d\u5927\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5bf9\u8bdd\u751f\u6210\uff0c\u4f46\u5728\u771f\u5b9e\u6027\u3001\u591a\u6837\u6027\u548c\u6c9f\u901a\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u63d0\u51faSafeTalkCoach\uff0c\u4e00\u4e2a\u4ee5\u591a\u4ee3\u7406\u4e3a\u57fa\u7840\u5e76\u5f3a\u8c03\u591a\u6837\u6027\u7684\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4f17\u5305\u4e0e\u5408\u6210\u573a\u666f\u3001\u6210\u719f\u6027\u5065\u5eb7\u6307\u5357\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u4eba\u7269\u8bbe\u5b9a\u3001\u81ea\u9002\u5e94\u63a7\u5236\u6a21\u5757\u548c\u5206\u5c42\u591a\u6837\u5316\u673a\u5236\uff0c\u65e8\u5728\u66f4\u597d\u5730\u6a21\u62df\u771f\u5b9e\u4eb2\u5b50\u6027\u5065\u5eb7\u5bf9\u8bdd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cSafeTalkCoach\u80fd\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u5bf9\u8bdd\uff0c\u5e76\u517c\u987e\u6c9f\u901a\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "SafeTalkCoach\u6846\u67b6\u53ca\u5176\u6570\u636e\u96c6\u53ef\u4e3aAI\u5bf9\u8bdd\u548c\u5065\u5eb7\u6c9f\u901a\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2602.00575", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00575", "abs": "https://arxiv.org/abs/2602.00575", "authors": ["Chaoqun Cui", "Jing Huang", "Shijing Wang", "Liming Zheng", "Qingchao Kong", "Zhixiong Zeng"], "title": "Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction", "comment": "21 pages, 11 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically \"easy to verify but hard to solve\", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VAGEN\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5f3a\u5316\u5b66\u4e60GUI\u4efb\u52a1\u4e2d\u81ea\u9a8c\u8bc1\u80fd\u529b\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u4e14\u4e3b\u52a8\u5730\u8bc4\u4f30\u667a\u80fd\u4f53\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u3001\u96be\u4ee5\u5904\u7406\u5f00\u653e\u6027\u4efb\u52a1\uff0c\u57fa\u4e8eLLM\u5224\u5b9a\u7684\u65b9\u6cd5\u4ec5\u80fd\u88ab\u52a8\u89c2\u5bdf\u89c6\u89c9\u7ed3\u679c\uff0c\u65e0\u6cd5\u611f\u77e5\u7cfb\u7edf\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aAgentic Interactive Verification\u7684\u65b0\u8303\u5f0f\uff0c\u5e76\u5177\u4f53\u5b9e\u73b0\u4e3aVAGEN\u6846\u67b6\u3002VAGEN\u901a\u8fc7\u96c6\u6210\u5177\u5907\u4ea4\u4e92\u5de5\u5177\u7684\u9a8c\u8bc1\u667a\u80fd\u4f53\uff0c\u4f7f\u5176\u80fd\u591f\u4e3b\u52a8\u5236\u5b9a\u9a8c\u8bc1\u8ba1\u5212\uff0c\u4e3b\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u4ee5\u5bfb\u627e\u4efb\u52a1\u5b8c\u6210\u7684\u8bc1\u636e\uff0c\u4ece\u800c\u7ed5\u5f00\u4f20\u7edf\u65b9\u6cd5\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u4f9d\u8d56\u3002", "result": "\u5728OSWorld-Verified\u548cAndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVAGEN\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684LLM-as-a-Judge\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5f15\u5165\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u540e\u8fdb\u4e00\u6b65\u589e\u5f3a\u6027\u80fd\u3002", "conclusion": "VAGEN\u6846\u67b6\u5b9e\u73b0\u4e86GUI\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u66f4\u4e3a\u4e3b\u52a8\u3001\u7cbe\u786e\u7684\u81ea\u9a8c\u8bc1\u80fd\u529b\uff0c\u514b\u670d\u4e86\u89c6\u89c9\u9650\u5236\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u80fd\u529b\uff0c\u4e3a\u540e\u7eedGUI\u667a\u80fd\u4f53\u7684\u6301\u7eed\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2602.00108", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00108", "abs": "https://arxiv.org/abs/2602.00108", "authors": ["Ren\u00e9 Peinl", "Vincent Tischler", "Patrick Schr\u00f6der", "Christian Groth"], "title": "SITUATE -- Synthetic Object Counting Dataset for VLM training", "comment": "accepted at 21st International Conference on Computer Vision Theory and Applications", "summary": "We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SITUATE\uff0c\u4e13\u4e3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5177\u6709\u7a7a\u95f4\u7ea6\u675f\u7684\u8ba1\u6570\u4efb\u52a1\u800c\u8bbe\u8ba1\uff0c\u5728\u6a21\u578b\u6cdb\u5316\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u6570\u4efb\u52a1\u6570\u636e\u96c6\u8981\u4e48\u8fc7\u4e8e\u7b80\u5355\uff08\u5982VLMCountBench\uff09\uff0c\u8981\u4e48\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u906e\u6321\u548c\u7a7a\u95f4\u6784\u56fe\uff08\u5982TallyQA\uff09\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u6cdb\u5316\u6027\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86SITUATE\u6570\u636e\u96c6\uff0c\u80fd\u591f\u5bf9\u906e\u6321\u548c\u7a7a\u95f4\u7ec4\u6210\u56e0\u7d20\u8fdb\u884c\u63a7\u5236\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u8ba1\u6570\u6a21\u578b\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528SITUATE\u5bf9Qwen VL 2.5 7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u5bf9\u5916\u90e8\u5206\u5e03\u7684Pixmo count\u6d4b\u8bd5\u6570\u636e\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u800c\u53cd\u8fc7\u6765\u5219\u6ca1\u6709\u63d0\u5347\u3002\u4f5c\u8005\u8fd8\u5c06SITUATE\u4e0e\u5176\u4ed6\u4e3b\u6d41\u8ba1\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u53ca\u6765\u6e90\u4e8ePixmo count\u7684\u540c\u7b49\u89c4\u6a21\u4f18\u5316\u96c6\u4f5c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5728\u591a\u7ec4\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0cSITUATE\u5fae\u8c03\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65b0\u7684\u8ba1\u6570\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4f7f\u7528Pixmo count\u5fae\u8c03\u5219\u8fbe\u4e0d\u5230\u540c\u7b49\u6548\u679c\u3002", "conclusion": "SITUATE\u6570\u636e\u96c6\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5177\u6709\u7a7a\u95f4\u7ea6\u675f\u548c\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u8ba1\u6570\u80fd\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u66f4\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00029", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00029", "abs": "https://arxiv.org/abs/2602.00029", "authors": ["Yao Zhang", "Hongyin Zhu"], "title": "Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management", "comment": null, "summary": "Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u672c\u4f53\u6a21\u578b(LOM)\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6784\u5efa-\u5bf9\u9f50-\u63a8\u7406\u6846\u67b6\uff0c\u6709\u6548\u6574\u5408\u591a\u6e90\u5f02\u6784\u6570\u636e\u5e76\u63d0\u5347\u672c\u4f53\u63a8\u7406\u6027\u80fd\uff0c\u5728\u590d\u6742\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u548c\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u9762\u5bf9\u4f01\u4e1a\u7ea7\u77e5\u8bc6\u7ba1\u7406\u4e2d\u591a\u6e90\u5f02\u6784\u6570\u636e\u96be\u6574\u5408\u3001\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u80fd\u529b\u5f31\u3001\u590d\u6742\u95ee\u9898\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u7edf\u4e00\u7684\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u7684\u77e5\u8bc6\u878d\u5408\u4e0e\u63a8\u7406\u3002", "method": "\uff081\uff09\u6784\u5efa\u7ed3\u6784\u5c42\u6b21\u4e30\u5bcc\u7684\u4f01\u4e1a\u672c\u4f53\uff08\u6574\u5408\u7ed3\u6784\u5316\u6570\u636e\u5e93\u4e0e\u975e\u7ed3\u6784\u5316\u6587\u672c\uff09\uff1b\uff082\uff09\u63d0\u51fa\u7edf\u4e00\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u672c\u4f53\u6307\u4ee4\u5fae\u8c03\uff08\u63d0\u5347\u7ed3\u6784\u7406\u89e3\uff09\u3001\u6587\u672c-\u672c\u4f53\u951a\u5b9a\uff08\u589e\u5f3a\u8bed\u4e49\u7f16\u7801\uff09\u3001\u591a\u4efb\u52a1\u6307\u4ee4\u8bad\u7ec3\uff08\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\uff09\uff1b\uff083\uff09\u5f00\u53d1\u8986\u76d6\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u4e0e\u8bc4\u6d4b\u96c6\u3002", "result": "\u6240\u63d04B\u53c2\u6570\u7684LOM\u6a21\u578b\u5728\u591a\u79cd\u672c\u4f53\u63a8\u7406\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e8689.47%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u590d\u6742\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8eDeepSeek-V3.2\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u4e0e\u8bed\u4e49\u878d\u5408\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "LOM\u6a21\u578b\u5b9e\u73b0\u4e86\u591a\u6e90\u4f01\u4e1a\u77e5\u8bc6\u7684\u6df1\u5ea6\u878d\u5408\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u590d\u6742\u95ee\u9898\u56de\u7b54\u80fd\u529b\uff0c\u4e3a\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u548c\u667a\u80fd\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00675", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00675", "abs": "https://arxiv.org/abs/2602.00675", "authors": ["Valerio Belcamino", "Mariya Kilina", "Alessandro Carf\u00ec", "Valeria Seidita", "Fulvio Mastrogiovanni", "Antonio Chella"], "title": "Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction", "comment": null, "summary": "Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u589e\u5f3a\u4eba\u673a\u5bf9\u8bdd\u7684\u8ba4\u77e5\u578b\u673a\u5668\u4eba\u52a9\u7406\u67b6\u6784JANUS\uff0c\u80fd\u6709\u6548\u4fdd\u6301\u7528\u6237\u4e0a\u4e0b\u6587\u3001\u5904\u7406\u4e0d\u5b8c\u6574\u8bf7\u6c42\uff0c\u5e76\u57fa\u4e8e\u5916\u90e8\u8bc1\u636e\u7ed9\u51fa\u53ef\u67e5\u8bc1\u56de\u590d\u3002JANUS\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u5bf9\u8bdd\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u7684\u8bb0\u5fc6\u4e0e\u63a8\u7406\u673a\u5236\u63d0\u5347\u5065\u58ee\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002\u5176\u5728\u81b3\u98df\u8f85\u52a9\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u53c2\u8003\u4e00\u81f4\u6027\u4e0e\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u578b\u673a\u5668\u4eba\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u3001\u5904\u7406\u4e0d\u5b8c\u6574\u8bf7\u6c42\u3001\u53ef\u9a8c\u8bc1\u63a8\u7406\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u652f\u6301\u957f\u65f6\u3001\u5177\u5907\u8bc1\u636e\u9a8c\u8bc1\u548c\u5ba1\u8ba1\u6027\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u578b\u8ba4\u77e5\u67b6\u6784\u63d0\u5347\u8fd9\u65b9\u9762\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86JANUS\u8ba4\u77e5\u67b6\u6784\uff1a1\uff09\u5c06\u6574\u4f53\u884c\u4e3a\u5206\u4e3a\u591a\u4e2a\u5b50\u6a21\u5757\uff0c\u6db5\u76d6\u8303\u56f4\u68c0\u6d4b\u3001\u610f\u56fe\u8bc6\u522b\u3001\u8bb0\u5fc6\u3001\u5185\u8bed\u3001\u67e5\u8be2\u751f\u6210\u548c\u5916\u90e8\u5bf9\u8bdd\u7b49\u529f\u80fd\u30022\uff09\u91c7\u7528\u5e26\u7c7b\u578b\u63a5\u53e3\u7684\u5206\u89e3\u63a7\u5236\u5668\uff1b3\uff09\u8bbe\u8ba1\u4e13\u95e8\u7684\u8bb0\u5fc6\u4f53 agent\uff0c\u7ed3\u5408\u77ed\u671f\u3001\u6838\u5fc3\u548c\u957f\u671f\u4e09\u5c42\u5b58\u50a8\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u548c\u53d7\u63a7\u6574\u5408\u4fee\u6b63\u7b56\u7565\u7ba1\u7406\u4e0a\u4e0b\u6587\uff1b4\uff09\u5f15\u5165\u4eff\u751f\u5185\u8bed\u6d41\uff0c\u63d0\u524d\u9a8c\u8bc1\u53c2\u6570\u5b8c\u6574\u6027\u5e76\u89e6\u53d1\u6f84\u6e05\uff0c\u63d0\u5347\u51b3\u7b56\u53ef\u9760\u6027\uff1b5\uff09\u8f93\u51fa\u4e0e\u5916\u90e8\u77e5\u8bc6\u5de5\u5177\u7d27\u5bc6\u7ed3\u5408\uff0c\u901a\u8fc7\u8bc1\u636e\u5305\u786e\u4fdd\u673a\u5668\u58f0\u660e\u53ef\u4fe1\u3002", "result": "\u5728\u81b3\u98df\u8f85\u52a9\u9886\u57df\u7684\u77e5\u8bc6\u56fe\u8c31\u73af\u5883\u4e0b\uff0cJANUS\u5404\u6a21\u5757\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\uff0c\u7ed3\u679c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u5ef6\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff0c\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "JANUS\u67b6\u6784\u8868\u660e\u57fa\u4e8e\u5206\u89e3\u6a21\u5757\u4e0e\u8bc1\u636e\u9a71\u52a8\u63a8\u7406\u7684\u8ba4\u77e5\u673a\u5668\u4eba\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u957f\u65f6\u4ea4\u4e92\u80fd\u529b\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u4e3a\u5b9e\u73b0\u9ad8\u6c34\u5e73\u5bf9\u8bdd\u52a9\u624b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2602.00109", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00109", "abs": "https://arxiv.org/abs/2602.00109", "authors": ["John J. Howard", "Richard O. Plesh", "Yevgeniy B. Sirotin", "Jerry L. Tipton", "Arun R. Vemury"], "title": "Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios", "comment": "Accepted to the IEEE/CVF WACV 2026 Workshop on Generative, Adversarial and Presentation Attacks in Biometrics (GAPBio). 8 pages, 6 figures, 4 tables", "summary": "Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4e\u5149\u73af\u5883\u548c\u81ea\u52a8\u56fe\u50cf\u91c7\u96c6\u5bf9\u8fdc\u7a0b\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\u5c55\u793a\u653b\u51fb\u68c0\u6d4b\uff08PAD\uff09\u529f\u80fd\u7684\u5f71\u54cd\uff0c\u663e\u793a\u5927\u591a\u6570\u5546\u7528PAD\u7cfb\u7edf\u5728\u8fd9\u4e9b\u60c5\u5f62\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u8fdc\u7a0b\u8eab\u4efd\u9a8c\u8bc1\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u63d0\u5347\u5176\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524dPAD\u7cfb\u7edf\u5728\u4e0d\u540c\u73af\u5883\u548c\u6d41\u7a0b\u4e0b\u7684\u7a33\u5065\u6027\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u4f4e\u5149\u548c\u81ea\u52a8\u91c7\u96c6\u7b49\u73b0\u5b9e\u573a\u666f\u3002\u4e86\u89e3\u8fd9\u4e9b\u56e0\u7d20\u5bf9PAD\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u5305\u542b\u4f4e\u5149\u548c\u81ea\u52a8\u91c7\u96c6\u4e24\u7c7b\u573a\u666f\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5bf9\u591a\u6b3e\u5546\u4e1aPAD\u7cfb\u7edf\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u548c\u5b9e\u9645\u7ed3\u679c\u8bc4\u4f30\u5b83\u4eec\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u8bc6\u522b\u9519\u8bef\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u5927\u591a\u6570PAD\u7cfb\u7edf\u7684\u9519\u8bef\u7387\u589e\u52a0\u4e86\u7ea64\u500d\uff0c\u81ea\u52a8\u91c7\u96c6\u6d41\u7a0b\u4e0b\u5219\u589e\u52a0\u4e86\u7ea6\u4e00\u500d\u3002\u4ec5\u6709\u4e00\u4e2a\u7cfb\u7edf\u5728\u6240\u6709\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5206\u7c7b\u9519\u8bef\u7387\u59cb\u7ec8\u4f4e\u4e8e3%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cPAD\u7cfb\u7edf\u5728\u4e0d\u540c\u5b9e\u9645\u73af\u5883\u4e0b\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u70c8\u5efa\u8bae\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u5145\u5206\u6d4b\u8bd5\uff0c\u4ee5\u786e\u4fdd\u7cfb\u7edf\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.00150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00150", "abs": "https://arxiv.org/abs/2602.00150", "authors": ["Xinyun Wang", "Min Zhang", "Sen Cui", "Zhikang Chen", "Bo Jiang", "Kun Kuang", "Mingbao Lin"], "title": "Reversible Diffusion Decoding for Diffusion Language Models", "comment": null, "summary": "Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9006\u6269\u6563\u89e3\u7801\uff08RDD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u65f6\u7684\u9c81\u68d2\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u91c7\u7528\u5206\u5757\u5e76\u884c\u751f\u6210\u4ee4\u724c\u65f6\uff0c\u4e00\u65e6\u505a\u51fa\u4e0d\u53ef\u9006\u51b3\u5b9a\uff0c\u5982\u679c\u5904\u4e8e\u6b21\u4f18\u4e0a\u4e0b\u6587\uff0c\u4f1a\u65e0\u6cd5\u7ee7\u7eed\u4f18\u5316\u751f\u6210\uff08\u79f0\u4e3a\u505c\u6ede\uff09\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faRDD\u89e3\u7801\u6846\u67b6\uff1a\u68c0\u6d4b\u505c\u6ede\u72b6\u6001\uff08\u5373\u53cd\u5411\u8fc7\u7a0b\u65e0\u6cd5\u8fdb\u884c\uff09\uff0c\u5141\u8bb8\u5728\u65e0\u9700\u91cd\u590d\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u56de\u6eaf\u5230\u524d\u9762\u7684\u751f\u6210\u5757\uff1b\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u518d\u63a9\u7801\uff0c\u5bf9\u4e0d\u786e\u5b9a\u7684\u4ee4\u724c\u8fdb\u884c\u6709\u9009\u62e9\u7684\u91cd\u521d\u59cb\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u53ef\u9760\u4e0a\u4e0b\u6587\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u53ef\u9006\u673a\u5236\u4ee5\u7ea0\u6b63\u65e9\u671f\u7684\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRDD\u65b9\u6cd5\u5728\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u80fd\u591f\u4ee5\u6781\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u63d0\u5347\u751f\u6210\u7684\u9c81\u68d2\u6027\u548c\u8d28\u91cf\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RDD\u901a\u8fc7\u5f15\u5165\u53ef\u9006\u6027\u4e0e\u52a8\u6001\u7ea0\u9519\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6269\u6563\u5e76\u884c\u751f\u6210\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u505c\u6ede\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u548c\u9c81\u68d2\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.00678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00678", "abs": "https://arxiv.org/abs/2602.00678", "authors": ["Tianyang Wu", "Hanwei Guo", "Yuhang Wang", "Junshu Yang", "Xinyang Sui", "Jiayi Xie", "Xingyu Chen", "Zeyang Liu", "Xuguang Lan"], "title": "Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion", "comment": null, "summary": "Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoE\uff09\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e0b\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u4ec5\u51ed\u672c\u4f53\u611f\u77e5\u5373\u53ef\u5b9e\u73b0\u9c81\u68d2\u884c\u8d70\uff0c\u5e76\u901a\u8fc7RoboGauge\u6307\u6807\u91cf\u5316\u5176\u8fc1\u79fb\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u7269\u7406\u6d4b\u8bd5\u9700\u6c42\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5df2\u5728\u56db\u8db3\u673a\u5668\u4eba\u884c\u52a8\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ee5\u53ca\u5728\u590d\u6742\u73af\u5883\u4e0b\u5956\u52b1\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u5f88\u591a\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5730\u4ece\u4eff\u771f\u8fc1\u79fb\u5230\u73b0\u5b9e\uff0c\u800c\u4e14\u5b9e\u4f53\u673a\u5668\u4eba\u6d4b\u8bd5\u5b58\u5728\u9ad8\u98ce\u9669\u4e0e\u4f4e\u6548\u7387\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u6df7\u5408\u4e13\u5bb6\uff08Mixture-of-Experts, MoE\uff09\u7b56\u7565\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u7ed3\u5408\u82e5\u5e72\u4e13\u957f\u4e13\u5bb6\uff0c\u5bf9\u4e0d\u540c\u5730\u5f62\u4e0e\u52a8\u4f5c\u6307\u4ee4\u8fdb\u884c\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86RoboGauge\u8bc4\u4f30\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u5730\u5f62\u3001\u96be\u5ea6\u548c\u968f\u673a\u5316\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u7eaf\u4eff\u771f\u65b9\u6cd5\u91cf\u5316MoE\u7b56\u7565\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4ece\u800c\u65e0\u9700\u5927\u91cf\u5b9e\u4f53\u6d4b\u8bd5\u5c31\u80fd\u9009\u62e9\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728Unitree Go2\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u5168\u65b0\u4e14\u590d\u6742\u7684\u5730\u5f62\uff08\u5982\u96ea\u3001\u6c99\u3001\u697c\u68af\u3001\u659c\u5761\u3001\u9ad8\u8fbe30\u5398\u7c73\u7684\u969c\u788d\uff09\u4e0a\u5b9e\u73b0\u7a33\u5065\u8fd0\u52a8\u3002\u5728\u9ad8\u901f\u5ea6\u6d4b\u8bd5\u4e0b\uff0c\u673a\u5668\u4eba\u8fbe\u52304 m/s\uff0c\u5e76\u81ea\u4e3b\u51fa\u73b0\u4e86\u5177\u6709\u66f4\u9ad8\u7a33\u5b9a\u6027\u7684\u7a84\u6b65\u5bbd\u6b65\u6001\u3002", "conclusion": "\u672c\u6587\u6240\u63d0\u51fa\u7684MoE-RoboGauge\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u4ec5\u51ed\u672c\u4f53\u611f\u77e5\u7684\u591a\u5730\u5f62\u9c81\u68d2\u884c\u8d70\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u53ef\u9760\u7684\u8fc1\u79fb\u6027\u8bc4\u4f30\u624b\u6bb5\u663e\u8457\u964d\u4f4e\u4e86\u7269\u7406\u5b9e\u9a8c\u7684\u4f9d\u8d56\uff0c\u6709\u529b\u63a8\u52a8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u4e0e\u5b89\u5168\u6027\u3002"}}
{"id": "2602.00110", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00110", "abs": "https://arxiv.org/abs/2602.00110", "authors": ["Yu Li", "Guilherme N. DeSouza", "Praveen Rao", "Chi-Ren Shyu"], "title": "Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5730\u7406\u7a7a\u95f4\u8f85\u52a9\u4fe1\u606f\u7684\u65b0\u578b\u89c6\u89c9Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u548c\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u9065\u611f\u56fe\u50cf\u5904\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u4e0e\u6587\u672c\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5730\u7406\u7a7a\u95f4\u5c42\u7ea7\u7ed3\u6784\u4fe1\u606f\u7684\u6709\u6548\u8868\u5f81\u548c\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u4e0e\u63a8\u65ad\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u7ed3\u5408\u591a\u6837\u5316\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u63d0\u5347\u9065\u611f\u573a\u666f\u4e0b\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u673a\u5236\uff0c\u5c06\u591a\u6e90\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u8f6c\u6362\u4e3a\u7a7a\u95f4\u5bf9\u9f50\u7684\u5d4c\u5165\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u6839\u636e\u4e0e\u8f85\u52a9\u6570\u636e\u7684\u76f8\u5173\u6027\u4e3a\u4e0d\u540c\u533a\u57df\u5206\u914d\u5173\u6ce8\u6743\u91cd\u3002\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u627f\u62c5\u4e0d\u540c\u4efb\u52a1\uff0c\u4ee5\u6355\u6349\u5f15\u5bfc\u4fe1\u606f\u7684\u4e92\u8865\u65b9\u9762\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u591a\u6a21\u6001\u878d\u5408\u53ca\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u75be\u75c5\u6d41\u884c\u5ea6\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\uff0c\u4f18\u4e8e\u4e3b\u6d41\u9884\u8bad\u7ec3\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u663e\u793a\u4e86\u5176\u591a\u6a21\u6001\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u6837\u5316\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u878d\u5165\u89c6\u89c9Transformer\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff0c\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u5904\u7406\u6548\u679c\uff0c\u62d3\u5bbd\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u5730\u7406\u7a7a\u95f4\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00238", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00238", "abs": "https://arxiv.org/abs/2602.00238", "authors": ["Tianyi Hu", "Niket Tandon", "Akhil Arora"], "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking", "comment": null, "summary": "Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIVERGE\u6846\u67b6\uff0c\u63d0\u5347RAG\u7cfb\u7edf\u5728\u591a\u4e2a\u5408\u7406\u7b54\u6848\u573a\u666f\u4e0b\u7684\u591a\u6837\u6027\uff0c\u517c\u987e\u591a\u6837\u6027\u4e0e\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u901a\u5e38\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6b63\u786e\u7b54\u6848\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u591a\u4e2a\u7b54\u6848\u7684\u573a\u666f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5185\u5bb9\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63d2\u62d4\u7684agentic RAG\u6846\u67b6DIVERGE\uff0c\u5f15\u5165\u53cd\u601d\u5f15\u5bfc\u7684\u751f\u6210\u548c\u8bb0\u5fc6\u589e\u5f3a\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u4ece\u800c\u9f13\u52b1\u591a\u6837\u5316\u89c2\u70b9\u7684\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u3002\u8fd8\u8bbe\u8ba1\u4e86\u65b0\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u7528\u5b83\u4eec\u5bf9\u7cfb\u7edf\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDIVERGE\u5728Infinity-Chat\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u7684\u6743\u8861\u4e0a\u8d85\u8d8a\u4e86\u5df2\u6709\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u7ebf\uff0c\u5e76\u4e14\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u5224\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u5f53\u524dLLM\u9a71\u52a8\u7684RAG\u7cfb\u7edf\u5728\u5f00\u653e\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e0a\u56fa\u6709\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u4f18\u5316\u5176\u8868\u73b0\u3002"}}
{"id": "2602.00686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00686", "abs": "https://arxiv.org/abs/2602.00686", "authors": ["Yujie Wei", "Jiahan Fan", "Jiyu Guo", "Ruichen Zhen", "Rui Shao", "Xiu Su", "Zeke Xie", "Shuo Yang"], "title": "Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u3001\u4efb\u52a1\u611f\u77e5\u7684\u7b56\u7565\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u4e0e\u8868\u73b0\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u63a8\u7406\u5f00\u9500\u5927\uff0c\u4e25\u91cd\u9650\u5236\u5b9e\u9645\u5e94\u7528\u3002\u63d0\u5347\u63a8\u7406\u6548\u7387\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u591a\u4e3a\u9759\u6001\u3001\u65e0\u4efb\u52a1\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u573a\u666f\u3002", "method": "\u4f5c\u8005\u5c06\u63a8\u7406\u52a0\u901f\u5efa\u6a21\u4e3a\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e24\u6a21\u5757\uff1a\u7f13\u5b58token\u9009\u62e9\u5668\uff08\u51b3\u5b9a\u54ea\u4e9btoken\u590d\u7528\uff09\u548c\u7f13\u5b58\u6bd4\u4f8b\u9884\u6d4b\u5668\uff08\u51b3\u5b9a\u590d\u7528\u6bd4\u4f8b\uff09\u3002\u8fd9\u4e24\u6a21\u5757\u901a\u8fc7\u53ef\u5fae\u677e\u5f1b\u6280\u672f\uff0c\u53ef\u7aef\u5230\u7aef\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5728LIBERO\u4e0eSIMPLER\u57fa\u51c6\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e861.76\u500d\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u5728LIBERO\u7684\u6210\u529f\u7387\u63d0\u53471.9\u4e2a\u767e\u5206\u70b9\uff0875.0%\u219276.9%\uff09\uff0c\u771f\u5b9e\u4efb\u52a1\u63d0\u53475\u4e2a\u767e\u5206\u70b9\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8ba1\u7b97\u5206\u914d\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u4e3a\u6253\u9020\u9ad8\u6548\u53c8\u5f3a\u5927\u7684VLA\u6a21\u578b\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.00111", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00111", "abs": "https://arxiv.org/abs/2602.00111", "authors": ["Haiyu Yang", "Heidi Lesscher", "Enhong Liu", "Miel Hostens"], "title": "From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves", "comment": null, "summary": "Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u8377\u517014\u5bb6\u5546\u4e1a\u5316\u4e73\u725b\u573a\u4e2d\uff0c\u5976\u725b\u728a\u5355\u53ea\u53ef\u7528\u7a7a\u95f4\u4e0e\u5176\u73a9\u800d\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u51fa\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u5b9e\u73b0\u5927\u89c4\u6a21\u6301\u7eed\u76d1\u6d4b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5355\u728a8-10\u5e73\u65b9\u7c73\u4e3a\u73a9\u800d\u884c\u4e3a\u6700\u591a\u7684\u7a7a\u95f4\uff0c\u8fc7\u5c0f\u6216\u8fc7\u5927\u7a7a\u95f4\u90fd\u6709\u6240\u964d\u4f4e\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5bf9\u4e8e\u6d3b\u8dc3\u73a9\u800d\u7684\u68c0\u6d4b\u8868\u73b0\u9ad8\u5ea6\u51c6\u786e\u3002", "motivation": "\u73a9\u800d\u884c\u4e3a\u662f\u5976\u725b\u728a\u798f\u5229\u72b6\u51b5\u7684\u91cd\u8981\u6b63\u9762\u6307\u6807\uff0c\u4f46\u5728\u5546\u4e1a\u5316\u73af\u5883\u4e0b\uff0c\u4e0d\u540c\uff08\u7279\u522b\u662f\u4e2d\u9ad8\uff09\u7a7a\u95f4\u989d\u5ea6\u5bf9\u8fd9\u79cd\u884c\u4e3a\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u51c6\u786e\u76d1\u6d4b\u65b9\u6cd5\u4ee5\u52a9\u4e8e\u5927\u89c4\u6a21\u3001\u5b9e\u65f6\u798f\u5229\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u9009\u53d614\u5bb6\u5546\u4e1a\u5316\u4e73\u725b\u573a\u300160\u7ec4\u5976\u725b\u728a\uff0c\u7a7a\u95f4\u8303\u56f42.66-17.98\u5e73\u65b9\u7c73/\u53ea\uff0c\u901a\u8fc7\u89c6\u9891\u53ca\u8be6\u7ec6\u884c\u4e3a\u8f9e\u5178\uff08ethogram\uff09\u4eba\u5de5\u89c2\u6d4b\uff0c\u5e76\u5e94\u7528\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u5206\u6790\u5f71\u54cd\u56e0\u7d20\u3002\u540c\u65f6\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u81ea\u52a8\u5206\u6790\u7ba1\u9053\u7528\u4e8e\u884c\u4e3a\u8bc6\u522b\u3002", "result": "\u53d1\u73b0\u73a9\u800d\u884c\u4e3a\u4e0e\u7a7a\u95f4\u989d\u5ea6\u5448\u975e\u7ebf\u6027\u5173\u7cfb\uff0c8-10\u5e73\u65b9\u7c73/\u53ea\u7a7a\u95f4\u65f6\u5976\u725b\u728a\u73a9\u800d\u884c\u4e3a\u6700\u9891\u7e41\uff081.6%\u89c2\u6d4b\u671f\uff09\uff0c\u592a\u5c0f\u6216\u8fc7\u5927\u7684\u7a7a\u95f4\u5219\u8f83\u5c11\uff08<0.6%\uff09\u3002\u63a7\u5236\u5e74\u9f84\u3001\u5065\u5eb7\u3001\u7fa4\u4f53\u89c4\u6a21\u540e\uff0c\u7a7a\u95f4\u5f71\u54cd\u4ecd\u663e\u8457\u3002\u81ea\u52a8\u5316\u7cfb\u7edf\u5bf9\u4e8e\u6d3b\u8dc3\u73a9\u800d\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\u8fbe97.6%\uff0c\u53ec\u56de\u738799.4%\u3002", "conclusion": "\u5efa\u8bae8-10\u5e73\u65b9\u7c73/\u53ea\u4f5c\u4e3a\u5976\u725b\u728a\u820d\u5408\u7406\u7a7a\u95f4\u76ee\u6807\uff0c\u8be5\u6807\u51c6\u5728\u52a8\u7269\u798f\u5229\u4e0e\u7ecf\u6d4e\u53ef\u884c\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u540c\u65f6\uff0c\u81ea\u52a8\u5316\u884c\u4e3a\u76d1\u63a7\u6280\u672f\u5df2\u5177\u5907\u5927\u89c4\u6a21\u6301\u7eed\u798f\u5229\u8bc4\u4f30\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u884c\u4e1a\u63a8\u5e7f\u5e94\u7528\u3002"}}
{"id": "2602.00279", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00279", "abs": "https://arxiv.org/abs/2602.00279", "authors": ["Philip M\u00fcller", "Nicholas Popovi\u010d", "Michael F\u00e4rber", "Peter Steinbach"], "title": "Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering", "comment": "Under Review", "summary": "Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u9762\u5411\u9700\u8981\u63a8\u7406\u7684\u79d1\u5b66\u95ee\u7b54\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\u7684\u6821\u51c6\u6548\u679c\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u82e5\u5e72\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u95ee\u7b54\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u751f\u6210\u7b54\u6848\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u5f81\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4ef7\u624b\u6bb5\uff0c\u5df2\u6709\u65b9\u6cd5\u5728\u79d1\u5b66\u95ee\u7b54\u8fd9\u4e00\u5bf9\u4e8b\u5b9e\u68c0\u7d22\u548c\u63a8\u7406\u9ad8\u5ea6\u4f9d\u8d56\u7684\u9886\u57df\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u4e2a\u7cfb\u7edf\u6027\u3001\u6807\u51c6\u5316\u7684UQ\u8bc4\u4ef7\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u6db5\u76d67\u4e2a\u79d1\u5b66\u95ee\u7b54\u6570\u636e\u96c6\uff08\u591a\u9879\u9009\u62e9\u53ca\u7b97\u672f\u7c7b\u95ee\u9898\uff09\uff0c\u5206\u6790\u591a\u8fbe20\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u5927\u6a21\u578b\uff08\u57fa\u7840\u3001\u6307\u4ee4\u5fae\u8c03\u3001\u63a8\u7406\u5fae\u8c03\uff09\u3002\u8bc4\u4f30\u8986\u76d6\u4e86\u4ee3\u8868\u4e3b\u6d41\u65b9\u6cd5\u7684685,000\u6761\u957f\u56de\u7b54\uff0c\u5206\u522b\u5728token\u7ea7\u548c\u5e8f\u5217\u7ea7\u63a2\u8ba8\u6821\u51c6\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6307\u4ee4\u5fae\u8c03\u4f1a\u9020\u6210\u6982\u7387\u8d28\u91cf\u6781\u7aef\u5316\uff0c\u5bfc\u81f4token\u7ea7\u7684\u7f6e\u4fe1\u5ea6\u5931\u53bb\u6709\u6548\u6027\u3002\u63a8\u7406\u5fae\u8c03\u6a21\u578b\u4e5f\u6709\u7c7b\u4f3c\u73b0\u8c61\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u6709\u7f13\u89e3\u4f5c\u7528\u3002\u5e8f\u5217\u7ea7\u4e0a\uff0c\u6587\u672c\u5316\u65b9\u6cd5\uff08\u5982\u8f93\u51fa\u4fe1\u5fc3\u8868\u8fbe\u8bed\uff09\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4e14\u4e0e\u6b63\u786e\u6027\u4f4e\u76f8\u5173\uff1b\u800c\u7b54\u6848\u4e00\u81f4\u6027\uff08\u7b54\u6848\u5728\u591a\u91c7\u6837\u95f4\u91cd\u590d\u51fa\u73b0\u7684\u9891\u7387\uff09\u6821\u51c6\u6700\u597d\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41UQ\u65b9\u6cd5\u548c\u8bc4\u6d4b\u5b9e\u8df5\u5728\u79d1\u5b66\u95ee\u7b54\u9886\u57df\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u5355\u72ec\u4f9d\u8d56ECE\u5ea6\u91cf\u5177\u6709\u8bef\u5bfc\u6027\u3002\u4e3a\u63a8\u52a8LLMs\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u5e94\u7528\uff0c\u9700\u53d1\u5c55\u66f4\u5065\u5168\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8bc4\u4ef7\u4f53\u7cfb\u3002"}}
{"id": "2602.00708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00708", "abs": "https://arxiv.org/abs/2602.00708", "authors": ["Weiqi Gai", "Yuman Gao", "Yuan Zhou", "Yufan Xie", "Zhiyang Liu", "Yuze Wu", "Xin Zhou", "Fei Gao", "Zhijun Meng"], "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation", "comment": null, "summary": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.", "AI": {"tldr": "\u63d0\u51faUSS-Nav\u6846\u67b6\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\uff0c\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u65f6\uff0c\u65e0\u4eba\u673a\u9762\u4e34\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u6709\u9650\u7b97\u529b\u7684\u77db\u76fe\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faIncremental Spatial Connectivity Graph\u7ed3\u5408\u591a\u9762\u4f53\u6269\u5c55\u6355\u6349\u5168\u5c40\u51e0\u4f55\u7ed3\u6784\uff0c\u901a\u8fc7\u56fe\u805a\u7c7b\u52a8\u6001\u5206\u533a\u4e3a\u8bed\u4e49\u533a\u57df\uff0c\u5e76\u5c06\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u8bed\u4e49\u951a\u5b9a\u5728\u8be5\u7ed3\u6784\u4e0a\u3002\u518d\u5229\u7528\u5206\u5c42\u573a\u666f\u56fe\u7ed3\u6784\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u8d1f\u8d23\u7c97\u7c92\u5ea6\u76ee\u6807\u533a\u57df\u5224\u522b\uff0c\u5c40\u90e8\u89c4\u5212\u5668\u6839\u636e\u4fe1\u606f\u589e\u76ca\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6709\u9650\u7b97\u529b\u5e73\u53f0\u4e0b\uff0c\u8ba1\u7b97\u6548\u7387\u4e0e\u5b9e\u65f6\u66f4\u65b0\u9891\u7387\u4f18\u4e8e\u73b0\u6709\u6700\u65b0\u65b9\u6cd5\uff0815 Hz\uff09\uff0c\u5e76\u5728Success weighted by Path Length (SPL)\u7b49\u6307\u6807\u4e0a\u53d6\u5f97\u660e\u663e\u63d0\u5347\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u5b50\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "USS-Nav\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e0b\u9ad8\u6548\u3001\u8bed\u4e49\u7406\u89e3\u9a71\u52a8\u7684\u76ee\u6807\u5bfc\u822a\u96be\u9898\uff0c\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7814\u7a76\u53d1\u5c55\u3002\u6e90\u7801\u5c06\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.00113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00113", "abs": "https://arxiv.org/abs/2602.00113", "authors": ["S. Kalaycioglu", "C. Hong", "K. Zhai", "H. Xie", "J. N. Wong"], "title": "AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment", "comment": "11 pages and 5 figures", "summary": "Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u591a\u89c6\u89d2\u6444\u5f71\u6d4b\u91cf\u3001\u4e09\u7ef4\u91cd\u5efa\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u7684AI\u70e7\u4f24\u8bc4\u4f30\u4e0e\u7ba1\u7406\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u5ba2\u89c2\u3001\u53ef\u91cf\u5316\u548c\u53ef\u8ffd\u8e2a\u7684\u70e7\u4f24\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u8089\u773c\u68c0\u67e5\u548c\u4e8c\u7ef4\u6444\u5f71\u5728\u70e7\u4f24\u8bc4\u4f30\u4e2d\u4e3b\u89c2\u6027\u5f3a\u3001\u96be\u4ee5\u5ba2\u89c2\u91cf\u5316\uff0c\u4e14\u4e0d\u4fbf\u4e8e\u75c5\u60c5\u968f\u65f6\u95f4\u5bf9\u6bd4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u51c6\u786e\u6cbb\u7597\u53ca\u6cd5\u5f8b\u6587\u6863\u9700\u6c42\u3002", "method": "\u5e73\u53f0\u5c06\u591a\u89d2\u5ea6\u666e\u901a\u76f8\u673a\u56fe\u7247\u7ecf\u4e09\u7ef4\u91cd\u5efa\u7b97\u6cd5\u6062\u590d\u60a3\u8005\u7279\u5f02\u6027\u7684\u70e7\u4f24\u8868\u9762\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u7b97\u6cd5\u8bc6\u522b\u70e7\u4f24\u533a\u57df\uff0c\u8ba1\u7b97\u9762\u79ef\u3001TBSA\u3001\u4f53\u79ef\u7b49\u5ba2\u89c2\u6307\u6807\u3002\u8fde\u7eed\u8bc4\u4f30\u7ed3\u679c\u53ef\u7a7a\u95f4\u914d\u51c6\uff0c\u5b9e\u73b0\u6108\u5408\u8fc7\u7a0b\u5b9a\u91cf\u8ddf\u8e2a\uff0c\u5e76\u8f85\u4ee5\u7528\u6237\u5f15\u5bfc\u3001\u5206\u6790\u3001\u63a8\u8350\u548c\u62a5\u544a\u81ea\u52a8\u751f\u6210\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u4e09\u7ef4\u91cd\u5efa\u7a33\u5b9a\uff0c\u8bc4\u4f30\u6307\u6807\u4e00\u81f4\uff0c\u7eb5\u5411\u8d8b\u52bf\u7b26\u5408\u4e34\u5e8a\u9884\u671f\uff0c\u8868\u660e\u65b9\u6cd5\u5177\u5907\u4e34\u5e8a\u53ef\u884c\u6027\u4e0e\u63a8\u5e7f\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u6025\u6027\u4e0e\u95e8\u8bca\u70e7\u4f24\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65e0\u521b\u3001\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u51e0\u4f55\u7684\u5ba2\u89c2\u8f85\u52a9\u51b3\u7b56\u5de5\u5177\uff0c\u6709\u671b\u63d0\u5347\u70e7\u4f24\u8bca\u7597\u6807\u51c6\u5316\u548c\u667a\u80fd\u5316\u6c34\u5e73\u3002"}}
{"id": "2602.00300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00300", "abs": "https://arxiv.org/abs/2602.00300", "authors": ["Xilin Gong", "Shu Yang", "Zehua Cao", "Lynne Billard", "Di Wang"], "title": "Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute \"purple\" for \"broccoli\", LLMs still generate \"green\" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\\% relative performance improvement.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7528Patchscopes\u89e3\u7801\u9690\u85cf\u8868\u793a\u5e76\u751f\u6210\u89e3\u91ca\u65f6\uff0c\u5e38\u5e38\u4f18\u5148\u8f93\u51fa\u56fa\u6709\u8bed\u8a00\u504f\u89c1\uff0c\u5bfc\u81f4\u5bf9\u771f\u5b9e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u89e3\u91ca\u4e0d\u5fe0\u5b9e\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBALOR\u7684\u5bf9\u6297\u504f\u89c1\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5c3d\u7ba1LLM\u901a\u8fc7Patchscopes\u5c55\u73b0\u4e86\u89e3\u91ca\u9690\u85cf\u8868\u793a\u7684\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u91ca\u65f6\u5bb9\u6613\u88ab\u8bad\u7ec3\u65f6\u7684\u8bed\u8a00\u60ef\u6027\u548c\u504f\u89c1\u4e3b\u5bfc\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u8f93\u5165\u7684\u53d8\u5316\u3002\u8fd9\u79cd\u201c\u4e0d\u5fe0\u5b9e\u201d\u4f1a\u6781\u5927\u5f71\u54cd\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u89e3\u91ca\u4e0e\u4fe1\u4efb\u5ea6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u5e76\u63a2\u7d22\u7ea0\u6b63\u624b\u6bb5\u3002", "method": "1. \u5148\u6784\u5efa\u4e86\u5e26\u6709\u523b\u610f\u504f\u89c1\u6d4b\u8bd5\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u7528\u4ee5\u68c0\u6d4bPatchscopes\u7684\u5fe0\u5b9e\u5ea6\u3002\n2. \u53d1\u73b0\u88ab\u8bed\u8a00\u4e60\u60ef\u4e3b\u5bfc\u65f6\uff0cPatchscopes\u5bf9\u504f\u7f6e\u4fe1\u606f\u7684\u53cd\u5e94\u660e\u663e\u964d\u4f4e\u3002\n3. \u63d0\u51faBias Alignment through Logit Recalibration\uff08BALOR\uff09\u65b9\u6cd5\uff1a\u5148\u5206\u522b\u83b7\u53d6\u6807\u51c6\u4e0e\u5e26\u5e72\u9884\uff08patched\uff09\u60c5\u51b5\u4e0b\u7684LLM\u8f93\u51falogit\uff0c\u901a\u8fc7\u4e24\u8005\u5bf9\u6bd4\u5e76\u91cd\u8c03\uff0c\u4ece\u800c\u6291\u5236\u6a21\u578b\u672c\u8eab\u7684\u504f\u89c1\uff0c\u589e\u5f3a\u5bf9\u771f\u5b9e\u4e0a\u4e0b\u6587\u7684\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPatchscopes\u5728\u504f\u89c1\u6d4b\u8bd5\u4e2d\u7684\u5e73\u5747\u5fe0\u5b9e\u5ea6\u4e0b\u964d18.84%\uff1bBALOR\u65b9\u6cd5\u5728\u591a\u79cd\u4e3b\u6d41LLM\u4e0a\u7684\u8868\u73b0\uff0c\u8f83\u73b0\u6709\u65b9\u6cd5\u53ef\u5b9e\u73b0\u6700\u9ad833%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLM\u5728\u89e3\u91ca\u5185\u90e8\u72b6\u6001\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5411\u539f\u6709\u8bed\u8a00\u5e38\u8bc6\u800c\u975e\u7f16\u7801\u4e0a\u4e0b\u6587\uff0c\u4f20\u7edfPatchscopes\u56e0\u6b64\u5b58\u5728\u663e\u8457\u5fe0\u5b9e\u6027\u95ee\u9898\u3002BALOR\u53ef\u6709\u6548\u6291\u5236\u8fd9\u4e00\u504f\u5dee\uff0c\u63d0\u9ad8\u89e3\u91ca\u7684\u5fe0\u5b9e\u4e0e\u53ef\u7528\u6027\u3002"}}
{"id": "2602.00743", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00743", "abs": "https://arxiv.org/abs/2602.00743", "authors": ["Xu Pan", "Zhenglin Wan", "Xingrui Yu", "Xianwei Zheng", "Youkai Ke", "Ming Sun", "Rui Wang", "Ziwei Wang", "Ivor Tsang"], "title": "SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning", "comment": "Version 1", "summary": "Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \\textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \\textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86SA-VLA\uff0c\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e0b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5904\u7406\u7a7a\u95f4\u5206\u5e03\u53d8\u5316\u65f6\uff0c\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u540e\u5e38\u51fa\u73b0\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\uff08spatial inductive bias\uff09\u5728\u5fae\u8c03\u4e2d\u53d7\u635f\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u4f9d\u8d56\u77ed\u671f\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "\u4f5c\u8005\u63d0\u51faSA-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u9690\u5f0f\u7a7a\u95f4\u8868\u793a\u4e0e\u89c6\u89c9tokens\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u51e0\u4f55\u8fdb\u5c55\u7684\u5bc6\u96c6\u5956\u52b1\uff0c\u5e76\u5f15\u5165SCAN\uff08\u7a7a\u95f4\u6761\u4ef6\u6e10\u8fdb\u5f0f\u63a2\u7d22\u7b56\u7565\uff09\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u65f6\u4fdd\u6301\u7a7a\u95f4\u57fa\u7840\u3002\u8fd9\u6837\u53ef\u4ee5\u5b9e\u73b0\u8868\u5f81\u5b66\u4e60\u3001\u5956\u52b1\u8bbe\u8ba1\u548c\u63a2\u7d22\u884c\u4e3a\u4e09\u65b9\u9762\u4e0e\u4efb\u52a1\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u7269\u4f53\u548c\u6742\u4e71\u64cd\u4f5c\u4efb\u52a1\u57fa\u51c6\u4e0a\uff0cSA-VLA\u5b9e\u73b0\u4e86\u66f4\u52a0\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5e76\u63d0\u5347\u4e86\u96f6\u6837\u672c\u4e0b\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u673a\u5668\u4eba\u83b7\u5f97\u66f4\u5f3a\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u9c81\u68d2\u6027\u8868\u73b0\u3002", "conclusion": "SA-VLA\u80fd\u591f\u663e\u8457\u7f13\u89e3RL\u5fae\u8c03\u4e0b\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u635f\u5931\uff0c\u63d0\u5347VLA\u6a21\u578b\u7684\u64cd\u7eb5\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00114", "abs": "https://arxiv.org/abs/2602.00114", "authors": ["Yunwei Bai", "Ying Kiat Tan", "Yao Shu", "Tsuhan Chen"], "title": "1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization", "comment": null, "summary": "Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a1S-DAug\u7684\u65b0\u578b\u751f\u6210\u5f0f\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5c11\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u5728\u65b0\u7c7b\u522b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u5e38\u89c4\u6d4b\u8bd5\u65f6\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u9762\u5bf9FSL\u4e2d\u6807\u6ce8\u6837\u672c\u6781\u5c11\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u6a21\u578b\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u96be\u4ee5\u594f\u6548\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u589e\u5f3a\u624b\u6bb5\uff0c\u5728\u4ec5\u67091\u4e2a\u6837\u672c\u65f6\u63d0\u5347\u6a21\u578b\u8bc6\u522b\u80fd\u529b\u3002", "method": "1S-DAug\u7ed3\u5408\u4e86\u4f20\u7edf\u51e0\u4f55\u6270\u52a8\u3001\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u3001\u548c\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6d4b\u8bd5\u65f6\u4ece\u5355\u4e2a\u6837\u672c\u5408\u6210\u591a\u6837\u4e14\u5fe0\u5b9e\u7684\u53d8\u4f53\u56fe\u7247\uff0c\u5e76\u4e0e\u539f\u59cb\u56fe\u7247\u5171\u540c\u7f16\u7801\u805a\u5408\u4e3a\u7ec4\u5408\u8868\u5f81\uff0c\u65e0\u9700\u66f4\u6539\u539f\u6709\u6a21\u578b\u53c2\u6570\uff0c\u4ec5\u4ee5\u63d2\u4ef6\u5f62\u5f0f\u96c6\u6210\u3002", "result": "\u57284\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u6807\u51c6\u6d4b\u8bd5\u4e0a\uff0c1S-DAug\u65e0\u987b\u8bad\u7ec3\u5373\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u5e94\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\uff0c\u5747\u663e\u8457\u63d0\u5347\u4e86FSL\u6027\u80fd\uff0c\u5c24\u5176\u5728miniImagenet 5-way-1-shot\u57fa\u51c6\u4e0a\u63d0\u5347\u4e8610%\u4ee5\u4e0a\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u3002", "conclusion": "1S-DAug\u662f\u4e00\u79cd\u6709\u6548\u3001\u6a21\u578b\u65e0\u5173\u3001\u8bad\u7ec3\u65e0\u9700\u6c42\u7684\u65b0\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347FSL\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5177\u6709\u826f\u597d\u5b9e\u7528\u63a8\u5e7f\u524d\u666f\u3002"}}
{"id": "2602.00316", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00316", "abs": "https://arxiv.org/abs/2602.00316", "authors": ["Rodrigo Batista", "Lu\u00eds Filipe Cunha", "Purifica\u00e7\u00e3o Silvano", "Nuno Guimar\u00e3es", "Al\u00edpio Jorge", "Evelin Amorim", "Ricardo Campos"], "title": "MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes", "comment": null, "summary": "Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7ba1\u9053\u65b9\u6cd5\uff0c\u4ece\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u4e2d\u9ad8\u6548\u63d0\u53d6\u5143\u6570\u636e\uff0c\u5e76\u5efa\u7acb\u4e86\u9886\u57df\u9996\u4e2a\u57fa\u51c6\uff0c\u5bf9\u591a\u79cd\u4e3b\u6d41\u6a21\u578b\u7684\u51c6\u786e\u7387\u3001\u63a8\u7406\u6210\u672c\u53ca\u78b3\u6392\u653e\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u7ed3\u6784\u591a\u6837\u3001\u683c\u5f0f\u5f02\u8d28\uff0c\u5143\u6570\u636e\uff08\u5982\u65e5\u671f\u3001\u5730\u70b9\u7b49\uff09\u901a\u5e38\u672a\u6807\u51c6\u5316\uff0c\u96be\u4ee5\u81ea\u52a8\u63d0\u53d6\uff0c\u73b0\u6709NER\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u6b64\u7c7b\u7279\u5b9a\u9886\u57df\u7684\u5143\u6570\u636e\u62bd\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u62bd\u53d6\u6d41\u7a0b\uff1a\u9996\u5148\u7528\u95ee\u7b54\u6a21\u578b\u5b9a\u4f4d\u5305\u542b\u5143\u6570\u636e\u7684\u7247\u6bb5\uff0c\u5176\u6b21\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08BERTimbau\u3001XLM-RoBERTa\u3001\u6709\u65e0CRF\u5c42\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u62bd\u53d6\uff0c\u5e76\u5229\u7528\u53bb\u8bcd\u6c47\u5316\u589e\u5f3a\u6548\u679c\u3002\u8fd8\u8bc4\u4f30\u4e86\u5f00\u6e90LLM\uff08Phi\uff09\u4e0e\u95ed\u6e90LLM\uff08Gemini\uff09\u7684\u8868\u73b0\u3001\u63a8\u7406\u6210\u672c\u548c\u78b3\u8db3\u8ff9\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u57df\u5185\u5e02\u653f\u7eaa\u8981\u7684\u5143\u6570\u636e\u62bd\u53d6\u4e0a\u4f18\u4e8e\u901a\u7528\u5927\u6a21\u578b\uff0c\u4f46\u8de8\u5e02\u8bc4\u4f30\u8868\u73b0\u4f1a\u964d\u4f4e\uff0c\u53cd\u6620\u4e86\u6587\u672c\u98ce\u683c\u548c\u8bed\u8a00\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u5efa\u7acb\u4e86\u9996\u4e2a\u6b64\u4efb\u52a1\u7684\u6807\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u57fa\u51c6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u5143\u6570\u636e\u62bd\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u548c\u91cd\u8981\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u540e\u7eed\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2602.00808", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00808", "abs": "https://arxiv.org/abs/2602.00808", "authors": ["Hang Zhou", "Qiang Zhang", "Peiran Liu", "Yihao Qin", "Zhaoxu Yan", "Yiding Ji"], "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving", "comment": null, "summary": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Mamba Transformer\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u5e76\u878d\u5165Port-Hamiltonian\u795e\u7ecf\u7f51\u7edc\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u7269\u7406\u53ef\u884c\u6027\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u867d\u7136\u80fd\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u5e38\u5e38\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u65f6\u5e8f\u4f9d\u8d56\u4e0e\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u9884\u6d4b\u8f68\u8ff9\u4e0d\u5408\u7406\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "1. \u63d0\u51faDiffusion Mamba Transformer\u67b6\u6784\uff0c\u5728\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165Mamba\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u66f4\u597d\u5730\u805a\u5408\u4f20\u611f\u5668\u4e0e\u5386\u53f2\u8f68\u8ff9\u7684\u65f6\u5e8f\u4e0a\u4e0b\u6587\u30022. \u8bbe\u8ba1Port-Hamiltonian\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u5c06\u7269\u7406\u80fd\u6e90\u7ea6\u675f\u65e0\u7f1d\u5d4c\u5165\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u6027\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u7269\u7406\u5408\u7406\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u8fd0\u52a8\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.00115", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00115", "abs": "https://arxiv.org/abs/2602.00115", "authors": ["David El-Chai Ben-Ezra", "Adar Tal", "Daniel Brisk"], "title": "Event Driven Clustering Algorithm", "comment": "~10 pages, 2 figures", "summary": "This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7684\u5c0f\u4e8b\u4ef6\u7c07\uff0c\u7b97\u6cd5\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027\u4e14\u4e0d\u53d7\u50cf\u7d20\u7ef4\u5ea6\u5f71\u54cd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4ea7\u751f\u7684\u6570\u636e\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7a00\u758f\u6027\uff0c\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u3001\u9ad8\u6548\u5730\u68c0\u6d4b\u4e8b\u4ef6\u7c07\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u91cf\u5927\u6216\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u5e94\u7528\u573a\u666f\u4e0b\u3002", "method": "\u57fa\u4e8e\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\uff0c\u91c7\u7528\u7c7b\u4f3c\u5206\u5c42\u805a\u7c7b\u7684\u65b9\u5f0f\uff0c\u5e76\u4f9d\u636e\u4e8b\u4ef6\u4e4b\u95f4\u7684\u65f6\u7a7a\u8ddd\u79bb\u8fdb\u884c\u5206\u7c07\u3002\u901a\u8fc7\u4f18\u5316\u51b3\u7b56\u673a\u5236\uff0c\u4f7f\u5f97\u805a\u7c7b\u7b97\u6cd5\u6574\u4f53\u590d\u6742\u5ea6\u8fbe\u5230O(n)\uff0c\u4e14\u4e0e\u50cf\u7d20\u9635\u5217\u5c3a\u5bf8\u65e0\u5173\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u5728\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u4e8b\u4ef6\u7c07\u68c0\u6d4b\uff0c\u65e0\u9700\u5173\u6ce8\u50cf\u7d20\u9635\u5217\u7684\u7ef4\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u7b97\u6548\u7387\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u5c0f\u4e8b\u4ef6\u7c07\u68c0\u6d4b\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u9002\u5408\u5b9e\u65f6\u3001\u5927\u89c4\u6a21\u4e8b\u4ef6\u6570\u636e\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.00319", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.00319", "abs": "https://arxiv.org/abs/2602.00319", "authors": ["Siyuan Shen", "Kai Wang"], "title": "Detecting AI-Generated Content in Academic Peer Reviews", "comment": null, "summary": "The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u4ece2022\u5e74\u8d77AI\u751f\u6210\u5185\u5bb9\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u5360\u6bd4\u8fc5\u901f\u589e\u52a0\uff0c\u52302025\u5e74ICLR\u548cNature Communications\uff08NC\uff09\u7684\u8bc4\u5ba1\u4e2dAI\u751f\u6210\u5185\u5bb9\u5206\u522b\u5360\u6bd4\u7ea620%\u548c12%\u3002", "motivation": "\u8fd1\u5e74\u6765\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2dAI\u89d2\u8272\u7684\u5173\u6ce8\uff0c\u4f5c\u8005\u60f3\u8981\u4e86\u89e3AI\u751f\u6210\u5185\u5bb9\u5728\u8bc4\u5ba1\u4e2d\u7684\u51fa\u73b0\u8d8b\u52bf\u53ca\u5176\u53d1\u5c55\u901f\u5ea6\u3002", "method": "\u4f5c\u8005\u5229\u7528\u8bad\u7ec3\u4e8e\u5386\u53f2\u8bc4\u5ba1\u7684AI\u68c0\u6d4b\u6a21\u578b\uff0c\u5bf9ICLR\u548cNC\u4e24\u4e2a\u4f1a\u8bae/\u671f\u520a\u5728\u4e0d\u540c\u5468\u671f\u7684\u8bc4\u5ba1\u6587\u672c\u8fdb\u884c\u68c0\u6d4b\uff0c\u8ffd\u8e2aAI\u5185\u5bb9\u7684\u6bd4\u4f8b\u53d8\u5316\u3002", "result": "2022\u5e74\u524dAI\u5185\u5bb9\u68c0\u6d4b\u6781\u5c11\uff0c2022\u5e74\u540e\u660e\u663e\u4e0a\u5347\uff0c\u81f32025\u5e74ICLR\u7ea620%\u3001NC\u7ea612%\u7684\u8bc4\u5ba1\u88ab\u68c0\u6d4b\u4e3aAI\u751f\u6210\u30022024\u5e74Q3\u81f3Q4\u671f\u95f4\uff0cNC\u4e2dAI\u751f\u6210\u8bc4\u5ba1\u7684\u589e\u957f\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "AI\u751f\u6210\u5185\u5bb9\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u5360\u6bd4\u6b63\u5feb\u901f\u4e0a\u5347\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5bf9\u5b66\u672f\u8bc4\u4ef7\u4f53\u7cfb\u5e26\u6765\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.00814", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00814", "abs": "https://arxiv.org/abs/2602.00814", "authors": ["Bomena Kim", "Hojun Lee", "Younsoo Park", "Yaoyu Hu", "Sebastian Scherer", "Inwook Shim"], "title": "SyNeT: Synthetic Negatives for Traversability Learning", "comment": null, "summary": "Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u5408\u6210\u7684\u4e0d\u53ef\u901a\u884c\uff08\u8d1f\uff09\u6837\u672c\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u5bfc\u822a\u4e2d\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u6a21\u578b\u8bc6\u522b\u5404\u79cd\u4e0d\u53ef\u901a\u884c\u533a\u57df\u7684\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5bf9\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u6846\u67b6\u4e3b\u8981\u57fa\u4e8e\u6b63\u6837\u672c\u548c\u672a\u6807\u8bb0\u6837\u672c\uff0c\u7f3a\u4e4f\u663e\u5f0f\u8d1f\u6837\u672c\uff08\u4e0d\u53ef\u901a\u884c\u533a\u57df\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u591a\u6837\u5316\u7684\u4e0d\u53ef\u901a\u884c\u533a\u57df\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5408\u6210\u8d1f\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u5c06\u8fd9\u4e9b\u4ee3\u8868\u201c\u770b\u4f3c\u53ef\u901a\u884c\u4f46\u5b9e\u9645\u4e0a\u4e0d\u53ef\u901a\u884c\u201d\u7684\u6837\u672c\u52a0\u5165\u8bad\u7ec3\uff0c\u9002\u7528\u4e8ePU\u548cPN\u67b6\u6784\uff0c\u65e0\u9700\u4fee\u6539\u63a8\u7406\u8fc7\u7a0b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u57fa\u4e8e\u76ee\u6807\u7684\u5047\u9633\u6027\u7387\uff08FPR\uff09\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u6a21\u578b\u5728\u5408\u6210\u8d1f\u6837\u672c\u63d2\u5165\u533a\u57df\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u624b\u5de5\u6807\u6ce8\u3002", "result": "\u5728\u516c\u5f00\u53ca\u81ea\u91c7\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5408\u6210\u8d1f\u6837\u672c\u5e76\u4f18\u5316\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edf\u5bf9\u4e8e\u4e0d\u53ef\u901a\u884c\u533a\u57df\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u65b9\u6cd5\u7b80\u5355\u6613\u7528\u4e14\u901a\u7528\u6027\u5f3a\u3002"}}
{"id": "2602.00117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00117", "abs": "https://arxiv.org/abs/2602.00117", "authors": ["Lamia Lahouel", "Laurynas Lopata", "Simon Gruening", "Gabriele Meoni", "Gaetan Petit", "Sylvain Lobry"], "title": "IC-EO: Interpretable Code-based assistant for Earth Observation", "comment": "15 pages, 1 figure", "summary": "Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u3001\u53ef\u5ba1\u8ba1Python\u4ee3\u7801\u5de5\u4f5c\u6d41\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5206\u6790\u7684\u4fbf\u6377\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u5206\u6790\u5bf9\u975e\u4e13\u4e1a\u4eba\u5458\u6765\u8bf4\u8fc7\u4e8e\u590d\u6742\u4e14\u4e13\u4e1a\u95e8\u69db\u9ad8\uff0c\u4e14\u73b0\u6709\u7cfb\u7edf\u591a\u4e3a\u201c\u9ed1\u7bb1\u201d\uff0c\u7f3a\u4e4f\u53ef\u5ba1\u8ba1\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u5206\u6790\u96be\u3001\u590d\u73b0\u96be\u3001\u89e3\u8bfb\u96be\u4e09\u5927\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6700\u65b0Tool LLM\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5bf9\u8bdd\u9a71\u52a8\u3001\u80fd\u751f\u6210Python\u4ee3\u7801\u7684\u667a\u80fd\u4ee3\u7406\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5206\u7c7b\u3001\u5206\u5272\u3001\u68c0\u6d4b\u3001\u5149\u8c31\u6307\u6570\u4e0e\u5730\u7406\u64cd\u4f5c\u7b49\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00API\u5f00\u653e\u8c03\u7528\u3002\u7ed3\u679c\u63a7\u5236\u5206\u4e3a\u5de5\u5177\u7ea7\u6027\u80fd\u3001\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u548c\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u4e09\u4e2a\u5c42\u9762\u3002", "result": "\u5728\u571f\u5730\u7ec4\u6210\u5236\u56fe\u548c\u706b\u707e\u540e\u635f\u5931\u8bc4\u4f30\u4e24\u5927\u5b9e\u9645\u4efb\u52a1\u4e0a\uff0c\u8be5\u4ee3\u7406\u7cfb\u7edf\u76f8\u8f83\u4e8e\u73b0\u6709\u5927\u6a21\u578b\uff08\u5982GPT-4o\u3001LLaVA\uff09\u53d6\u5f97\u4e86\u66f4\u9ad8\u51c6\u786e\u7387\uff08\u5206\u522b\u4e3a64.2%\u5bf951.7%\uff0c50%\u5bf90%\uff09\uff0c\u4e14\u8f93\u51fa\u4ee3\u7801\u900f\u660e\u6613\u4e8e\u7406\u89e3\u3002", "conclusion": "\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86EO\u6570\u636e\u5206\u6790\u7684\u53ef\u590d\u73b0\u6027\u4e0e\u900f\u660e\u5ea6\uff0c\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u548c\u4e13\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u5ba1\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00352", "abs": "https://arxiv.org/abs/2602.00352", "authors": ["Li Siyan", "Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning", "comment": null, "summary": "When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u52a0\u771f\u5b9e\u6a21\u62df\u2018\u4f3c\u66fe\u76f8\u8bc6\u2019\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\u7684\u53cc\u667a\u80fd\u4f53\u6d4b\u8bc4\u57fa\u51c6DETOUR\uff0c\u5e76\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u6b20\u4f73\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u667a\u80fd\u4f53\u2018\u4f3c\u66fe\u76f8\u8bc6\u2019\u5f0f\u591a\u8f6e\u68c0\u7d22\u8bc4\u6d4b\u591a\u6570\u53ea\u8003\u8651\u5355\u8f6e\u573a\u666f\uff0c\u8fd9\u4e0e\u771f\u5b9e\u4ea4\u6d41\u573a\u666f\u4e0d\u7b26\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u8d34\u5408\u5b9e\u9645\u7684\u591a\u8f6e\u8bc4\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86DETOUR\u53cc\u667a\u80fd\u4f53\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u542b1011\u4e2a\u63d0\u793a\u8bed\u3002\u6838\u5fc3\u8bbe\u8ba1\u4e3a\u8ba9\u5f85\u8bc4\u4f30\u7684\u2018\u4e3b\u667a\u80fd\u4f53\u2019\u901a\u8fc7\u4e0e\u56fa\u5b9a\u8bb0\u5fc6\u667a\u80fd\u4f53\u7684\u591a\u8f6e\u95ee\u7b54\uff0c\u63a8\u65ad\u51fa\u76ee\u6807\u4fe1\u606f\u3002\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u4e0e\u89c6\u9891\u591a\u6a21\u6001\u3002", "result": "\u73b0\u6709\u6700\u524d\u6cbf\u7684\u6a21\u578b\u5728DETOUR\u5168\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u53ca\u89c6\u9891\u6a21\u6001\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a36%\u3002", "conclusion": "\u4e3b\u6d41\u6a21\u578b\u5728\u591a\u8f6e\u3001\u4e0d\u5145\u5206\u63d0\u793a\u4e0b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u4ecd\u8f83\u5f31\uff0c\u672a\u6765\u9700\u8981\u7740\u91cd\u63d0\u5347\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u63a8\u7406\u4e0e\u68c0\u7d22\u80fd\u529b\u3002"}}
{"id": "2602.00823", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00823", "abs": "https://arxiv.org/abs/2602.00823", "authors": ["Spyridon Syntakas", "Kostas Vlachos"], "title": "Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the \"helpfulness\" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative \"gliding\". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bMPC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6d77\u6d0b\u6d0b\u6d41\u63d0\u5347AUV\u7eed\u822a\u4e0e\u80fd\u6548\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u80fd\u8017\u5927\u5e45\u4e0b\u964d\u3002", "motivation": "AUV\u53d7\u9650\u4e8e\u80fd\u6548\u548c\u7eed\u822a\u80fd\u529b\uff0c\u6d0b\u6d41\u5e26\u6765\u673a\u4f1a\uff0c\u4f46\u76ee\u524d\u63a7\u5236\u65b9\u6cd5\u5229\u7528\u6709\u9650\u3002\u56e0\u6b64\u9700\u5f00\u53d1\u66f4\u6709\u6548\u5229\u7528\u6d0b\u6d41\u7684\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\u7b56\u7565\u3002", "method": "\u63d0\u51faCurrent-Harnessing Stage-Gated MPC\u65b9\u6cd5\uff0c\u5728\u9884\u6d4b\u65f6\u57df\u4e2d\u8ba1\u7b97\u6d0b\u6d41\u5bf9\u63a7\u5236\u76ee\u6807\u7684\u201c\u6709\u5229\u7a0b\u5ea6\u201d\u6807\u91cf\uff0c\u4ec5\u5728\u201c\u6709\u5229\u201d\u65f6\u523b\u8c03\u6574\u76ee\u6807\u51fd\u6570\u3002\u65b9\u6cd5\u7ed3\u6784\u5305\u62ec\uff1a\u4e00\u662f\u6709\u5e2e\u52a9\u95e8\u63a7\u7684\u5355\u8c03\u6210\u672c\u5851\u578b\uff0c\u7528\u4e8e\u653e\u5bbd\u4f4d\u7f6e\u8bef\u5dee\u5e76\u5e26\u6765\u6709\u754c\u7684\u80fd\u91cf\u8fd4\u8fd8\uff0c\u4fdd\u8bc1\u76ee\u6807\u51fd\u6570\u4f18\u4e8e\u57fa\u7ebf\uff1b\u4e8c\u662f\u901f\u5ea6\u5339\u914d\u6210\u672c\uff0c\u9f13\u52b1\u63a8\u8fdb\u529b\u51cf\u5c11\u3001\u5b9e\u73b0\u4e0e\u6d0b\u6d41\u901f\u5ea6\u5339\u914d\uff0c\u8fbe\u5230\u8fd1\u96f6\u6c34\u76f8\u5bf9\u201c\u6ed1\u7fd4\u201d\u3002\u6240\u6709\u6210\u672c\u9879\u5747\u4e3aC1\u8fde\u7eed\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u8fdbMPC\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u6d77\u6d0b\u6d41\u573a\u4e0b\uff0c\u7528BlueROV2\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u4eff\u771f\u3002\u65b0\u65b9\u6cd5\u4e0e\u5e38\u89c4\u9884\u6d4b\u63a7\u5236\u5bf9\u6bd4\uff0c\u8868\u73b0\u4e3a\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u5230\u8fbe\u65f6\u95f4\u4e0e\u7ea6\u675f\u6ee1\u8db3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u964d\u4f4eAUV\u80fd\u8017\uff0c\u53ef\u5e7f\u6cdb\u96c6\u6210\u5230\u5404\u7c7bMPC\u4e2d\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AUV\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.00122", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00122", "abs": "https://arxiv.org/abs/2602.00122", "authors": ["Hongzhu Yi", "Yujia Yang", "Yuanxiang Wang", "Zhenyu Guan", "Jiahuan Chen", "Chenxi Bao", "Tiankun Yang", "Yixuan Yuan", "Tianyu Zong", "Xinming Wang", "Tao Yu", "Ruiwen Tao", "Haijin Liang", "Jin Ma", "Jinwen Luo", "Yeshani Xinyu Zuo", "Jungang Xu"], "title": "VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents", "comment": null, "summary": "In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \\textbf{V}isual \\textbf{D}oc \\textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VDE Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u6d4b\u591a\u8bed\u79cd\u3001\u5bc6\u96c6\u6587\u672c\u89c6\u89c9\u6587\u6863\u7f16\u8f91\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u4e2d\u82f1\u6587\u6587\u672c\u56fe\u50cf\uff0c\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u805a\u7126\u4e8e\u82f1\u8bed\u548c\u7a00\u758f\u6587\u672c\uff0c\u5bf9\u4e8e\u7ed3\u6784\u590d\u6742\u3001\u5bc6\u96c6\u6587\u672c\u4ee5\u53ca\u975e\u62c9\u4e01\u6587\u79cd\uff08\u5982\u4e2d\u6587\uff09\u7684\u7f16\u8f91\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86VDE Bench\u57fa\u51c6\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u4e2d\u82f1\u6587\u5bc6\u96c6\u6587\u672c\u6587\u6863\uff08\u5982\u8bba\u6587\u3001\u6d77\u62a5\u3001\u5e7b\u706f\u7247\u3001\u8003\u8bd5\u8d44\u6599\u548c\u62a5\u7eb8\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u89e3\u8026\u7684OCR\u7ea7\u7f16\u8f91\u6027\u80fd\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u53ef\u7ec6\u81f4\u91cf\u5316\u6587\u672c\u4fee\u6539\u51c6\u786e\u6027\u3002", "result": "\u4f5c\u8005\u57fa\u4e8eVDE Bench\u5bf9\u591a\u79cd\u4e3b\u6d41\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u6d4b\uff0c\u4eba\u5de5\u9a8c\u8bc1\u8868\u660e\u81ea\u52a8\u5316\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u5224\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "VDE Bench\u9996\u6b21\u4e3a\u591a\u8bed\u79cd\u3001\u5bc6\u96c6\u6587\u672c\u89c6\u89c9\u6587\u6863\u7684\u7f16\u8f91\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u3001\u53ef\u9760\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e3a\u8be5\u9886\u57df\u540e\u7eed\u7814\u7a76\u548c\u6a21\u578b\u8fdb\u6b65\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.00377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00377", "abs": "https://arxiv.org/abs/2602.00377", "authors": ["Zhaochen Hong", "Jiaxuan You"], "title": "DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models", "comment": null, "summary": "Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.", "AI": {"tldr": "DecompressionLM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u72b6\u6001\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u9884\u8bbe\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\u96f6\u6837\u672c\u63d0\u53d6\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u77e5\u8bc6\u56fe\u8c31\uff0c\u540c\u65f6\u514b\u670d\u4e86\u4f20\u7edf\u63a2\u67e5\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u3002\u5b83\u8fd8\u5c55\u793a\u4e86\u4e0d\u540c\u91cf\u5316\u7b56\u7565\u5bf9\u6a21\u578b\u77e5\u8bc6\u8986\u76d6\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u63a2\u67e5\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u53ea\u80fd\u63d0\u53d6\u5df2\u77e5\u6982\u5ff5\uff0c\u4e14\u5e38\u89c1\u7684\u89e3\u7801\u63a2\u67e5\u65b9\u6cd5\u5b58\u5728\u8de8\u5e8f\u5217\u8026\u5408\u3001\u9ad8\u9891\u4f18\u5148\u4e0e\u6269\u5c55\u6027\u5dee\u7b49\u95ee\u9898\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u65e0\u76d1\u7763\u3001\u65e0\u9700\u67e5\u8be2\u3001\u53ef\u6269\u5c55\u5730\u63d0\u53d6\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDecompressionLM\u6846\u67b6\uff0c\u5229\u7528Van der Corput\u4f4e\u5dee\u5f02\u5e8f\u5217\u548c\u7b97\u672f\u89e3\u7801\uff0c\u5b9e\u73b0\u65e0\u72b6\u6001\u3001\u53ef\u5e76\u884c\uff08\u4e14\u786e\u5b9a\u6027\uff09\u5730\u4ece\u6a21\u578b\u4e2d\u751f\u6210\u5305\u542b\u6982\u5ff5\u5173\u7cfb\u7684\u56fe\u8c31\uff0c\u65e0\u9700\u8de8\u6837\u672c\u5171\u4eab\u72b6\u6001\u3002\u6bd4\u8f83\u4e0d\u540c\u91cf\u5316\uff08AWQ-4bit\u4e0eGPTQ-Int4\uff09\u65b9\u6cd5\u4e0b\u7684\u77e5\u8bc6\u8986\u76d6\u7387\uff0c\u5e76\u901a\u8fc7\u8bed\u6599\u9a8c\u8bc1\u6a21\u578b\u771f\u5b9e\u4e0e\u5e7b\u89c9\u77e5\u8bc6\u7684\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAWQ-4bit\u91cf\u5316\u80fd\u63d0\u5347\u6982\u5ff5\u8986\u76d6\u738730-170%\uff1b\u800cGPTQ-Int4\u91cf\u5316\u5219\u5bfc\u81f4\u8986\u76d6\u7387\u4e0b\u964d71-86%\u3002\u8fd9\u79cd\u5dee\u5f02\u65e0\u6cd5\u5355\u7eaf\u901a\u8fc7\u56f0\u60d1\u5ea6\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u77e5\u8bc6\u8986\u76d6\u5e7f\u5ea6\u8bc4\u4f30\u63ed\u793a\u9ad8\u5206\u4e0e\u4f4e\u5206\u6a21\u578b\u5b58\u572817\u5206\u7684\u201c\u5e7b\u89c9\u201d\u5dee\u8ddd\u3002", "conclusion": "DecompressionLM\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u8bc4\u4f30\u538b\u7f29\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u5e7f\u5ea6\u4e0e\u4e8b\u5b9e\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u8fd8\u8865\u5145\u4e86\u5f53\u524d\u4ee5\u56f0\u60d1\u5ea6\u4e3a\u4e3b\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u63a8\u52a8\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u524d\u7684\u53ef\u7528\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2602.00868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00868", "abs": "https://arxiv.org/abs/2602.00868", "authors": ["Nikhil Uday Shinde", "Dylan Hirsch", "Michael C. Yip", "Sylvia Herbert"], "title": "Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects", "comment": null, "summary": "Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5728\u5177\u6709\u968f\u673a\u52a8\u6001\u7684\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u673a\u5668\u4eba\u5b89\u5168\u63a2\u7d22\u7684\u65b0\u65b9\u6cd5\uff08S.S.Explorer\uff09\uff0c\u901a\u8fc7\u6743\u8861\u5b89\u5168\u4e0e\u4fe1\u606f\u6536\u96c6\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u673a\u5668\u4eba\u5728\u5b9e\u9645\u672a\u77e5\u6216\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5e38\u5e38\u9762\u4e34\u52a8\u6001\u672a\u77e5\u548c\u968f\u673a\u6027\uff0c\u4e0d\u6613\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u800c\u73b0\u6709\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u5df2\u77e5\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u968f\u673a\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u63a2\u7d22\u5b89\u5168\u3001\u53c8\u80fd\u5e94\u5bf9\u968f\u673a\u52a8\u6001\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faS.S.Explorer\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u5bf9\u672a\u77e5\u5b89\u5168\u51fd\u6570\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u673a\u5668\u4eba\u91c7\u53d6\u4fe1\u606f\u6536\u96c6\u884c\u4e3a\u4ee5\u51cf\u5c11\u5173\u4e8e\u5b89\u5168\u6027\u7684\u672a\u77e5\u3002\u540c\u65f6\u5206\u522b\u9488\u5bf9\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u548c\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u8bbe\u8ba1\u4e86\u7b97\u6cd5\uff0c\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u4e86\u4e0e\u591a\u4e2a\u672a\u77e5\u7269\u4f53\u7684\u7269\u7406\u4ea4\u4e92\u5b89\u5168\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u6a21\u62df\u5b9e\u9a8c\u548c\u5b9e\u9645\u786c\u4ef6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u79cd\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u5b89\u5168\u6027\u548c\u63a2\u7d22\u6548\u7387\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "S.S.Explorer\u4e3a\u5728\u672a\u77e5\u548c\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u548c\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u73af\u5883\u4e2d\u5e7f\u6cdb\u5b89\u5168\u5e94\u7528\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.00124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00124", "abs": "https://arxiv.org/abs/2602.00124", "authors": ["Divya Acharya", "Pierre Bernab'e", "Antoine Chevrot", "Helge Spieker", "Arnaud Gotlieb", "Bruno Legeard"], "title": "Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance", "comment": null, "summary": "The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6539\u8fdb\u6d77\u4e8b\u8239\u53ea\u4ea4\u901a\u76d1\u63a7\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6d77\u4e8b\u8239\u8236\u4ea4\u901a\u76d1\u63a7\u4e2d\u5f02\u5e38\u68c0\u6d4b\u5bf9\u5b89\u5168\u4fdd\u969c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u96be\u4ee5\u68c0\u6d4b\u96c6\u4f53\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5f02\u5e38\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d56\u8239\u53ea\u81ea\u62a5\u544aAIS\u6d88\u606f\u7684\u6570\u636e\u73af\u5883\u4e2d\uff0c\u5f02\u5e38\u5e38\u5e38\u4e0e\u5177\u4f53\u8239\u8236\u76f8\u5173\u8054\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u81ea\u7f16\u7801\u5668\u5728\u8fd9\u4e9b\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u9608\u503c\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u5f02\u5e38\u7684\u8bc6\u522b\u80fd\u529b\u548c\u6548\u7387\u3002\u8bba\u6587\u5bf9\u6bd4\u4e864\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\u53d8\u4f53\u548c\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u5728\u6e14\u8239\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u9645\u6848\u4f8b\u4e2d\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0a\u4e0b\u6587\u5bf9\u91cd\u6784\u635f\u5931\u548c\u5f02\u5e38\u68c0\u6d4b\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\u901a\u8fc7\u8003\u8651\u4e0a\u4e0b\u6587\u91cd\u8981\u6027\u548c\u91c7\u7528\u7279\u5b9a\u9608\u503c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6d77\u4e8b\u8239\u8236\u5f02\u5e38\u76d1\u63a7\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00380", "abs": "https://arxiv.org/abs/2602.00380", "authors": ["Sercan Karaka\u015f"], "title": "Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models", "comment": "15 pages", "summary": "This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u7406\u89e3\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u7684\u6307\u4ee3\u5173\u7cfb\u3002\u901a\u8fc7\u5bf9\u4e24\u79cd\u4e0d\u540c\u6a21\u578b\u5728\u672c\u5730\u4e0e\u8fdc\u8ddd\u79bb\u6307\u4ee3\u8868\u73b0\u4e0a\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u7684\u6307\u4ee3\u5173\u7cfb\uff08\u7ed1\u5b9a\u5173\u7cfb\uff09\u8f83\u4e3a\u590d\u6742\uff0c\u7136\u800c\u76ee\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u7c7b\u7ed3\u6784\u4e0a\u7684\u7406\u89e3\u80fd\u529b\u8fd8\u4e0d\u660e\u786e\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u548c\u91cf\u5316\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u5904\u7406\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u9886\u57df\u7684\u8868\u73b0\u53ca\u6f5c\u5728\u5dee\u5f02\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86100\u4e2a\u5747\u8861\u53e5\u5b50\u96c6\uff0c\u53e5\u5b50\u4e2d\u53cd\u8eab\u4ee3\u8bcdkendi\u548ckendisi\u5728\u672c\u5730\u4e0e\u975e\u672c\u5730\u6307\u4ee3\u5173\u7cfb\u4e2d\u8fdb\u884c\u5bf9\u6297\u3002\u8bc4\u6d4b\u5bf9\u8c61\u4e3aOpenAI\u94fe\u5f0f\u63a8\u7406\u6a21\u578b\u548c\u571f\u8033\u5176\u8bed\u7ec6\u8c03\u7684Trendyol-LLM\uff0c\u901a\u8fc7\u53e5\u5b50\u56f0\u60d1\u5ea6\u548c\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u5bf9\u5148\u884c\u8bcd\u7684\u503e\u5411\u3002", "result": "Trendyol-LLM\u5728\u7ea670%\u7684\u8bd5\u9a8c\u4e2d\u652f\u6301\u672c\u5730\u6307\u4ee3\uff0c\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u672c\u5730\u6027\u504f\u597d\uff1b\u800co1 Mini\u6a21\u578b\u5bf9\u672c\u5730\u4e0e\u8fdc\u8ddd\u79bb\u6307\u4ee3\u9009\u62e9\u57fa\u672c\u5747\u5300\uff0c\u4e24\u79cd\u6a21\u578b\u7684\u7ed1\u5b9a\u884c\u4e3a\u6709\u663e\u8457\u5bf9\u6bd4\u3002", "conclusion": "\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u7ed1\u5b9a\u5173\u7cfb\u7684\u5904\u7406\u65b9\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5176\u4e2d\u9488\u5bf9\u672c\u5730\u6570\u636e\u91cf\u5927\u3001\u7ec6\u81f4\u5fae\u8c03\u7684\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u672c\u5730\u6307\u4ee3\u3002\u8fd9\u63d0\u9192\u6211\u4eec\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u53e5\u6cd5\u7ed3\u6784\u7684\u638c\u63e1\u53d7\u5230\u8bad\u7ec3\u8bed\u6599\u548c\u5fae\u8c03\u7b56\u7565\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.00877", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00877", "abs": "https://arxiv.org/abs/2602.00877", "authors": ["Zhipeng Zhao", "Taimeng Fu", "Shaoshu Su", "Qiwei Du", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury", "Chen Wang"], "title": "Learning When to Jump for Off-road Navigation", "comment": null, "summary": "Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5730\u5f62\u53ef\u901a\u884c\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u8d8a\u91ce\u73af\u5883\u4e0b\u80fd\u5b89\u5168\u4e14\u9ad8\u6548\u5730\u52a8\u6001\u89c4\u5212\u8def\u5f84\uff0c\u6709\u6548\u51cf\u5c11\u7ed5\u884c\u5e76\u63d0\u5347\u901a\u884c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u591a\u4ec5\u8003\u8651\u4f4d\u7f6e\u6216\u56fa\u5b9a\u901f\u5ea6\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u901f\u5ea6\u4e0b\u5730\u5f62\u5bf9\u673a\u5668\u4eba\u52a8\u6001\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u5730\u5f62\uff08\u5982\u58d5\u6c9f\uff09\u4e0b\u4f4e\u901f\u53cd\u800c\u66f4\u5371\u9669\u3002\u4e3a\u5b9e\u73b0\u901f\u5ea6\u52a8\u6001\u76f8\u5173\u7684\u5b89\u5168\u9ad8\u6548\u8d8a\u91ce\u5bfc\u822a\uff0c\u6709\u5fc5\u8981\u5efa\u7acb\u66f4\u7cbe\u7ec6\u7684\u8fd0\u52a8-\u5730\u5f62\u5173\u8054\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMotion-aware Traversability (MAT)\u8868\u793a\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u5730\u5f62\u533a\u57df\u7684\u53ef\u901a\u884c\u6027\u5efa\u6a21\u4e3a\u901f\u5ea6\u7684\u9ad8\u65af\u51fd\u6570\u3002\u5177\u4f53\u5305\u62ec\uff1a(1)\u611f\u77e5\u8f93\u5165\u4e0b\u5355\u6b21\u524d\u9988\u9884\u6d4b\u6bcf\u4e2a\u533a\u57df\u7684\u9ad8\u65af\u53c2\u6570\uff1b(2)\u8def\u5f84\u89c4\u5212\u9636\u6bb5\u6839\u636e\u5f53\u524d\u52a8\u529b\u5b66\u72b6\u6001\uff0c\u7528\u4e0a\u8ff0\u9ad8\u65af\u51fd\u6570\u9ad8\u6548\u5730\u8ba1\u7b97\u4e0d\u540c\u901f\u5ea6\u4e0b\u7684\u5730\u5f62\u6210\u672c\uff0c\u65e0\u9700\u53cd\u590d\u63a8\u7406\u3002", "result": "MAT\u7cfb\u7edf\u5728\u4eff\u771f\u548c\u771f\u5b9e\u8d8a\u91ce\u969c\u788d\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u53ef\u5927\u5e45\u7f29\u77ed\u8def\u7ebf\uff08\u7ed5\u884c\u51cf\u5c1175%\uff09\uff0c\u540c\u65f6\u4fdd\u969c\u590d\u6742\u5730\u5f62\u4e0b\u7684\u884c\u9a76\u5b89\u5168\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u89c4\u5212\u6548\u7387\u3002", "conclusion": "MAT\u6709\u6548\u63d0\u5347\u4e86\u8d8a\u91ce\u5bfc\u822a\u7cfb\u7edf\u5bf9\u52a8\u6001\u5730\u5f62\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5c11\u7ed5\u884c\u4e0e\u66f4\u9ad8\u5b89\u5168\u6027\u7684\u7edf\u4e00\uff0c\u4f18\u4e8e\u4f20\u7edf\u4ec5\u57fa\u4e8e\u4f4d\u7f6e\u548c\u56fa\u5b9a\u901f\u5ea6\u7684\u65b9\u6848\u3002"}}
{"id": "2602.00126", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00126", "abs": "https://arxiv.org/abs/2602.00126", "authors": ["Dmytro Filatov", "Valentyn Fedorov", "Vira Filatova", "Andrii Zelenchuk"], "title": "D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection", "comment": "9 pages", "summary": "Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6D3R-Net\uff0c\u901a\u8fc7\u5f15\u5165\u9891\u57df\u635f\u5931\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u7f3a\u9677\u533a\u57df\u7684\u5206\u5272\u4e0e\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91cd\u6784\u7684\u65b9\u6cd5\u5bf9\u9ad8\u9891\u7ec6\u8282\u5904\u7406\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5fae\u5c0f\u7f3a\u9677\u65e0\u6cd5\u51c6\u786e\u533a\u5206\uff0c\u8fdb\u800c\u9650\u5236\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u6539\u8fdb\u91cd\u6784\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u8bc6\u522b\u548c\u7a81\u51fa\u7ec6\u5fae\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u4e86D3R-Net\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u7ed3\u5408\u81ea\u76d1\u7763\u7684\u56fe\u50cf\u201c\u4fee\u590d\u201d\u4efb\u52a1\u4e0e\u9891\u57df\u611f\u77e5\u6b63\u5219\u5316\uff08FFT\u635f\u5931\uff09\u3002\u8bad\u7ec3\u65f6\u6a21\u578b\u9700\u5c06\u5408\u6210\u635f\u574f\u7684\u6b63\u5e38\u56fe\u50cf\u91cd\u6784\u4e3a\u5e72\u51c0\u56fe\u50cf\uff0c\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u5230\u6052\u7b49\u6620\u5c04\u3002\u635f\u5931\u51fd\u6570\u540c\u65f6\u5305\u542b\u7a7a\u95f4\u57df\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u9891\u7387\u57dfFFT\u5e45\u503c\u635f\u5931\uff0c\u53ef\u9009\u7ed3\u6784\u76f8\u4f3c\u6027\uff08SSIM\uff09\u4f5c\u4e3a\u6d88\u878d\u7814\u7a76\u3002\u6b64\u5916\uff0cD3R-Net\u91c7\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u3002", "result": "\u5728MVTec AD Hazelnut\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u5165FFT\u635f\u5931\u540e\uff0c\u5206\u5272\u5b9a\u4f4d\u6027\u80fd\uff08PRO AUC\uff09\u75310.603\u63d0\u5347\u81f30.687\uff0c\u56fe\u50cf\u7ea7ROC AUC\u4fdd\u6301\u5f3a\u5065\u3002\u5728MVTec\u5341\u4e94\u6708\u7c7b\u522b\u5e73\u5747\u4e0a\uff0c\u50cf\u7d20\u7ea7ROC AUC\u75310.733\u63d0\u5347\u81f30.751\uff0cPRO AUC\u75310.417\u63d0\u5347\u81f30.468\uff0c\u5355GPU\u63a8\u7406\u901f\u5ea6\u7ea6\u4e3a20 FPS\u3002", "conclusion": "D3R-Net\u6846\u67b6\u80fd\u5728\u4e0d\u4f9d\u8d56\u5927\u6a21\u578b\u7279\u5f81\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u4e2d\u7684\u5f02\u5e38\u5206\u5272\u548c\u5b9a\u4f4d\u6548\u679c\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00425", "abs": "https://arxiv.org/abs/2602.00425", "authors": ["Siyuan Wang", "Yanchen Liu", "Xiang Ren"], "title": "Segment-Level Attribution for Selective Learning of Long Reasoning Traces", "comment": "Accepted to ICLR 2026. 16 pages, 5 figures. Code available at https://github.com/SiyuanWangw/SegmentSelectiveSFT", "summary": "Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \\textit{attribution strength} measures the overall attribution magnitude; and (2) \\textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f52\u56e0\u65b9\u6cd5\u8bc6\u522b\u63a8\u7406\u957f\u94fe\u4e2d\u6709\u6548\u4fe1\u606f\u6bb5\u843d\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u91cd\u8981\u6bb5\u843d\u8fdb\u884c\u9009\u62e9\u6027\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u51c6\u786e\u6027\u548c\u8f93\u51fa\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u957f\u63a8\u7406\u94fe\u5305\u542b\u5927\u91cf\u5197\u4f59\u3001\u4e0d\u91cd\u8981\u7684\u5185\u5bb9\uff0c\u8fd9\u4e9b\u5197\u4f59\u5185\u5bb9\u7ecf\u6709\u76d1\u7763\u5fae\u8c03\u540e\u88ab\u6a21\u578b\u5b66\u4e60\u5e76\u6a21\u4eff\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b9\u6cd5\u6392\u9664\u65e0\u7528\u5185\u5bb9\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u9645\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u96c6\u6210\u68af\u5ea6\u5f52\u56e0\uff08Integrated Gradient Attribution\uff09\u91cf\u5316\u6bcf\u4e2atoken\u5bf9\u6700\u7ec8\u7b54\u6848\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u5176\u805a\u5408\u4e3a\u4e24\u4e2a\u7247\u6bb5\u7ea7\u6307\u6807\uff1a\uff081\uff09\u5f52\u56e0\u5f3a\u5ea6\uff08attribution strength\uff09\u8868\u793a\u7247\u6bb5\u7684\u91cd\u8981\u6027\uff1b\uff082\uff09\u65b9\u5411\u4e00\u81f4\u6027\uff08direction consistency\uff09\u8861\u91cf\u5f52\u56e0\u65b9\u5411\u7684\u7edf\u4e00\u6027\u3002\u57fa\u4e8e\u8fd9\u4e24\u4e2a\u6307\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7247\u6bb5\u7ea7\u9009\u62e9\u6027\u5b66\u4e60\u6846\u67b6\u2014\u2014\u5bf9\u5177\u6709\u9ad8\u5f52\u56e0\u5f3a\u5ea6\u4f46\u4e00\u81f4\u6027\u4e2d\u7b49\u7684\u7247\u6bb5\uff08\u4ee3\u8868\u66f4\u6df1\u5c42\u63a8\u7406\uff09\u8fdb\u884c\u6709\u9009\u62e9\u7684SFT\u8bad\u7ec3\uff0c\u5176\u4f59\u5219\u906e\u853d\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\u53ca\u8f93\u51fa\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u5f52\u56e0\u6307\u6807\u7684\u7247\u6bb5\u7ea7\u9009\u62e9\u6027\u8bad\u7ec3\u80fd\u591f\u6709\u6548\u6316\u6398\u548c\u653e\u5927\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6709\u6548\u4fe1\u606f\uff0c\u51cf\u5c11\u5197\u4f59\u5185\u5bb9\uff0c\u8fdb\u800c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u957f\u63a8\u7406\u94fe\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2602.00886", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00886", "abs": "https://arxiv.org/abs/2602.00886", "authors": ["Amitesh Vatsa", "Zhixian Xie", "Wanxin Jin"], "title": "RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback", "comment": null, "summary": "Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5RoDiF\uff0c\u80fd\u5728\u5b58\u5728\u9519\u8bef\u4eba\u7c7b\u504f\u597d\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6269\u6563\u7b56\u7565\u8fdb\u884c\u7a33\u5065\u7684\u5fae\u8c03\uff0c\u4e14\u4f18\u4e8e\u5df2\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u7684\u6269\u6563\u7b56\u7565\u96be\u4ee5\u5229\u7528\u4eba\u7c7b\u504f\u597d\u4fe1\u606f\u8fdb\u884c\u4f18\u5316\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u7684\u591a\u6b65\u7ed3\u6784\uff0c\u5c24\u5176\u5728\u504f\u597d\u6570\u636e\u5b58\u5728\u9519\u8bef\u6807\u7b7e\u65f6\u66f4\u96be\u5904\u7406\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6846\u67b6\uff0c\u5c06\u6269\u6563\u53bb\u566a\u94fe\u4e0e\u73af\u5883\u52a8\u529b\u5b66\u7ed3\u5408\uff0c\u5b9e\u73b0\u65e0\u5956\u52b1\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6587\u4e2d\u63d0\u51faRoDiF\u7b97\u6cd5\uff0c\u5c06DPO\u76ee\u6807\u91cd\u65b0\u89e3\u91ca\u4e3a\u51e0\u4f55\u5047\u8bbe\u5207\u5272\u89c6\u89d2\uff0c\u5e76\u91c7\u7528\u4fdd\u5b88\u7684\u5207\u5272\u7b56\u7565\u6765\u63d0\u5347\u5bf9\u9519\u8bef\u504f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5047\u5b9a\u566a\u58f0\u5206\u5e03\u3002", "result": "RoDiF\u5728\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u504f\u597d\u6807\u7b7e\u670930%\u9519\u8bef\u7684\u6761\u4ef6\u4e0b\uff0c\u4e5f\u80fd\u7a33\u5b9a\u9ad8\u6548\u5730\u5fae\u8c03\u4e0d\u540c\u7ed3\u6784\u7684\u9884\u8bad\u7ec3\u6269\u6563\u7b56\u7565\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "RoDiF\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u4f18\u5316\u7684\u65b0\u9014\u5f84\uff0c\u52a0\u5f3a\u4e86\u7b97\u6cd5\u5bf9\u6b64\u7c7b\u566a\u58f0\u5e72\u6270\u4e0b\u7684\u8868\u73b0\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00131", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00131", "abs": "https://arxiv.org/abs/2602.00131", "authors": ["Fraser Robinson", "Souren Pashangpour", "Matthew Lisondra", "Goldie Nejat"], "title": "PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living", "comment": "Submitted to Advanced Robotics (Taylor & Francis)", "summary": "A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86POVNet+\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u5bf9\u591a\u79cd\u65e5\u5e38\u6d3b\u52a8\uff08ADLs\uff09\u7684\u8bc6\u522b\uff0c\u4ece\u800c\u4e3b\u52a8\u63d0\u4f9b\u8f85\u52a9\u3002", "motivation": "\u957f\u671f\u90e8\u7f72\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u7684\u4e3b\u8981\u96be\u9898\u5728\u4e8e\u673a\u5668\u4eba\u65e0\u6cd5\u611f\u77e5\u53ca\u8f85\u52a9\u591a\u9879ADL\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u7a81\u7834\u6d3b\u52a8\u8bc6\u522b\u7684\u969c\u788d\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u771f\u5b9e\u5730\u4e3a\u7528\u6237\u591a\u65b9\u4f4d\u8f85\u52a9\u3002", "method": "POVNet+\u7ed3\u5408ADL\u548c\u52a8\u4f5c\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u533a\u5206\u5df2\u77e5ADL\u3001\u65b0\u51fa\u73b0\u7684ADL\uff0c\u4ee5\u53ca\u975e\u5178\u578b\u6267\u884c\u7684ADL\u3002\u5e76\u5229\u7528\u521b\u65b0\u7528\u6237\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e8e\u52a8\u4f5c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76d1\u6d4b\u65b0\u6d3b\u52a8\u548c\u7528\u6237\u8868\u73b0\uff0c\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u4e3b\u52a8\u53d1\u8d77\u673a\u5668\u4eba\u8f85\u52a9\u884c\u4e3a\u3002", "result": "\u4e0e\u5148\u8fdb\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u5bf9\u6bd4\uff0cPOVNet+\u5728ADL\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u66f4\u9ad8\u3002\u5728\u591a\u7528\u6237\u3001\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\uff0c\u534f\u52a9\u673a\u5668\u4ebaLeia\u5b9e\u9a8c\u8868\u660e\u8be5\u67b6\u6784\u80fd\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u5e38\u89c1\u548c\u65b0\u51fa\u73b0\u7684ADL\u53ca\u5176\u5f02\u5e38\u6267\u884c\uff0c\u540c\u65f6\u5408\u7406\u53d1\u8d77\u8f85\u52a9\u4e92\u52a8\u3002", "conclusion": "POVNet+\u4e3a\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u611f\u77e5\u548c\u8f85\u52a9\u590d\u6742\u591a\u6837\u7684\u65e5\u5e38\u6d3b\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4e3b\u52a8\u8f85\u52a9\u80fd\u529b\uff0c\u6709\u671b\u63a8\u52a8\u5176\u957f\u671f\u771f\u5b9e\u573a\u666f\u90e8\u7f72\u3002"}}
{"id": "2602.00428", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00428", "abs": "https://arxiv.org/abs/2602.00428", "authors": ["Naen Xu", "Hengyu An", "Shuo Shi", "Jinghuai Zhang", "Chunyi Zhou", "Changjiang Li", "Tianyu Du", "Zhihui Fu", "Jun Wang", "Shouling Ji"], "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems", "comment": "ICLR 2026", "summary": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u8d4b\u80fd\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u88ab\u7fa4\u4f53\u6027\u8ba4\u77e5\u504f\u5dee\uff08\u5982\u66fc\u5fb7\u62c9\u6548\u5e94\uff09\u5f71\u54cd\u7684\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u8bc4\u6d4b\u57fa\u51c6\u4e0e\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u5176\u5728\u534f\u4f5c\u4e2d\u6613\u53d7\u96c6\u4f53\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u201c\u66fc\u5fb7\u62c9\u6548\u5e94\u201d\uff0c\u5bfc\u81f4\u96c6\u4f53\u6027\u8bb0\u5fc6\u9519\u8bef\u548c\u8bef\u4fe1\u606f\u4f20\u64ad\u3002\u672c\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u65e2\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u4e5f\u5e26\u6765\u4f26\u7406\u98ce\u9669\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86MANBENCH\u57fa\u51c6\uff0c\u9488\u5bf9\u56db\u7c7b\u6613\u53d7\u66fc\u5fb7\u62c9\u6548\u5e94\u5f71\u54cd\u7684\u4efb\u52a1\u548c\u4e94\u79cd\u4e0d\u540c\u4e92\u52a8\u534f\u8bae\uff0c\u7cfb\u7edf\u8bc4\u6d4bLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7684\u96c6\u4f53\u8bb0\u5fc6\u504f\u5dee\uff1b\u5e76\u8bbe\u8ba1\u4e86\u8bf8\u5982\u8ba4\u77e5\u5b9a\u4f4d\u3001\u4fe1\u606f\u6765\u6e90\u68c0\u9a8c\u7b49\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u53ca\u6a21\u578b\u5bf9\u9f50\u7b49\u9632\u5fa1\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u5728MANBENCH\u5e73\u53f0\u4e0a\u5bf9\u4e0d\u540cLLMs\u80fd\u529b\u8bc4\u6d4b\uff0c\u91cf\u5316\u4e86\u66fc\u5fb7\u62c9\u6548\u5e94\u7684\u53d1\u751f\u548c\u6210\u56e0\uff1b\u901a\u8fc7\u63d0\u793a\u5c42\u548c\u6a21\u578b\u5c42\u9632\u5fa1\u63aa\u65bd\uff0c\u6709\u6548\u51cf\u7f13\u8fd9\u4e00\u6548\u5e94\uff0c\u5e73\u5747\u964d\u4f4e74.40%\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3\u548c\u51cf\u5c11LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u504f\u5dee\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u4e0e\u5b9e\u7528\u9632\u5fa1\u65b9\u6cd5\uff0c\u5bf9\u63d0\u5347\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u4f26\u7406\u5408\u89c4\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00915", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00915", "abs": "https://arxiv.org/abs/2602.00915", "authors": ["Zhiyuan Wu", "Xiangyu Zhang", "Zhuo Chen", "Jiankang Deng", "Rolandos Alexandros Potamias", "Shan Luo"], "title": "UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation", "comment": null, "summary": "Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \\textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faUniMorphGrasp\uff0c\u901a\u8fc7\u5f15\u5165\u624b\u90e8\u5f62\u6001\u4fe1\u606f\u548c\u7edf\u4e00\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u7ed3\u6784\u673a\u68b0\u624b\u7684\u901a\u7528\u6293\u53d6\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u548c\u96f6\u6837\u672c\u6cdb\u5316\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u7075\u5de7\u6293\u53d6\u65b9\u6cd5\u5927\u591a\u53ea\u9002\u7528\u4e8e\u7279\u5b9a\u7684\u673a\u5668\u4eba\u624b\u7ed3\u6784\uff0c\u9762\u5bf9\u65b0\u578b\u6216\u672a\u89c1\u8fc7\u7684\u673a\u68b0\u624b\u65f6\u6cdb\u5316\u80fd\u529b\u5f88\u5f31\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4ee5\u6269\u6563\u6a21\u578b\u4e3a\u6838\u5fc3\u7684UniMorphGrasp\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u4e0d\u540c\u673a\u68b0\u624b\u7684\u6293\u53d6\u52a8\u4f5c\u7edf\u4e00\u6620\u5c04\u5230\u7c7b\u4f3c\u4eba\u624b\u7684\u6807\u51c6\u59ff\u6001\u7a7a\u95f4\u4e2d\uff0c\u518d\u7ed3\u5408\u7531\u673a\u68b0\u624b\u7ed3\u6784\u751f\u6210\u7684\u56fe\u8868\u793a\u548c\u7269\u4f53\u51e0\u4f55\u4fe1\u606f\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u8fdb\u884c\u6293\u53d6\u751f\u6210\u3002\u540c\u65f6\uff0c\u5229\u7528\u624b\u90e8\u8fd0\u52a8\u5b66\u7684\u5c42\u6b21\u7ed3\u6784\u4fe1\u606f\u8bbe\u8ba1\u76d1\u7763\u635f\u5931\uff0c\u5f15\u5bfc\u66f4\u7cbe\u7ec6\u7684\u5173\u8282\u63a7\u5236\u5b66\u4e60\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u7075\u5de7\u6293\u53d6\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6548\u679c\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u8fc7\u673a\u68b0\u624b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "UniMorphGrasp\u663e\u8457\u63d0\u5347\u4e86\u673a\u68b0\u624b\u8de8\u7ed3\u6784\u6293\u53d6\u7684\u7edf\u4e00\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7075\u5de7\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00132", "abs": "https://arxiv.org/abs/2602.00132", "authors": ["Jiao Li", "Jian Lang", "Xikai Tang", "Wenzheng Shu", "Ting Zhong", "Qiang Gao", "Yong Wang", "Leiting Chen", "Fan Zhou"], "title": "Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation", "comment": "Accepted by AAAI2026 main track", "summary": "Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u4ec7\u6068\u89c6\u9891\u5185\u5bb9\u5728\u5206\u5e03\u548c\u8868\u73b0\u5f62\u5f0f\u4e0a\u4e0d\u65ad\u53d8\u5316\u5bfc\u81f4\u68c0\u6d4b\u6a21\u578b\u5931\u6548\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u4e3a\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\uff08HVD\uff09\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u6846\u67b6SCANNER\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e25\u91cd\u8bed\u4e49\u6f02\u79fb\u4e0b\u7684\u68c0\u6d4b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u636e\u5206\u5e03\u4e00\u81f4\uff0c\u7136\u800c\u4ec7\u6068\u5185\u5bb9\u4f1a\u4e0d\u65ad\u6f14\u5316\u4ee5\u89c4\u907f\u5ba1\u67e5\uff0c\u5bfc\u81f4\u8bed\u4e49\u53d8\u5316\u663e\u8457\uff0c\u539f\u6709\u6a21\u578b\u8868\u73b0\u5927\u5e45\u4e0b\u964d\u3002\u4f20\u7edfTTA\u65b9\u6cd5\u4ec5\u5e94\u5bf9\u8f7b\u5fae\u5206\u5e03\u6f02\u79fb\uff0c\u96be\u4ee5\u5904\u7406HVD\u573a\u666f\u4e0b\u7684\u4e25\u91cd\u8bed\u4e49\u6f02\u79fb\u3002", "method": "SCANNER\u4ee5\u4ec7\u6068\u5185\u5bb9\u6df1\u5c42\u6838\u5fc3\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\u7b49\u4e3a\u653b\u51fb\u76ee\u6807\uff09\u5177\u6709\u7a33\u5b9a\u6027\u4e3a\u51fa\u53d1\u70b9\uff0c\u901a\u8fc7\u4e2d\u5fc3\u5f15\u5bfc\u7684\u5bf9\u9f50\u673a\u5236\u63ed\u793a\u8fdb\u5316\u540e\u5185\u5bb9\u4e2d\u7684\u7a33\u5b9a\u6838\u5fc3\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7c07\u5185\u5bf9\u9f50\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\uff0c\u63d0\u5347\u9002\u5e94\u80fd\u529b\u3001\u5e94\u5bf9\u5f02\u5e38\u6837\u672c\u3001\u51cf\u5c11\u8bed\u4e49\u584c\u7f29\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSCANNER\u5728\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u5747\u53d6\u5f97\u66f4\u4f18\u8868\u73b0\uff0cMacro-F1\u5747\u503c\u63d0\u53474.69%\u3002", "conclusion": "SCANNER\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u4e2d\u7684\u4e25\u91cd\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u9886\u57df\u4e0b\u5177\u5907\u66f4\u5f3a\u7684\u9002\u5e94\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.00459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00459", "abs": "https://arxiv.org/abs/2602.00459", "authors": ["Yongxin Zhou", "Changshun Wu", "Philippe Mulhem", "Didier Schwab", "Maxime Peyrard"], "title": "What Matters to an LLM? Behavioral and Computational Evidences from Summarization", "comment": "Findings of EACL 2026", "summary": "Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.", "AI": {"tldr": "\u672c\u6587\u63a2\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u9009\u62e9\u4fe1\u606f\u7684\u5185\u5728\u6807\u51c6\uff0c\u901a\u8fc7\u884c\u4e3a\u548c\u8ba1\u7b97\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u91cd\u8981\u6027\u5224\u522b\u7684\u89c4\u5f8b\u548c\u673a\u5236\u3002", "motivation": "\u867d\u7136LLM\u5728\u6587\u672c\u6458\u8981\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5982\u4f55\u5224\u5b9a\u54ea\u4e9b\u4fe1\u606f\u91cd\u8981\u4ecd\u662f\u4e0d\u900f\u660e\u7684\uff0c\u7406\u89e3\u8fd9\u4e00\u8fc7\u7a0b\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u751f\u6210\u591a\u79cd\u957f\u5ea6\u53d7\u63a7\u7684\u6458\u8981\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u4ece\u6a21\u578b\u9009\u62e9\u54ea\u4e9b\u4fe1\u606f\u5355\u4f4d\u51fa\u73b0\u7684\u9891\u7387\u5f52\u7eb3\u51fa\u201c\u91cd\u8981\u6027\u5206\u5e03\u201d\uff1b\u6b64\u5916\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7279\u522b\u662f\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u4ee5\u53ca\u4e2d\u540e\u5c42\u7684\u8868\u5f81\uff0c\u63a2\u8ba8\u5176\u4e0e\u5916\u90e8\u91cd\u8981\u6027\u5206\u5e03\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLM\u5728\u4fe1\u606f\u7b5b\u9009\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u4e14\u72ec\u7279\u7684\u201c\u91cd\u8981\u6027\u6a21\u5f0f\u201d\uff0c\u8fd9\u4e00\u70b9\u660e\u663e\u533a\u522b\u4e8e\u65e7\u6709\u57fa\u7ebf\uff0c\u4e14\u4e0d\u540cLLM\u5bb6\u65cf\u7684\u9009\u62e9\u98ce\u683c\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u4e3a\u7c7b\u4f3c\u3002\u53e6\u5916\uff0c\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u548c\u4e2d\u540e\u5c42\u7684\u8868\u73b0\uff0c\u4e0e\u5b9e\u8bc1\u91cd\u8981\u6027\u5206\u5e03\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "LLM\u5728\u6458\u8981\u65f6\u6709\u7a33\u5b9a\u7684\u5185\u5728\u4fe1\u606f\u4f18\u5148\u7ea7\u673a\u5236\uff0c\u53ef\u501f\u52a9\u5185\u5c42\u6ce8\u610f\u529b\u7b49\u8868\u5f81\u624b\u6bb5\u52a0\u4ee5\u89e3\u91ca\u3002\u8fd9\u4e3a\u7406\u89e3\u3001\u89e3\u91ca\u53ca\u672a\u6765\u63a7\u5236\u6a21\u578b\u7684\u4fe1\u606f\u7b5b\u9009\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.00919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00919", "abs": "https://arxiv.org/abs/2602.00919", "authors": ["I. Apanasevich", "M. Artemyev", "R. Babakyan", "P. Fedotova", "D. Grankin", "E. Kupryashin", "A. Misailidi", "D. Nerus", "A. Nutalapati", "G. Sidorov", "I. Efremov", "M. Gerasyov", "D. Pikurov", "Y. Senchenko", "S. Davidenko", "D. Kulikov", "M. Sultankin", "K. Askarbek", "O. Shamanin", "D. Statovoy", "E. Zalyaev", "I. Zorin", "A. Letkin", "E. Rusakov", "A. Silchenko", "V. Vorobyov", "S. Sobolnikov", "A. Postnikov"], "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "comment": "22 pages, 14 figures", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6Green-VLA\uff0c\u5b9e\u73b0\u4e86\u5728Green\u4eba\u5f62\u673a\u5668\u4eba\u7b49\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u7684\u6cdb\u5316\u548c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\uff0c\u53d6\u5f97\u4e86\u5f3a\u6cdb\u5316\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u9700\u8981\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u786c\u4ef6\u548c\u73af\u5883\u4e2d\u6cdb\u5316\uff0c\u540c\u65f6\u4fdd\u969c\u5b89\u5168\u6027\u548c\u9ad8\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u3001\u591a\u6a21\u6001\u7406\u89e3\u548c\u591a\u6837\u673a\u5668\u4eba\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "Green-VLA\u91c7\u7528\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5305\u62ec\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u57fa\u7840\u5bf9\u9f50\u3001\u591a\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u3001\u7279\u5b9a\u673a\u5668\u4eba\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6821\u51c6\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e863,000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u7edf\u4e00\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u63a5\u53e3\uff0c\u4ee5\u53ca\u591a\u7ef4\u63a8\u7406\u65f6\u5b89\u5168\u4e0e\u6307\u5bfc\u673a\u5236\u3002", "result": "\u5728\u4e0d\u540c\u4eff\u771f\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5305\u62ecWidowX\u3001CALVIN\u7b49\uff09\u5927\u91cf\u5b9e\u9a8c\uff0cGreen-VLA\u5728\u6210\u529f\u7387\u3001\u5065\u58ee\u6027\u548c\u957f\u7a0b\u4efb\u52a1\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u63d0\u5347\u663e\u8457\u3002", "conclusion": "Green-VLA\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u591a\u673a\u5668\u4eba\u6cdb\u5316\u63a7\u5236\uff0c\u4e3a\u73b0\u5b9e\u673a\u5668\u4eba\u5e7f\u6cdb\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00135", "abs": "https://arxiv.org/abs/2602.00135", "authors": ["Pengcheng Zheng", "Chaoning Zhang", "Jiarong Mo", "GuoHui Li", "Jiaquan Zhang", "Jiahao Zhang", "Sihan Cao", "Sheng Zheng", "Caiyan Qin", "Guoqing Wang", "Yang Yang"], "title": "LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models", "comment": "Accepted by ICLR 2026", "summary": "Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u538b\u7f29\u65b9\u6cd5LLaVA-FA\uff0c\u901a\u8fc7\u5728\u9891\u57df\u5185\u8054\u5408\u4f4e\u79e9\u5206\u89e3\u548c\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347\u6743\u91cd\u91cf\u5316\u7684\u7d27\u51d1\u6027\u548c\u51c6\u786e\u6027\uff0c\u7ed3\u5408\u6781\u5750\u6807\u91cf\u5316\u4e0e\u5bf9\u89d2\u6821\u51c6\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LMMs\u5728\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u53d6\u5f97\u5353\u8d8a\u8868\u73b0\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u5c06\u4f4e\u79e9\u5206\u89e3\u548c\u91cf\u5316\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u8de8\u6a21\u6001\u5197\u4f59\u7684\u591a\u6a21\u6001\u7ed3\u6784\u4e2d\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002\u56e0\u800c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u5347\u6a21\u578b\u6548\u7387\u3001\u51cf\u5c11\u8bef\u5dee\u7684\u8054\u5408\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLLaVA-FA\u65b9\u6cd5\uff0c\u5728\u9891\u57df\u4e0b\u5bf9\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u8054\u5408\u4f4e\u79e9\u5206\u89e3\u548c\u91cf\u5316\uff0c\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u7684\u53bb\u76f8\u5173\u4e0e\u5171\u8f6d\u5bf9\u79f0\u7279\u6027\uff0c\u589e\u5f3a\u6743\u91cd\u8868\u8fbe\u7684\u7d27\u51d1\u6027\u4e0e\u51c6\u786e\u6027\u3002\u540c\u65f6\u63d0\u51faPolarQuant\u6781\u5750\u6807\u91cf\u5316\u65b9\u6cd5\uff0c\u4e13\u4e3a\u590d\u6570\u77e9\u9635\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165\u53ef\u9009\u7684\u5bf9\u89d2\u6821\u51c6\u673a\u5236\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6821\u51c6\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLLaVA-FA\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u538b\u7f29\u540e\u6fc0\u6d3b\u53c2\u6570\u66f4\u5c11\u3001\u8ba1\u7b97\u91cf\u66f4\u4f4e\uff0c\u4f46\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "LLaVA-FA\u7efc\u5408\u4e86\u9891\u57df\u4f4e\u79e9\u5206\u89e3\u4e0e\u91cf\u5316\u7684\u4f18\u52bf\uff0c\u662f\u9ad8\u6548\u538b\u7f29\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6709\u6548\u65b0\u65b9\u6848\uff0c\u53ef\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2602.00469", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00469", "abs": "https://arxiv.org/abs/2602.00469", "authors": ["Abhinav Gupta", "Toben H. Mintz", "Jesse Thomason"], "title": "Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations", "comment": "5 pages, 2 figures, codebase can be found at: https://github.com/abhinav-usc/SENSE-model/tree/main", "summary": "While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\\text{SENSE}$ $(\\textbf{S}\\text{ensorimotor }$ $\\textbf{E}\\text{mbedding }$ $\\textbf{N}\\text{orm }$ $\\textbf{S}\\text{coring }$ $\\textbf{E}\\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSENSE\u7684\u6a21\u578b\uff0c\u5c06\u8bcd\u8bed\u7684\u8bcd\u5d4c\u5165\u6295\u5c04\u5230\u611f\u89c9\u8fd0\u52a8\u89c4\u8303\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u8bcd\u6c47\u4e0e\u611f\u89c9\u8fd0\u52a8\u4f53\u9a8c\u4e4b\u95f4\u7684\u7cfb\u7edf\u8054\u7cfb\u3002", "motivation": "\u4f20\u7edf\u8bcd\u5d4c\u5165\u4e3b\u8981\u57fa\u4e8e\u8bcd\u5171\u73b0\u5173\u7cfb\u83b7\u5f97\u8bcd\u4e49\uff0c\u4f46\u4eba\u7c7b\u7406\u89e3\u8bed\u8a00\u8fd8\u4f9d\u8d56\u4e8e\u611f\u89c9\u548c\u52a8\u4f5c\u7ecf\u9a8c\u3002\u4e3a\u8ba9\u8ba1\u7b97\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u65b9\u5f0f\uff0c\u9700\u8981\u5c06\u611f\u89c9\u8fd0\u52a8\u4fe1\u606f\u5f15\u5165\u8bcd\u8868\u793a\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SENSE\u6a21\u578b\uff0c\u5373\u4f20\u611f\u8fd0\u52a8\u5d4c\u5165\u89c4\u8303\u8bc4\u5206\u5f15\u64ce\uff0c\u7528\u5b66\u4e60\u5230\u7684\u6295\u5f71\u5c06\u8bcd\u5d4c\u5165\u8f6c\u6362\u4e3aLancaster\u611f\u89c9\u8fd0\u52a8\u89c4\u8303\u5206\u6570\u3002\u53e6\u5916\uff0c\u8bbe\u8ba1\u4e86\u884c\u4e3a\u5b9e\u9a8c\uff0c\u8ba9281\u540d\u53c2\u4e0e\u8005\u57fa\u4e8e\u81ea\u521b\u8bcd\u8bed\u9009\u62e9\u4e0e\u7279\u5b9a\u611f\u89c9\u8fd0\u52a8\u76f8\u5173\u7684\u8bcd\uff0c\u7ed3\u5408SENSE\u6253\u5206\u8fdb\u4e00\u6b65\u5206\u6790\u3002", "result": "\u884c\u4e3a\u5b9e\u9a8c\u53d1\u73b0\uff0c\u53c2\u4e0e\u8005\u9009\u62e9\u4e0eSENSE\u8bc4\u5206\u572811\u79cd\u611f\u89c9\u8fd0\u52a8\u6a21\u6001\u4e2d\u76846\u79cd\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\u3002\u5bf9\u81ea\u521b\u8bcd\u6c47\u7684\u5b50\u8bcd\u7d20\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u4e0e\u5185\u611f\u53d7\u76f8\u5173\u7684\u7cfb\u7edf\u97f3\u7d20-\u610f\u4e49\u6a21\u5f0f\u3002", "conclusion": "SENSE\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u8bcd\u7684\u611f\u89c9\u8fd0\u52a8\u5c5e\u6027\uff0c\u4e14\u884c\u4e3a\u5b9e\u9a8c\u652f\u6301\u8bcd\u6c47\u4e0e\u611f\u89c9\u8fd0\u52a8\u4f53\u9a8c\u4e4b\u95f4\u7684\u7cfb\u7edf\u8054\u7cfb\uff0c\u4e3a\u81ea\u52a8\u53d1\u73b0\u97f3\u7d20-\u610f\u4e49\u5bf9\u5e94\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00923", "abs": "https://arxiv.org/abs/2602.00923", "authors": ["Jincheng Wang", "Lingfan Bao", "Tong Yang", "Diego Martinez Plasencia", "Jianhao Jiao", "Dimitrios Kanoulas"], "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation", "comment": "Under review. 11 pages", "summary": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.", "AI": {"tldr": "SanD-Planner\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u548c\u4eff\u5c04B\u6837\u6761\u7684\u5c40\u90e8\u89c4\u5212\u65b9\u6cd5\uff0c\u53ef\u5728\u6781\u5c11\u4e13\u5bb6\u793a\u8303\u4e0b\u5b9e\u73b0\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u5bfc\u822a\u89c4\u5212\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u8d1f\u62c5\uff0c\u53d6\u5f97\u9886\u5148\u6548\u679c\u3002", "motivation": "\u5728\u9ad8\u5ea6\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u6781\u5176\u56f0\u96be\uff0c\u4e3b\u8981\u74f6\u9888\u4e3a\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6f14\u793a\u548c\u4f4e\u6548\u7684\u6570\u636e\u5229\u7528\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6837\u672c\u9700\u6c42\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "method": "\u63d0\u51faSanD-Planner\uff0c\u4e00\u79cd\u5728\u7ea6\u675fB\u6837\u6761\u7a7a\u95f4\u5185\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u6df1\u5ea6\u56fe\u50cf\u8f93\u5165\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u7684\u5c40\u90e8\u89c4\u5212\u5668\u3002\u65b9\u6cd5\u81ea\u7136\u4fdd\u8bc1\u8f93\u51fa\u5e8f\u5217\u7684\u5e73\u6ed1\u4e0e\u8bef\u5dee\u754c\uff0c\u5e76\u4e14\u878d\u5165\u57fa\u4e8eESDF\u7684\u5b89\u5168\u68c0\u67e5\u5668\uff0c\u7528\u660e\u786e\u7684\u5b89\u5168\u4e0e\u5b8c\u6210\u65f6\u95f4\u6307\u6807\u66ff\u4ee3\u590d\u6742\u7684\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\u3002\u5bf9\u6bd4\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u4e13\u5bb6\u6f14\u793a\u91cf\u3002", "result": "\u4ec5\u7528500\u7ec4\u8bad\u7ec3\uff08\u4e3a\u57fa\u7ebf\u6a21\u578b\u6f14\u793a\u89c4\u6a21\u76840.25%\uff09\uff0cSanD-Planner\u5728\u5f00\u653e\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e8690.1%\u7684\u4eff\u771f\u6210\u529f\u7387\u548c72.0%\u7684\u5ba4\u5185\u4eff\u771f\u6210\u529f\u7387\uff0c\u8fdc\u8d85\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u5e76\u80fd\u76f4\u63a5\u65e0\u5fae\u8c03\u8fc1\u79fb\u5230\u771f\u5b9e2D\u548c3D\u5b9e\u9a8c\u573a\u666f\u3002", "conclusion": "SanD-Planner\u6781\u5927\u63d0\u5347\u4e86\u5c40\u90e8\u89c4\u5212\u7b97\u6cd5\u7684\u6570\u636e\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4eba\u5de5\u793a\u8303\u6210\u672c\uff0c\u53d6\u5f97SOTA\u6027\u80fd\u5e76\u5177\u5907\u4f18\u826f\u8fc1\u79fb\u6027\u3002\u76f8\u5173\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4e5f\u5c06\u5f00\u6e90\uff0c\u4e3a\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2602.00144", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00144", "abs": "https://arxiv.org/abs/2602.00144", "authors": ["Xuan Rao", "Mingming Ha", "Bo Zhao", "Derong Liu", "Cesare Alippi"], "title": "Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers", "comment": null, "summary": "Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\\mathcal{O}(Cd^2)$ to $\\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \\ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u5206\u7c7b\u5668LR-RGDA\u548c\u8bad\u7ec3\u514d\u8d39\u5206\u5e03\u8865\u507f\u5668HopDC\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u5927\u89c4\u6a21\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u548c\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "motivation": "\u57fa\u4e8eViT\u7684\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u5728\u5206\u7c7b\u5668\u91cd\u5efa\u9636\u6bb5\u56e0\u4f9d\u8d56\u8fed\u4ee3SGD\u8bad\u7ec3\u800c\u9762\u4e34\u5de8\u5927\u8ba1\u7b97\u8d1f\u62c5\u3002\u867d\u7136\u5206\u6790\u6027RGDA\u80fd\u63d0\u4f9bBayes\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u4e8c\u6b21\u63a8\u7406\u590d\u6742\u5ea6\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4efb\u52a1\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u8bbe\u8ba1\u517c\u987e\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\u7684\u5206\u7c7b\u5668\u3002", "method": "1\uff09\u63d0\u51fa\u57fa\u4e8eWoodbury\u6052\u7b49\u5f0f\u7684\u4f4e\u79e9\u5206\u89e3RGDA\uff08LR-RGDA\uff09\u5206\u7c7b\u5668\uff0c\u5c06\u5224\u522b\u51fd\u6570\u5206\u89e3\u4e3a\u5168\u5c40\u4eff\u5c04\u9879\u4e0e\u4f4e\u79e9\u4e8c\u6b21\u6270\u52a8\u9879\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff1b2\uff09\u4e3a\u7f13\u89e3\u9aa8\u5e72\u7f51\u7edc\u66f4\u65b0\u5bfc\u81f4\u7684\u8868\u793a\u6f02\u79fb\uff0c\u63d0\u51fa\u57fa\u4e8eHopfield\u7f51\u7edc\u7684\u5206\u5e03\u8865\u507f\u5668\uff08HopDC\uff09\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u951a\u70b9\u6821\u51c6\u5386\u53f2\u7c7b\u522b\u7edf\u8ba1\uff0c\u5e76\u7ed9\u51fa\u4f30\u8ba1\u8bef\u5dee\u7406\u8bba\u754c\u3002", "result": "\u5728\u591a\u4e2a\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u57fa\u51c6\uff08\u5982ImageNet\u7b49\uff09\u4e0a\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LR-RGDA\u4e0eHopDC\u7684\u7ec4\u5408\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u65b0\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u8fed\u4ee3\u4f18\u5316\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u548c\u91cd\u8bad\u7ec3\u5f00\u9500\uff0c\u4e3a\u57fa\u4e8eViT\u7684\u5927\u89c4\u6a21\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00477", "abs": "https://arxiv.org/abs/2602.00477", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Intention-Adaptive LLM Fine-Tuning for Text Revision Generation", "comment": "In the Conference of the European Chapter of the Association for Computational Linguistics (EACL), March 2026", "summary": "Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Intention-Tuning \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728LLM\u4e2d\u6709\u9009\u62e9\u5730\u5fae\u8c03\u90e8\u5206\u5c42\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u201c\u610f\u56fe\u9a71\u52a8\u201d\u6539\u5199\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5728\u5c0f\u89c4\u6a21\u8bed\u6599\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5728\u6458\u8981\u751f\u6210\u3001\u63a8\u7406\u7b49\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u4efb\u52a1\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u660e\u786e\u53cd\u6620\u5199\u4f5c\u610f\u56fe\u7684\u6539\u5199\u751f\u6210\u7b49\u201c\u610f\u56fe\u9a71\u52a8\u201d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u8f83\u5c11\u3002\u610f\u56fe\u8bc6\u522b\u548c\u6309\u610f\u56fe\u8fdb\u884c\u6539\u5199\u751f\u6210\u5341\u5206\u590d\u6742\uff0c\u4e14\u591a\u610f\u56fe\u573a\u666f\u4e0b\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u610f\u56fe\u6307\u4ee4\u5fae\u8c03\u53c8\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86 Intention-Tuning \u6846\u67b6\uff1a\u8be5\u65b9\u6cd5\u5728LLM\u7684\u90e8\u5206\u5c42\u52a8\u6001\u9009\u62e9\u8fdb\u884c\u9488\u5bf9\u610f\u56fe\u7684\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u5728\u5b66\u4e60\u610f\u56fe\u65b9\u9762\u66f4\u6709\u6548\u7387\uff0c\u5e76\u80fd\u5c06\u8fd9\u79cd\u80fd\u529b\u8fc1\u79fb\u5230\u5b9e\u9645\u7684\u6587\u672c\u6539\u5199\u751f\u6210\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cIntention-Tuning \u5728\u5c0f\u89c4\u6a21\u6539\u5199\u8bed\u6599\u5e93\u4e0a\u6548\u679c\u4f18\u4e8e\u591a\u79cd\u4e3b\u6d41\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u6709\u6548\u3002", "conclusion": "Intention-Tuning \u6709\u6548\u89e3\u51b3\u4e86\u591a\u610f\u56fe\u6539\u5199\u96be\u9898\uff0c\u964d\u4f4e\u4e86\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\uff0c\u5728\u8d44\u6e90\u6709\u9650\u80cc\u666f\u4e0b\u63d0\u5347\u4e86LLM\u7684\u201c\u610f\u56fe\u611f\u77e5\u201d\u6587\u672c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2602.00935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00935", "abs": "https://arxiv.org/abs/2602.00935", "authors": ["Mohamed Sorour", "Barbara Webb"], "title": "Minimal Footprint Grasping Inspired by Ants", "comment": null, "summary": "Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.", "AI": {"tldr": "\u672c\u8bba\u6587\u501f\u9274\u8682\u8681\u524d\u8db3\u7ed3\u6784\uff0c\u8bbe\u8ba1\u51fa\u4e00\u79cd\u5177\u6709\u9ad8\u6469\u64e6\u529b\u548c\u4eff\u751f\u7ed3\u6784\u7684\u4f4e\u6210\u672c\u673a\u68b0\u5939\u6301\u5668\uff0c\u63d0\u5347\u4e86\u5728\u6742\u4e71\u73af\u5883\u4e0b\u6293\u53d6\u7269\u4f53\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u8bbe\u8ba1\u9ad8\u6548\u4e14\u53ef\u9760\u3002", "motivation": "\u73b0\u6709\u5939\u6301\u5668\u5728\u5e94\u5bf9\u5806\u79ef\u590d\u6742\u6216\u6742\u4e71\u7269\u4f53\u6293\u53d6\u65f6\u6027\u80fd\u6709\u9650\u3002\u89c2\u5bdf\u5230\u8682\u8681\u80fd\u9ad8\u6548\u5e94\u5bf9\u7c7b\u4f3c\u95ee\u9898\uff0c\u4e14\u5176\u524d\u8db3\u7ed3\u6784\uff08\u9ad8\u6469\u64e6\u57ab\u3001\u6bdb\u53d1\u3001\u67d4\u6027\u722a\u90e8\uff09\u8d77\u5230\u5173\u952e\u4f5c\u7528\uff0c\u56e0\u6b64\u5e0c\u671b\u501f\u9274\u5176\u539f\u7406\u6539\u8fdb\u5939\u6301\u5668\u8bbe\u8ba1\u3002", "method": "\u5c06\u8682\u8681\u524d\u8db3\u7684\u5fae\u89c2\u7ed3\u6784\u7279\u5f81\uff08\u9ad8\u6469\u64e6\u57ab\u3001\u8868\u9762\u6bdb\u53d1\u3001\u67d4\u6027\u672b\u7aef\uff09\u62bd\u8c61\u5316\uff0c\u8bbe\u8ba1\u51fa\u4eff\u751f\u5939\u6301\u5668\u3002\u8be5\u88c5\u7f6e\u91c7\u7528\u7ec6\u957f\u5e26\u9ad8\u6469\u64e6\u57ab\u7684\u524d\u81c2\uff0c\u4f4e\u6469\u64e6\u6bdb\u53d1\uff0c\u4ee5\u53ca\u7c7b\u4f3c\u5173\u8282\u7684\u7ed3\u6784\uff0c\u6a21\u62df\u8682\u8681\u524d\u8db3\u7684\u7075\u6d3b\u6027\u4e0e\u6469\u64e6\u7279\u6027\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u4eff\u751f\u5939\u6301\u5668\u80fd\u7a33\u5065\u6293\u53d6\u5404\u79cd\u6d88\u8d39\u54c1\uff0c\u6bcf\u6b21\u5c1d\u8bd5\u5747\u6210\u529f\u3002\u5728\u590d\u6742\u5bc6\u96c6\u7684\u73af\u5883\u4e0b\uff0c\u4e5f\u80fd\u9ad8\u6548\u5b8c\u6210\u5355\u4ef6\u6293\u53d6\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6742\u7269\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u5939\u6301\u5668\u901a\u8fc7\u6a21\u4eff\u8682\u8681\u524d\u8db3\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7269\u4f53\u6293\u53d6\u80fd\u529b\uff0c\u540c\u65f6\u9610\u660e\u4e86\u6606\u866b\u6bdb\u53d1\u7ed3\u6784\u4e0e\u5173\u8282\u67d4\u6027\u7684\u673a\u68b0\u5b66\u610f\u4e49\uff0c\u63a8\u52a8\u4e86\u5939\u6301\u4e0e\u4eff\u751f\u9886\u57df\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2602.00145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00145", "abs": "https://arxiv.org/abs/2602.00145", "authors": ["Siva Teja Kakileti", "Geetha Manjunath"], "title": "DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images", "comment": null, "summary": "Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86DensiThAI\uff0c\u4e00\u79cd\u5229\u7528\u7ea2\u5916\u70ed\u6210\u50cf\u548c\u591a\u89c6\u89d2\u6df1\u5ea6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u65e0\u9700X\u5c04\u7ebf\uff0c\u907f\u514d\u7535\u79bb\u8f90\u5c04\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u5177\u6709\u8f83\u597d\u8868\u73b0\u3002", "motivation": "\u4e73\u817a\u5bc6\u5ea6\u662f\u4e73\u817a\u764c\u98ce\u9669\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56X\u5c04\u7ebf\u6444\u5f71\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u7535\u79bb\u8f90\u5c04\u98ce\u9669\uff0c\u56e0\u6b64\u6025\u9700\u4e00\u79cd\u65e0\u521b\u3001\u65e0\u8f90\u5c04\u7684\u65b0\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e73\u817a\u5bc6\u5ea6\u3002", "method": "\u63d0\u51faDensiThAI\u591a\u89c6\u89d2\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u7ea2\u5916\u70ed\u6210\u50cf\u4e2d\u5206\u7c7b\u4e73\u817a\u5bc6\u5ea6\u3002\u5229\u75283,500\u540d\u5973\u6027\u7684\u591a\u4e2d\u5fc3\u6570\u636e\u548c\u4e73\u817a\u6444\u5f71\u6807\u7b7e\u4f5c\u4e3a\u53c2\u7167\uff0c\u901a\u8fc7\u4e94\u4e2a\u6807\u51c6\u70ed\u6210\u50cf\u89c6\u56fe\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "DensiThAI\u572810\u6b21\u968f\u673a\u5212\u5206\u4e2d\u8fbe\u5230\u4e86\u5e73\u5747AUROC\u4e3a0.73\uff0c\u4e14\u4e0d\u540c\u4e73\u817a\u5bc6\u5ea6\u7c7b\u522b\u4e4b\u95f4\u5177\u6709\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6027\uff08p<<0.05\uff09\uff1b\u5728\u4e0d\u540c\u5e74\u9f84\u7ec4\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u7ea2\u5916\u70ed\u6210\u50cf\u7ed3\u5408AI\u80fd\u591f\u8f83\u597d\u5730\u5b9e\u73b0\u4e73\u817a\u5bc6\u5ea6\u65e0\u521b\u8bc4\u4f30\uff0c\u5728\u63d0\u5347\u60a3\u8005\u4f53\u9a8c\u548c\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2602.00491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00491", "abs": "https://arxiv.org/abs/2602.00491", "authors": ["Zhaokun Yan", "Zhaohan Liu", "Wuzheng Dong", "Lijie Feng", "Chengxiao Dai"], "title": "From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas", "comment": null, "summary": "Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \\textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86GlobalHealthAtlas\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d615\u4e2a\u516c\u5171\u536b\u751f\u9886\u57df\u300117\u79cd\u8bed\u8a00\u3001\u517128\u4e07\u591a\u5b9e\u4f8b\u7684\u5927\u578b\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u5927\u6a21\u578b\u8f85\u52a9\u6d41\u7a0b\uff0c\u63a8\u52a8\u5b89\u5168\u76f8\u5173\u7684\u516c\u5171\u536b\u751f\u63a8\u7406\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u516c\u5171\u536b\u751f\u9886\u57df\u7684\u63a8\u7406\u8981\u6c42\u4f9d\u8d56\u79d1\u5b66\u8bc1\u636e\u3001\u4e13\u5bb6\u5171\u8bc6\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u4f46\u5c06\u5176\u4f5c\u4e3a\u7ed3\u6784\u5316\u673a\u5668\u5b66\u4e60\u95ee\u9898\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\uff0c\u73b0\u6709\u76d1\u7763\u4fe1\u53f7\u4e0e\u57fa\u51c6\u6570\u636e\u6709\u9650\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u6db5\u76d6\u591a\u9886\u57df\u3001\u5206\u96be\u5ea6\u7684\u5927\u578b\u6570\u636e\u96c6GlobalHealthAtlas\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u5927\u6a21\u578b\u7684\u6784\u5efa\u53ca\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\uff0c\u5305\u62ec\u4fe1\u606f\u68c0\u7d22\u3001\u53bb\u91cd\u3001\u8bc1\u636e\u6838\u67e5\u3001\u6807\u7b7e\u9a8c\u8bc1\u7b49\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ee5\u5927\u578b\u6a21\u578b\u9ad8\u7f6e\u4fe1\u8bc4\u5224\u84b8\u998f\u800c\u6765\u7684\u9886\u57df\u5bf9\u9f50\u8bc4\u4ef7\u5668\uff0c\u4ece\u516d\u4e2a\u7ef4\u5ea6\u5bf9\u8f93\u51fa\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6210\u529f\u63a8\u51fa\u4e86\u5305\u542b280,210\u4e2a\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u6d4b\u6d41\u7a0b\u548c\u9ad8\u4e00\u81f4\u6027\u7684\u6807\u7b7e\u53ca\u6570\u636e\u8d28\u91cf\u63a7\u5236\u4f53\u7cfb\u3002\u7528\u4e8e\u8bc4\u6d4b\u7684\u591a\u7ef4\u6307\u6807\u63d0\u5347\u4e86\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u8bc4\u4ef7\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002", "conclusion": "\u672c\u5de5\u4f5c\u586b\u8865\u4e86\u516c\u5171\u536b\u751f\u63a8\u7406\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u4f53\u7cfb\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u5927\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u7684\u516c\u5171\u536b\u751f\u9886\u57df\u63a8\u7406\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u4e3a\u65e5\u540e\u76f8\u5173\u76d1\u7763\u5b66\u4e60\u4e0e\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.00937", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00937", "abs": "https://arxiv.org/abs/2602.00937", "authors": ["I-Chun Arthur Liu", "Krzysztof Choromanski", "Sandy Huang", "Connor Schenck"], "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining", "comment": null, "summary": "Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u70b9\u4e91\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u4fe1\u606f\u76843D\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08CLAMP\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u5728\u4eff\u771f\u4e0e\u771f\u5b9e\u5404\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u76842D\u56fe\u50cf\u8868\u5f81\uff0c\u4f462D\u4fe1\u606f\u65e0\u6cd5\u5145\u5206\u523b\u753b\u64cd\u4f5c\u6240\u9700\u7684\u91cd\u89813D\u65f6\u7a7a\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u51653D\u7a7a\u95f4\u4fe1\u606f\u6539\u5584\u673a\u5668\u4eba\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u4f5c\u8005\u63d0\u51faCLAMP\u6846\u67b6\uff0c\u9996\u5148\u5c06RGB-D\u56fe\u50cf\u4e0e\u76f8\u673a\u5916\u53c2\u8f6c\u6362\u4e3a3D\u70b9\u4e91\uff0c\u751f\u6210\u5305\u542b\u6df1\u5ea6\u548c3D\u5750\u6807\u7684\u591a\u89c6\u89d2\u56db\u901a\u9053\u89c2\u6d4b\u56fe\u50cf\uff08\u542b\u52a8\u6001\u624b\u8155\u89c6\u89d2\uff09\uff0c\u4f7f\u6a21\u578b\u83b7\u5f97\u5173\u952e\u4e09\u7ef4\u51e0\u4f55\u548c\u4f4d\u7f6e\u7ebf\u7d22\u3002\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4eff\u771f\u8f68\u8ff9\u4e0a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u5e76\u5c06Diffusion Policy\u7528\u4e8e\u521d\u59cb\u5316\u7b56\u7565\u6743\u91cd\uff0c\u63d0\u5347\u5fae\u8c03\u9636\u6bb5\u7684\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u6027\u80fd\u3002\u6700\u540e\u5728\u6709\u9650\u7684\u771f\u5b9e\u6f14\u793a\u6570\u636e\u4e0a\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCLAMP\u80fd\u591f\u5927\u5e45\u63d0\u5347\u5bf9\u65b0\u4efb\u52a1\u7684\u5b66\u4e60\u901f\u5ea6\u548c\u7b56\u7565\u8868\u73b0\uff0c\u5e76\u5728\u516d\u9879\u4eff\u771f\u4efb\u52a1\u548c\u4e94\u9879\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5f15\u51653D\u591a\u89c6\u89d2\u52a8\u4f5c\u6761\u4ef6\u7f16\u7801\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\u6709\u52a9\u4e8e\u5145\u5206\u6316\u6398\u4e09\u7ef4\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u64cd\u63a7\u7684\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u81ea\u76d1\u7763\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.00148", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00148", "abs": "https://arxiv.org/abs/2602.00148", "authors": ["Shiqian Li", "Ruihong Shen", "Junfeng Ni", "Chang Pan", "Chi Zhang", "Yixin Zhu"], "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields", "comment": "43 pages, ICLR 2026", "summary": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5168\u65b0\u7684\u795e\u7ecf\u9ad8\u65af\u529b\u573a\uff08NGFF\uff09\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u591a\u89c6\u89d2RGB\u8f93\u5165\u5230\u7269\u7406\u771f\u5b9e\u7684\u89c6\u9891\u751f\u6210\uff0c\u6781\u5927\u63d0\u5347\u4e86\u6a21\u62df\u901f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524dAI\u5728\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u9884\u6d4b\u7269\u7406\u52a8\u6001\u65b9\u9762\u5c1a\u6709\u8f83\u5927\u6311\u6218\u3002\u867d\u7136\u89c6\u9891\u751f\u6210\u6a21\u578b\u6709\u8f83\u597d\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u7f3a\u5c11\u7269\u7406\u6cd5\u5219\u5efa\u6a21\uff0c\u5bfc\u81f4\u65e0\u6cd5\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u89c6\u9891\u3002\u7ed3\u54083D\u9ad8\u65af\u4e0e\u7269\u7406\u5f15\u64ce\u7684\u65b9\u6cd5\u867d\u7136\u7269\u7406\u4e0a\u66f4\u53ef\u4fe1\uff0c\u4f46\u56e0\u91cd\u5efa\u3001\u4eff\u771f\u5f00\u9500\u5927\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faNGFF\u795e\u7ecf\u9ad8\u65af\u529b\u573a\u6846\u67b6\uff0c\u5c063D\u9ad8\u65af\u611f\u77e5\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u7ed3\u5408\uff0c\u53ef\u7aef\u5230\u7aef\u751f\u6210\u4ea4\u4e92\u5f0f\u4e14\u7269\u7406\u771f\u5b9e\u76844D\u89c6\u9891\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u4eff\u771f\u901f\u5ea6\u3002\u4e3a\u8bad\u7ec3\u8be5\u6a21\u578b\uff0c\u8fd8\u6784\u5efa\u4e86GSCollision\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4e92\u52a8\u548c\u590d\u6742\u573a\u666f\u7684\u5927\u89c4\u6a214D\u9ad8\u65af\u7269\u7406\u89c6\u9891\u3002", "result": "NGFF\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u4e09\u7ef4\u573a\u666f\u7684\u8bc4\u6d4b\u4e2d\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u7269\u7406\u63a8\u7406\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4eff\u771f\u901f\u5ea6\u6bd4\u4ee5\u5f80\u9ad8\u65af\u6a21\u62df\u5668\u5feb\u4e24\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u89c6\u9891\u9884\u6d4b\u548c\u4e16\u754c\u5efa\u6a21\u7684\u53d1\u5c55\uff0c\u4e3a\u7269\u7406\u611f\u77e5\u3001\u63a8\u7406\u53ca\u76f8\u5173\u89c6\u89c9\u4efb\u52a1\u5e26\u6765\u65b0\u7684\u7a81\u7834\u3002"}}
{"id": "2602.00497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00497", "abs": "https://arxiv.org/abs/2602.00497", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design", "comment": null, "summary": "Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u672c\u5730\u6587\u5316\u4e0e\u8bed\u8a00\u8fb9\u7f18\u7fa4\u4f53\u9762\u4e34\u7cfb\u7edf\u6027\u6cbb\u7406\u98ce\u9669\uff0c\u73b0\u6709\u6cbb\u7406\u6846\u67b6\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u3002\u4f5c\u8005\u63d0\u51fa\u9700\u8981\u4ee5\u6587\u5316\u4e3a\u57fa\u7840\u7684\u6cbb\u7406\u65b9\u6cd5\u6765\u907f\u514d\u73b0\u6709\u7684\u4e0d\u5e73\u7b49\u56e0\u6280\u672f\u6269\u5927\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5e7f\u6cdb\u90e8\u7f72\u65f6\uff0c\u5e38\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u3001\u5ffd\u89c6\u591a\u5143\u6587\u5316\u4e0e\u8bed\u8a00\u73af\u5883\uff0c\u9020\u6210\u6570\u636e\u3001\u8d23\u4efb\u3001\u6a21\u578b\u884c\u4e3a\u4e0e\u672c\u5730\u9884\u671f\u4e0d\u7b26\uff0c\u7279\u522b\u5bf9\u4f4e\u8d44\u6e90\u548c\u8fb9\u7f18\u7fa4\u4f53\u52a0\u5267\u4e0d\u516c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7efc\u8ff0\u591a\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u3001\u6570\u636e\u4e0d\u5bf9\u79f0\u4e0e\u793e\u4f1a\u6280\u672f\u5371\u5bb3\u7684\u73b0\u6709\u8bc1\u636e\uff0c\u5229\u7528\u8de8\u6587\u5316\u89c6\u89d2\uff0c\u63d0\u51fa\u4e00\u4e2a\u6587\u5316\u6839\u57fa\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e09\u5927\u6311\u6218\u3002\u91cd\u70b9\u662f\u7406\u8bba\u548c\u6982\u5ff5\u6846\u67b6\u68b3\u7406\u800c\u975e\u6280\u672f\u5b9e\u73b0\u3002", "result": "\u7ed3\u679c\u4e3a\u5f52\u7eb3\u51fa\u4e09\u5927\u6cbb\u7406\u6311\u6218\uff1a1\uff09\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u4e2d\u7684\u6587\u5316\u4e0e\u8bed\u8a00\u4e0d\u516c\u5e73\uff0c2\uff09\u5168\u7403\u90e8\u7f72\u4e0e\u672c\u5730\u89c4\u8303\u4ef7\u503c\u9519\u4f4d\uff0c3\uff09\u5bf9\u53d7\u5bb3\u8fb9\u7f18\u7fa4\u4f53\u7684\u95ee\u8d23\u673a\u5236\u6709\u9650\u3002\u63d0\u51fa\u6846\u67b6\u548c\u8bae\u7a0b\u800c\u975e\u65b0\u6280\u672f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u591a\u8bed\u8a00AI\u6cbb\u7406\u5e94\u4ece\u793e\u4f1a\u6587\u5316\u4e0e\u6743\u5229\u51fa\u53d1\uff0c\u5f3a\u8c03\u6570\u636e\u6cbb\u7406\u3001\u900f\u660e\u5ea6\u548c\u53c2\u4e0e\u5f0f\u95ee\u8d23\uff0c\u9632\u6b62AI\u5728\u4e2d\u7acb\u6027\u7684\u8868\u8c61\u4e0b\u52a0\u5267\u5168\u7403\u4e0d\u5e73\u7b49\u3002"}}
{"id": "2602.00980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00980", "abs": "https://arxiv.org/abs/2602.00980", "authors": ["Yichen Cai", "Yuan Gao", "Pengpeng Li", "Wei Wang", "Guibin Sun", "Jinhu L\u00fc"], "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution", "comment": null, "summary": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5206\u5e03\u5f0f\u4e14\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\u7684\u96c6\u7fa4\u5f62\u72b6\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u8d28\u91cf\u5206\u5e03\u5efa\u6a21\u4e0e\u5206\u5e03\u5f0f\u63a7\u5236\u5f8b\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u5f62\u72b6\u91cd\u6784\u548c\u89c4\u6a21\u9002\u5e94\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bc6\u5ea6\u5206\u5e03\u7684\u96c6\u7fa4\u63a7\u5236\u65b9\u6cd5\u867d\u80fd\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\uff0c\u4f46\u5728\u590d\u6742\u5f62\u72b6\u8868\u8fbe\u548c\u5206\u6563\u5b9e\u73b0\u65b9\u9762\u5b58\u5728\u96be\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5b9e\u9645\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5b9a\u4e49\u8fde\u7eed\u5bc6\u5ea6\u51fd\u6570\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u65b9\u6cd5\uff0c\u517c\u987e\u590d\u6742\u5f62\u72b6\u5f62\u6210\u548c\u89c4\u6a21\u9002\u5e94\u80fd\u529b\u3002", "method": "\u9996\u5148\u4ee5\u4e00\u7ec4\u79bb\u6563\u91c7\u6837\u70b9\u5b9a\u4e49\u7fa4\u4f53\u7684\u8d28\u91cf\u5206\u5e03\u51fd\u6570\u7528\u4e8e\u5efa\u6a21\uff1b\u7136\u540e\u5229\u7528\u5206\u5e03\u5f0f\u5747\u503c\u6f02\u79fb\u63a7\u5236\u5f8b\uff0c\u7ed3\u5408\u5c40\u90e8\u8d28\u91cf\u4f30\u7b97\u53cd\u9988\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u96c6\u7fa4\u7684\u5206\u5e03\u5411\u91c7\u6837\u70b9\u5206\u5e03\u6536\u655b\u3002\u6240\u6709\u70b9\u7684\u8d28\u91cf\u4f30\u7b97\u7531\u673a\u5668\u4eba\u901a\u8fc7\u5206\u5e03\u5f0f\u4f30\u7b97\u5668\u534f\u4f5c\u5b8c\u6210\uff0c\u5e76\u8bc1\u660e\u4f30\u7b97\u80fd\u6e10\u8fd1\u6536\u655b\u5230\u5168\u5c40\u771f\u5b9e\u503c\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u5f62\u72b6\u6784\u5efa\u53ca\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\u4e0b\u7684\u9ad8\u6548\u7387\u548c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u65b9\u6848\u80fd\u6709\u6548\u5b9e\u73b0\u590d\u6742\u5f62\u72b6\u5f62\u6210\u5e76\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\uff0c\u4e14\u4e0d\u9700\u8981\u96be\u4ee5\u5b9a\u4e49\u7684\u8fde\u7eed\u5bc6\u5ea6\u51fd\u6570\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u7684\u673a\u5668\u4eba\u96c6\u7fa4\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00149", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00149", "abs": "https://arxiv.org/abs/2602.00149", "authors": ["Shucong Li", "Xiaoluo Zhou", "Yuqian He", "Zhenyu Liu"], "title": "SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles", "comment": null, "summary": "3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDCM\u7684\u57fa\u4e8e4D\u96f7\u8fbe\u4e0e\u89c6\u89c9\u878d\u5408\u7684\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a00\u758f\u96f7\u8fbe\u70b9\u4e91\u548c\u4f4e\u8d28\u89c6\u89c9\u6570\u636e\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "4D\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u5bfc\u81f4\u4e09\u7ef4\u8868\u5f81\u80fd\u529b\u5dee\uff0c\u89c6\u89c9\u4fe1\u606f\u5728\u5f31\u5149\u3001\u8fdc\u8ddd\u548c\u906e\u6321\u7b49\u573a\u666f\u6613\u9000\u5316\uff0c\u4f7f\u76ee\u6807\u68c0\u6d4b\u548c\u611f\u77e5\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faSDCM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a1\uff09SimDen\u6a21\u5757\u91c7\u75283D KDE\u548c\u9ad8\u65af/\u66f2\u7387\u6a21\u62df\u5bf9\u96f7\u8fbe\u70b9\u4e91\u8fdb\u884c\u7a20\u5bc6\u5316\uff1b2\uff09RCM\u6a21\u5757\u5229\u7528\u96f7\u8fbe\u5b9e\u65f6\u3001\u5168\u5929\u5019\u4f18\u52bf\u8865\u507f\u89c6\u89c9\u4fe1\u606f\u9000\u5316\uff1b3\uff09MMIF\u6a21\u5757\u5bf9\u591a\u6a21\u6001\u7279\u5f81\u5f20\u91cf\u5dee\u5f02\u5efa\u6a21\uff0c\u4fc3\u8fdb\u5f02\u8d28\u4fe1\u606f\u4ea4\u4e92\u4e0e\u878d\u5408\u3002", "result": "\u5728VoD\u3001TJ4DRadSet\u548cAstyx\u6570\u636e\u96c6\u4e0a\uff0cSDCM\u5728\u7cbe\u5ea6\u3001\u53c2\u6570\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\uff0c\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "SDCM\u6709\u6548\u89e3\u51b3\u4e864D\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u548c\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u96f7\u8fbe-\u89c6\u89c9\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\uff0c\u5177\u6709\u8f83\u597d\u7684\u5e94\u7528\u524d\u666f\u548c\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2602.00543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00543", "abs": "https://arxiv.org/abs/2602.00543", "authors": ["Seho Pyo", "Jiheon Seok", "Jaejin Lee"], "title": "Reasoning by Commented Code for Table Question Answering", "comment": null, "summary": "Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\\% accuracy on the WikiTableQuestions benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6ce8\u91ca\u3001\u9010\u6b65\u751f\u6210\u4ee3\u7801\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u8868\u683c\u95ee\u7b54(TableQA)\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u7ebf\u6027\u5316\u65b9\u6cd5\u4f1a\u7834\u574f\u6570\u636e\u7684\u4e8c\u7ef4\u7ed3\u6784\u5173\u7cfb\uff0c\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4f4e\u3001\u63a8\u7406\u4e0d\u900f\u660e\u3002\u73b0\u6709\u591a\u6570\u65b9\u6cd5\u7684\u6570\u503c\u51c6\u786e\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u5747\u4e0d\u7406\u60f3\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6b65\u3001\u5e26\u6ce8\u91ca\u7684Python\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684TableQA\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u591a\u884c\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u5728\u6bcf\u6b65\u4e2d\u52a0\u5165\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u5b9e\u73b0\u663e\u5f0f\u63a8\u7406\uff1b\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7b54\u6848\u7b5b\u9009\u673a\u5236\uff0c\u5c06\u8be5\u6846\u67b6\u4e0e\u73b0\u6709\u7684\u5f3a\u5927TableQA\u6a21\u578b\u6574\u5408\u3002", "result": "\u5728WikiTableQuestions\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u4f7f\u7528Qwen2.5-Coder-7B-Instruct\u6a21\u578b\u83b7\u5f97\u4e8670.9%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86Repanda\u57fa\u7ebf\uff0867.6%\uff09\u3002\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u878d\u5408\u540e\uff0c\u51c6\u786e\u7387\u8fdb\u4e00\u6b65\u63d0\u5347\u81f384.3%\u3002", "conclusion": "\u5206\u6b65\u3001\u5e26\u6ce8\u91ca\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86TableQA\u4efb\u52a1\u7684\u6b63\u786e\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a8\u7406\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u901a\u8fc7\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u7ed3\u5408\u53ef\u53d6\u5f97\u66f4\u4f18\u6548\u679c\u3002"}}
{"id": "2602.00992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00992", "abs": "https://arxiv.org/abs/2602.00992", "authors": ["Phone Thiha Kyaw", "Jonathan Kelly"], "title": "Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds", "comment": "Submitted to WAFR 2026 (17th World Symposium on the Algorithmic Foundations of Robotics (WAFR))", "summary": "In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u8fdb\u884c\u91c7\u6837\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u6709\u6548\u517c\u987e\u4e86\u9ad8\u7ef4\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u51e0\u4f55\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8fd0\u52a8\u89c4\u5212\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6b27\u6c0f\u6216\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bb8\u591a\u8fd0\u52a8\u89c4\u5212\u5668\u4f7f\u7528\u7b80\u5355\u7684\u6b27\u6c0f\u8ddd\u79bb\uff0c\u672a\u80fd\u5229\u7528\u4efb\u52a1\u76ee\u6807\u548c\u7269\u7406\u7ea6\u675f\u5728\u914d\u7f6e\u7a7a\u95f4\u5185\u8bf1\u5bfc\u4ea7\u751f\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u8fd9\u5bfc\u81f4\u8def\u5f84\u4e0d\u4f18\u6216\u4e0d\u73b0\u5b9e\u3002\u5e38\u89c4\u6570\u503c\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u81f3\u9ad8\u7ef4\uff0c\u91c7\u6837\u65b9\u6cd5\u5219\u727a\u7272\u4e86\u51e0\u4f55\u7cbe\u5ea6\u3002\u8be5\u8bba\u6587\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e24\u8005\u7684\u6743\u8861\u3002", "method": "\u65b9\u6cd5\u65b9\u9762\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u914d\u7f6e\u76f8\u5173\u7684\u9ece\u66fc\u6d41\u5f62\u4e0a\u8fdb\u884c\u64cd\u4f5c\u3002\u901a\u8fc7\u9ad8\u6548\u7684\u4e2d\u70b9\u578b\u9ece\u66fc\u6d4b\u5730\u7ebf\u8ddd\u79bb\u8fd1\u4f3c\uff08\u8bc1\u660e\u5177\u6709\u4e09\u9636\u7cbe\u5ea6\uff09\uff0c\u7ed3\u5408\u4e00\u9636\u91cd\u7f29\u6620\u5c04\u4e0e\u9ece\u66fc\u81ea\u7136\u68af\u5ea6\u8bbe\u8ba1\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u5668\u3002", "result": "\u5b9e\u9a8c\u5728\u4e8c\u8fde\u6746\u673a\u68b0\u81c2\u3001\u4e03\u81ea\u7531\u5ea6Franka\u673a\u68b0\u81c2\u53ca\u5e26\u975e\u5b8c\u6574\u7ea6\u675f\u7684SE(2)\u521a\u4f53\u573a\u666f\u4e0b\u8fdb\u884c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u672c\u6587\u65b9\u6cd5\u80fd\u5728\u52a8\u529b\u5b66\u80fd\u91cf\u5ea6\u91cf\u7b49\u590d\u6742\u5ea6\u91cf\u4e0b\u83b7\u5f97\u6bd4\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u3001\u4f20\u7edf\u9ece\u66fc\u6d4b\u5730\u7ebf\u6570\u503c\u89e3\u6cd5\u66f4\u4f4e\u5f00\u9500\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ed3\u5408\u9ad8\u7ef4\u53ef\u6269\u5c55\u6027\u4e0e\u6d41\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u7ea6\u675f\u89c4\u5212\u4efb\u52a1\u7684\u8def\u5f84\u8d28\u91cf\uff0c\u76f8\u8f83\u5f53\u524d\u4e3b\u6d41\u6280\u672f\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2602.00151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00151", "abs": "https://arxiv.org/abs/2602.00151", "authors": ["Alexander Blezinger", "Wolfgang Nejdl", "Ming Tang"], "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency", "comment": "9 pages, 7 figures and 5 tables. Initialy submitted for IJCAI 2026", "summary": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u56de\u5f52\u578b\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728HRD\uff08\u540c\u6e90\u91cd\u7ec4\u7f3a\u9677\uff09\u8bc4\u5206\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u8bc1\u5b9e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u4f18\u4e8e\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5f31\u52bf\u7fa4\u4f53\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5df2\u5728\u591a\u4e2a\u4efb\u52a1\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u5728\u6301\u7eed\u578b\uff08\u56de\u5f52\u578b\uff09\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u9886\u57df\u7684\u6548\u679c\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002\u5c24\u5176\u662f\u9488\u5bf9\u4e2a\u4f53\u5316\u764c\u75c7\u6cbb\u7597\u5173\u952e\u6307\u6807HRD\u8bc4\u5206\uff0c\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u6548\u679c\u4ecd\u9700\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528\u4e94\u79cd\u6700\u65b0\u7684\u7ec4\u7ec7\u75c5\u7406\u57fa\u7840\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u4eceWSI\u4e2d\u63d0\u53d6Patch\u7ea7\u7279\u5f81\uff0c\u5e76\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7279\u5f81\u8fdb\u884c\u5bf9\u6bd4\u3002\u5728\u516c\u5f00\u7684\u4e73\u817a\u764c\u3001\u5b50\u5bab\u5185\u819c\u764c\u548c\u80ba\u764c\u961f\u5217\u4e2d\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u8fde\u7eed\u7684HRD\u8bc4\u5206\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u4e0a\u91c7\u6837\u7b56\u7565\u7f13\u89e3\u6837\u672c\u4e0d\u5747\u8861\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u7814\u7a76\u4e0d\u540c\u91c7\u6837\u7b56\u7565\u548c\u5305\u5927\u5c0f\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8bad\u7ec3\u7684\u56de\u5f52\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7279\u5f81\u57fa\u7ebf\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u5728\u7279\u5f81\u8868\u73b0\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u4e0a\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u4e34\u5e8a\u91cd\u8981\u4f46\u6837\u672c\u4e0d\u8db3\u7fa4\u4f53\u7684\u53ec\u56de\u7387\u548c\u5747\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u5927\u89c4\u6a21\u7ec4\u7ec7\u75c5\u7406\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6709\u52a9\u4e8e\u63d0\u5347\u751f\u7269\u6807\u5fd7\u7269\u56de\u5f52\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u8f6c\u79fb\u6027\uff0c\u6709\u671b\u63a8\u52a8AI\u9a71\u52a8\u7cbe\u786e\u80bf\u7624\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00554", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00554", "abs": "https://arxiv.org/abs/2602.00554", "authors": ["Liu Kaipeng", "Wu Ling"], "title": "A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora", "comment": "14 pages, 5 figures", "summary": "This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86BERT\u6a21\u578b\u5982\u4f55\u5904\u7406\u56db\u79cd\u57fa\u672c\u8bba\u5143\u7ed3\u6784\u5efa\u6784\uff0c\u63ed\u793a\u4e86\u5176\u5206\u5c42\u7684\u8868\u5f81\u7ed3\u6784\u548c\u4e0d\u540c\u5c42\u4e4b\u95f4\u7684\u4fe1\u606f\u5206\u5316\u8fc7\u7a0b\u3002", "motivation": "\u7406\u89e3BERT\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u8bed\u8a00\u7ed3\u6784\u65f6\u7684\u5185\u90e8\u673a\u5236\uff0c\u7279\u522b\u662f\u5bf9\u6838\u5fc3\u53e5\u6cd5\u7ed3\u6784\u7684\u4fe1\u606f\u8868\u5f81\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u591a\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ecMDS\u548ct-SNE\u964d\u7ef4\u3001GDV\u4f5c\u4e3a\u805a\u7c7b\u5206\u79bb\u5ea6\u6307\u6807\u3001FDR\u7ebf\u6027\u8bca\u65ad\u63a2\u67e5\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\uff0c\u7efc\u5408\u63ed\u793a\u5404\u5c42\u5bf9\u7ed3\u6784\u4fe1\u606f\u7684\u7f16\u7801\u60c5\u51b5\u3002", "result": "\u53d1\u73b0BERT\u5728\u65e9\u671f\u5c42\u4e2d\u663e\u73b0\u51fa\u5efa\u6784\u7279\u5f02\u6027\u4fe1\u606f\uff0c\u4e8e\u4e2d\u95f4\u5c42\u80cc\u666f\u4e0b\u6837\u672c\u53ef\u5f62\u6210\u533a\u5206\u6027\u6781\u9ad8\u7684\u805a\u7c7b\uff0c\u5e76\u5728\u540e\u671f\u5c42\u4e2d\u6301\u7eed\u4fdd\u6301\u8fd9\u4e9b\u4fe1\u606f\u3002", "conclusion": "BERT\u6a21\u578b\u5177\u5907\u5206\u5c42\u7ed3\u6784\uff0c\u4e0d\u540c\u53e5\u6cd5\u5efa\u6784\u4fe1\u606f\u5728\u5404\u5c42\u4e2d\u4ee5\u4e0d\u540c\u5f62\u5f0f\u8868\u73b0\u548c\u4f20\u9012\uff0c\u5c55\u793a\u4e86\u6df1\u5c42\u6a21\u578b\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u7cbe\u7ec6\u6355\u6349\u80fd\u529b\u3002"}}
{"id": "2602.00993", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00993", "abs": "https://arxiv.org/abs/2602.00993", "authors": ["Weizhe Tang", "Junwei You", "Jiaxi Liu", "Zhaoyi Wang", "Rui Gan", "Zilin Huang", "Feng Wei", "Bin Ran"], "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HERMES\u6846\u67b6\uff0c\u5728\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5927\u6a21\u578b\u6ce8\u91ca\u7684\u98ce\u9669\u63d0\u793a\uff0c\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u867d\u7136\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u589e\u5f3a\u4e86\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u3001\u7f55\u89c1\uff08\u957f\u5c3e\uff09\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e0b\uff0c\u4f9d\u7136\u96be\u4ee5\u4fdd\u8bc1\u5b89\u5168\u548c\u51c6\u786e\uff0c\u5c24\u5176\u5728\u9700\u8981\u5e94\u5bf9\u591a\u6837\u5316\u8def\u51b5\u4e0e\u53c2\u4e0e\u8005\u65f6\u98ce\u9669\u8f83\u9ad8\u3002", "method": "HERMES\u6846\u67b6\u521b\u65b0\u6027\u5730\u63d0\u51fa foundation-model\uff08\u5927\u6a21\u578b\uff09\u8f85\u52a9\u7684\u6ce8\u91ca\u6d41\u7a0b\uff0c\u83b7\u53d6\u5e76\u7ed3\u6784\u5316\u957f\u5c3e\u573a\u666f\u548c\u89c4\u5212\u4e0a\u4e0b\u6587\uff0c\u5c06\u4ee5\u98ce\u9669\u4e3a\u4e2d\u5fc3\u7684\u4fe1\u606f\u3001\u9a7e\u9a76\u610f\u56fe\u548c\u5b89\u5168\u504f\u597d\u6ce8\u5165\u5230\u8f68\u8ff9\u89c4\u5212\u4e2d\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e09\u6a21\u6001\u9a7e\u9a76\u6a21\u5757\uff0c\u5c06\u591a\u89c6\u89d2\u611f\u77e5\u3001\u5386\u53f2\u8fd0\u52a8\u7ebf\u7d22\u548c\u8bed\u4e49\u5f15\u5bfc\u8fdb\u884c\u878d\u5408\uff0c\u4ece\u800c\u52a0\u5f3a\u5728\u957f\u5c3e\u590d\u6742\u4ea4\u901a\u4e0b\u7684\u98ce\u9669\u611f\u77e5\u4e0e\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHERMES\u5728\u5404\u79cd\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u4ee3\u8868\u6027\u7aef\u5230\u7aef\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5404\u5173\u952e\u6a21\u5757\u7684\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "HERMES\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u98ce\u9669\u63d0\u793a\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u957f\u5c3e\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u65b0\u4e00\u4ee3\u98ce\u9669\u611f\u77e5\u4e0e\u51b3\u7b56\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2602.00152", "categories": ["cs.CV", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00152", "abs": "https://arxiv.org/abs/2602.00152", "authors": ["Boyu Li", "Kuangji Zuo", "Lincong Li", "Yonghui Wu"], "title": "Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion", "comment": "24 pages, 6 figures. The manusrcipt is under review at Measurement", "summary": "The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5206\u5c42\u5e76\u884c\u4f2a\u56fe\u50cf\u589e\u5f3a\u878d\u5408\u7f51\u7edc\uff08HPPI-Net\uff09\uff0c\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff0c\u517c\u987e\u7cbe\u5ea6\u3001\u8ba1\u7b97\u8d44\u6e90\u4e0e\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8fb9\u7f18\u7aef\u6a21\u5f0f\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u5728\u6709\u9650\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\uff0c\u5c24\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u3001\u53ef\u7a7f\u6234\u548c\u667a\u80fd\u5bb6\u5c45\u7b49\u5bf9\u80fd\u8017\u53ca\u90e8\u7f72\u8d44\u6e90\u8981\u6c42\u82db\u523b\u7684\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u8feb\u5207\u9700\u8981\u517c\u5177\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u548c\u826f\u597d\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHPPI-Net\uff1a\u5206\u5c42\u8bbe\u8ba1\uff0c\u9996\u5c42\u5229\u7528FFT\u8c31\u56fe\u63d0\u53d6\u521d\u6b65\u7279\u5f81\uff1b\u7b2c\u4e8c\u5c42\u6839\u636e\u9759\u6001\u6216\u52a8\u6001\u6d3b\u52a8\uff0c\u9009\u62e9\u9759\u6b62\u6d3b\u52a8\u8bc6\u522b\u6a21\u5757\u6216\u5e76\u884cLSTM-MobileNet\uff08PLMN\uff09\u3002PLMN\u878d\u5408FFT\u3001\u5c0f\u6ce2\u548cGabor\u8c31\u56fe\uff0c\u901a\u8fc7\u4e09\u8defLSTM\u7f16\u7801\u5e76\u5229\u7528\u901a\u9053\u6ce8\u610f\u529b\uff08ECA\uff09\u4e0e\u6df1\u5ea6\u53ef\u5206\u5377\u79ef\uff08DSC\uff09\u7cbe\u70bc\u7279\u5f81\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u4f4e\u7b97\u529b\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u3002\u4f18\u5316\u540e\u5728ARM Cortex-M4\u5fae\u63a7\u5236\u5668\u4e0a\u90e8\u7f72\u3002", "result": "\u5728ARM Cortex-M4\u5fae\u63a7\u5236\u5668\u4e0a\uff0cHPPI-Net\u5728\u4ec5\u5360\u752822.3KiB RAM\u4e0e439.5KiB ROM\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e8696.70%\u7684\u51c6\u786e\u7387\uff0c\u8f83MobileNetV3\u63d0\u5347\u4e861.22%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6RAM\u548cROM\u5f00\u9500\u5206\u522b\u964d\u4f4e71.2%\u548c42.1%\u3002", "conclusion": "HPPI-Net\u517c\u5177\u9ad8\u51c6\u786e\u7387\u3001\u4f4e\u8d44\u6e90\u5360\u7528\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u53ef\u7a7f\u6234\u3001\u5de5\u4e1a\u548c\u667a\u80fd\u5bb6\u5c45\u7b49\u53d7\u5185\u5b58\u9650\u5236\u7684\u8fb9\u7f18\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00588", "abs": "https://arxiv.org/abs/2602.00588", "authors": ["Thiago Dumont Oliveira"], "title": "The French Drama Revolution: Political Economy and Literary Production, 1700-1900", "comment": null, "summary": "This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u4e3b\u9898\u5efa\u6a21\u548c\u6587\u672c\u76f8\u4f3c\u6027\u65b9\u6cd5\u5206\u6790\u4e861700-1900\u5e74\u95f4\u6cd5\u56fd\u620f\u5267\u5185\u5bb9\u7684\u6f14\u53d8\uff0c\u63ed\u793a\u4e86\u6cd5\u56fd\u5927\u9769\u547d\u540e\u620f\u5267\u4e3b\u9898\u7684\u91cd\u5927\u53d8\u5316\u53ca\u5176\u4e0e\u7ecf\u6d4e\u589e\u957f\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7a76\u6cd5\u56fd\u620f\u5267\u5728\u4e24\u4e2a\u4e16\u7eaa\u5185\u5982\u4f55\u968f\u5386\u53f2\u3001\u653f\u6cbb\u4e0e\u7ecf\u6d4e\u53d8\u9769\u53d1\u751f\u4e3b\u9898\u53d8\u5316\uff0c\u5c24\u5176\u5173\u6ce8\u6cd5\u56fd\u5927\u9769\u547d\u53ca\u5de5\u4e1a\u5316\u8fdb\u7a0b\u5bf9\u620f\u5267\u5185\u5bb9\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528Latent Dirichlet Allocation\uff08LDA\uff09\u4e3b\u9898\u5efa\u6a21\u548cJensen-Shannon Divergence\u8bc4\u4f30\u620f\u5267\u6587\u672c\u4e3b\u9898\u5206\u5e03\u7684\u53d8\u5316\uff0c\u5e76\u5c06\u620f\u5267\u4e3b\u9898\u6f14\u53d8\u4e0e\u540c\u671f\u6cd5\u56fdGDP\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0\u6cd5\u56fd\u5927\u9769\u547d\u540e\uff0c\u5c24\u5176\u662f1789\u81f31850\u5e74\u95f4\uff0c\u620f\u5267\u7684\u4e3b\u9898\u5206\u5e03\u53d1\u751f\u4e86\u663e\u8457\u53d8\u5316\uff0c\u8d44\u4ea7\u9636\u7ea7\u4e3b\u9898\u81ea18\u4e16\u7eaa\u672b\u8d77\u6210\u4e3a\u4e3b\u6d41\u3002\u620f\u5267\u4e3b\u9898\u7684\u53d8\u5316\u4e0e\u6cd5\u56fd\u7ecf\u6d4e\u3001\u653f\u6cbb\u73af\u5883\u53d8\u8fc1\u76f8\u547c\u5e94\u3002", "conclusion": "\u6cd5\u56fd\u620f\u5267\u5185\u5bb9\u5728\u653f\u6cbb\u9769\u547d\u548c\u7ecf\u6d4e\u589e\u957f\u7684\u80cc\u666f\u4e0b\u4e0d\u65ad\u6f14\u5316\uff0c\u5c55\u73b0\u51fa\u793e\u4f1a\u7ed3\u6784\u548c\u4ef7\u503c\u89c2\u8f6c\u53d8\u7684\u75d5\u8ff9\uff0c\u620f\u5267\u4e0e\u793e\u4f1a\u53d8\u8fc1\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u4e92\u52a8\u548c\u5171\u6f14\u5173\u7cfb\u3002"}}
{"id": "2602.01018", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01018", "abs": "https://arxiv.org/abs/2602.01018", "authors": ["Chongyu Zhu", "Mithun Vanniasinghe", "Jiayu Chen", "Chi-Guhn Lee"], "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories", "comment": null, "summary": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.", "AI": {"tldr": "\u63d0\u51faLOKI\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u5728\u65e0\u5956\u52b1\u3001\u591a\u4efb\u52a1\u7684\u79bb\u7ebf\u6570\u636e\u4e2d\u81ea\u52a8\u53d1\u73b0\u53ef\u590d\u7528\u6280\u80fd\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u7684\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\u96be\u4ee5\u5728\u7f3a\u4e4f\u5956\u52b1\u548c\u5b50\u4efb\u52a1\u6807\u6ce8\u7684\u957f\u65f6\u5e8f\u3001\u591a\u4efb\u52a1\u79bb\u7ebf\u6570\u636e\u4e2d\u53d1\u73b0\u548c\u5229\u7528\u53ef\u590d\u7528\u6280\u80fd\u3002\u9700\u8981\u4e00\u79cd\u7aef\u5230\u7aef\u65b9\u6cd5\u81ea\u52a8\u5206\u5272\u5e76\u7ec4\u7ec7\u6280\u80fd\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5b66\u4e60\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6LOKI\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u5e26\u5f31\u6807\u7b7e\u7684\u5411\u91cf\u91cf\u5316VAE\u505a\u7c97\u7c92\u5ea6\u7684\u5b8f\u89c2\u5206\u6bb5\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u81ea\u76d1\u7763\u987a\u5e8f\u6a21\u578b\u5fae\u8c03\u5206\u6bb5\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u805a\u7c7b\u7cbe\u786e\u6280\u80fd\u8fb9\u754c\uff1b\u7b2c\u4e09\u9636\u6bb5\u5229\u7528\u7cbe\u51c6\u6280\u80fd\u8fb9\u754c\uff0c\u5728option\u65b9\u6cd5\u6846\u67b6\u4e0b\u5b66\u4e60\u5177\u5907\u663e\u5f0f\u7ec8\u6b62\u6761\u4ef6\u7684\u5206\u5c42\u7b56\u7565\u3002", "result": "\u5728D4RL Kitchen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLOKI\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "LOKI\u4e0d\u4ec5\u80fd\u81ea\u52a8\u53d1\u73b0\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e00\u81f4\u548c\u53ef\u7ec4\u5408\u7684\u8bed\u4e49\u6280\u80fd\uff0c\u8fd8\u80fd\u901a\u8fc7\u6280\u80fd\u7ec4\u5408\u89e3\u51b3\u65b0\u9896\u672a\u89c1\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8f83\u5f3a\u7684\u6cdb\u5316\u548c\u53ef\u590d\u7528\u6027\u3002"}}
{"id": "2602.00153", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00153", "abs": "https://arxiv.org/abs/2602.00153", "authors": ["Axel Duch\u00e9", "Cl\u00e9ment Chatelain", "Gilles Gasso"], "title": "See Without Decoding: Motion-Vector-Based Tracking in Compressed Video", "comment": null, "summary": "We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u538b\u7f29\u57df\u76ee\u6807\u8ddf\u8e2a\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u5728\u89c6\u9891\u6d41\u7684\u538b\u7f29\u6570\u636e\uff08\u800c\u975e\u5b8c\u5168\u89e3\u7801\u540e\u7684RGB\u89c6\u9891\uff09\u4e0a\u8fd0\u884c\uff0c\u5229\u7528\u8fd0\u52a8\u77e2\u91cf\u548c\u53d8\u6362\u7cfb\u6570\u8fdb\u884c\u76ee\u6807\u6846\u7684\u65f6\u5e8f\u4f20\u64ad\u3002\u5728MOTS15/17/20\u6570\u636e\u96c6\u4e0a\uff0c\u8fd9\u4e00\u65b9\u6cd5\u6bd4\u4f20\u7edfRGB\u65b9\u6cd5\u8ba1\u7b97\u63d0\u901f\u6700\u9ad8\u53ef\u8fbe3.7\u500d\uff0cmAP@0.5\u4ec5\u4e0b\u964d4%\u3002", "motivation": "\u5f53\u524d\u76d1\u63a7\u7b49\u5927\u89c4\u6a21\u7cfb\u7edf\u5bf9\u89c6\u9891\u5206\u6790\u7684\u5b9e\u65f6\u6027\u548c\u8ba1\u7b97\u6548\u7387\u6709\u5f88\u9ad8\u9700\u6c42\uff0c\u800c\u4f20\u7edf\u57fa\u4e8eRGB\u89c6\u9891\u7684\u5206\u6790\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u3001\u6548\u7387\u4f4e\u3002\u4e3a\u4e86\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u76ee\u6807\u8ddf\u8e2a\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u76f4\u63a5\u5229\u7528\u538b\u7f29\u57df\u6570\u636e\u8fdb\u884c\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u538b\u7f29\u57df\u6df1\u5ea6\u8ddf\u8e2a\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5904\u7406\u89c6\u9891\u538b\u7f29\u6570\u636e\u4e2d\u7684\u8fd0\u52a8\u77e2\u91cf\u548c\u53d8\u6362\u7cfb\u6570\uff0c\u800c\u65e0\u9700\u89e3\u7801\u5b8c\u6574RGB\u5e27\u3002\u6a21\u578b\u6839\u636e\u538b\u7f29\u4fe1\u606f\uff0c\u5728\u5e27\u4e4b\u95f4\u4f20\u64ad\u548c\u8c03\u6574\u76ee\u6807\u68c0\u6d4b\u6846\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ddf\u8e2a\u3002", "result": "\u5728MOTS15/17/20\u6570\u636e\u96c6\u4e0a\uff0c\u538b\u7f29\u57df\u8ddf\u8e2a\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6700\u9ad8\u63d0\u53473.7\u500d\uff0cmAP@0.5\u6307\u6807\u53ea\u4e0b\u964d\u4e864%\u3002", "conclusion": "\u538b\u7f29\u57df\u7684\u8fd0\u52a8\u5efa\u6a21\u4e3a\u5927\u89c4\u6a21\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u5728\u76d1\u63a7\u7b49\u9886\u57df\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00594", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00594", "abs": "https://arxiv.org/abs/2602.00594", "authors": ["Zhijie Huang", "Stephen McIntosh", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling", "comment": null, "summary": "A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u5c42\u97f3\u9891\u5206\u79bb\u5206\u8bcd\u5668Kanade\uff0c\u80fd\u6709\u6548\u5206\u79bb\u8bed\u97f3\u4e2d\u7684\u53d1\u97f3\u548c\u97f5\u5f8b\u4fe1\u606f\uff0c\u540c\u65f6\u6291\u5236\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u4fe1\u606f\uff0c\u5982\u8bf4\u8bdd\u4eba\u8eab\u4efd\uff0c\u63d0\u5347\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5206\u8bcd\u6280\u672f\u96be\u4ee5\u540c\u65f6\u5206\u79bb\u51fa\u4e30\u5bcc\u7684\u8bed\u97f3\u7279\u5f81\uff08\u5982\u53d1\u97f3\u3001\u97f5\u5f8b\uff09\u5e76\u6291\u5236\u8bf4\u8bdd\u4eba\u7b49\u975e\u8bed\u8a00\u56e0\u7d20\uff0c\u4e14\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u8f85\u52a9\u65b9\u6cd5\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKanade\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u5355\u5c42\u7ed3\u6784\u5206\u79bb\u51fa\u8bed\u97f3\u4e2d\u7684\u58f0\u5b66\u5e38\u6570\uff0c\u751f\u6210\u4ec5\u5305\u542b\u8bed\u97f3\u548c\u97f5\u5f8b\u4fe1\u606f\u7684token\u6d41\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u8f85\u52a9\u5206\u79bb\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKanade\u5728\u8bf4\u8bdd\u4eba\u5206\u79bb\u3001\u8bcd\u6c47\u53ef\u7528\u6027\u65b9\u9762\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u5e76\u4e14\u80fd\u591f\u4fdd\u6301\u51fa\u8272\u7684\u8bed\u97f3\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "Kanade\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7ed3\u6784\u7b80\u5355\u7684\u8bed\u97f3\u5206\u8bcd\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5177\u6709\u53ef\u7528\u8bed\u97f3\u7279\u5f81\u7684token\uff0c\u5bf9\u8bed\u97f3\u5efa\u6a21\u4e0e\u751f\u6210\u5177\u6709\u5f88\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.01040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01040", "abs": "https://arxiv.org/abs/2602.01040", "authors": ["Yuhang Zhang", "Chao Yan", "Jiaxi Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration", "comment": null, "summary": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContrAstive Prompt Orchestration\uff08CAPO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9-\u8fd0\u52a8\u7b56\u7565\u5728\u4e0d\u540c\u8eab\u4f53\u5e73\u53f0\u95f4\u7684\u9002\u5e94\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8de8\u57df\u4e0e\u96f6\u6837\u672c\u9002\u5e94\u4e0a\u6548\u679c\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u5728\u9762\u4e34\u4e0d\u540c\u4f20\u611f\u5668\u3001\u73af\u5883\u6216\u7cfb\u7edf\u7269\u7406\u5c5e\u6027\u53d8\u5316\u65f6\uff0c\u4e00\u822c\u96be\u4ee5\u5206\u79bb\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u548c\u9886\u57df\u7279\u6709\u5e72\u6270\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\u548c\u6837\u672c\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faCAPO\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0eprompt\u673a\u5236\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u63d0\u51fa\u6df7\u5408\u578b\u5bf9\u6bd4\u5b66\u4e60\uff0c\u878d\u5408\u89c6\u89c9\u3001\u52a8\u4f5c\u65f6\u5e8f\u548c\u6587\u672c\u76ee\u6807\uff0c\u751f\u6210\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6c60\u6765\u8868\u8fbe\u7ec6\u7c92\u5ea6\u9886\u57df\u4fe1\u606f\uff1b\uff082\uff09\u6839\u636e\u5f53\u524d\u89c2\u6d4b\uff0c\u81ea\u9002\u5e94\u5730\u805a\u5408\u548c\u9009\u7528\u8fd9\u4e9b\u63d0\u793a\uff0c\u4ece\u800c\u52a8\u6001\u4f18\u5316\u72b6\u6001\u8868\u793a\uff1b\uff083\uff09\u6700\u7ec8\u7528\u4e8e\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cCAPO\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u65b9\u6cd5\uff0c\u4e14\u5728\u7167\u660e\u3001\u89c6\u573a\u53d8\u5316\u6216\u65cb\u8f6c\u7b49\u5267\u70c8\u73af\u5883\u4e0e\u7269\u7406\u53d8\u52a8\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u81ea\u9002\u5e94\u3002", "conclusion": "CAPO\u6709\u52a9\u4e8e\u63d0\u5347\u4e0d\u540c\u5d4c\u5165\u5f0f\u4f53\u5e73\u53f0\u4e0a\u89c6\u89c9-\u8fd0\u52a8\u51b3\u7b56\u7684\u6cdb\u5316\u548c\u9002\u5e94\u6027\uff0c\u662f\u8de8\u4f53\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u9886\u57df\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00163", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.00163", "abs": "https://arxiv.org/abs/2602.00163", "authors": ["Laura Cif", "Diane Demailly", "Gabriella A. Horv\u00e0th", "Juan Dario Ortigoza Escobar", "Nathalie Dorison", "Mayt\u00e9 Castro Jim\u00e9nez", "C\u00e9cile A. Hubsch", "Thomas Wirth", "Gun-Marie Hariz", "Sophie Huby", "Morgan Dornadic", "Zohra Souei", "Muhammad Mushhood Ur Rehman", "Simone Hemm", "Mehdi Boulayme", "Eduardo M. Moraud", "Jocelyne Bloch", "Xavier Vasques"], "title": "Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders", "comment": null, "summary": "Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u59ff\u6001\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5e38\u89c4\u4e34\u5e8a\u89c6\u9891\u4e2d\u5ba2\u89c2\u533a\u5206\u591a\u79cd\u9ad8\u8fd0\u52a8\u6027\u969c\u788d\u3002", "motivation": "\u9ad8\u8fd0\u52a8\u6027\u969c\u788d\uff08HMDs\uff09\u5982\u808c\u5f20\u529b\u969c\u788d\u3001\u9707\u98a4\u3001\u821e\u8e48\u75c7\u3001\u808c\u9635\u631b\u548c\u62bd\u52a8\u75c7\u5728\u5404\u5e74\u9f84\u5c42\u5747\u6709\u53d1\u751f\uff0c\u7531\u4e8e\u8868\u73b0\u5f62\u5f0f\u591a\u53d8\u4e14\u5e38\u5e38\u4ea4\u7ec7\u51fa\u73b0\uff0c\u4e34\u5e8a\u8bc6\u522b\u53ca\u968f\u8bbf\u76d1\u6d4b\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u5224\uff0c\u5b58\u5728\u8f83\u5927\u4e3b\u89c2\u8bef\u5dee\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u5ba2\u89c2\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u5957\u57fa\u4e8e\u89c6\u9891\u52a8\u4f5c\u59ff\u6001\u548c\u673a\u5668\u5b66\u4e60\u7684\u5206\u6790\u6846\u67b6\uff0c\u5c06\u666e\u901a\u95e8\u8bca\u89c6\u9891\u8f6c\u6362\u4e3a\u89e3\u5256\u5173\u952e\u70b9\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u63d0\u53d6\u6db5\u76d6\u7edf\u8ba1\u5b66\u3001\u65f6\u5e8f\u3001\u9891\u8c31\u53ca\u9ad8\u9636\u590d\u6742\u6027\u7684\u4e0d\u540c\u884c\u4e3a\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u7528\u4e8e\u8fdb\u4e00\u6b65\u533a\u5206\u4e0d\u540c\u7684\u9ad8\u8fd0\u52a8\u6027\u969c\u788d\u8868\u578b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u5e38\u89c4\u89c6\u9891\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u63d0\u53d6\u5e76\u91cf\u5316\u8fd9\u4e9b\u590d\u6742\u8fd0\u52a8\u969c\u788d\u7684\u5173\u952e\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u533a\u5206\u548c\u8bc6\u522b\u4e0d\u540c\u7c7b\u578b\u7684\u9ad8\u8fd0\u52a8\u6027\u969c\u788d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edf\u4f9d\u8d56\u4e3b\u89c2\u5224\u65ad\u7684\u4e34\u5e8a\u9ad8\u8fd0\u52a8\u6027\u969c\u788d\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u81ea\u52a8\u5316\u7684\u65b0\u9014\u5f84\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00597", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00597", "abs": "https://arxiv.org/abs/2602.00597", "authors": ["Chaoqun Cui", "Shijing Wang", "Liangbin Huang", "Qingqing Gu", "Zhaolong Huang", "Xiao Zeng", "Wenji Mao"], "title": "Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling", "comment": "Accepted to The Web Conference (WWW) 2026", "summary": "Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Hermes\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5b57\u5e55\u7ffb\u8bd1\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u5b57\u5e55\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u672f\u8bed\u548c\u4ee3\u8bcd\u7ffb\u8bd1\u53ca\u8868\u8fbe\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u5f53\u524d\u6c34\u5e73\u3002", "motivation": "\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u5bf9\u4e8e\u5f71\u89c6\u672c\u5730\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e2d\u7279\u6709\u7684\u5b57\u5e55\u6587\u672c\u7279\u70b9\uff08\u5982\u7b80\u6d01\u3001\u8bed\u5883\u4f9d\u8d56\u5f3a\u7b49\uff09\u4e3a\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u672f\u8bed\u548c\u4ee3\u8bcd\u7ffb\u8bd1\u53ca\u8868\u8fbe\u80fd\u529b\u7b49\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u81ea\u52a8\u7ffb\u8bd1\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u96be\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Hermes\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e09\u5927\u6a21\u5757\uff1a\u8bf4\u8bdd\u4eba\u5206\u79bb\uff08Speaker Diarization\uff09\u3001\u672f\u8bed\u8bc6\u522b\uff08Terminology Identification\uff09\u548c\u8868\u8fbe\u529b\u589e\u5f3a\uff08Expressiveness Enhancement\uff09\uff0c\u4ee5\u63d0\u5347\u5b57\u5e55\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHermes\u5728\u8bf4\u8bdd\u4eba\u5206\u79bb\u65b9\u9762\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u52a0\u8868\u8fbe\u4e30\u5bcc\u3001\u8bed\u5883\u8fde\u8d2f\u7684\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u3002", "conclusion": "Hermes\u6846\u67b6\u89e3\u51b3\u4e86\u5b57\u5e55\u7ffb\u8bd1\u7684\u5173\u952e\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u81ea\u52a8\u5316\u7814\u7a76\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.01041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01041", "abs": "https://arxiv.org/abs/2602.01041", "authors": ["Akinosuke Tsutsumi", "Tomoya Itsuka", "Yuichiro Kasahara", "Tomoya Kouno", "Kota Akinari", "Genki Yamauchi", "Daisuke Endo", "Taro Abe", "Takeshi Hashimoto", "Keiji Nagatani", "Ryo Kurazume"], "title": "LLM-Based Behavior Tree Generation for Construction Machinery", "comment": "7 pages, 7 figures", "summary": "Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u884c\u4e3a\u6811\uff08BT\uff09\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5730\u534f\u8c03\u591a\u53f0\u65bd\u5de5\u673a\u68b0\u7684\u534f\u4f5c\uff0c\u63d0\u9ad8\u571f\u65b9\u4f5c\u4e1a\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5de5\u5730\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u571f\u65b9\u4f5c\u4e1a\u5bf9\u81ea\u52a8\u5316\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u7531\u4e8e\u5de5\u4eba\u8001\u9f84\u5316\u548c\u6280\u80fd\u6d41\u5931\uff0c\u73b0\u6709\u57fa\u4e8e\u624b\u5de5\u8bbe\u8ba1\u884c\u4e3a\u6811\u7684\u65b9\u6cd5\u5728\u5f02\u6784\u673a\u5668\u534f\u4f5c\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u53d7\u9650\uff0c\u9700\u8981\u65b0\u7684\u81ea\u52a8\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\uff1a\u7b2c\u4e00\u6b65\u7531LLM\u8fdb\u884c\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u5e76\u751f\u6210\u540c\u6b65\u6807\u5fd7\uff0c\u7528\u4e8e\u591a\u673a\u5b89\u5168\u534f\u4f5c\uff1b\u7b2c\u4e8c\u6b65\u6309\u7167\u7ed3\u6784\u5316\u6a21\u677f\u751f\u6210\u884c\u4e3a\u6811\u3002\u5229\u7528\u7cfb\u7edf\u6570\u636e\u5e93\u7684\u53c2\u6570\u5b9e\u73b0\u5b89\u5168\u89c4\u5212\uff0c\u5e76\u652f\u6301\u5f02\u6784\u5efa\u7b51\u673a\u68b0\u7684\u81ea\u52a8\u5316\u534f\u4f5c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u73af\u5883\u4e0b\u548c\u5b9e\u9645\u590d\u6742\u5de5\u5730\u591a\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u4efb\u52a1\u89c4\u5212\u4e0e\u534f\u4f5c\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8eLLM\u81ea\u52a8\u751f\u6210\u884c\u4e3a\u6811\u7684\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u571f\u6728\u5de5\u7a0b\u65bd\u5de5\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4fdd\u969c\u591a\u673a\u68b0\u5b89\u5168\u534f\u4f5c\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2602.00168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00168", "abs": "https://arxiv.org/abs/2602.00168", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation", "comment": null, "summary": "This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.", "AI": {"tldr": "\u63d0\u51faYOLOE-26\uff0c\u4e00\u4e2a\u7ed3\u5408YOLOv26\u9ad8\u6548\u67b6\u6784\u4e0e\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u7684\u65b0\u578b\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u53ef\u5728\u6ee1\u8db3\u9ad8\u6548\u4e0e\u786e\u5b9a\u6027\u7684\u540c\u65f6\u5177\u5907\u5f00\u653e\u7c7b\u522b\u8bc6\u522b\u4e0e\u5206\u5272\u80fd\u529b\u3002", "motivation": "\u5e38\u89c4YOLO\u7cfb\u5217\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5c01\u95ed\u96c6\uff08\u56fa\u5b9a\u7c7b\u522b\uff09\u68c0\u6d4b\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u5b9e\u9645\u573a\u666f\u7684\u5f00\u653e\u7c7b\u522b\u9700\u6c42\uff0c\u540c\u65f6\u9700\u517c\u987e\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u6253\u7834YOLO\u7684\u56fa\u5b9a\u7c7b\u522b\u5c40\u9650\uff0c\u5b9e\u73b0\u5bf9\u4efb\u610f\u6587\u672c/\u89c6\u89c9\u63cf\u8ff0\u76ee\u6807\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u548c\u90e8\u7f72\u53cb\u597d\u3002", "method": "\u63d0\u51faYOLOE-26\u67b6\u6784\uff0c\u878d\u5408YOLOv26\u65e0NMS\u7aef\u5230\u7aef\u8bbe\u8ba1\u3001PAN/FPN\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u5e76\u4ee5\u5bf9\u8c61\u5d4c\u5165\u66ff\u4ee3\u56fa\u5b9a\u7c7b\u522blogits\uff0c\u901a\u8fc7\u4e0e\u6587\u672c\u3001\u89c6\u89c9\u6216\u5185\u7f6e\u8bcd\u6c47\u7684\u63d0\u793a\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u5ea6\u5339\u914d\u3002\u5f15\u5165RepRTA\u5b9e\u73b0\u96f6\u5f00\u9500\u6587\u672c\u63d0\u793a\uff0cSAVPE\u5b9e\u73b0\u793a\u4f8b\u9a71\u52a8\u5206\u5272\uff0c\u4ee5\u53caLazy Region Prompt Contrast\u652f\u6301\u514d\u63d0\u793a\u63a8\u7406\uff0c\u4e09\u8005\u7edf\u4e00\u4e8e\u5bf9\u8c61\u5d4c\u5165\u7a7a\u95f4\u3002\u5229\u7528\u5927\u89c4\u6a21\u68c0\u6d4b\u4e0e\u951a\u5b9a\u6570\u636e\u3001\u591a\u4efb\u52a1\u4f18\u5316\u8bad\u7ec3\uff0c\u5e76\u517c\u5bb9Ultralytics\u751f\u6001\u5168\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\u3001\u63d0\u793a\u4e0e\u514d\u63d0\u793a\u573a\u666f\u4e2d\uff0cYOLOE-26\u5c55\u73b0\u4e86\u4e00\u81f4\u7684\u6269\u5c55\u6027\u548c\u4f18\u826f\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5404\u7c7b\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "YOLOE-26\u662f\u9762\u5411\u52a8\u6001\u5b9e\u9645\u73af\u5883\u3001\u5177\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u76ee\u6807\u68c0\u6d4b\u5206\u5272\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.00612", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00612", "abs": "https://arxiv.org/abs/2602.00612", "authors": ["Yitong Zhang", "Yongmin Li", "Yuetong Liu", "Jia Li", "Xiaoran Jia", "Zherui Li", "Ge Li"], "title": "Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.\n  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAVE\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u4e3a\u6269\u6563\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8f93\u51fa\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u867d\u7136dLLMs\u5728\u751f\u6210\u5f62\u5f0f\u5316\u8bed\u8a00\uff08\u5982\u4ee3\u7801\u3001\u5316\u5b66\u8868\u8fbe\u5f0f\uff09\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u7531\u4e8e\u5176\u6982\u7387\u751f\u6210\u7279\u6027\uff0c\u751f\u6210\u7ed3\u679c\u5e38\u5e38\u4e0d\u7b26\u5408\u8bed\u6cd5\u89c4\u8303\u3002\u73b0\u6709\u7684\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u800cdLLMs\u7684\u5e76\u884c\u751f\u6210\u65b9\u5f0f\u4f7f\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002\u6b64\u5916\uff0c\u4e13\u4e3adLLMs\u8bbe\u8ba1\u7684\u4e00\u4e9b\u65b9\u6cd5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u4e2d\u95f4\u8f93\u51fa\u540e\u7eed\u80fd\u6269\u5c55\u4e3a\u5408\u6cd5\u53e5\u5b50\uff0c\u964d\u4f4e\u4e86\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u9002\u7528\u4e8edLLMs\u7684\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u6280\u672f\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86LAVE\u65b9\u6cd5\uff0c\u5145\u5206\u5229\u7528dLLMs\u6bcf\u8f6e\u8fed\u4ee3\u80fd\u5e76\u884c\u9884\u6d4b\u6240\u6709\u4f4d\u7f6etoken\u5206\u5e03\u7684\u7279\u6027\u3002\u5f53\u6a21\u578b\u63d0\u51fa\u65b0token\u65f6\uff0cLAVE\u80fd\u57fa\u4e8e\u8fd9\u4e9b\u5206\u5e03\u8fdb\u884c\u524d\u77bb\u68c0\u67e5\uff08lookahead\uff09\uff0c\u9ad8\u6548\u68c0\u9a8c\u65b0token\u7684\u5408\u6cd5\u6027\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e2d\u95f4\u8f93\u51fa\u4e00\u5b9a\u6709\u53ef\u80fd\u88ab\u6269\u5c55\u6210\u7b26\u5408\u8bed\u6cd5\u7684\u5b8c\u6574\u53e5\u5b50\u3002", "result": "\u4f5c\u8005\u5728\u56db\u79cd\u4e3b\u6d41dLLMs\u548c\u4e09\u4e2a\u6807\u51c6\u8bc4\u6d4b\u96c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aLAVE\u5728\u8bed\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u8fdc\u8d85\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u57fa\u672c\u4e0d\u4f1a\u589e\u52a0\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "LAVE\u4e3adLLMs\u5e26\u6765\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86dLLMs\u5728\u4ee3\u7801\u7b49\u8bed\u6cd5\u4e25\u683c\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u8bed\u6cd5\u7ea6\u675f\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2602.01067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01067", "abs": "https://arxiv.org/abs/2602.01067", "authors": ["Fanqi Lin", "Kushal Arora", "Jean Mercat", "Haruki Nishimura", "Paarth Shah", "Chen Xu", "Mengchao Zhang", "Mark Zolotas", "Maya Angeles", "Owen Pfannenstiehl", "Andrew Beaulieu", "Jose Barreiros"], "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation", "comment": null, "summary": "Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u6a21\u578b\u7684\u8054\u5408\u8bad\u7ec3\u6570\u636e\u6a21\u6001\u548c\u7b56\u7565\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u53ca\u8de8\u673a\u5668\u4eba\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u5927\u89c4\u6a21\u884c\u4e3a\u6a21\u578b\u867d\u7136\u663e\u793a\u51fa\u5f3a\u5927\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u673a\u5668\u4eba\u6570\u636e\u8986\u76d6\u6709\u9650\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u76f4\u63a5\u91c7\u96c6\u66f4\u591a\u673a\u5668\u4eba\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u4e9f\u9700\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8054\u5408\u591a\u79cd\u5f02\u8d28\u6570\u636e\u6a21\u6001\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\uff0c\u660e\u786e\u4e0d\u540c\u8054\u5408\u8bad\u7ec3\u6570\u636e\u53ca\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u6db5\u76d6\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u3001\u8f68\u8ff9\u5bc6\u96c6\u8bed\u8a00\u6807\u6ce8\u3001\u8de8\u673a\u4f53\u673a\u5668\u4eba\u6570\u636e\u3001\u4eba\u7c7b\u89c6\u9891\u3001\u79bb\u6563\u673a\u5668\u4eba\u52a8\u4f5c\u6807\u8bb0\u4e94\u79cd\u6570\u636e\u6a21\u6001\uff0c\u4ee5\u53ca\u5355\u9636\u6bb5\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002\u5171\u75284000\u5c0f\u65f6\u673a\u5668\u4eba\u53ca\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u30015000\u4e07\u89c6\u89c9-\u8bed\u8a00\u6837\u672c\uff0c\u8bad\u7ec3\u51fa\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\uff0c\u5728\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u6d4b\u8bd5\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u5404\u8bad\u7ec3\u7b56\u7565\u53ca\u6570\u636e\u7ec4\u5408\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u548c\u8de8\u673a\u4f53\u673a\u5668\u4eba\u6570\u636e\u7684\u8054\u5408\u8bad\u7ec3\u5927\u5e45\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u8f6c\u79fb\u3001\u672a\u89c1\u4efb\u52a1\u3001\u8bed\u8a00\u8ddf\u968f\u7b49\u65b9\u9762\u3002\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u6548\u76ca\u6709\u9650\uff0c\u591a\u6a21\u6001\u7ed3\u5408\u4ea7\u751f\u9012\u589e\u589e\u76ca\uff0c\u6709\u52a9\u4e8e\u5bf9\u5168\u65b0\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u5feb\u901f\u9002\u5e94\u3002\u5355\u7528\u673a\u5668\u4eba\u6570\u636e\u5219\u635f\u5bb3\u6a21\u578b\u7684\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff0c\u8054\u5408\u8bad\u7ec3\u53ef\u6062\u590d\u6b64\u80fd\u529b\u3002\u4ee5chain-of-thought\u94fe\u5f0f\u63a8\u7406\u6570\u636e\u663e\u5f0f\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\u5bf9\u6027\u80fd\u65e0\u76ca\u3002", "conclusion": "\u6709\u6548\u6570\u636e\u6a21\u6001\u7684\u8054\u5408\u8bad\u7ec3\u5bf9\u4e8e\u6784\u5efa\u53ef\u6269\u5c55\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u672c\u7814\u7a76\u4e3a\u5982\u4f55\u9ad8\u6548\u9009\u7528\u5f02\u8d28\u6570\u636e\u63d0\u5347\u673a\u5668\u4eba\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6027\u6307\u5bfc\u3002"}}
{"id": "2602.00174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00174", "abs": "https://arxiv.org/abs/2602.00174", "authors": ["Jiajun Zhao", "Xuan Yang"], "title": "Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation", "comment": "5 pages, 7 figures, accepted by ICASSP 2026", "summary": "We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u7c7b\u5185\u7ec6\u5206\u50cf\u7d20\u5bf9\u6bd4\u5b66\u4e60\uff08SPCL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u201c\u65e0\u5173\u6837\u672c\u201d\u6982\u5ff5\u548c\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u8d28\u91cf\u548c\u8fb9\u754c\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u7c7b\u5185\u8fb9\u754c\u9644\u8fd1\u7684\u8868\u5f81\u6613\u88ab\u6c61\u67d3\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u6548\u679c\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u65b9\u5f0f\u533a\u5206\u540c\u4e00\u7c7b\u522b\u5185\u8fb9\u754c\u4e0e\u5185\u90e8\u533a\u57df\u7684\u50cf\u7d20\u3002", "method": "\u4f5c\u8005\u63d0\u51faSPCL\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u65e0\u5173\u6837\u672c\u201d\u6982\u5ff5\uff0c\u5c06\u540c\u7c7b\u4e2d\u5185\u90e8\u4e0e\u8fb9\u754c\u533a\u57df\u50cf\u7d20\u533a\u5206\u5f00\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\uff0c\u589e\u5f3a\u8fb9\u754c\u8868\u793a\u7684\u5224\u522b\u80fd\u529b\u3002\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u8be5\u65b9\u6cd5\u7ec4\u4ef6\u7684\u4f18\u52bf\u3002", "result": "\u5728\u516c\u5f00\u5fc3\u810f\u6570\u636e\u96c6\u4e0a\uff0cSPCL\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u65e0\u8bba\u5728\u5206\u5272\u8d28\u91cf\u8fd8\u662f\u8fb9\u754c\u7cbe\u5ea6\u4e0a\u5747\u8d85\u8fc7\u4e86\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "SPCL\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u8fb9\u754c\u533a\u57df\u8868\u5f81\u6c61\u67d3\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00613", "abs": "https://arxiv.org/abs/2602.00613", "authors": ["Nsrin Ashraf", "Mariam Labib", "Hamada Nayel"], "title": "Transformer-Based Model for Multilingual Hope Speech Detection", "comment": "5 pages, 1 figure, PolyHope-M shared task at RANLP2025", "summary": "This paper describes a system that has been submitted to the \"PolyHope-M\" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u7528\u4e8e\u82f1\u8bed\u548c\u5fb7\u8bed\u5e0c\u671b\u8bed\u68c0\u6d4b\u7684transformer\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u8f83\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u5e0c\u671b\u8bed\u68c0\u6d4b\u662fNLP\u9886\u57df\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u63d0\u5347\u76f8\u5173\u6a21\u578b\u6027\u80fd\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u79ef\u6781\u6b63\u5411\u7684\u8bed\u8a00\u73af\u5883\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76\u4e0d\u540ctransformer\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u9488\u5bf9\u5e0c\u671b\u8bed\u68c0\u6d4b\u4efb\u52a1\uff0c\u4f5c\u8005\u5bf9\u82f1\u8bed\u4f7f\u7528RoBERTa\u6a21\u578b\uff0c\u82f1\u8bed\u548c\u5fb7\u8bed\u5219\u4f7f\u7528\u591a\u8bed\u8a00XLM-RoBERTa\u6a21\u578b\uff0c\u5728\u76f8\u5173\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "RoBERTa\u6a21\u578b\u5728\u82f1\u8bed\u4e0a\u83b7\u5f97\u52a0\u6743F1\u5206\u65700.818\u3001\u51c6\u786e\u738781.8%\uff1bXLM-RoBERTa\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u4e0a\u83b7\u5f97F1\u5206\u65700.786\u3001\u51c6\u786e\u738778.5%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u5347\u5e0c\u671b\u8bed\u68c0\u6d4b\u4efb\u52a1\u8868\u73b0\u6548\u679c\u663e\u8457\uff0c\u5c55\u793a\u4e86transformer\u6a21\u578b\u5bf9\u591a\u79cdNLP\u4efb\u52a1\u7684\u4fc3\u8fdb\u4f5c\u7528\u3002"}}
{"id": "2602.01085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01085", "abs": "https://arxiv.org/abs/2602.01085", "authors": ["Qi Jing Chen", "Shilin Shan", "Timothy Bretl", "Quang-Cuong Pham"], "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes", "comment": "7 pages, 4 figures", "summary": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u89c2\u6d4b\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08\u5982\u7535\u7ebf\uff09\u5f62\u72b6\u6765\u68c0\u6d4b\u548c\u4f30\u7b97\u5176\u6240\u53d7\u5916\u529b\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u7aef\u6267\u884c\u5668\u6216\u6602\u8d35\u529b\u4f20\u611f\u5668\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u4e0e\u7535\u7ebf\u7b49\u7ebf\u6027\u7269\u4f53\u7684\u4ea4\u4e92\u4e2d\uff0c\u63a5\u89e6\u70b9\u5e38\u5e38\u4e0d\u5728\u672b\u7aef\u6267\u884c\u5668\uff0c\u800c\u662f\u5728\u7ebf\u4f53\u7684\u5176\u4ed6\u4f4d\u7f6e\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5916\u90e8\u4f20\u611f\u5668\u4e14\u4e3b\u8981\u5173\u6ce8\u672b\u7aef\u529b\uff0c\u9650\u5236\u5927\u3001\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002\u51c6\u786e\u68c0\u6d4b\u63a5\u89e6\u5bf9\u4e8e\u5b89\u5168\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\uff08\u5982\u907f\u514d\u635f\u4f24\u3001\u8fd0\u52a8\u53d7\u9650\u6216\u5371\u9669\u60c5\u51b5\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u6df1\u5ea6\u6444\u50cf\u5934\u83b7\u53d6\u7ebf\u6027\u7269\u4f53\uff08\u5982\u7535\u7ebf\uff09\u7684\u5f62\u72b6\uff0c\u5047\u5b9a\u5176\u5904\u4e8e\u6216\u63a5\u8fd1\u9759\u529b\u5e73\u8861\u72b6\u6001\u3002\u901a\u8fc7\u5efa\u7acb\u529b\u77e9\u5e73\u8861\u5173\u7cfb\uff0c\u63a8\u5bfc\u4e00\u81f4\u6027\u6761\u4ef6\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u4ee5\u540c\u65f6\u4f30\u7b97\u5916\u529b\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5747\u80fd\u51c6\u786e\u4f30\u7b97\u5916\u529b\u7684\u5927\u5c0f\u548c\u4f4d\u7f6e\uff0c\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u80fd\u591f\u65e0\u9700\u4e13\u7528\u529b\u4f20\u611f\u5668\uff0c\u4ec5\u51ed\u89c6\u89c9\u4fe1\u606f\u5b9e\u73b0\u5bf9\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u5916\u529b\u7684\u68c0\u6d4b\u4e0e\u4f30\u7b97\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u7ebf\u6027\u7269\u4f53\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.00176", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00176", "abs": "https://arxiv.org/abs/2602.00176", "authors": ["Feng Tian", "Yixuan Li", "Weili Zeng", "Weitian Zhang", "Yichao Yan", "Xiaokang Yang"], "title": "Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation", "comment": null, "summary": "Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u566a\u58f0\u4e0e\u9891\u7387\u7684\u8fde\u7eed\u91c7\u6837\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u6269\u6563\u540e\u9a8c\u91c7\u6837\u5728\u56fe\u50cf\u9006\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7ec6\u8282\u6062\u590d\u53ca\u6d4b\u91cf\u4e00\u81f4\u6027\u65b9\u9762\uff0c\u53d6\u5f97\u4e86\u5728\u8d85\u5206\u8fa8\u7387\u3001\u4fee\u8865\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u7684\u65b0\u6027\u80fd\u9ad8\u70b9\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u540e\u9a8c\u91c7\u6837\u5728\u9ad8\u566a\u58f0\u9636\u6bb5\u6307\u5bfc\u4e0d\u7cbe\u786e\uff0c\u7ec6\u8282\u6062\u590d\u548c\u6d4b\u91cf\u4e00\u81f4\u6027\u6548\u679c\u5dee\uff0c\u5e38\u51fa\u73b0\u9ad8\u9891\u4f2a\u5f71\uff0c\u5bf9\u8d85\u53c2\u6570\u4e0e\u4e0d\u9002\u5b9a\u7b97\u5b50\u654f\u611f\u3002\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u7ed3\u5408\u6d4b\u91cf\u4e00\u81f4\u6027\u4e0e\u6269\u6563\u8fc7\u7a0b\uff0c\u7279\u522b\u80fd\u533a\u5206\u5bf9\u4e0d\u540c\u9891\u6bb5\u91c7\u7528\u4e0d\u540c\u5f3a\u5ea6\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u566a\u58f0-\u9891\u7387\u8fde\u7eed\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u566a\u58f0\u76f8\u5173\u9891\u5e26\u7684\u4e00\u7cfb\u5217\u4e2d\u95f4\u540e\u9a8c\u5206\u5e03\uff0c\u4ec5\u9488\u5bf9\u7279\u5b9a\u5e26\u5bbd\u5185\u9891\u7387\u5206\u91cf\u65bd\u52a0\u6d4b\u91cf\u4e00\u81f4\u6027\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u91c7\u6837\u5668\u91c7\u7528\u591a\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u7b56\u7565\uff1a\u5bf9\u4e8e\u53ef\u9760\u7684\u4f4e\u9891\u4fe1\u606f\u79ef\u6781\u7ea0\u6b63\uff0c\u800c\u9ad8\u9891\u7ec6\u8282\u5219\u4ec5\u5728\u53ef\u8fa8\u8bc6\u65f6\u8c28\u614e\u4fee\u6b63\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u3001\u56fe\u50cf\u4fee\u8865\u3001\u53bb\u6a21\u7cca\u7b49\u9006\u95ee\u9898\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f18\u6216\u9886\u5148\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u7279\u522b\u662f\u5728\u8fd0\u52a8\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\uff0c\u76f8\u8f83\u4e8e\u5df2\u6709\u5f3a\u57fa\u7ebf\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe5dB\u3002", "conclusion": "\u566a\u58f0-\u9891\u7387\u8fde\u7eed\u91c7\u6837\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u9006\u95ee\u9898\u4e2d\u7684\u5b9e\u7528\u6027\u4e0e\u91cd\u5efa\u7ec6\u8282\u6062\u590d\u80fd\u529b\uff0c\u663e\u793a\u51fa\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u96be\u4ee5\u5904\u7406\u7684\u9ad8\u9891\u6062\u590d\u65b9\u9762\u4f18\u52bf\u7a81\u51fa\u3002"}}
{"id": "2602.00619", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00619", "abs": "https://arxiv.org/abs/2602.00619", "authors": ["Yuxuan Lu", "Yongkang Guo", "Yuqing Kong"], "title": "Jailbreaking LLMs via Calibration", "comment": null, "summary": "Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower \"Jailbreak Tax\" compared with existing methods, especially on the safety-hardened gpt-oss-120b.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u89c6\u4e3a\u5bf9\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u7cfb\u7edf\u6027\u626d\u66f2\uff0c\u5e76\u636e\u6b64\u6539\u8fdb\u4e86\u7834\u89e3\uff08jailbreaking\uff09\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7834\u89e3\u6210\u529f\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u529f\u80fd\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u4f7f\u6a21\u578b\u8f93\u51fa\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4ea7\u751f\u5dee\u5f02\uff0c\u5f71\u54cd\u4e86\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u4e5f\u4e3a\u7834\u89e3\u8005\u5229\u7528\u8fd9\u4e9b\u5dee\u5f02\u8fdb\u884c\u653b\u51fb\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u7406\u89e3\u5e76\u523b\u753b\u8fd9\u79cd\u626d\u66f2\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u5730\u5b9e\u65bd\u7834\u89e3\u653b\u51fb\u53ca\u8c03\u6574\u9632\u5fa1\u63aa\u65bd\u3002", "method": "\u4f5c\u8005\u5c06\u7834\u89e3\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9884\u6d4b\u805a\u5408\uff08forecast aggregation\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u635f\u5931\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u504f\u79fb\uff08Gradient Shift\uff09\u6765\u8fbe\u5230\u6700\u4f18\u805a\u5408\uff0c\u5e76\u5c06\u5e38\u89c1logit-arithmetic\u7834\u89e3\u65b9\u6cd5\u6846\u5b9a\u4e3a\u4ea4\u53c9\u71b5\u635f\u5931\u60c5\u5f62\u4e0b\u7684\u7279\u4f8b\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63a8\u5e7f\u51fa\u5bf9\u5e94\u5176\u4ed6\u635f\u5931\u51fd\u6570\u7684\u805a\u5408\u89c4\u5219\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6df7\u5408\u578b\u7684\u65b0\u89c4\u5219\u3002", "result": "\u5728\u591a\u9879red-teaming\u57fa\u51c6\u4e0e\u6570\u5b66\u4efb\u52a1\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\uff08Attack Success Rate\uff09\u548c\u201c\u7834\u89e3\u4ee3\u4ef7\u201d\uff08Jailbreak Tax\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b89\u5168\u6027\u66f4\u5f3a\u7684gpt-oss-120b\u6a21\u578b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5efa\u6a21\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f5c\u8005\u8bc1\u660e\u4e86\u65b0\u6846\u67b6\u548c\u65b0\u65b9\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5bf9\u5b89\u5168\u5bf9\u9f50\u7684LLMs\u8fdb\u884c\u7834\u89e3\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u6a21\u578b\u529f\u80fd\u7684\u635f\u5bb3\u3002\u8fd9\u4e3a\u672a\u6765\u7834\u89e3\u7814\u7a76\u548c\u9632\u5fa1\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2602.01092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01092", "abs": "https://arxiv.org/abs/2602.01092", "authors": ["Peng Zhou", "Zhongxuan Li", "Jinsong Wu", "Jiaming Qi", "Jun Hu", "David Navarro-Alarcon", "Jia Pan", "Lihua Xie", "Shiyao Zhang", "Zeqing Zhang"], "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance", "comment": null, "summary": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u7684\u3001\u5177\u5907\u5931\u8d25\u611f\u77e5\u80fd\u529b\u7684\u53cc\u624b\u8fdc\u7a0b\u64cd\u4f5c\u8f85\u52a9\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u89e6\u89c9\u8f85\u52a9\u5f15\u5bfc\u64cd\u4f5c\u5458\u8fdc\u79bb\u6613\u5931\u8d25\u52a8\u4f5c\uff0c\u4ece\u800c\u5728\u590d\u6742\u3001\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u5347\u6210\u529f\u7387\u5e76\u964d\u4f4e\u64cd\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5ea6\u8fdc\u7a0b\u64cd\u4f5c\u7531\u4e8e\u6210\u529f\u5bb9\u5dee\u5c0f\u548c\u63a5\u89e6\u52a8\u6001\u590d\u6742\uff0c\u64cd\u4f5c\u5458\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u96be\u4ee5\u9884\u5224\u5931\u8d25\uff0c\u5bb9\u6613\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4e0d\u5265\u593a\u4eba\u5de5\u4e3b\u5bfc\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u4e3b\u52a8\u611f\u77e5\u548c\u89c4\u907f\u64cd\u4f5c\u98ce\u9669\u7684\u6280\u672f\u3002", "method": "\u5229\u7528\u5305\u542b\u6210\u529f\u4e0e\u5931\u8d25\u4f8b\u5b50\u7684\u5f02\u6784\u79bb\u7ebf\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\uff0c\u901a\u8fc7\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\uff08Conservative Value Learning, CVL\uff09\u8bad\u7ec3\u51fa\u4efb\u52a1\u53ef\u884c\u6027\u7684\u4fdd\u5b88\u6210\u529f\u5206\u6570\uff0c\u5e76\u5728\u5728\u7ebf\u64cd\u4f5c\u65f6\u7528\u8be5\u5206\u6570\u8c03\u8282\u8f85\u52a9\u5f3a\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u5b66\u4e60\u5f97\u5230\u7684\u884c\u4e3a\u4f53\u63d0\u4f9b\u4fee\u6b63\u8fd0\u52a8\u65b9\u5411\u3002\u8fd9\u4e24\u4e2a\u4fe1\u53f7\u901a\u8fc7\u4e3b\u63a7\u7aef\u7684\u5173\u8282\u7a7a\u95f4\u963b\u6297\u63a5\u53e3\u6574\u5408\uff0c\u4e3a\u64cd\u4f5c\u5458\u63d0\u4f9b\u987a\u4ece\u4e14\u4e0d\u4f1a\u5f3a\u884c\u8986\u76d6\u610f\u56fe\u7684\u8fde\u7eed\u5f15\u5bfc\u3002", "result": "\u5728\u5bcc\u542b\u63a5\u89e6\u7684\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u8fdc\u7a0b\u63a7\u5236\u4e0e\u5171\u4eab\u81ea\u6cbb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6240\u63d0\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u964d\u4f4e\u64cd\u4f5c\u5458\u8d1f\u8377\u3002", "conclusion": "\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u4e3a\u8fdc\u7a0b\u53cc\u5411\u64cd\u4f5c\u5f15\u5165\u5931\u8d25\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4fdd\u969c\u4eba\u7c7b\u4e3b\u63a7\u6743\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2602.00181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00181", "abs": "https://arxiv.org/abs/2602.00181", "authors": ["Hang Wu", "Yujun Cai", "Zehao Li", "Haonan Ge", "Bowen Sun", "Junsong Yuan", "Yiwei Wang"], "title": "CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning", "comment": null, "summary": "Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86CamReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u63d0\u5347\u89c6\u9891\u6444\u50cf\u673a\u52a8\u6001\u7406\u89e3\u80fd\u529b\uff0c\u514b\u670d\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u8868\u9762\u89c6\u89c9\u7279\u5f81\u4f9d\u8d56\u3001\u5ffd\u89c6\u7269\u7406\u51e0\u4f55\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u6444\u50cf\u673a\u52a8\u6001\u65f6\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u89c9\u8868\u9762\u7279\u5f81\uff0c\u96be\u4ee5\u533a\u5206\u5b9e\u9645\u7269\u7406\u8fd0\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u51e0\u4f55\u4e0e\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u6ce8\uff0c\u5bfc\u81f4\u6a21\u578b\u5e38\u5e38\u6df7\u6dc6\u4e0d\u540c\u7c7b\u578b\u7684\u6444\u50cf\u673a\u8fd0\u52a8\u3002", "method": "CamReasoner\u91c7\u7528 OTA\uff08Observation-Thinking-Answer\uff0c\u89c2\u5bdf-\u601d\u8003-\u56de\u7b54\uff09\u7ed3\u6784\u6027\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u6a21\u578b\u663e\u5f0f\u5f15\u5bfc\u5229\u7528\u65f6\u7a7a\u7ebf\u7d22\uff08\u5982\u6444\u50cf\u673a\u8f68\u8ff9\u3001\u89c6\u9525\u4f53\uff09\uff0c\u914d\u5408\u5927\u89c4\u6a21\u8f68\u8ff9\u63a8\u7406\u6570\u636e\u53ca\u53cd\u9988\u6837\u672c\u8bad\u7ec3\uff0c\u5e76\u9996\u521b\u6027\u5730\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u8c03\u6574\u63a8\u7406\u94fe\uff0c\u5b9e\u73b0\u7269\u7406\u8fd0\u52a8\u903b\u8f91\u4e0e\u6a21\u578b\u63a8\u7406\u7ed3\u679c\u7684\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u8054\u5408\u5927\u89c4\u6a21\u63a8\u7406\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u672c\u65b9\u6cd5\u80fd\u663e\u8457\u6291\u5236\u5e7b\u89c9\u56de\u7b54\uff0c\u5e76\u5728\u591a\u4e2a\u6444\u50cf\u673a\u8fd0\u52a8\u7406\u89e3\u76f8\u5173\u7684\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u52a0\u7269\u7406\u51e0\u4f55\u7ea6\u675f\u4e0eRL\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u6444\u50cf\u673a\u8fd0\u52a8\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u4ece\u7c7b\u9ed1\u76d2\u5206\u7c7b\u6a21\u5f0f\u5411\u53ef\u89e3\u91ca\u7269\u7406\u63a8\u7406\u7684\u8f6c\u578b\u3002"}}
{"id": "2602.00638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00638", "abs": "https://arxiv.org/abs/2602.00638", "authors": ["Yingji Zhang"], "title": "Formal Semantic Control over Language Models", "comment": null, "summary": "This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u63d0\u5347\u8bed\u4e49\u8868\u793a\u5b66\u4e60\uff0c\u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u7684\u8868\u793a\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u5e76\u901a\u8fc7\u6709\u610f\u8bc6\u5730\u5851\u9020\u6f5c\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u5b9e\u73b0\u5c40\u90e8\u3001\u7c7b\u7b26\u53f7\u5f0f\u7684\u7ec4\u5408\u63a7\u5236\u3002\u4f5c\u8005\u5728VAE\u6846\u67b6\u4e0b\uff0c\u5206\u522b\u9488\u5bf9\u53e5\u5b50\u7ea7\u522b\u548c\u63a8\u7406\u7ea7\u522b\u5f00\u5c55\u4e86\u7814\u7a76\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u7cfb\u5217\u521b\u65b0\u7406\u8bba\u4e0e\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u591f\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u6f5c\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u63a7\u6027\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u7684\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8bed\u4e49\u8868\u793a\u96be\u4ee5\u89e3\u91ca\u548c\u63a7\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u4e8e\u590d\u6742\u8bed\u8a00\u4efb\u52a1\uff08\u5982\u751f\u6210\u4e0e\u63a8\u7406\uff09\u4e2d\u7684\u8868\u73b0\u548c\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u63d0\u5347\u6f5c\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u671d\u5411\u66f4\u7cbe\u7ec6\u3001\u7cfb\u7edf\u7684\u89e3\u91ca\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u53d1\u5c55\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6846\u67b6\uff0c\u4ece\u4e24\u4e2a\u65b9\u5411\u5165\u624b\uff1a\uff081\uff09\u53e5\u5b50\u7ea7\u522b\uff1a\u5728\u6f5c\u7a7a\u95f4\u4e2d\u89e3\u8026\u5e76\u64cd\u63a7\u7279\u5b9a\u8bed\u4e49\u7279\u5f81\uff0c\u6307\u5bfc\u53e5\u5b50\u751f\u6210\uff0c\u4ee5\u89e3\u91ca\u6027\u6587\u672c\u4efb\u52a1\u4e3a\u5b9e\u9a8c\u5e73\u53f0\uff1b\uff082\uff09\u63a8\u7406\u7ea7\u522b\uff1a\u5728\u6f5c\u7a7a\u95f4\u4e2d\u9694\u79bb\u548c\u64cd\u63a7\u63a8\u7406\u884c\u4e3a\uff0c\u63a7\u5236\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\uff0c\u7279\u522b\u805a\u7126\u4e8e\u89e3\u91ca\u6027NLI\u4efb\u52a1\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u9645\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6f5c\u7a7a\u95f4\u7684\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u4e0e\u51e0\u4f55\u7ed3\u6784\u53ef\u63a7\u6027\uff0c\u4e0d\u4ec5\u80fd\u4f7f\u6a21\u578b\u751f\u6210\u53d7\u63a7\u7684\u3001\u5e26\u6709\u6307\u5b9a\u8bed\u4e49\u7279\u5f81\u7684\u6587\u672c\uff0c\u8fd8\u80fd\u5728\u63a8\u7406\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5bf9\u6f5c\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u4e0e\u64cd\u63a7\uff0c\u53ef\u4ee5\u660e\u663e\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u5916\u90e8\u53ef\u63a7\u6027\uff0c\u8fd9\u4e3a\u66f4\u900f\u660e\u3001\u53ef\u64cd\u4f5c\u7684\u8bed\u8a00\u667a\u80fd\u7cfb\u7edf\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01100", "abs": "https://arxiv.org/abs/2602.01100", "authors": ["Hang Wu", "Tongqing Chen", "Jiasen Wang", "Xiaotao Li", "Lu Fang"], "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating", "comment": null, "summary": "Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a \"Lock-and-Gated\" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.", "AI": {"tldr": "StreamVLA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u4eba\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u533a\u5206\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u5927\u5e45\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u63d0\u5347\u64cd\u4f5c\u7a33\u5065\u6027\uff0c\u53d6\u5f97\u4e86\u524d\u6cbf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u6bcf\u4e00\u6b65\u90fd\u505a\u5197\u4f59\u63a8\u7406\uff0c\u5bfc\u81f4\u51b3\u7b56\u5ef6\u8fdf\u9ad8\u3001\u76ee\u6807\u4e0d\u7a33\u5b9a\u3002\u5982\u4f55\u517c\u987e\u9ad8\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u4f4e\u5c42\u8fde\u7eed\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u957f\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faStreamVLA\u53cc\u7cfb\u7edf\u67b6\u6784\uff0c\u5c06\u4efb\u52a1\u6587\u672c\u5206\u89e3\u3001\u89c6\u89c9\u76ee\u6807\u60f3\u8c61\u548c\u52a8\u4f5c\u751f\u6210\u7edf\u4e00\u4e8e\u4e00\u4e2a\u9ad8\u6548\u9aa8\u5e72\u7f51\u7edc\u3002\u521b\u65b0\u8bbe\u8ba1\u201cLock-and-Gated\u201d\u673a\u5236\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u5b50\u4efb\u52a1\u5207\u6362\u65f6\u624d\u8fdb\u884c\u8be6\u7ec6\u63a8\u7406\uff0c\u751f\u6210\u6587\u672c\u6307\u4ee4\u548c\u5177\u4f53\u89c6\u89c9\u5b8c\u6210\u72b6\u6001\uff08\u800c\u975e\u6cdb\u5316\u672a\u6765\u5e27\uff09\uff0c\u5176\u4f59\u65f6\u95f4\u9501\u5b9a\u9ad8\u5c42\u610f\u56fe\uff0c\u7528Flow Matching\u5934\u9ad8\u6548\u751f\u6210\u52a8\u4f5c\uff0c\u8282\u770172%\u7684\u89e3\u7801\u8ba1\u7b97\u3002", "result": "StreamVLA\u5728LIBERO\u57fa\u51c6\u4e0a\u53d6\u5f9798.5%\u7684\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u73af\u5883\u5e72\u6270\u4e0b\u5177\u5907\u5f3a\u6062\u590d\u529b\uff0c\u63a8\u7406\u5ef6\u8fdf\u6bd4\u5168\u7a0b\u63a8\u7406\u57fa\u7ebf\u4f4e48%\u3002", "conclusion": "StreamVLA\u901a\u8fc7\u7ed3\u6784\u521b\u65b0\uff0c\u6210\u529f\u5c06\u4efb\u52a1\u5206\u89e3\u4e0e\u8fde\u7eed\u63a7\u5236\u9ad8\u6548\u8854\u63a5\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u9c81\u68d2\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u663e\u8457\u9886\u5148\u540c\u7c7b\u65b9\u6848\u3002"}}
{"id": "2602.00192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00192", "abs": "https://arxiv.org/abs/2602.00192", "authors": ["Elif Nebioglu", "Emirhan Bilgi\u00e7", "Adrian Popescu"], "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange", "comment": "21 pages, 15 figures, 6 tables", "summary": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\uff0c\u73b0\u6709\u4fee\u590d\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u5168\u5c40\u4f2a\u5f71\u800c\u975e\u5c40\u90e8\u4fee\u590d\u5185\u5bb9\uff0c\u56e0\u6b64\u5bb9\u6613\u88ab\u7ed5\u8fc7\u3002\u4f5c\u8005\u63d0\u51fa\u4e86INP-X\u64cd\u4f5c\u548c\u65b0\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u4e3b\u6d41\u68c0\u6d4b\u5668\u68c0\u6d4b\u6548\u679c\u5927\u5e45\u4e0b\u964d\u3002\u7406\u8bba\u5206\u6790\u6307\u51fa\u539f\u56e0\u662fVAE\u91cd\u5efa\u5f15\u8d77\u7684\u9891\u8c31\u53d8\u5316\u3002\u57fa\u4e8e\u65b0\u6570\u636e\u8bad\u7ec3\u53ef\u63d0\u5347\u68c0\u6d4b\u7684\u6cdb\u5316\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4fee\u590d\u5de5\u5177\u80fd\u751f\u6210\u6781\u4e3a\u903c\u771f\u7684\u5c40\u90e8\u56fe\u7247\u6539\u52a8\uff0c\u5bfc\u81f4\u4f2a\u9020\u68c0\u6d4b\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u4f5c\u8005\u53d1\u73b0\u68c0\u6d4b\u5668\u53ef\u80fd\u6ca1\u6709\u771f\u6b63\u68c0\u6d4b\u4fee\u590d\u5185\u5bb9\u672c\u8eab\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7a76\u5176\u673a\u5236\u548c\u5c40\u9650\uff0c\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u53d1\u5c55\u3002", "method": "\u63d0\u51faINP-X\u64cd\u4f5c\uff0c\u5c06\u672a\u7f16\u8f91\u533a\u57df\u7684\u50cf\u7d20\u6062\u590d\u4e3a\u539f\u56fe\uff0c\u53ea\u4fdd\u7559\u5c40\u90e8\u4fee\u590d\u751f\u6210\u533a\u57df\uff0c\u5e76\u636e\u6b64\u5236\u4f5c\u4e86\u5305\u542b9\u4e07\u5f20\u56fe\u7247\u7684\u6570\u636e\u96c6\u3002\u7528INP-X\u5e72\u9884\u5404\u7c7b\u4e3b\u6d41\uff08\u5305\u62ec\u5546\u7528\uff09\u68c0\u6d4b\u5668\uff0c\u5206\u6790\u68c0\u6d4b\u51c6\u786e\u7387\u5e76\u8fdb\u884c\u7406\u8bba\u89e3\u91ca\u3002", "result": "\u4e3b\u6d41\u68c0\u6d4b\u5668\u5728\u901a\u5e38\u51c6\u786e\u7387\u9ad8\u8fbe91%\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9INP-X\u5904\u7406\u540e\u7684\u56fe\u7247\u51c6\u786e\u7387\u9aa4\u964d\u81f355%\uff0c\u51e0\u4e4e\u7b49\u4e8e\u968f\u673a\u3002\u7406\u8bba\u5206\u6790\u8868\u660eVAE\u91cd\u5efa\u4f1a\u5f15\u8d77\u5168\u5c40\u9ad8\u9891\u8870\u51cf\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5668\u4f9d\u8d56\u5168\u5c40\u7279\u5f81\u800c\u975e\u5185\u5bb9\u672c\u8eab\u3002", "conclusion": "\u4f9d\u8d56\u5168\u5c40\u4f2a\u5f71\u7684\u68c0\u6d4b\u65b9\u5f0f\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u4fee\u590d\uff0c\u5fc5\u987b\u63d0\u5347\u68c0\u6d4b\u5668\u5bf9\u5c40\u90e8\u4fee\u590d\u5185\u5bb9\u7684\u611f\u77e5\u80fd\u529b\u3002\u4f5c\u8005\u516c\u5f00\u7684\u65b0\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u65b9\u6cd5\u4e3a\u6539\u8fdb\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u8bc4\u6d4b\u57fa\u51c6\u3002"}}
{"id": "2602.00642", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00642", "abs": "https://arxiv.org/abs/2602.00642", "authors": ["Haitao Li", "Yifan Chen", "Shuo Miao", "Qian Dong", "Jia Chen", "Yiran Hu", "Junjie Chen", "Minghao Qin", "Qingyao Ai", "Yiqun Liu", "Cheng Luo", "Quan Zhou", "Ya Zhang", "Jikun Hu"], "title": "LegalOne: A Family of Foundation Models for Reliable Legal Reasoning", "comment": "25 pages, v1", "summary": "While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5f00\u6e90\u4e86LegalOne\uff0c\u4e00\u5957\u4e13\u4e3a\u4e2d\u6587\u6cd5\u5f8b\u9886\u57df\u8bbe\u8ba1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u548c\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u80fd\u529b\u51fa\u4f17\uff0c\u4f46\u5728\u6cd5\u5f8b\u9886\u57df\u56e0\u7f3a\u4e4f\u7cbe\u51c6\u9886\u57df\u77e5\u8bc6\u548c\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u76f4\u63a5\u5e94\u7528\u53d7\u9650\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u4e2d\u671f\u8bad\u7ec3\u5f15\u5165\u4e86\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684PAS\u91c7\u6837\u65b9\u6cd5\uff0c\u5e73\u8861\u65b0\u77e5\u8bc6\u5438\u6536\u548c\u539f\u6709\u80fd\u529b\u4fdd\u6301\uff1b2\uff09\u6709\u76d1\u7763\u5fae\u8c03\u4f7f\u7528\u4e86Legal Agentic CoT Distillation\uff08LEAD\uff09\uff0c\u5c06\u590d\u6742\u53f8\u6cd5\u6d41\u7a0b\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff1b3\uff09\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u8bb0\u5fc6\u3001\u7406\u89e3\u5230\u63a8\u7406\uff0c\u9010\u6b65\u63d0\u5347\u6a21\u578b\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u3002", "result": "LegalOne\u5728\u591a\u9879\u6cd5\u5f8b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u4e8e\u66f4\u5927\u53c2\u6570\u901a\u7528\u6a21\u578b\u7684\u8868\u73b0\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u77e5\u8bc6\u5bc6\u5ea6\u4e0e\u63a8\u7406\u6548\u7387\u3002", "conclusion": "LegalOne\u4e3a\u6cd5\u5f8bAI\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u9ad8\u98ce\u9669\u53f8\u6cd5\u573a\u666f\u4e0b\u7684\u6a21\u578b\u90e8\u7f72\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u6743\u91cd\u548c\u8bc4\u6d4b\u5de5\u5177\uff0c\u4ee5\u63a8\u52a8\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2602.01115", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01115", "abs": "https://arxiv.org/abs/2602.01115", "authors": ["Zhihao Chen", "Yiyuan Ge", "Ziyang Wang"], "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV", "comment": "Accepted By ICRA2026", "summary": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\\textcolor{red}{link}}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u76843D\u89c6\u89c9-\u52a8\u4f5c\u7b56\u7565\u6a21\u578b\uff0c\u79f0\u4e3aKAN-We-Flow\uff0c\u901a\u8fc7\u65b0\u9896\u67b6\u6784\u5927\u5e45\u51cf\u5c11\u53c2\u6570\uff0c\u5e76\u5728\u591a\u4e2a\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u4e0a\u53d6\u5f97\u9886\u5148\u8868\u73b0\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u4e0a\u6548\u679c\u7a81\u51fa\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4f4e\u6548\uff0c\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u548c\u5e9e\u5927\u7f51\u7edc\u7ed3\u6784\uff0c\u96be\u4ee5\u5b9e\u7528\u90e8\u7f72\u3002\u73b0\u6709Flow Matching\u65b9\u6cd5\u867d\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff0c\u5374\u4f9d\u8d56\u5927\u578bUNet\u67b6\u6784\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u7b56\u7565\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u3001\u53c2\u6570\u91cf\u5927\u96be\u9898\u3002", "method": "\u63d0\u51faKAN-We-Flow\u7b56\u7565\uff0c\u7ed3\u5408RWKV\u548cKAN\u67b6\u6784\u3002\u901a\u8fc7RWKV-KAN\u5757\u5148\u8fdb\u884c\u9ad8\u6548\u65f6\u5e8f\u4e0e\u901a\u9053\u4fe1\u606f\u6df7\u5408\uff0c\u518d\u7528GroupKAN\u8fdb\u884c\u57fa\u4e8e\u6837\u6761\u7684\u7279\u5f81\u975e\u7ebf\u6027\u8c03\u6574\u3002\u540c\u65f6\u52a0\u5165ACR\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u9884\u6d4b\u52a8\u4f5c\u8f68\u8ff9\u4e0e\u4e13\u5bb6\u793a\u8303\u5bf9\u9f50\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u65b9\u6cd5\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5927\u578bUNet\uff0c\u5c06\u53c2\u6570\u91cf\u51cf\u5c1186.8%\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb\uff0c\u5e76\u5728Adroit\u3001Meta-World\u548cDexArt\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6210\u529f\u7387\u3002", "conclusion": "KAN-We-Flow\u8bc1\u660e\u4e86\u8f7b\u91cf\u9ad8\u6548\u9aa8\u5e72\u7f51\u5bf9\u4e8e\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u653f\u7b56\u5b66\u4e60\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2602.00202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00202", "abs": "https://arxiv.org/abs/2602.00202", "authors": ["Shanwen Wang", "Xin Sun", "Danfeng Hong", "Fei Zhou"], "title": "Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images", "comment": null, "summary": "The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u534a\u76d1\u7763\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6a21\u578bSemiEarth\uff0c\u901a\u8fc7\u5f15\u5165VLM-PP\u6a21\u5757\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u548c\u826f\u597d\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4eba\u5de5\u6807\u6ce8\u4ee3\u4ef7\u9ad8\uff0c\u534a\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4f2a\u6807\u7b7e\uff0c\u4f46\u4f2a\u6807\u7b7e\u8d28\u91cf\u666e\u904d\u8f83\u5dee\uff0c\u5c24\u5176\u5728\u590d\u6742\u591a\u7c7b\u8fb9\u754c\u533a\u57df\uff0c\u5f71\u54cd\u5b66\u751f\u7f51\u7edc\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faSemiEarth\u6a21\u578b\uff0c\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f15\u5165\u534a\u76d1\u7763\u6846\u67b6\uff0c\u8bbe\u8ba1VLM-PP\u7ed3\u6784\u6e05\u6d17\u6559\u5e08\u7f51\u7edc\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u3002VLM-PP\u76f4\u63a5\u7ea0\u6b63\u4e0d\u81ea\u4fe1/\u6709\u6b67\u4e49\u7684\u4f2a\u6807\u7b7e\uff0c\u63d0\u5347\u8fb9\u754c\u5730\u533a\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5f15\u5bfc\u5b66\u751f\u6a21\u578b\u66f4\u597d\u5b66\u4e60\u3002VLM-PP\u4e0e\u5177\u4f53\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u62d3\u5c55\u5e76\u5e26\u6765\u5f00\u653e\u57df\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSemiEarth\u5728\u5206\u5272\u51c6\u786e\u7387\u7b49\u6307\u6807\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002\u4e0e\u4ee5\u5f80SOTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u4f18\u5f02\u8fd8\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SemiEarth\u901a\u8fc7VLM-PP\u6781\u5927\u63d0\u5347\u4e86\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u8fb9\u754c\u548c\u591a\u7c7b\u60c5\u5f62\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u9065\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.00665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00665", "abs": "https://arxiv.org/abs/2602.00665", "authors": ["Lakshan Cooray", "Deshan Sumanathilaka", "Pattigadapa Venkatesh Raju"], "title": "Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation", "comment": "Submission is under review with Computational Linguistics", "summary": "Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u591a\u8f6e\u5ba2\u670d\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u90e8\u5206\u6a21\u578b\u63a5\u8fd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6548\u679c\uff0c\u4f46\u6574\u4f53\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u5ba2\u670d\u5bf9\u8bdd\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u548c\u90e8\u7f72\u96be\u5ea6\u8ba9\u5176\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u96be\u4ee5\u5e94\u7528\u3002SLM\u4f5c\u4e3a\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u9009\u62e9\uff0c\u5176\u5728\u591a\u8f6e\u3001\u9700\u8bed\u5883\u5ef6\u7eed\u7684\u5ba2\u670d\u573a\u666f\u4e0b\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u6307\u4ee4\u5fae\u8c03\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u7ed3\u5408\u5bf9\u8bdd\u5386\u53f2\u6458\u8981\u7b56\u7565\uff0c\u7528\u4e8e\u591a\u8f6e\u5ba2\u670d\u95ee\u7b54\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5bf9\u8bdd\u9636\u6bb5\u7684\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u4ece\u4e0d\u540c\u9636\u6bb5\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002\u901a\u8fc7\u8bcd\u6c47\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\uff0c\u4ee5\u53ca\u4eba\u5de5\u4e0eLLM\u6253\u5206\u7b49\u8d28\u6027\u65b9\u6cd5\uff0c\u5bf99\u4e2aSLM\u548c3\u4e2a\u5546\u4e1aLLM\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540cSLM\u4e4b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u6709\u4e9b\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u63a5\u8fd1LLM\u7684\u8868\u73b0\uff0c\u4f46\u6709\u4e9b\u96be\u4ee5\u6301\u7eed\u5bf9\u8bdd\u8fde\u8d2f\u6027\u4e0e\u8bed\u5883\u5207\u5408\u5ea6\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5ba2\u670d\u95ee\u7b54\u7cfb\u7edf\u4e2d\u5177\u5907\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u5728\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u628a\u63e1\u65b9\u9762\u8fd8\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002"}}
{"id": "2602.01153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01153", "abs": "https://arxiv.org/abs/2602.01153", "authors": ["Zhuo Chen", "Fei Ni", "Kaiyao Luo", "Zhiyuan Wu", "Xuyang Zhang", "Emmanouil Spyrakos-Papastavridis", "Lorenzo Jamone", "Nathan F. Lepora", "Jiankang Deng", "Shan Luo"], "title": "UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors", "comment": null, "summary": "Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniForce\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u591a\u79cd\u4e0d\u540c\u7c7b\u578b\u89e6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u5171\u4eab\u529b\u7a7a\u95f4\u8868\u5f81\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u5bf9\u529b\u7684\u611f\u77e5\u4f9d\u8d56\u7279\u5b9a\u7c7b\u578b\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff08\u5982\u5149\u5b66\u3001\u78c1\u6027\u7b49\uff09\uff0c\u4e0d\u540c\u4f20\u611f\u5668\u4e4b\u95f4\u5728\u539f\u7406\u3001\u6750\u6599\u3001\u5f62\u6001\u7b49\u65b9\u9762\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u6a21\u578b\u3001\u6821\u51c6\u3001\u6570\u636e\u91c7\u96c6\u90fd\u9700\u5b9a\u5236\uff0c\u5236\u7ea6\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "UniForce\u901a\u8fc7\u540c\u65f6\u5efa\u6a21\u9006\u52a8\u529b\u5b66\uff08\u56fe\u50cf\u5230\u529b\uff09\u548c\u6b63\u52a8\u529b\u5b66\uff08\u529b\u5230\u56fe\u50cf\uff09\uff0c\u7ed3\u5408\u529b\u5e73\u8861\u548c\u56fe\u50cf\u91cd\u5efa\u635f\u5931\uff0c\u5b66\u4e60\u4f20\u611f\u5668\u65e0\u5173\u7684\u529b\u7a7a\u95f4\u8868\u793a\u3002\u6570\u636e\u91c7\u96c6\u91c7\u7528\u9759\u529b\u5e73\u8861\u65b9\u6cd5\uff0c\u5229\u7528\u4f20\u611f\u5668-\u7269\u4f53-\u4f20\u611f\u5668\u76f4\u63a5\u63a5\u89e6\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5916\u90e8\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff0c\u4fbf\u4e8e\u8de8\u4f20\u611f\u5668\u914d\u51c6\u3002", "result": "\u5728GelSight\u3001TacTip\u3001uSkin\u7b49\u591a\u79cd\u5f02\u6784\u89e6\u89c9\u4f20\u611f\u5668\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cUniForce\u5728\u529b\u4f30\u8ba1\u4e0a\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u652f\u6301Vision-Tactile-Language-Action\u6a21\u578b\u5728\u673a\u5668\u4eba\u64e6\u62ed\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u8de8\u4f20\u611f\u5668\u534f\u4f5c\u3002", "conclusion": "UniForce\u5b9e\u73b0\u4e86\u5f02\u6784\u89e6\u89c9\u4f20\u611f\u5668\u95f4\u7684\u7edf\u4e00\u529b\u8868\u5f81\uff0c\u5927\u5e45\u7b80\u5316\u4e86\u6570\u636e\u548c\u6a21\u578b\u8fc1\u79fb\u6d41\u7a0b\uff0c\u6709\u52a9\u4e8e\u529b\u611f\u77e5\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u666e\u53ca\u63a8\u5e7f\u3002"}}
{"id": "2602.00211", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00211", "abs": "https://arxiv.org/abs/2602.00211", "authors": ["Zafar Iqbal", "Anwar Ul Haq", "Srimannarayana Grandhi"], "title": "Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning", "comment": null, "summary": "Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u65e0\u76d1\u7763\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u2014\u2014\u591a\u8df3\u89c6\u89c9\u63a8\u7406\u94fe\uff08VCoR\uff09\u6846\u67b6\uff0c\u517c\u987e\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u5c3d\u7ba1\u51c6\u786e\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u8bef\u5dee\u79ef\u7d2f\u548c\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u964d\u4f4e\u3002\u4f5c\u8005\u5e0c\u671b\u4ee5\u66f4\u5177\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u5f0f\u63d0\u5347\u914d\u51c6\u53ef\u9760\u6027\u3002", "method": "\u4f5c\u8005\u5c06\u914d\u51c6\u4efb\u52a1\u91cd\u5851\u4e3a\u4e00\u4e2a\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u3002VCoR\u6846\u67b6\u5728\u6bcf\u4e00\u6b65\uff08hop\uff09\u4e2d\u7ed3\u5408\u4e86\u201c\u5c40\u90e8\u7a7a\u95f4\u7ec6\u5316\uff08LSR\uff09\u201d\u6a21\u5757\u548c\u201c\u4ea4\u53c9\u53c2\u8003\u6ce8\u610f\u529b\uff08CRA\uff09\u201d\u673a\u5236\u3002\u901a\u8fc7\u591a\u8df3\u7b56\u7565\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u3001\u6301\u7eed\u4f18\u5316\u53d8\u5f62\u573a\uff0c\u5e76\u751f\u6210\u6bcf\u4e00\u6b65\u7684\u4e2d\u95f4\u9884\u6d4b\uff0c\u4fbf\u4e8e\u8ffd\u8e2a\u548c\u89e3\u91ca\u914d\u51c6\u8fc7\u7a0b\u3002\u8fd8\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u901a\u8fc7\u5404\u6b65\u7ed3\u679c\u7684\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u6027\uff0c\u91cf\u5316\u9884\u6d4b\u4fe1\u5fc3\u3002", "result": "\u5728DIR-Lab 4D CT\uff08\u80ba\uff09\u548cIXI T1\u8111MRI\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cVCoR\u4e0d\u4ec5\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u914d\u51c6\u51c6\u786e\u7387\uff0c\u8fd8\u80fd\u8f93\u51fa\u4e30\u5bcc\u7684\u4e2d\u95f4\u53ef\u89c6\u5316\u7ed3\u679c\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\u3002", "conclusion": "VCoR\u6846\u67b6\u4e3a\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u91cf\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u65b9\u6cd5\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u4fe1\u8d56\u5ea6\u3002"}}
{"id": "2602.00733", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00733", "abs": "https://arxiv.org/abs/2602.00733", "authors": ["Yinuo Zhang", "Dingcheng Huang", "Haifeng Suo", "Yizhuo Li", "Ziya Zhao", "Junhao Xu", "Zhiying Tu", "Dianhui Chu", "Deming Zhai", "Xianming Liu", "Xiaoyan Yu", "Dianbo Sui"], "title": "EchoReview: Learning Peer Review from the Echoes of Scientific Citations", "comment": null, "summary": "As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f15\u7528\u4e0a\u4e0b\u6587\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u672f\u5f15\u7528\u4e2d\u7684\u96c6\u4f53\u8bc4\u4ef7\u4fe1\u53f7\u751f\u6210\u7ed3\u6784\u5316\u8bc4\u8bba\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86\u81ea\u52a8\u5316\u5ba1\u7a3f\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5ba1\u7a3f\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u5b66\u672f\u6295\u7a3f\u6570\u91cf\u7684\u5feb\u901f\u589e\u957f\uff0c\u4f20\u7edf\u4eba\u5de5\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u9762\u4e34\u53ef\u6269\u5c55\u6027\u538b\u529b\uff0c\u8feb\u5207\u9700\u8981\u65e2\u5177\u89c4\u6a21\u53c8\u53ef\u9760\u7684\u81ea\u52a8\u5316\u5ba1\u7a3f\u65b9\u6cd5\u3002\u800c\u73b0\u6709\u57fa\u4e8e\u771f\u5b9e\u8bc4\u8bba\u7684\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u5355\u4e00\u6765\u6e90\u53ca\u4eba\u4e3a\u8bc4\u8bba\u7684\u4e3b\u89c2\u6027\u3001\u4e0d\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u81ea\u52a8\u5ba1\u7a3f\u3002", "method": "\u63d0\u51fa\u4e86EchoReview\u6846\u67b6\uff0c\u5229\u7528\u5b66\u672f\u8bba\u6587\u5f15\u7528\u4e2d\u7684\u9690\u5f0f\u96c6\u4f53\u8bc4\u4f30\u4fe1\u53f7\u81ea\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u8bc4\u8bba\u98ce\u683c\u6570\u636e\u3002\u57fa\u4e8e\u8be5\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u8de8\u4f1a\u8bae\u3001\u8de8\u5e74\u4efd\u7684\u5f15\u7528\u9a71\u52a8\u8bc4\u8bba\u6570\u636e\u96c6EchoReview-16K\uff0c\u5e76\u636e\u6b64\u8bad\u7ec3\u81ea\u52a8\u5316\u5ba1\u7a3f\u6a21\u578bEchoReviewer-7B\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cEchoReviewer-7B\u5728\u8bc1\u636e\u652f\u6301\u548c\u8bc4\u8bba\u5168\u9762\u6027\u7b49\u6838\u5fc3\u7ef4\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4e14\u7a33\u5b9a\u7684\u63d0\u5347\uff0c\u8868\u660e\u5f15\u7528\u4e0a\u4e0b\u6587\u53ef\u4ee5\u4f5c\u4e3a\u9c81\u68d2\u6709\u6548\u7684\u81ea\u52a8\u5316\u5ba1\u7a3f\u6570\u636e\u6765\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u5f15\u7528\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u6570\u636e\u8303\u5f0f\u5728\u81ea\u52a8\u5316\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u73b0\u53ef\u9760\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u5316\u5b66\u672f\u8bc4\u5ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01166", "abs": "https://arxiv.org/abs/2602.01166", "authors": ["Shuanghao Bai", "Jing Lyu", "Wanqi Zhou", "Zhe Li", "Dakai Wang", "Lei Xing", "Xiaoguang Zhao", "Pengwei Wang", "Zhongyuan Wang", "Cheng Chi", "Badong Chen", "Shanghang Zhang"], "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \\href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.", "AI": {"tldr": "LaRA-VLA\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u591a\u6a21\u6001\u94fe\u5f0f\u63a8\u7406\u5185\u5316\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u7684VLA\u65b0\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u4e0e\u63a7\u5236\uff0c\u5e76\u6781\u5927\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9700\u8981\u751f\u6210\u663e\u5f0f\u7684\u94fe\u5f0f\u63a8\u7406\u6b65\u9aa4\uff0c\u4e0d\u4ec5\u589e\u52a0\u4e86\u8ba1\u7b97\u5ef6\u8fdf\uff0c\u800c\u4e14\u79bb\u6563\u7684\u63a8\u7406\u65b9\u5f0f\u4e0e\u673a\u5668\u7684\u8fde\u7eed\u611f\u77e5\u548c\u63a7\u5236\u4e0d\u5339\u914d\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u66f4\u9002\u4e8e\u5b9e\u9645\u90e8\u7f72\u7684VLA\u63a8\u7406\u673a\u5236\u3002", "method": "LaRA-VLA\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u65b9\u5f0f\uff0c\u9010\u6b65\u4ece\u663e\u5f0f\u94fe\u5f0f\u63a8\u7406\u76d1\u7763\u8fc7\u6e21\u5230\u9690\u5f0f\u7684\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\uff0c\u5e76\u6700\u7ec8\u5c06\u8fd9\u79cd\u63a8\u7406\u80fd\u529b\u7528\u4e8e\u9a71\u52a8\u884c\u52a8\u7684\u751f\u6210\u3002\u6a21\u578b\u5728\u63a8\u7406\u548c\u9884\u6d4b\u9636\u6bb5\uff0c\u5168\u90e8\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u5b8c\u6210\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u751f\u6210\u663e\u5f0f\u7684\u6587\u672c\u6216\u89c6\u89c9\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u6784\u5efa\u7684\u94fe\u5f0f\u63a8\u7406\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e0a\uff0cLaRA-VLA\u5747\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u7684VLA\u65b9\u6cd5\u3002\u5728\u63a8\u7406\u5ef6\u8fdf\u4e0a\uff0c\u8f83\u65e8\u5728\u663e\u5f0f\u94fe\u5f0f\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe90%\u3002", "conclusion": "\u5c06\u94fe\u5f0f\u63a8\u7406\u6f5c\u5728\u5316\u662f\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u4efb\u52a1\u9ad8\u6548\u3001\u5b9e\u65f6\u63a7\u5236\u7684\u6709\u6548\u65b0\u8303\u5f0f\uff0c\u4e3a\u590d\u6742\u673a\u5668\u884c\u52a8\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2602.00212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00212", "abs": "https://arxiv.org/abs/2602.00212", "authors": ["Sathish Krishna Anumula", "Vetrivelan Tamilmani", "Aniruddha Arjun Singh", "Dinesh Rajendran", "Venkata Deepak Namburi"], "title": "Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images", "comment": "17 Pages, 2 Tables, 6 Figures", "summary": "Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u80f8\u90e8X\u5149\u7247\u4e2d\u7684\u80ba\u708e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u8bca\u65ad\u3002", "motivation": "\u80ba\u708e\u5728\u5168\u7403\u81f4\u75c5\u548c\u81f4\u6b7b\u7387\u8f83\u9ad8\uff0c\u5c24\u5176\u5728\u513f\u7ae5\u548c\u8001\u5e74\u4eba\u7fa4\u4f53\u4e2d\u66f4\u4e3a\u660e\u663e\uff0c\u8d44\u6e90\u6709\u9650\u5730\u533a\u5c24\u751a\u3002\u4f20\u7edf\u80f8\u7247\u4eba\u5de5\u8bca\u65ad\u65b9\u6cd5\u53d7\u5230\u533b\u751f\u75b2\u52b3\u3001\u4e13\u5bb6\u4e0d\u8db3\u548c\u4e3b\u89c2\u5dee\u5f02\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u7387\u53d7\u9650\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u51c6\u786e\u3001\u5feb\u901f\u4e14\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e13\u95e8\u4f18\u5316\u4e8e\u7070\u5ea6\u533b\u5b66\u56fe\u50cf\u7eb9\u7406\u7279\u5f81\u7684\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7ed3\u6784\u7684\u81ea\u5b9a\u4e49CNN\u67b6\u6784\u3002\u91c7\u7528CLAHE\u548c\u51e0\u4f55\u589e\u5f3a\u4f5c\u4e3a\u9884\u5904\u7406\uff0c\u51cf\u7f13\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u901a\u7528\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u6b64\u65b9\u6cd5\u53c2\u6570\u66f4\u52a0\u7d27\u51d1\u3002", "result": "\u57285863\u5f20\u6b63\u4f4d\u80f8\u90e8X\u5149\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8be5\u7cfb\u7edf\uff0c\u8868\u660e\u5176\u80fd\u4ee5\u8f83\u9ad8\u7684\u7cbe\u5ea6\u548c\u8f83\u4f4e\u7684\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\uff0c\u81ea\u52a8\u8bc6\u522b\u80ba\u708e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bca\u65ad\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u80ba\u708e\u7684\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u5177\u6709\u8f83\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u533b\u7597\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u80ba\u708e\u7b5b\u67e5\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.00740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00740", "abs": "https://arxiv.org/abs/2602.00740", "authors": ["Ziyan Xiao", "Yinghao Zhu", "Liang Peng", "Lequan Yu"], "title": "ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement", "comment": null, "summary": "Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns \"how to revise\" rather than just \"what to revise\". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5ExperienceWeaver\uff0c\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u4e0b\u63d0\u5347\u4e34\u5e8a\u6587\u672c\u6539\u5199\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u6587\u672c\u6539\u5199\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\uff0c\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u4e14\u5bb9\u6613\u53ea\u505a\u8868\u5c42\u4fee\u6b63\uff0c\u7f3a\u4e4f\u6df1\u5c42\u6539\u5199\u903b\u8f91\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faExperienceWeaver\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u4e0d\u4f9d\u8d56\u76f4\u63a5\u68c0\u7d22\u6848\u4f8b\uff0c\u800c\u662f\u5c06\u591a\u6e90\u533b\u7597\u53cd\u9988\uff08\u5982\u9519\u8bef\u4fee\u6b63\u5efa\u8bae\u548c\u9ad8\u5c42\u6b21\u6539\u5199\u7b56\u7565\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u548c\u63d0\u70bc\uff0c\u5e76\u5728\u751f\u6210\u6a21\u578b\u6d41\u7a0b\u4e2d\u6ce8\u5165\u8fd9\u4e9b\u6539\u5199\u7ecf\u9a8c\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u201c\u5982\u4f55\u6539\u5199\u201d\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u5e7f\u6cdb\u8bc4\u6d4b\uff0cExperienceWeaver\u5728\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u6587\u672c\u6539\u5199\u8d28\u91cf\u548c\u5408\u7406\u6027\u5747\u8d85\u8fc7\u4e86Gemini-3 Pro\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u548c\u5c42\u6b21\u7ecf\u9a8c\u6ce8\u5165\uff0cExperienceWeaver\u514b\u670d\u4e86\u4f20\u7edf\u68c0\u7d22\u6216\u8c03\u4f18\u65b9\u6cd5\u5bf9\u6570\u636e\u91cf\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u3001\u66f4\u667a\u80fd\u7684\u4e34\u5e8a\u6587\u672c\u6539\u5199\uff0c\u5bf9\u5b9e\u9645\u533b\u7597\u6587\u6863\u5904\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.01189", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01189", "abs": "https://arxiv.org/abs/2602.01189", "authors": ["Astik Srivastava", "Thomas J Chackenkulam. Bitla Bhanu Teja", "Antony Thomas", "Madhava Krishna"], "title": "SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment", "comment": null, "summary": "We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5efa\u56fe\u3001\u4f9d\u8d56\u89c6\u89c9\u611f\u77e5\u7684\u56db\u7ef4\u65f6\u7a7a\u89c4\u5212\u65b9\u6cd5\uff0c\u4f7f\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff08UAV\uff09\u80fd\u5728\u52a8\u6001\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u907f\u969c\u548c\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u56db\u65cb\u7ffc\u907f\u969c\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e0e\u5730\u56fe\u6784\u5efa\u878d\u5408\uff0c\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u3001\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5728\u9762\u5bf9\u52a8\u6001\u672a\u77e5\u73af\u5883\u548c\u969c\u788d\u7269\u65f6\u4e0d\u591f\u7075\u6d3b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6784\u5efa\u5168\u5c40\u5730\u56fe\u3001\u53cd\u5e94\u901f\u5ea6\u5feb\u4e14\u80fd\u5e94\u5bf9\u52a8\u6001\u969c\u788d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54084\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\u8def\u5f84\u89c4\u5212\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u5b89\u5168\u98de\u884c\u901a\u9053\uff08SFC\uff09\u751f\u6210\u4e0e\u8f68\u8ff9\u4f18\u5316\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u89c9\u5206\u5272\u4e0e\u76ee\u6807\u8ddf\u8e2a\u7ba1\u9053\uff0c\u533a\u5206\u52a8\u6001\u4e0e\u9759\u6001\u5143\u7d20\uff0c\u5e76\u96c6\u6210\u5907\u4efd\u89c4\u5212\u6a21\u5757\uff0c\u5728\u9047\u5230\u6b7b\u9501\u6216\u969c\u788d\u65f6\u80fd\u5feb\u901f\u505a\u51fa\u5e94\u6025\u53cd\u5e94\uff0c\u65e0\u9700\u4f9d\u8d56\u5168\u5c40\u5730\u56fe\u3002", "result": "\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u786c\u4ef6\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u591a\u79cd\u4e3b\u6d41\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u663e\u793a\u51fa\u5728\u52a8\u6001\u672a\u77e5\u73af\u5883\u4e0b\u5b9e\u73b0\u53cd\u5e94\u5f0f\u5bfc\u822a\u7684\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89c6\u89c9\u611f\u77e5\u4e0e\u56db\u7ef4\u89c4\u5212\u4e00\u4f53\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\u6846\u67b6\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u52a8\u6001\u3001\u672a\u77e5\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u98de\u884c\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u907f\u969c\u80fd\u529b\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.00214", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00214", "abs": "https://arxiv.org/abs/2602.00214", "authors": ["Juan A. Olmos", "Antoine Manzanera", "Fabio Mart\u00ednez"], "title": "A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification", "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578bMFM-Geom\uff0c\u5c06bp-MRI\u5f71\u50cf\u4e0e\u4e34\u5e8a\u62a5\u544a\u8054\u5408\u5efa\u6a21\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u524d\u5217\u817a\u764c\u8bc6\u522b\uff0c\u4e14\u5728\u5c0f\u6837\u672c\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u524d\u5217\u817a\u764c\u7684\u8bca\u65ad\u4f9d\u8d56\u4e13\u5bb6\u4e3b\u89c2\u5224\u8bfb\uff0c\u540c\u65f6\u4e3b\u6d41\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e34\u5e8a\u53d8\u91cf\uff0c\u4ec5\u4f9d\u8d56\u5f71\u50cf\u4fe1\u606f\uff0c\u5e76\u4e14\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\uff0c\u96be\u4ee5\u5b66\u4e60\u5230\u9c81\u68d2\u8868\u8fbe\uff0c\u56e0\u6b64\u4e9f\u9700\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u8bca\u65ad\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MFM-Geom\uff0c\u5c06bp-MRI\u5f71\u50cf\u4e0e\u4e34\u5e8a\u62a5\u544a\u4fe1\u606f\u5171\u540c\u7f16\u7801\uff0c\u5e76\u5728\u5206\u7c7b\u5934\u5229\u7528\u5bf9\u79f0\u6b63\u5b9a(SPD)\u77e9\u9635\u4e0e\u9ece\u66fc\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5f71\u50cf\u4e0e\u6587\u672c\u8868\u5f81\u7684\u878d\u5408\u548c\u5206\u7c7b\u3002", "result": "\u4ec5\u752810%\u7684\u8bad\u7ec3\u6570\u636e\uff0cMFM-Geom\u5728\u5206\u7c7b\u4efb\u52a1\u4e2dAUC-PR\u8fbe90.67\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b8.3%\uff0c\u5e76\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u8868\u73b0\u826f\u597d\uff08AUC-PR 90.6\uff09\u3002", "conclusion": "MFM-Geom\u6709\u6548\u7ed3\u5408\u4e86\u5f71\u50cf\u4e0e\u4e34\u5e8a\u4fe1\u606f\uff0c\u5728\u524d\u5217\u817a\u764c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00742", "abs": "https://arxiv.org/abs/2602.00742", "authors": ["Liang Wang", "Xinyi Mou", "Xiaoyou Liu", "Xuanjing Huang", "Zhongyu Wei"], "title": "CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs", "comment": null, "summary": "User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u7269\u5efa\u6a21\u6846\u67b6 CURP\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u4ece\u7528\u6237\u7684\u504f\u597d\u548c\u884c\u4e3a\u6570\u636e\u4e2d\u62bd\u53d6\u4e2a\u6027\u5316\u7279\u5f81\uff0c\u5b9e\u73b0\u5927\u6a21\u578b\u4e0b\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u5efa\u6a21\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u8d28\u91cf\u548c\u8ba1\u7b97/\u6570\u636e\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u80fd\u4fdd\u8bc1\u4e2a\u6027\u5316\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "CURP \u6846\u67b6\u5f15\u5165\u4e86\u53cc\u5411\u7528\u6237\u7f16\u7801\u5668\u548c\u79bb\u6563\u539f\u578b\u7801\u4e66\uff0c\u80fd\u591f\u63d0\u53d6\u591a\u7ef4\u5ea6\u7684\u7528\u6237\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u7ea62000\u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u5360\u6a21\u578b\u603b\u53c2\u6570\u7ea60.2%\uff09\uff0c\u652f\u6301\u5373\u63d2\u5373\u7528\u7684\u4e2a\u6027\u5316\u3002", "result": "\u5927\u91cf\u751f\u6210\u76f8\u5173\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0cCURP\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u7684\u5f3a\u57fa\u7ebf\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CURP\u5728\u4fdd\u8bc1\u9ad8\u4e2a\u6027\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6781\u5927\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u5efa\u6a21\u4e0e\u4e2a\u6027\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2602.01226", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01226", "abs": "https://arxiv.org/abs/2602.01226", "authors": ["Aditya Shibu", "Marah Saleh", "Mohamed Al-Musleh", "Nidhal Abdulaziz"], "title": "SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., \"Form a circle\") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SkySim\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9ad8\u5c42\u6b21\u89c4\u5212\u4e0e\u4f4e\u5c42\u6b21\u5b89\u5168\u7ba1\u63a7\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u975e\u4e13\u5bb6\u5bf9\u65e0\u4eba\u673a\u7f16\u961f\u7684\u81ea\u7136\u8bed\u8a00\u63a7\u5236\uff0c\u517c\u987e\u667a\u80fd\u6027\u4e0e\u98de\u884c\u5b89\u5168\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u7269\u6d41\u3001\u519c\u4e1a\u3001\u5b89\u9632\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u64cd\u4f5c\u590d\u6742\u3001\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u8981\u6c42\u9ad8\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u5927\u91cf\u8bed\u8a00\u6a21\u578b\u5177\u5907\u9ad8\u5c42\u6307\u4ee4\u7406\u89e3\u80fd\u529b\uff0c\u5374\u56e0\u4e3a\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u5bfc\u81f4\u8f68\u8ff9\u4e0d\u5b89\u5168\u3002\u6025\u9700\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u6613\u7528\u6027\uff0c\u53c8\u80fd\u4fdd\u969c\u5b89\u5168\u548c\u53ef\u884c\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faSkySim\u6a21\u62df\u5e73\u53f0\uff1a\u4f7f\u7528Gemini 3.5 Pro\u6a21\u578b\u5c06\u7528\u6237\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u4e09\u7ef4\u7a7a\u95f4\u822a\u70b9\uff0c\u5e76\u57fa\u4e8eROS2\u548cGazebo\u5b9e\u73b0\u4eff\u771f\uff1b\u7ed3\u5408\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5bf9\u822a\u70b9\u8fdb\u884c\u78b0\u649e\u89c4\u907f\u3001\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u5730\u7406\u56f4\u680f\u7684\u5b89\u5168\u6821\u6b63\uff1b\u4ee520Hz\u7684\u901f\u7387\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u57283\u300110\u300130\u53f0Crazyflie\u65e0\u4eba\u673a\u7684\u96c6\u7fa4\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cSkySim\u80fd100%\u51c6\u786e\u5b8c\u6210\u51e0\u4f55\u56fe\u6848\u7a7a\u95f4\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u78b0\u649e\u9884\u9632\u4e0e\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SkySim\u6846\u67b6\u6709\u6548\u5730\u5c06\u5927\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5212\u80fd\u529b\u4e0e\u65e0\u4eba\u673a\u5b89\u5168\u7ba1\u63a7\u878d\u5408\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65e0\u4eba\u673a\u96c6\u7fa4\u64cd\u4f5c\u95e8\u69db\uff0c\u5e76\u517c\u987e\u667a\u80fd\u3001\u53ef\u7528\u4e0e\u5b89\u5168\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u771f\u5b9e\u786c\u4ef6\u3002"}}
{"id": "2602.00216", "categories": ["cs.CV", "cs.CY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00216", "abs": "https://arxiv.org/abs/2602.00216", "authors": ["Zaldy Pagaduan", "Jason Occidental", "Nathaniel Duro", "Dexielito Badilles", "Eleonor Palconit"], "title": "Development of a Cacao Disease Identification and Management App Using Deep Learning", "comment": "6 pages, 8 figures, preprint", "summary": "Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u6b3e\u9002\u7528\u4e8e\u83f2\u5f8b\u5bbe\u5c0f\u519c\u6237\u7684\u79bb\u7ebf\u53ef\u7528\u79fb\u52a8\u5e94\u7528\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u5bf9\u53ef\u53ef\u4e3b\u8981\u75c5\u5bb3\u7684\u8bc6\u522b\u4e0e\u7ba1\u7406\uff0c\u5e2e\u52a9\u519c\u6237\u63d0\u5347\u7530\u95f4\u8bca\u65ad\u80fd\u529b\u548c\u751f\u4ea7\u529b\u3002", "motivation": "\u83f2\u5f8b\u5bbe\u5c0f\u519c\u6237\u666e\u904d\u7f3a\u4e4f\u5148\u8fdb\u519c\u6280\u4e0e\u6570\u636e\u652f\u6301\uff0c\u519c\u4e1a\u75c5\u5bb3\u548c\u866b\u5bb3\u7ba1\u7406\u53d7\u9650\uff0c\u5236\u7ea6\u4e86\u53ef\u53ef\u4ea7\u91cf\u548c\u54c1\u8d28\u63d0\u5347\u3002\u5f53\u524d\u7f3a\u5c11\u9762\u5411\u5c0f\u519c\u6237\u7684\u4fbf\u6377\u3001\u5b9e\u7528\u6280\u672f\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8bad\u7ec3\u53ef\u53ef\u75c5\u5bb3\u8bc6\u522b\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u53ef\u79bb\u7ebf\u8fd0\u884c\u7684\u624b\u673a\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u901a\u8fc7\u519c\u7530\u5b9e\u5730\u91c7\u96c6\u56fe\u7247\u548c\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "result": "\u75c5\u5bb3\u8bc6\u522b\u6a21\u578b\u9a8c\u8bc1\u7cbe\u5ea6\u8fbe96.93%\uff0c\u9ed1\u835a\u75c5\u611f\u67d3\u7b49\u7ea7\u68c0\u6d4b\u6a21\u578b\u7cbe\u5ea6\u4e3a79.49%\uff1b\u73b0\u573a\u5e94\u7528\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\u7387\u4e3a84.2%\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u79bb\u7ebf\u53ef\u7528\u624b\u673a\u5e94\u7528\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u519c\u6237\u7684\u75c5\u5bb3\u8bca\u65ad\u4e0e\u7ba1\u7406\u80fd\u529b\uff0c\u4e3a\u63a8\u52a8\u53ef\u53ef\u4ea7\u4e1a\u667a\u80fd\u5316\u3001\u79d1\u5b66\u5316\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00747", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00747", "abs": "https://arxiv.org/abs/2602.00747", "authors": ["Shengrui Li", "Fei Zhao", "Kaiyan Zhao", "Jieying Ye", "Haifeng Liu", "Fangcheng Shi", "Zheyong Xie", "Yao Hu", "Shaosheng Cao"], "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training", "comment": "17 pages, 5 figures", "summary": "Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u641c\u7d22\u65b9\u6cd5 DeMix\uff0c\u901a\u8fc7\u6a21\u578b\u878d\u5408\u9884\u6d4b\u6700\u4f73\u6570\u636e\u6bd4\u4f8b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a2\u7d22\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u5bf9\u4e8e\u6a21\u578b\u65e2\u5177\u5907\u901a\u7528\u80fd\u529b\u53c8\u80fd\u5728\u6570\u5b66\u3001\u4ee3\u7801\u7b49\u96be\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e\u6548\u7684\u4ee3\u7406\u5b9e\u9a8c\u6216\u6602\u8d35\u7684\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u96be\u4ee5\u627e\u5230\u6700\u4f18\u6df7\u5408\u6bd4\u4f8b\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6df7\u5408\u6bd4\u4f8b\u641c\u7d22\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86DeMix\u6846\u67b6\u3002\u4e0e\u4f20\u7edf\u6bcf\u6b21\u91c7\u6837\u90fd\u9700\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u4e0d\u540c\uff0cDeMix\u5148\u5206\u522b\u5bf9\u5019\u9009\u6570\u636e\u96c6\u8bad\u7ec3\u591a\u4e2a\u7ec4\u4ef6\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u52a0\u6743\u6a21\u578b\u878d\u5408\u6765\u6a21\u62df\u4e0d\u540c\u6570\u636e\u6df7\u5408\u4e0b\u7684\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u4ece\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5373\u53ef\u5feb\u901f\u8bc4\u4f30\u4efb\u610f\u591a\u79cd\u6570\u636e\u6df7\u5408\u65b9\u6848\u3002\u8fd9\u6837\u5927\u5927\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u7b97\u529b\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeMix\u80fd\u591f\u6253\u7834\u4ee5\u5f80\u5728\u6570\u636e\u5145\u5206\u6027\u3001\u51c6\u786e\u6027\u548c\u641c\u7d22\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u56f0\u5883\uff0c\u5728\u66f4\u4f4e\u7684\u641c\u7d22\u6210\u672c\u4e0b\u83b7\u5f97\u66f4\u4f18\u7684\u6df7\u5408\u6bd4\u4f8b\uff0c\u5e76\u5e26\u6765\u66f4\u4f73\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u3002", "conclusion": "DeMix\u6709\u6548\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u641c\u7d22\u7684\u80fd\u529b\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u5e26\u6765\u63d0\u5347\u3002\u8bba\u6587\u8fd8\u516c\u5f00\u4e86\u5305\u542b\u9ad8\u8d28\u91cf\u6570\u636e\u53ca\u9a8c\u8bc1\u6df7\u5408\u6bd4\u4f8b\u7684DeMix Corpora\u6570\u636e\u96c6\uff0c\u5c06\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2602.01266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01266", "abs": "https://arxiv.org/abs/2602.01266", "authors": ["Grzegorz Malczyk", "Mihir Kulkarni", "Kostas Alexis"], "title": "Reinforcement Learning for Active Perception in Autonomous Navigation", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u4e3b\u52a8\u611f\u77e5\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u907f\u969c\u7684\u540c\u65f6\u80fd\u591f\u4e3b\u52a8\u8c03\u6574\u6444\u50cf\u673a\u4ee5\u63d0\u5347\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5728\u590d\u6742\u3001\u672a\u77e5\u73af\u5883\u4e0b\u81ea\u4e3b\u5bfc\u822a\u65f6\uff0c\u5355\u7eaf\u4f9d\u8d56\u88ab\u52a8\u89c6\u89c9\u96be\u4ee5\u5168\u9762\u611f\u77e5\u73af\u5883\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bfc\u822a\u4e0d\u5b89\u5168\u6216\u6548\u7387\u4f4e\u3002\u57fa\u4e8e\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u548c\u5bfc\u822a\u9c81\u68d2\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8f93\u5165\u5305\u62ec\u673a\u5668\u4eba\u81ea\u8eab\u72b6\u6001\u3001\u5f53\u524d\u6df1\u5ea6\u56fe\u50cf\u548c\u6700\u8fd1\u4e00\u6bb5\u6df1\u5ea6\u89c2\u6d4b\u6784\u5efa\u7684\u5c40\u90e8\u51e0\u4f55\u8868\u793a\u3002\u5c06\u7ed3\u5408\u540c\u6b65\u7684\u907f\u969c\u8def\u5f84\u89c4\u5212\u548c\u4fe1\u606f\u9a71\u52a8\u7684\u4e3b\u52a8\u6444\u50cf\u5934\u63a7\u5236\uff0c\u5e76\u5728\u5956\u52b1\u4e2d\u5f15\u5165\u4f53\u7d20\u5316\u4fe1\u606f\u91cf\u6307\u6807\u4ee5\u6fc0\u52b1\u63a2\u7d22\u6027\u611f\u77e5\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u56fa\u5b9a\u6444\u50cf\u5934\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u98de\u884c\uff0c\u540c\u65f6\u80fd\u81ea\u4e3b\u4ea7\u751f\u63a2\u7d22\u6027\u884c\u4e3a\u4ee5\u63d0\u5347\u5bf9\u73af\u5883\u7684\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e3b\u52a8\u611f\u77e5\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u5bfc\u822a\u7cfb\u7edf\uff0c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u80fd\u66f4\u597d\u5730\u517c\u987e\u76ee\u6807\u5bfc\u5411\u79fb\u52a8\u548c\u73af\u5883\u4fe1\u606f\u83b7\u53d6\uff0c\u63d0\u9ad8\u5bfc\u822a\u5b89\u5168\u6027\u548c\u63a2\u7d22\u80fd\u529b\u3002"}}
{"id": "2602.00247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00247", "abs": "https://arxiv.org/abs/2602.00247", "authors": ["Samyak Jha", "Junho Kim"], "title": "CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models", "comment": null, "summary": "Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u901a\u8fc7\u5bf9\u89c6\u89c9Token\u7684\u9009\u62e9\u6027\u526a\u679d\u548c\u524d\u9988\u7f51\u7edc\u9ad8\u6548\u8fd1\u4f3c\uff0c\u663e\u8457\u51cf\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u6a21\u578b\u89c6\u89c9\u8f93\u5165\u7684Token\u6570\u91cf\u5e9e\u5927\uff0c\u4f7f\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u8fa8\u8bc6\u54ea\u4e9b\u89c6\u89c9Token\u548c\u76f8\u5173\u8ba1\u7b97\u53ef\u88ab\u5b89\u5168\u79fb\u9664\u7684\u673a\u5236\u3002\u65e2\u6709\u65b9\u6cd5\u5e38\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6570\u8bc4\u4f30Token\u91cd\u8981\u6027\uff0c\u4f46\u5176\u4e0e\u5b9e\u9645\u8d21\u732e\u5e76\u4e0d\u5b8c\u5168\u543b\u5408\u3002", "method": "\u672c\u6587\u63d0\u51faAttention Contribution\u6307\u6807\uff0c\u5c06\u6ce8\u610f\u529b\u6982\u7387\u4e0evalue\u5411\u91cf\u6a21\u957f\u76f8\u7ed3\u5408\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u89c6\u89c9Token\u8d21\u732e\uff0c\u5e76\u636e\u6b64\u533a\u5206\u53ef\u5b89\u5168\u79fb\u9664\u7684Probability Dumps\u548c\u5fc5\u987b\u4fdd\u7559\u7684Structural Anchors\u3002\u53e6\u5916\uff0c\u5b9e\u9a8c\u8bc1\u660eFFN\u5b58\u5728\u5197\u4f59\uff0c\u5c24\u5176\u5728\u56fe\u50cfToken\u5448\u73b0\u7ebf\u6027\u884c\u4e3a\u7684\u4e2d\u95f4\u5c42\u3002\u57fa\u4e8e\u4e0a\u8ff0\u5206\u6790\uff0c\u4f5c\u8005\u63d0\u51faCAPA\u6846\u67b6\uff1a\u5728\u5173\u952e\u529f\u80fd\u8f6c\u53d8\u5904\u7528Attention Contribution\u526a\u679d\u89c6\u89c9Token\uff0c\u540c\u65f6\u4ee5\u7ebf\u6027\u8fd1\u4f3c\u9ad8\u6548\u51cf\u5c11FFN\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCAPA\u7b56\u7565\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u6a21\u578b\u4e0a\u53ef\u4ee5\u5728\u727a\u7272\u6781\u5c0f\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e14\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u66f4\u51c6\u786e\u7684\u89c6\u89c9Token\u8bc4\u4f30\u65b9\u6cd5\u548c\u9488\u5bf9FFN\u7684\u9ad8\u6548\u8fd1\u4f3c\u624b\u6bb5\uff0cCAPA\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u4e0e\u9ad8\u6027\u80fd\u7684\u6298\u4e2d\uff0c\u5bf9\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u5177\u6709\u5e7f\u6cdb\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2602.00758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00758", "abs": "https://arxiv.org/abs/2602.00758", "authors": ["Ali El Lahib", "Ying-Jieh Xia", "Zehan Li", "Yuxuan Wang", "Xinyu Pi"], "title": "Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting", "comment": "9 pages, 6 figures", "summary": "Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u4f7f\u7528\u641c\u7d22\u5f15\u64ce\u7684\u65e5\u671f\u8fc7\u6ee4\u5668\u8fdb\u884c\u201c\u9884\u622a\u6b62\u201d\u68c0\u7d22\u5e38\u88ab\u7528\u6765\u8bc4\u4f30\u4e0e\u641c\u7d22\u7ed3\u5408\u7684\u9884\u6d4b\u7cfb\u7edf\uff0c\u4f46\u8fd9\u79cd\u505a\u6cd5\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5927\u91cf\u7ed3\u679c\u5b58\u5728\u622a\u6b62\u65e5\u671f\u540e\u7684\u4fe1\u606f\u6cc4\u9732\uff0c\u5bfc\u81f4\u8bc4\u6d4b\u7ed3\u679c\u88ab\u9ad8\u4f30\u3002\u4f5c\u8005\u5efa\u8bae\u91c7\u7528\u66f4\u4e25\u8c28\u7684\u6570\u636e\u68c0\u7d22\u7b56\u7565\u6216\u76f4\u63a5\u4f7f\u7528\u65f6\u95f4\u5feb\u7167\u6570\u636e\u3002", "motivation": "\u5728\u8bc4\u4f30\u641c\u7d22\u589e\u5f3a\u7c7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u5e38\u7528\u7684\u505a\u6cd5\u662f\u5bf9\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c\u65e5\u671f\u8fc7\u6ee4\uff0c\u4ee5\u4fdd\u8bc1\u6a21\u578b\u83b7\u53d6\u7684\u4fe1\u606f\u53ea\u622a\u6b62\u5230\u67d0\u4e00\u65f6\u95f4\u70b9\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u4fdd\u969c\u516c\u5e73\u3001\u51c6\u786e\u7684\u540e\u9a8c\u68c0\u7d22\u8bc4\u4f30\u3002\u4f46\u4f5c\u8005\u8d28\u7591\u8be5\u505a\u6cd5\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u5ba1\u67e5\u4e86Google Search\u7684before:\u65e5\u671f\u8fc7\u6ee4\u7ed3\u679c\uff0c\u91cf\u5316\u4e86\u641c\u7d22\u540e\u4fe1\u606f\u6cc4\u9732\u7684\u60c5\u51b5\uff0c\u5e76\u5206\u6790\u51fa\u73b0\u6cc4\u9732\u7684\u5e38\u89c1\u673a\u5236\u3002\u540c\u65f6\uff0c\u4ed6\u4eec\u7528\u4e00\u4e2a\u5927\u6a21\u578b\uff08gpt-oss-120b\uff09\u5728\u542b\u6709\u6cc4\u9732\u4fe1\u606f\u548c\u65e0\u6cc4\u9732\u4fe1\u606f\u7684\u6570\u636e\u96c6\u4e0a\u505a\u9884\u6d4b\u51c6\u786e\u6027\u5bf9\u6bd4\u3002", "result": "71%\u7684\u6d4b\u8bd5\u95ee\u9898\u8fd4\u56de\u7684\u7ed3\u679c\u4e2d\u81f3\u5c11\u6709\u4e00\u6761\u9875\u9762\u542b\u6709\u622a\u6b62\u65e5\u671f\u540e\u7684\u5f3a\u4fe1\u606f\u6cc4\u9732\uff1b41%\u7684\u95ee\u9898\u66f4\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u3002\u6a21\u578b\u5728\u542b\u6cc4\u9732\u6570\u636e\u548c\u65e0\u6cc4\u9732\u6570\u636e\u7684Brier\u5206\u6570\u5206\u522b\u4e3a0.108\u548c0.242\uff0c\u51fa\u73b0\u4e25\u91cd\u9ad8\u4f30\u3002", "conclusion": "\u5355\u9760\u65e5\u671f\u8fc7\u6ee4\u65e0\u6cd5\u675c\u7edd\u4fe1\u606f\u6cc4\u9732\uff0c\u4e25\u91cd\u5f71\u54cd\u641c\u7d22\u589e\u5f3a\u7c7b\u6a21\u578b\u7684\u8bc4\u6d4b\u516c\u6b63\u6027\u3002\u5efa\u8bae\u91c7\u7528\u66f4\u5b89\u5168\u7684\u68c0\u7d22\u63aa\u65bd\uff0c\u6216\u76f4\u63a5\u4f9d\u8d56\u5b58\u6863\u548c\u5e26\u65f6\u95f4\u6233\u7684\u9759\u6001\u5feb\u7167\u3002"}}
{"id": "2602.01385", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01385", "abs": "https://arxiv.org/abs/2602.01385", "authors": ["Xiangyu Li", "Mingwei Lai", "Mengke Zhang", "Junxiao Lin", "Tiancheng Lai", "Junping Zhi", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design", "comment": null, "summary": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e09\u6816\u673a\u5668\u4eba\uff0c\u80fd\u5728\u7a7a\u4e2d\u3001\u9646\u5730\u548c\u6c34\u4e2d\u8fd0\u52a8\uff0c\u4e14\u7ed3\u6784\u7b80\u7ea6\uff0c\u65e0\u9700\u989d\u5916\u6267\u884c\u5668\uff0c\u540c\u65f6\u514b\u670d\u4e86\u5e38\u89c1\u4e09\u6a21\u6001\u673a\u5668\u4eba\u6548\u7387\u4f4e\u6216\u7ed3\u6784\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u591a\u6570\u673a\u5668\u4eba\u53ea\u80fd\u5728\u4e24\u79cd\u73af\u5883\u4e0b\u8fd0\u52a8\uff0c\u4e14\u73b0\u6709\u4e09\u6816\u8bbe\u8ba1\u8981\u4e48\u7ed3\u6784\u590d\u6742\u3001\u8981\u4e48\u63a8\u8fdb\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u3001\u7ed3\u6784\u66f4\u7b80\u7ea6\u7684\u4e09\u6816\u673a\u5668\u4eba\u8bbe\u8ba1\u3002", "method": "\u8be5\u673a\u5668\u4eba\u57fa\u4e8e\u56db\u65cb\u7ffc\u52a0\u4e24\u4e2a\u88ab\u52a8\u8f6e\u7684\u6781\u7b80\u7ed3\u6784\uff0c\u901a\u8fc7\u504f\u5fc3\u91cd\u5fc3\u8bbe\u8ba1\u6ee1\u8db3\u9ad8\u6548\u5730\u9762/\u6c34\u4e0b\u63a8\u8fdb\uff0c\u65e0\u9700\u590d\u6742\u673a\u68b0\u53d8\u6362\u3002\u91c7\u7528\u57fa\u4e8e\u78c1\u573a\u5b9a\u5411\u63a7\u5236\uff08FOC\uff09\u7684\u7edf\u4e00\u63a8\u8fdb\u7cfb\u7edf\uff0c\u89e3\u51b3\u7a7a\u6c14\u548c\u6c34\u4e2d\u8f6c\u77e9\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u53cc\u5411\u63a8\u8fdb\u3002\u63a7\u5236\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236- PID\u7cfb\u7edf\uff0c\u4fdd\u8bc1\u5168\u57df\u7a33\u5b9a\u8fd0\u52a8\u548c\u65e0\u7f1d\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u6709\u6548\u5b9e\u73b0\u7a7a-\u9646-\u6c34\u591a\u73af\u5883\u8fd0\u52a8\u53ca\u987a\u7545\u8f6c\u53d8\uff0c\u63a8\u8fdb\u7cfb\u7edf\u5177\u6709\u8f83\u9ad8\u6548\u7387\u548c\u81ea\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u673a\u5668\u4eba\u65b9\u6848\u5728\u6781\u7b80\u7ed3\u6784\u4e0b\u5b9e\u73b0\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u591a\u57df\uff08\u4e09\u6816\uff09\u8fd0\u52a8\u548c\u6a21\u5f0f\u5207\u6362\uff0c\u63a8\u8fdb\u53ca\u63a7\u5236\u7b56\u7565\u63d0\u5347\u4e86\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5bf9\u591a\u73af\u5883\u81ea\u4e3b\u673a\u5668\u4eba\u53d1\u5c55\u5177\u6709\u63a8\u52a8\u610f\u4e49\u3002"}}
{"id": "2602.00249", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00249", "abs": "https://arxiv.org/abs/2602.00249", "authors": ["Rishav Pramanik", "Ian E. Nielsen", "Jeff Smith", "Saurav Pandit", "Ravi P. Ramachandran", "Zhaozheng Yin"], "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis", "comment": null, "summary": "The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SANEval\u57fa\u51c6\u4f53\u7cfb\uff0c\u4e00\u79cd\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u5c5e\u6027\u3001\u591a\u7269\u4f53\u53ca\u7a7a\u95f4\u5173\u7cfb\u6587\u672c\u751f\u6210\u56fe\u7247\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u6d4b\u5de5\u5177\u3002SANEval\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6765\u8fdb\u884c\u66f4\u7cbe\u7ec6\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u8bc4\u6d4b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u751f\u6210\u56fe\u7247\uff08T2I\uff09\u6a21\u578b\u5728\u9762\u5bf9\u5305\u542b\u591a\u4e2a\u7269\u4f53\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u63d0\u793a\u8bcd\u65f6\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002\u5176\u4e3b\u8981\u74f6\u9888\u4e4b\u4e00\u5728\u4e8e\u7f3a\u5c11\u5408\u9002\u7684\u8bc4\u6d4b\u624b\u6bb5\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5c01\u95ed\u8bcd\u6c47\uff0c\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u53cd\u9988\u96be\u4ee5\u76f4\u89c2\u89e3\u91ca\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u4f18\u8d28\u3001\u66f4\u5f00\u653e\u3001\u66f4\u5177\u89e3\u91ca\u6027\u7684\u8bc4\u6d4b\u65b9\u6cd5\u4ee5\u63a8\u52a8\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u4f5c\u8005\u63d0\u51faSANEval\u8bc4\u6d4b\u57fa\u51c6\uff1a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u672c\u63d0\u793a\u8fdb\u884c\u6df1\u5ea6\u7406\u89e3\uff0c\u518d\u7ed3\u5408\u7531LLM\u589e\u5f3a\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5bf9\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u7a7a\u95f4\u5173\u7cfb\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u6d4b\u3002\u8be5\u65b9\u6cd5\u6452\u5f03\u56fa\u5b9a\u8bcd\u8868\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u5f00\u653e\u5f0f\u3001\u7ec6\u7c92\u5ea6\u548c\u53ef\u89e3\u91ca\u7684\u7efc\u5408\u8bc4\u6d4b\u3002", "result": "\u5728\u516d\u4e2a\u4e3b\u6d41T2I\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aSANEval\u7684\u81ea\u52a8\u8bc4\u6d4b\u6307\u6807\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\uff08Spearman\u76f8\u5173\u7cfb\u6570\uff09\u9ad8\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u4e14\u5728\u5c5e\u6027\u7ed1\u5b9a\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u6570\u91cf\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7edf\u8ba1\u4e0a\u663e\u8457\u4e0d\u540c\u7684\u7ed3\u679c\u3002", "conclusion": "SANEval\u663e\u8457\u63d0\u5347\u4e86T2I\u6a21\u578b\u7684\u590d\u6742\u7ec4\u5408\u80fd\u529b\u8bc4\u6d4b\u6c34\u5e73\uff0c\u80fd\u591f\u4e3a\u6a21\u578b\u7814\u53d1\u63d0\u4f9b\u66f4\u53ef\u4fe1\u3001\u66f4\u7cbe\u7ec6\u7684\u8bca\u65ad\u548c\u53cd\u9988\u3002\u4f5c\u8005\u8fd8\u5c06\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u7ba1\u7ebf\uff0c\u4fc3\u8fdb\u540e\u7eed\u590d\u6742\u6587\u672c-\u56fe\u7247\u751f\u6210\u4e0e\u8bc4\u6d4b\u7814\u7a76\u3002"}}
{"id": "2602.00759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00759", "abs": "https://arxiv.org/abs/2602.00759", "authors": ["Zhipeng Chen", "Xiaobo Qin", "Wayne Xin Zhao", "Youbin Wu", "Ji-Rong Wen"], "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning", "comment": "21 pages, Working in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA^2D\u7684\u81ea\u9002\u5e94\u80fd\u529b\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u7684\u6548\u679c\u3002\u901a\u8fc7\u8ba9\u6a21\u578b\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u7b80\u5355\u5b50\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5b50\u95ee\u9898\u6307\u5bfc\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u4e0e\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7ed3\u5408(LRVR)\u7684\u573a\u666f\u4e0b\uff0c\u6a21\u578b\u83b7\u5f97\u7684\u4fe1\u606f\u6709\u9650\uff0c\u5bfc\u81f4\u53ea\u80fd\u8fdb\u884c\u76f2\u76ee\u63a2\u7d22\uff0c\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002\u4e3a\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u4f5c\u8005\u60f3\u8981\u5728\u4e0d\u4f9d\u8d56\u6559\u5e08\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u989d\u5916\u7684\u77e5\u8bc6\u4fe1\u53f7\u4ee5\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\uff0c\u901a\u8fc7RLVR\uff08\u4e0d\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\uff09\u8bad\u7ec3\u4e00\u4e2a\u5206\u89e3\u5668\uff0c\u8be5\u5206\u89e3\u5668\u80fd\u591f\u628a\u590d\u6742\u95ee\u9898\u62c6\u89e3\u6210\u4e00\u7cfb\u5217\u5b50\u95ee\u9898\uff1b\u5176\u6b21\uff0c\u5229\u7528\u5206\u89e3\u5668\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u96c6\u95ee\u9898\u6807\u6ce8\u5b50\u95ee\u9898\uff0c\u518d\u5bf9\u63a8\u7406\u5668(reasoner)\u5728\u8bbe\u6709\u5b50\u95ee\u9898\u6307\u5bfc\u4e0b\u8fdb\u884cRLVR\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u63d2\u62d4\u6a21\u5757\u53ef\u5e94\u7528\u4e8e\u591a\u79cdRLVR\u7b97\u6cd5\u3002", "result": "A^2D\u4e0e\u73b0\u6709\u5f3a\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\uff0c\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6548\u679c\u3002\u65b9\u6cd5\u6a21\u5757\u5316\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540cRLVR\u7b97\u6cd5\u3002\u5206\u6790\u53d1\u73b0\u5206\u89e3\u5668\u7684\u6307\u5bfc\u4fc3\u8fdb\u4e86\u63a8\u7406\u5668\u5bf9\u73af\u5883\u7684\u63a2\u7d22\u548c\u5229\u7528\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7ed3\u679c\u3002", "conclusion": "A^2D\u65b9\u6cd5\u80fd\u81ea\u52a8\u4e3aLLM\u751f\u6210\u5b50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86RLVR\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u8868\u73b0\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u3002\u8be5\u65b9\u6848\u4e3a\u5927\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u63a2\u7d22\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2602.01389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01389", "abs": "https://arxiv.org/abs/2602.01389", "authors": ["Michele Antonazzi", "Lorenzo Signorelli", "Matteo Luperto", "Nicola Basilico"], "title": "Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation", "comment": "Accepted for publication at ICRA 2026", "summary": "Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u5730\u56fe\u548c\u57fa\u7840\u6a21\u578b\u7684\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u5b9e\u9645\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u65e0\u9700\u76ee\u6807\u57df\u6807\u6ce8\u3002", "motivation": "\u8bed\u4e49\u5206\u5272\u6a21\u578b\u901a\u5e38\u5728\u65b0\u73af\u5883\uff08\u76ee\u6807\u57df\uff09\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u6570\u636e\uff08\u6e90\u57df\uff09\u4e0e\u5b9e\u9645\u5e94\u7528\u73af\u5883\u89c6\u89c9\u5206\u5e03\u4e0d\u4e00\u81f4\u65f6\u3002\u8fd9\u79cd\u57df\u504f\u79fb\u9650\u5236\u4e86\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\uff0c\u9700\u8981\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\u5229\u7528\u673a\u5668\u4eba\u5728\u73af\u5883\u4e2d\u6536\u96c6\u7684\u5927\u91cf\u6570\u636e\u8fdb\u884c\u81ea\u9002\u5e94\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u673a\u5668\u4eba\u6536\u96c6\u7684\u4e09\u7ef4\u4f53\u7d20\u5730\u56fe\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u4f2a\u6807\u7b7e\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982\u5927\u6a21\u578b\uff09\u7684\u96f6\u6837\u672c\u5b9e\u4f8b\u5206\u5272\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u7ec6\u5316\u4f2a\u6807\u7b7e\u4ee5\u4fdd\u8bc1\u5b9e\u4f8b\u7ea7\u4e00\u81f4\u6027\u3002\u6700\u7ec8\u5229\u7528\u8fd9\u4e9b\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u5bf9\u611f\u77e5\u6a21\u578b\u8fdb\u884c\u81ea\u76d1\u7763\u5fae\u8c03\uff0c\u5b9e\u73b0\u90e8\u7f72\u65f6\u7684\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u57fa\u4e8e\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u6700\u5148\u8fdbUDA\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u76ee\u6807\u57df\u7684\u771f\u5b9e\u6807\u7b7e\u3002", "conclusion": "\u7ed3\u54083D\u5730\u56fe\u5efa\u56fe\u4e0e\u57fa\u7840\u6a21\u578b\u5b9e\u4f8b\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u591a\u89c6\u89d2UDA\u4e2d\u5b9e\u4f8b\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd8\u63d0\u9ad8\u4e86\u76ee\u6807\u57df\u8bed\u4e49\u5206\u5272\u6548\u679c\uff0c\u6709\u52a9\u4e8e\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.00262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00262", "abs": "https://arxiv.org/abs/2602.00262", "authors": ["Huanran Li", "Daniel Pimentel-Alarc\u00f3n"], "title": "Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning", "comment": null, "summary": "Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7f3a\u5931\u6570\u636e\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\uff08CSC\uff09\uff0c\u901a\u8fc7\u751f\u6210\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u7684\u63a9\u7801\u89c6\u56fe\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u805a\u7c7b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u666e\u904d\u5047\u8bbe\u6570\u636e\u5b8c\u6574\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u7f3a\u5931\u6570\u636e\u73af\u5883\u4e0b\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u73b0\u5b9e\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u5347\u7b97\u6cd5\u5e94\u5bf9\u7f3a\u5931\u503c\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Contrastive Subspace Clustering\uff08CSC\uff09\u6846\u67b6\uff0c\u9488\u5bf9\u4e0d\u5b8c\u6574\u6570\u636e\u751f\u6210\u4e0d\u540c\u906e\u7f69\u89c6\u89d2\uff0c\u5229\u7528SimCLR\u98ce\u683c\u7684\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u83b7\u5f97\u4e0d\u53d8\u5d4c\u5165\u8868\u793a\uff0c\u4e4b\u540e\u91c7\u7528\u7a00\u758f\u5b50\u7a7a\u95f4\u805a\u7c7b\u6cd5\u5bf9\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCSC\u5728\u805a\u7c7b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u53ca\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "CSC\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b50\u7a7a\u95f4\u805a\u7c7b\u5728\u7f3a\u5931\u6570\u636e\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e0d\u5b8c\u6574\u6570\u636e\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u53ef\u884c\u7684\u65b9\u6848\u3002"}}
{"id": "2602.00760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00760", "abs": "https://arxiv.org/abs/2602.00760", "authors": ["Kaiyan Chang", "Chenwei Zhu", "Yingfeng Luo", "Yifu Huo", "Chenglong Wang", "Xiaoqian Liu", "Qiaozhi He", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards", "comment": "Under Review", "summary": "Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u73b0\u8c61\u8fdb\u884c\u4e86\u7ec6\u81f4\u5256\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u951a\u70b9\u7684\u5956\u52b1\u7b56\u7565\uff08APR\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u867d\u7136Test-Time Scaling\uff08TTS\uff09\u589e\u5f3a\u4e86\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u4f34\u968f\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u526f\u4f5c\u7528\uff0c\u5373\u6a21\u578b\u5728\u627e\u5230\u7b54\u6848\u540e\u4ecd\u91cd\u590d\u81ea\u6211\u9a8c\u8bc1\u3002\u8feb\u5207\u9700\u8981\u7406\u89e3\u548c\u6539\u8fdb\u8be5\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7cbe\u7ec6\u5206\u6790\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u5f15\u5165\u4e86Reasoning Anchor\uff08\u63a8\u7406\u951a\u70b9\uff09\u7684\u5b9a\u4e49\uff0c\u5373\u6a21\u578b\u7b54\u6848\u7b2c\u4e00\u6b21\u7a33\u5b9a\u7684\u4f4d\u7f6e\u3002\u968f\u540e\u63d0\u51faAnswer-Stable Tail\uff08AST\uff09\uff0c\u5b9a\u4e49\u4e3a\u951a\u70b9\u540e\u7684\u5197\u4f59\u91cd\u590d\u9a8c\u8bc1\u884c\u4e3a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faAnchor-based Process Reward\uff08APR\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4f4d\u951a\u70b9\u5e76\u5bf9AST\u90e8\u5206\u52a0\u4ee5\u60e9\u7f5a\uff0c\u7ed3\u5408\u9002\u7528\u4e8e\u957f\u5ea6\u60e9\u7f5a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u8bad\u7ec3\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684APR\u6a21\u578b\u57281.5B\u548c7B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u4e8e\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6027\u80fd-\u6548\u7387\u6298\u4e2d\u6700\u4f18\uff0c\u65e0\u9700\u6d88\u8017\u5927\u91cf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8d44\u6e90\u3002", "conclusion": "\u951a\u70b9\u8bc6\u522b\u4e0e\u7ed3\u6784\u5316\u5956\u60e9\u673a\u5236\u80fd\u663e\u8457\u51cf\u5c11\u65e0\u610f\u4e49\u7684\u63a8\u7406\u5197\u4f59\uff0c\u5728\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.01429", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01429", "abs": "https://arxiv.org/abs/2602.01429", "authors": ["Gonzalo Olguin", "Javier Ruiz-del-Solar"], "title": "Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors", "comment": "8 pages, 5 figures", "summary": "This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5730\u56fe\u7684\u5168\u5c40\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408CVAEs\u751f\u6210\u8f68\u8ff9\u548c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8bed\u4e49\u5206\u5272\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5728\u6237\u5916\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u5bfc\u822a\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u8be6\u7ec6\u5730\u56fe\uff0c\u7136\u800c\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e0b\u83b7\u53d6\u548c\u66f4\u65b0\u5730\u56fe\u5341\u5206\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u7cfb\u7edf\u7684\u5e94\u7528\u8303\u56f4\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u65e0\u9700\u5730\u56fe\u4e5f\u80fd\u53ef\u9760\u5de5\u4f5c\u7684\u5bfc\u822a\u65b9\u6848\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u7528\u4e8e\u5927\u91cf\u751f\u6210\u591a\u6837\u6027\u8f68\u8ff9\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u4e0b\u6309\u5f00\u653e\u8bcd\u6c47\u8bc4\u4f30\u3001\u9009\u62e9\u6700\u4f18\u8f68\u8ff9\uff0c\u6700\u540e\u5229\u7528\u73b0\u6709\u7684\u5c40\u90e8\u89c4\u5212\u5668\u6267\u884c\u901f\u5ea6\u63a7\u5236\u6307\u4ee4\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5bfc\u822a\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u771f\u5b9e\u6237\u5916\u73af\u5883\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5bfc\u822a\u6210\u529f\u7387\u548c\u6548\u7387\u4f18\u4e8e\u4e3b\u6d41\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u548c\u9009\u62e9\u591a\u6837\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u5730\u56fe\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5728\u6237\u5916\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00265", "abs": "https://arxiv.org/abs/2602.00265", "authors": ["Dong Liang", "Yuhao Liu", "Jinyuan Jia", "Youjun Zhao", "Rynson W. H. Lau"], "title": "World-Shaper: A Unified Framework for 360\u00b0 Panoramic Editing", "comment": null, "summary": "Being able to edit panoramic images is crucial for creating realistic 360\u00b0 visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360\u00b0 visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u7b49\u8ddd\u6295\u5f71\uff08ERP\uff09\u57df\u4e0b\u8fdb\u884c\u5168\u666f\u56fe\u50cf\u7f16\u8f91\u7684\u65b0\u65b9\u6cd5World-Shaper\uff0c\u663e\u8457\u63d0\u5347\u4e86360\u00b0\u5168\u666f\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u5168\u666f\u56fe\u50cf\u7f16\u8f91\u7684\u65b9\u6cd5\u591a\u57fa\u4e8e\u900f\u89c6\u56fe\uff0c\u96be\u4ee5\u5904\u7406\u5168\u666f\u56fe\u7684\u7a7a\u95f4\u7ed3\u6784\uff1b\u800c\u7acb\u65b9\u4f53\u6295\u5f71\u867d\u7136\u5c1d\u8bd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u51e0\u4f55\u4e0d\u5339\u914d\uff0c\u5e26\u6765\u5168\u5c40\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5728\u5168\u666f\u7684\u539f\u751f\u8868\u8fbe\u4e0a\u8fdb\u884c\u7f16\u8f91\u4e14\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u5728\u7b49\u8ddd\u6295\u5f71\u57df\u63d0\u51faWorld-Shaper\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1\uff09\u63d0\u51fagenerate-then-edit\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u5168\u666f\u751f\u6210\u9636\u6bb5\u5408\u6210\u591a\u6837\u7684\u7f16\u8f91\u6570\u636e\uff0c\u52a9\u529b\u76d1\u7763\u5f0f\u5b66\u4e60\uff1b2\uff09\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u91c7\u7528\u4f4d\u7f6e\u611f\u77e5\u7684\u5f62\u72b6\u76d1\u7763\u53ca\u9010\u6b65\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u663e\u5f0f\u548c\u9690\u5f0f\u7ed3\u5408\u5f3a\u5316\u5168\u666f\u7ed3\u6784\u5efa\u6a21\u3002", "result": "\u5728\u81ea\u5efa\u5168\u666f\u7f16\u8f91\u57fa\u51c6\uff08PEBench\uff09\u4e0a\uff0cWorld-Shaper\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u7f16\u8f91\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u53ef\u63a7\u6027\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u65b0\u65b9\u6cd5\uff08SOTA\uff09\u3002", "conclusion": "World-Shaper\u5b9e\u73b0\u4e86\u5728\u5355\u4e00\u6846\u67b6\u4e0b\u9ad8\u4e00\u81f4\u6027\u3001\u7075\u6d3b\u53ef\u63a7\u7684360\u00b0\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u5bf9\u865a\u62df\u73b0\u5b9e\u7b49\u9886\u57df\u7684\u5168\u666f\u5185\u5bb9\u521b\u4f5c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00762", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00762", "abs": "https://arxiv.org/abs/2602.00762", "authors": ["Yuheng Shao", "Junjie Xiong", "Chaoran Wu", "Xiyuan Wang", "Ziyu Zhou", "Yang Ouyang", "Qinyi Tao", "Quan Li"], "title": "WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs", "comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI' 26), April 13--17, 2026, Barcelona, Spain", "summary": "Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u4e92\u52a8\u5de5\u5177WordCraft\uff0c\u65e8\u5728\u8f85\u52a9\u4ee5\u4e2d\u6587\u4e3a\u6bcd\u8bed\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u66f4\u6709\u6548\u5730\u8fd0\u7528\u5173\u952e\u8bcd\u6cd5\u8bb0\u5fc6\u5355\u8bcd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u5de5\u5177\u6709\u6548\u63d0\u5347\u4e86\u8bb0\u5fc6\u6548\u679c\u53ca\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4ee5\u4e2d\u6587\u4e3a\u6bcd\u8bed\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u5728\u7528\u5173\u952e\u8bcd\u6cd5\u8bb0\u5fc6\u8bcd\u6c47\u65f6\u5b58\u5728\u660e\u663e\u56f0\u96be\uff0c\u5982\u96be\u4ee5\u6784\u601d\u6070\u5f53\u7684\u5173\u952e\u8bcd\u3001\u5173\u8054\u53ca\u610f\u8c61\uff0c\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u6307\u5bfc\u4e0d\u8db3\uff1b\u73b0\u6709\u65b9\u6cd5\u6216\u727a\u7272\u4e86\u5b66\u4e60\u8005\u4e3b\u52a8\u6027\uff0c\u6216\u7f3a\u4e4f\u8fc7\u7a0b\u5c42\u9762\u7684\u8f85\u52a9\u3002", "method": "\u4f5c\u8005\u9996\u5148\u901a\u8fc7\u5bf918\u540dL1\u4e2d\u6587\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u548c\u6559\u5e08\u7684\u8bbf\u8c08\uff0c\u5206\u6790\u5173\u952e\u8bcd\u6cd5\u5b9e\u8df5\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002\u968f\u540e\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u5f00\u53d1\u4e86WordCraft\u5de5\u5177\uff0c\u5f15\u5bfc\u5b66\u4e60\u8005\u5206\u6b65\u9009\u62e9\u5173\u952e\u8bcd\u3001\u6784\u5efa\u5173\u8054\u5e76\u5f62\u6210\u610f\u8c61\uff0c\u540c\u65f6\u5229\u7528MLLMs\u589e\u5f3a\u4e92\u52a8\u4f53\u9a8c\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4e24\u8f6e\u7528\u6237\u5b9e\u9a8c\u8bc4\u4f30\u5176\u6548\u679c\u53ca\u53ef\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWordCraft\u65e2\u80fd\u4fdd\u62a4\u5173\u952e\u8bcd\u6cd5\u7684\u751f\u6210\u6548\u5e94\uff0c\u53c8\u5177\u6709\u8f83\u9ad8\u7684\u8bb0\u5fc6\u6548\u679c\u548c\u7528\u6237\u53ef\u7528\u6027\u3002", "conclusion": "WordCraft\u4f5c\u4e3a\u4e00\u4e2a\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u3001\u57fa\u4e8eMLLMs\u7684\u4e92\u52a8\u5de5\u5177\uff0c\u6709\u6548\u5f25\u8865\u4e86\u4f20\u7edf\u5173\u952e\u8bcd\u6cd5\u8f85\u5bfc\u7684\u4e0d\u8db3\uff0c\u4e3a\u4e8c\u8bed\u8bcd\u6c47\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.01448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01448", "abs": "https://arxiv.org/abs/2602.01448", "authors": ["Harshith Jella", "Pejman Kheradmand", "Joseph Klein", "Behnam Moradkhani", "Yash Chitalia"], "title": "Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression", "comment": null, "summary": "This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable \"ring mechanism\", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5305\u62ec\u592a\u7a7a\u7ad9\u7b49\u7279\u6b8a\u73af\u5883\u7684\u7d27\u6025\u60c5\u51b5\u4e0b\u5904\u7406\u5927\u51fa\u8840\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u53ef\u53d8\u5f62\u73af\u7ed3\u6784\uff0c\u901a\u8fc7\u6c14\u56ca\u548c\u5145\u6c14\u73af\u63d0\u4f9b\u5747\u5300\u538b\u529b\uff0c\u6709\u6548\u6b62\u8840\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u529b\u5b66\u6027\u80fd\u548c\u6b62\u8840\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6b62\u8840\u63aa\u65bd\u5982\u6b62\u8840\u5e26\u5728\u67d0\u4e9b\u7279\u6b8a\u73af\u5883\uff08\u5982\u592a\u7a7a\u7ad9\uff09\u6216\u89e3\u5256\u90e8\u4f4d\uff08\u5982\u8179\u90e8\u3001\u80cc\u90e8\u3001\u9888\u90e8\u7b49\uff09\u96be\u4ee5\u6709\u6548\u5e94\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u66f4\u5177\u9002\u5e94\u6027\u7684\u6b62\u8840\u6280\u672f\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u80fd\u4ece\u5706\u5f62\u53d8\u4e3a\u692d\u5706\u5f62\u7684\u53ef\u53d8\u5f62\u73af\u673a\u5236\uff0c\u5e76\u914d\u5408\u4e0d\u540c\u67d4\u97e7\u6027\u7684\u73af\u81c2\u548c\u5145\u6c14\u6c14\u56ca\uff0c\u63d0\u5347\u5176\u5bf9\u8eab\u4f53\u4e0d\u540c\u90e8\u4f4d\u7684\u9002\u5e94\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u73af\u81c2\u7684\u5f2f\u66f2\u521a\u5ea6\uff0c\u4ee5\u53ca\u6c14\u56ca\u7cfb\u7edf\u7684\u538b\u529b\u8f93\u51fa\uff0c\u6700\u540e\u5728\u6a21\u62df\u4f24\u5458\u88c5\u7f6e\u4e0a\u8bc4\u4f30\u4e86\u6b62\u8840\u6548\u679c\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u4e0d\u540c\u914d\u7f6e\u7684\u7cfb\u7edf\u5747\u80fd\u63d0\u4f9b\u6709\u6548\u4e14\u5747\u5300\u7684\u538b\u529b\uff0c\u5177\u5907\u4e00\u5b9a\u6b62\u8840\u80fd\u529b\u3002\u4f46\u8bbe\u5907\u5728\u8986\u76d6\u9762\u79ef\u548c\u5bf9\u590d\u6742\u89e3\u5256\u90e8\u4f4d\u7684\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u5728\u6a21\u62df\u73af\u5883\u4e0b\u6210\u529f\u63a7\u5236\u5927\u51fa\u8840\uff0c\u663e\u793a\u51fa\u5728\u6781\u7aef\u6216\u7279\u6b8a\u73af\u5883\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u5176\u5bf9\u590d\u6742\u90e8\u4f4d\u7684\u9002\u5e94\u6027\u548c\u8986\u76d6\u80fd\u529b\u3002"}}
{"id": "2602.00267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00267", "abs": "https://arxiv.org/abs/2602.00267", "authors": ["Gemma Canet Tarr\u00e9s", "Manel Baradad", "Francesc Moreno-Noguer", "Yumeng Li"], "title": "PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories", "comment": null, "summary": "Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PLACID\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u751f\u6210\u5f0fAI\u5728\u591a\u7269\u4f53\u5408\u6210\u65f6\u5b58\u5728\u7684\u4e3b\u4f53\u9519\u6f0f\u3001\u5e03\u5c40\u6df7\u4e71\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u4f53\u8eab\u4efd\u3001\u80cc\u666f\u548c\u8272\u5f69\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u9ad8\u8d28\u91cf\u56fe\u7247\u751f\u6210AI\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u7269\u4f53\u7684\u4e13\u4e1a\u7ea7\u5408\u6210\u4efb\u52a1\u65f6\uff0c\u5e38\u5e38\u51fa\u73b0\u7269\u4f53\u7ec6\u8282\u4e22\u5931\u3001\u6f0f\u5217\u3001\u91cd\u590d\u548c\u5e03\u5c40\u6bd4\u4f8b\u4e0d\u5f53\u7b49\u5173\u952e\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u8bbe\u8ba1\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u5408\u6210\u8d28\u91cf\u548c\u63a7\u5236\u529b\u3002", "method": "\u63d0\u51faPLACID\u6846\u67b6\uff1a1\uff09\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08I2V\uff09\u5e76\u7ed3\u5408\u6587\u672c\u63a7\u5236\uff0c\u901a\u8fc7\u89c6\u9891\u65f6\u95f4\u5148\u9a8c\u589e\u5f3a\u5408\u6210\u4e2d\u591a\u7269\u4f53\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u4e0e\u80cc\u666f\u8fd8\u539f\uff1b2\uff09\u63d0\u51fa\u5168\u65b0\u6570\u636e\u6574\u7406\u65b9\u5f0f\uff0c\u5408\u6210\u5e8f\u5217\u6837\u672c\u4f7f\u7269\u4f53\u4ece\u968f\u673a\u4f4d\u7f6e\u5e73\u6ed1\u79fb\u52a8\u5230\u76ee\u6807\u5e03\u5c40\uff0c\u63d0\u5347\u6a21\u578b\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\uff0c\u5b9e\u73b0\u968f\u673a\u5206\u5e03\u7269\u4f53\u81ea\u52a8\u751f\u6210\u5408\u7406\u5e03\u5c40\u7684\u5408\u6210\u56fe\u3002", "result": "\u4e30\u5bcc\u7684\u5b9a\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u8c03\u7814\u8868\u660e\uff0cPLACID\u65b9\u6cd5\u5728\u591a\u7269\u4f53\u5408\u6210\u7684\u7269\u4f53\u8eab\u4efd\u3001\u80cc\u666f\u8272\u5f69\u8fd8\u539f\uff0c\u4ee5\u53ca\u7269\u4f53\u5b8c\u6574\u6027\u4e0e\u7f8e\u89c2\u6027\u65b9\u9762\uff0c\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "PLACID\u663e\u8457\u6539\u5584\u4e86\u591a\u7269\u4f53\u56fe\u50cf\u5408\u6210\u5728\u8eab\u4efd\u4fdd\u7559\u3001\u5e03\u5c40\u63a7\u5236\u548c\u6574\u4f53\u7f8e\u89c2\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u5927\u89c4\u6a21\u4ea7\u54c1\u5c55\u793a\u6216\u8bbe\u8ba1\u5408\u6210\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00769", "abs": "https://arxiv.org/abs/2602.00769", "authors": ["Siyu Yan", "Lusha Zhu", "Jian-Qiao Zhu"], "title": "Eliciting Trustworthiness Priors of Large Language Models via Economic Games", "comment": null, "summary": "One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u7684\u4fe1\u4efb\u503e\u5411\uff0c\u53d1\u73b0GPT-4.1\u7684\u4fe1\u4efb\u5ea6\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u5e76\u901a\u8fc7\u4fe1\u4efb\u535a\u5f08\u63a2\u8ba8\u5176\u5bf9\u4e0d\u540c\u89d2\u8272\u7684\u4fe1\u4efb\u51b3\u7b56\u3002", "motivation": "\u5728\u6784\u5efa\u4ee5\u4eba\u4e3a\u672c\u3001\u503c\u5f97\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u65f6\uff0c\u7ef4\u6301\u6070\u5f53\u7684\u4fe1\u4efb\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9AI\u672c\u8eab\u4fe1\u4efb\u7a0b\u5ea6\u7684\u523b\u753b\u65b9\u6cd5\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u4e3a\u4e86\u89e3\u548c\u91cf\u5316AI\u6a21\u578b\u7684\u4fe1\u4efb\u503e\u5411\u63d0\u4f9b\u65b9\u6cd5\u5b66\u521b\u65b0\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u884c\u4e3a\u535a\u5f08\u8bba\u4e2d\u7684\u4fe1\u4efb\u535a\u5f08\uff0c\u5e94\u7528\u8fed\u4ee3\u6027in-context learning\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u5148\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u4fe1\u4efb\u503e\u5411\uff08trustworthiness priors\uff09\uff0c\u5e76\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u7684\u4fe1\u4efb\u5ea6\u8868\u73b0\u3002\u540c\u65f6\u7814\u7a76\u6a21\u578b\u5bf9\u4e0d\u540c\u73a9\u5bb6\u8eab\u4efd\u7684\u4fe1\u4efb\u53cd\u5e94\uff0c\u5e76\u7528\u57fa\u4e8e\u523b\u677f\u5370\u8c61\u7684\u6a21\u578b\uff08\u57fa\u4e8e\u6e29\u6696\u611f\u548c\u80fd\u529b\u611f\u77e5\uff09\u8fdb\u884c\u89e3\u91ca\u3002", "result": "GPT-4.1\u7684\u4fe1\u4efb\u503e\u5411\u4e0e\u4eba\u7c7b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u53ef\u4ee5\u533a\u5206\u4e0d\u540c\u89d2\u8272\u95f4\u7684\u4fe1\u4efb\u5ea6\uff0c\u800c\u8fd9\u79cd\u4fe1\u4efb\u5ea6\u5dee\u5f02\u53ef\u4ee5\u901a\u8fc7\u6e29\u6696\u548c\u80fd\u529b\u4e24\u4e2a\u7ef4\u5ea6\u7684\u523b\u677f\u5370\u8c61\u6709\u6548\u9884\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u65b9\u6cd5\u80fd\u6709\u6548\u91cf\u5316AI\u6a21\u578b\u4e2d\u7684\u4fe1\u4efb\u503e\u5411\uff0cGPT-4.1\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u4efb\u535a\u5f08\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u3002\u8fd9\u4e3a\u7406\u89e3\u4e0e\u4f18\u5316AI\u4e0e\u4eba\u7684\u534f\u4f5c\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.01501", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01501", "abs": "https://arxiv.org/abs/2602.01501", "authors": ["Minwoo Jung", "Nived Chebrolu", "Lucas Carvalho de Lima", "Haedam Oh", "Maurice Fallon", "Ayoung Kim"], "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching", "comment": "An 8-page paper with 7 tables and 8 figures, accepted to ICRA 2026", "summary": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TreeLoc\uff0c\u4e00\u79cd\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u68ee\u6797\u573a\u666f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u80fd\u591f\u5728GPS\u4fe1\u53f7\u5dee\u3001\u7ed3\u6784\u590d\u6742\u7684\u68ee\u6797\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5168\u5c40\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u7684\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u57ce\u5e02\u73af\u5883\uff0c\u5047\u8bbe\u6709\u72ec\u7279\u7684\u7ed3\u6784\u7279\u5f81\u7528\u4e8e\u8bc6\u522b\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5e76\u4e0d\u9002\u7528\u4e8e\u7279\u5f81\u91cd\u590d\u3001\u906e\u6321\u548c\u7ed3\u6784\u590d\u6742\u7684\u68ee\u6797\u3002\u9700\u8981\u4e13\u95e8\u9762\u5411\u68ee\u6797\u73af\u5883\u7684\u5b9a\u4f4d\u65b9\u6cd5\u6765\u63d0\u5347\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "method": "TreeLoc \u4ee5\u6811\u5e72\u53ca\u5176\u80f8\u5f84(DBH)\u4e3a\u7279\u5f81\uff0c\u901a\u8fc7\u5176\u8f74\u7ebf\u5bf9\u9f50\u5b9e\u73b0\u573a\u666f\u6807\u51c6\u5316\u3002\u5148\u7528\u6811\u5206\u5e03\u76f4\u65b9\u56fe\u8fdb\u884c\u7c97\u5339\u914d\uff0c\u7136\u540e\u5229\u75282D\u4e09\u89d2\u5f62\u63cf\u8ff0\u5b50\u8fdb\u884c\u7cbe\u7ec6\u5339\u914d\uff0c\u6700\u540e\u7ed3\u5408\u4e24\u6b65\u51e0\u4f55\u9a8c\u8bc1\u5b9e\u73b06\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u3002\u7b97\u6cd5\u7ecf\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9010\u6b65\u9a8c\u8bc1\u5404\u6a21\u5757\u8d21\u732e\u3002", "result": "\u5728\u591a\u4e2a\u68ee\u6797\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0cTreeLoc \u660e\u663e\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5b9a\u4f4d\u7cbe\u5ea6\u9ad8\u3002\u6b64\u5916\uff0c\u6d88\u878d\u5b9e\u9a8c\u5c55\u793a\u4e86\u5404\u90e8\u5206\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TreeLoc \u4e3a\u68ee\u6797\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u5e26\u6765\u4e86\u9c81\u68d2\u3001\u5b9e\u7528\u7684\u65b0\u65b9\u6848\u3002\u7ed3\u5408\u7d27\u51d1\u7684\u5168\u5c40\u6811\u6570\u636e\u5e93\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u68ee\u6797\u7ba1\u7406\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u5b66\u672f\u4e0e\u5de5\u7a0b\u793e\u533a\u3002"}}
{"id": "2602.00268", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00268", "abs": "https://arxiv.org/abs/2602.00268", "authors": ["Ariel Shaulov", "Eitan Shaar", "Amit Edenzon", "Lior Wolf"], "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation", "comment": null, "summary": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7f13\u89e3\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u95f4\u6f02\u79fb\uff08Temporal Drift\uff09\u7684\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u662f\u5728\u63a8\u7406\u65f6\u8bc6\u522b\u5e76\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728token\uff0c\u4ee5\u9632\u6b62\u9519\u8bef\u79ef\u7d2f\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u5e8f\u5217\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u8fed\u4ee3\u751f\u6210\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u5373\u8bef\u5dee\u968f\u65f6\u95f4\u7d2f\u79ef\u5e76\u653e\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u672a\u6709\u6548\u89e3\u51b3\u9519\u8bef\u4f20\u64ad\u5bfc\u81f4\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u7f13\u89e3\u673a\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\uff0c\u65f6\u95f4\u6f02\u79fb\u4e3b\u8981\u6e90\u81ea\u63a8\u7406\u9636\u6bb5\u4e0d\u53d7\u63a7\u5730\u91cd\u590d\u4f7f\u7528\u53d7\u635f\u7684\u6f5c\u5728token\u3002\u4e3a\u6b64\uff0c\u8bba\u6587\u5728\u63a8\u7406\u65f6\u68c0\u6d4b\u5e76\u79fb\u9664\u8868\u73b0\u51fa\u5411\u524d\u4e00\u5e27\u663e\u8457\u504f\u5dee\u7684\u4e0d\u7a33\u5b9a\u6f5c\u5728token\uff0c\u4ece\u800c\u907f\u514d\u5df2\u53d7\u635f\u7684\u4fe1\u606f\u8fdb\u5165\u540e\u7eed\u751f\u6210\u6b65\u9aa4\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u3001\u8bad\u7ec3\u8fc7\u7a0b\u6216\u79bb\u5f00\u6f5c\u7a7a\u95f4\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u5e8f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u65f6\u95f4\u6f02\u79fb\u73b0\u8c61\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u548c\u8bad\u7ec3\u6d41\u7a0b\u7684\u524d\u63d0\u4e0b\uff0c\u4e5f\u80fd\u5e26\u6765\u53ef\u89c2\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u63a8\u7406\u65f6token\u7b5b\u9664\u673a\u5236\uff0c\u80fd\u663e\u8457\u7f13\u89e3\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u957f\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u601d\u8def\uff0c\u540c\u65f6\u5177\u5907\u4f4e\u4fb5\u5165\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.00770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00770", "abs": "https://arxiv.org/abs/2602.00770", "authors": ["Siyuan Zhang", "Jialian Li", "Yichi Zhang", "Xiao Yang", "Yinpeng Dong", "Hang Su"], "title": "Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models", "comment": "30 pages, 27 figures, 8 tables", "summary": "Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5185\u90e8\u8868\u793a\u53d8\u5316\uff0c\u53d1\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8868\u5f81\u5206\u5e03\u8f6c\u79fb\uff0c\u8bad\u7ec3\u540e\u6a21\u578b\u80fd\u66f4\u597d\u5730\u9a71\u52a8\u8fd9\u4e00\u8f6c\u79fb\uff0c\u5185\u90e8\u8868\u5f81\u4e0e\u8f93\u51fa\u6b63\u786e\u7387\u5f3a\u76f8\u5173\uff0c\u63a8\u7406\u8868\u73b0\u63d0\u5347\u4e3b\u8981\u4f9d\u8d56\u751f\u6210token\u7684\u8bed\u4e49\u800c\u975e\u53c2\u6570\u672c\u8eab\u6216\u63a8\u7406\u65f6\u8ba1\u7b97\u8fc7\u7a0b\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u591a\u628a\u5927\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u53ea\u5206\u6790\u8f93\u5165\u8f93\u51fa\uff0c\u65e0\u6cd5\u63ed\u793a\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65f6\u5185\u90e8\u72b6\u6001\u7684\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u7684\u8868\u793a\u52a8\u6001\uff0c\u6df1\u5165\u7406\u89e3\u63a8\u7406\u8868\u73b0\u63d0\u5347\u7684\u672c\u8d28\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7528\u8868\u793a\u89c6\u89d2\u5206\u6790\u63a8\u7406\u80fd\u529b\u53d8\u5316\uff0c\u8ddf\u8e2a\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u7684\u6a21\u578b\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u76d1\u6d4b\u5176\u5185\u90e8\u8868\u793a\u5206\u5e03\uff0c\u5e76\u7528\u7edf\u8ba1\u548c\u53cd\u4e8b\u5b9e\u5b9e\u9a8c\u5206\u6790\u5185\u90e8\u8868\u5f81\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4ece\u800c\u627e\u5230\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "1\uff09\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u521d\u59cb\u9759\u6001\u8868\u5f81\u8d28\u91cf\u63d0\u5347\u6709\u9650\uff1b2\uff09\u63a8\u7406\u8fc7\u7a0b\u8868\u5f81\u5206\u5e03\u4f1a\u6301\u7eed\u8f6c\u79fb\uff0c\u4e14\u8fd9\u79cd\u8f6c\u79fb\u533a\u522b\u4e8e\u975e\u63a8\u7406\u4efb\u52a1\uff1b3\uff09\u8bad\u7ec3\u63d0\u5347\u4e86\u6a21\u578b\u9a71\u52a8\u8868\u5f81\u8f6c\u79fb\u4ee5\u83b7\u5f97\u66f4\u4f18\u4efb\u52a1\u5206\u5e03\u7684\u80fd\u529b\uff1b4\uff09\u6700\u7ec8\u8f93\u51fa\u6b63\u786e\u6027\u4e0e\u5185\u90e8\u8868\u5f81\u9ad8\u5ea6\u76f8\u5173\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e3b\u8981\u6e90\u81ea\u751f\u6210token\u7684\u8bed\u4e49\u800c\u975e\u53c2\u6570\u6216\u63a8\u7406\u65f6\u989d\u5916\u8ba1\u7b97\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u7684\u5185\u90e8\u673a\u5236\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u5206\u6790\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5373\u5173\u6ce8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8868\u5f81\u8f6c\u79fb\u548c\u8f93\u51fatoken\u8bed\u4e49\u5bf9\u63a8\u7406\u7ed3\u679c\u7684\u9a71\u52a8\u4f5c\u7528\u3002"}}
{"id": "2602.01515", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01515", "abs": "https://arxiv.org/abs/2602.01515", "authors": ["Humphrey Munn", "Brendan Tidd", "Peter Bohm", "Marcus Gallagher", "David Howard"], "title": "RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots", "comment": null, "summary": "Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAPT\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u3001\u8f7b\u91cf\u7ea7\u7684\u90e8\u7f72\u65f6\u76d1\u63a7\u5668\uff0c\u80fd\u9ad8\u6548\u76d1\u6d4b\u548c\u89e3\u91ca\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u4eff\u771f\u5230\u73b0\u5b9e\uff08Sim-to-Real\uff09\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5e76\u652f\u6301\u9ad8\u9891\uff0850Hz\uff09\u63a7\u5236\u3002RAPT\u4e0d\u4ec5\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u8fd8\u80fd\u6301\u7eed\u91cf\u5316\u90e8\u7f72\u4e0e\u8bad\u7ec3\u7684\u504f\u5dee\uff0c\u5e76\u81ea\u52a8\u5206\u6790\u6545\u969c\u6839\u6e90\u3002", "motivation": "\u73b0\u6709\u7c7b\u4eba\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u5728\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u65f6\uff0c\u6613\u5bfc\u81f4\u2018\u6c89\u9ed8\u5f0f\u2019\u5931\u6548\uff0c\u9020\u6210\u786c\u4ef6\u98ce\u9669\u3002\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u9ad8\u9891\u63a7\u5236\uff0c\u5bf9\u6781\u4f4e\u8bef\u62a5\u8981\u6c42\u4e0b\u6027\u80fd\u8f83\u5dee\uff0c\u4e14\u5927\u591a\u4e3a\u2018\u9ed1\u7bb1\u2019\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u56e0\u6b64\u4e9f\u9700\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u4e0e\u6545\u969c\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51faRAPT\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5efa\u6a21\u4eff\u771f\u4e2d\u7684\u7a7a\u95f4-\u65f6\u95f4\u6982\u7387\u6d41\u5f62\uff0c\u5e76\u5728\u90e8\u7f72\u65f6\u5bf9\u6bcf\u7ef4\u4fe1\u53f7\u8fdb\u884c\u6821\u51c6\u7684\u9884\u6d4b\u504f\u5dee\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5728\u7ebfOOD\uff08\u5206\u5e03\u5916\uff09\u68c0\u6d4b\u4e0e\u6f02\u79fb\u5ea6\u91cf\u3002\u8fd8\u7ed3\u5408\u57fa\u4e8e\u68af\u5ea6\u7684\u65f6\u5e8f\u663e\u8457\u6027\u5206\u6790\u548cLLM\u63a8\u7406\uff0c\u5b9e\u73b0\u81ea\u52a8\u6545\u969c\u6839\u56e0\u8bca\u65ad\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4eff\u771f\u4e2d\uff0cRAPT\u5728\u56fa\u5b9a0.5%\u96c6\u6570\u8bef\u62a5\u7387\u4e0b\u5c06TPR\u63d0\u5347\u4e8637%\uff0c\u5b9e\u7269\u90e8\u7f72\u4e2dTPR\u63d0\u534712.5%\u3002RAPT\u8fd8\u5b9e\u73b0\u4e86\u5bf916\u6b21\u771f\u5b9e\u6545\u969c\u768475%\u6839\u56e0\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e14\u4ec5\u7528\u672c\u4f53\u4f20\u611f\u6570\u636e\u3002", "conclusion": "RAPT\u80fd\u9ad8\u6548\u3001\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u5730\u76d1\u63a7\u7c7b\u4eba\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7387\uff0c\u5e76\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u6839\u56e0\u8bca\u65ad\u548c\u6301\u7eed\u6027\u80fd\u8ffd\u8e2a\uff0c\u5bf9\u5b89\u5168\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00288", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00288", "abs": "https://arxiv.org/abs/2602.00288", "authors": ["Baiqi Li", "Kangyi Zhao", "Ce Zhang", "Chancharik Mitra", "Jean de Dieu Nyandwi", "Gedas Bertasius"], "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs", "comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/", "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TimeBlind\u57fa\u51c6\uff0c\u7528\u4e8e\u8bca\u65ad\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u89c6\u9891\u65f6\u7a7a\u7406\u89e3\u4e2d\u7684\u77ed\u677f\uff0c\u5c24\u5176\u662f\u5bf9\u65f6\u5e8f\u52a8\u6001\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u65f6\u5e8f\u7406\u89e3\u4efb\u52a1\u4e0a\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u5185\u5bb9\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u52a8\u6001\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u5173\u7cfb\u7406\u89e3\u8584\u5f31\u3002\u7f3a\u4e4f\u80fd\u7cbe\u7ec6\u533a\u5206\u6a21\u578b\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u7684\u4e13\u4e1a\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u63d0\u51faTimeBlind\u57fa\u51c6\uff1a\u4ee5\u89c6\u9891\u6700\u5c0f\u5bf9\uff08\u5185\u5bb9\u9759\u6001\u89c6\u89c9\u4fe1\u606f\u4e00\u81f4\uff0c\u4ec5\u65f6\u5e8f\u7ed3\u6784\u4e0d\u540c\uff09\u548c\u4e92\u8865\u5f0f\u95ee\u9898\u6d88\u9664\u8bed\u8a00\u504f\u89c1\u3002\u65f6\u95f4\u7406\u89e3\u5206\u4e3a\u4e09\u5c42\u6b21\uff1a\u4e8b\u4ef6\u8fa8\u8bc6\u3001\u4e8b\u4ef6\u5c5e\u6027\u63cf\u8ff0\u3001\u4e8b\u4ef6\u95f4\u5173\u7cfb\u63a8\u7406\u3002\u8bc4\u4f30\u4e8620\u591a\u79cd\u524d\u6cbf\u591a\u6a21\u6001\u6a21\u578b\u5171600\u7ec4\uff082400\u5bf9\uff09\u89c6\u9891\u95ee\u9898\u5bf9\u3002", "result": "\u6700\u4f73\u591a\u6a21\u6001\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u533a\u5206\u51c6\u786e\u7387\u4ec5\u4e3a48.2%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768498.2%\u3002\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u66f4\u4f9d\u8d56\u4e8e\u9759\u6001\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u975e\u771f\u6b63\u7684\u65f6\u5e8f\u903b\u8f91\u3002", "conclusion": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u65f6\u7a7a\u7406\u89e3\u4e0a\u4ecd\u6709\u5de8\u5927\u77ed\u677f\uff0cTimeBlind\u6210\u4e3a\u63a8\u52a8\u4e0b\u4ee3\u89c6\u9891\u7406\u89e3\u548c\u6a21\u578b\u8bca\u65ad\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.00777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00777", "abs": "https://arxiv.org/abs/2602.00777", "authors": ["Xuan Ai", "Qingqing Yang", "Peng Wang", "Lei Deng", "Lin Zhang", "Renhai Chen", "Gong Zhang"], "title": "HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference", "comment": null, "summary": "Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\\bf HyLRA} ({\\bf Hy}brid {\\bf L}ayer {\\bf R}euse {\\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \\textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \\textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\\%--46\\% while maintaining comparable performance (with $<1\\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \\href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\\texttt{/r/unified-cache-management-CF80/}}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyLRA\uff0c\u4e00\u79cd\u7ed3\u5408\u5c42\u95f4\u548c\u5c42\u5185\u7a00\u758f\u7279\u6027\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u4e86KV\u7f13\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u969c\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u3002", "motivation": "\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u4ee5\u53caKV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u867d\u53ef\u6539\u5584\u4f46\u5f80\u5f80\u6548\u7387\u548c\u51c6\u786e\u7387\u96be\u4e24\u5168\uff0c\u4e9f\u9700\u5bfb\u6c42\u66f4\u4f18\u6298\u4e2d\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5b58\u5728\u201c\u5c42\u5185\u654f\u611f\u6027\u201d\uff08\u90e8\u5206\u5c42\u9700\u5b8c\u6574\u6ce8\u610f\u529b\u4ee5\u9632\u7279\u5f81\u635f\u5931\uff09\u548c\u201c\u5c42\u95f4\u76f8\u4f3c\u6027\u201d\uff08\u8fde\u7eed\u5c42\u5173\u952etoken\u9ad8\u5ea6\u4e00\u81f4\uff09\uff0c\u636e\u6b64\u8bbe\u8ba1HyLRA\u6df7\u5408\u673a\u5236\uff1a\u4e3a\u654f\u611f\u5c42\u4fdd\u7559\u5168\u6ce8\u610f\u529b\uff0c\u5bb9\u5fcd\u5c42\u76f4\u63a5\u91cd\u7528\u4e0a\u5c42top-k\u7d22\u5f15\uff0c\u4ece\u800c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u3002\u5177\u4f53\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u79bb\u7ebf\u83b7\u5f97\u3002", "result": "\u5728\u5b9e\u9645\u5927\u6a21\u578b\u957f\u6587\u672c\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cHyLRA\u76f8\u6bd4\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u53476%-46%\uff0c\u4e14\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e1%\uff1b\u5728\u5404\u79cd\u57fa\u51c6\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "HyLRA\u80fd\u517c\u987e\u63a8\u7406\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u6709\u6548\u7a81\u7834\u7a20\u5bc6\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u662f\u63d0\u5347\u5927\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u7684\u4f18\u9009\u65b9\u6848\u3002"}}
{"id": "2602.01535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01535", "abs": "https://arxiv.org/abs/2602.01535", "authors": ["Huzaifa Mustafa Unjhawala", "Khizar Shaikh", "Luning Bakke", "Radu Serban", "Dan Negrut"], "title": "Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations", "comment": "19 pages, 15 figures", "summary": "While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u4fdd\u771f\u3001\u5168\u8f66\u95ed\u73af\u4eff\u771f\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u53ef\u53d8\u5f62\u5730\u5f62\u4e0a\u534f\u540c\u4f18\u5316\u8d8a\u91ce\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8f6e\u5b50\u8bbe\u8ba1\u548c\u63a7\u5236\u5668\u53c2\u6570\u3002\u91c7\u7528\u8fde\u7eed\u4ecb\u8d28\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u6548\u7387\uff0c\u652f\u6301\u5728\u590d\u6742\u8f68\u8ff9\u4e0a\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u7406\u60f3\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u5e73\u8861\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4eff\u771f\u65b9\u6848\u7684\u5f00\u6e90\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u5143\u4eff\u771f\uff08DEM\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5168\u8f66\u4eff\u771f\u7684\u60c5\u5f62\uff0c\u9650\u5236\u4e86\u673a\u68b0\u4e0e\u63a7\u5236\u534f\u540c\u4f18\u5316\u7684\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8fde\u7eed\u4ecb\u8d28\u5efa\u6a21\uff08CRM\uff09\uff0c\u6784\u5efa\u9ad8\u4fdd\u771f\u7684\u5168\u8f66\u7ea7\u95ed\u73af\u4eff\u771f\u5e73\u53f0\uff0c\u8054\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\u534f\u540c\u8c03\u6574\u8f6e\u5b50\u53c2\u6570\uff08\u5982\u534a\u5f84\u3001\u5bbd\u5ea6\u3001\u51f8\u8d77\u5f62\u72b6\uff09\u548c\u8f6c\u5411PID\u63a7\u5236\u53c2\u6570\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u51fd\u6570\uff08\u901f\u5ea6\u3001\u8ddf\u8e2a\u8bef\u5dee\u3001\u80fd\u8017\uff09\u5f15\u5bfc\u8bbe\u8ba1\u3002\u6bd4\u8f83\u4e86\u8f6e\u5b50\u4e0e\u63a7\u5236\u5668\u540c\u6b65\u4e0e\u5206\u6b65\u4f18\u5316\u7684\u4e24\u79cd\u7b56\u7565\uff0c\u5e76\u5728\u5927\u91cf\u4eff\u771f\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\u548c\u8ba1\u7b97\u6d88\u8017\u3002", "result": "\u57283,000\u7ec4\u5168\u8f66\u7ea7\u4eff\u771f\u4e2d\uff0c\u4f18\u5316\u6d41\u7a0b\u4ec5\u97005-9\u5929\u5b8c\u6210\uff0c\u6bd4\u539f\u672cDEM\u65b9\u6848\u8282\u7701\u6570\u6708\u65f6\u95f4\u3002\u5b9e\u7269\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4eff\u771f\u4f18\u5316\u8d8b\u52bf\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u65b0\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u8d8a\u91ce\u8f66\u8f86\u5728\u53ef\u53d8\u5f62\u5730\u5f62\u4eff\u771f\u4e2d\u7684\u8bbe\u8ba1\u4f18\u5316\u6548\u7387\uff0c\u4f7f\u5f97\u8f6e\u5b50\u4e0e\u63a7\u5236\u5668\u534f\u540c\u4f18\u5316\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002\u5f00\u6e90\u7684\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2602.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00289", "abs": "https://arxiv.org/abs/2602.00289", "authors": ["Alan Yuille", "Daniel Kersten"], "title": "Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory", "comment": null, "summary": "This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.", "AI": {"tldr": "\u672c\u6587\u7528\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\uff08BDT\uff09\u4e3a\u7406\u8bba\u89c6\u89d2\uff0c\u7b80\u8981\u4ecb\u7ecd\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u91cd\u70b9\u6bd4\u8f83\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e24\u8005\u7ed3\u5408\u7684\u524d\u666f\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u5df2\u6210\u4e3a\u5e9e\u5927\u800c\u590d\u6742\u7684\u9886\u57df\uff0c\u5176\u4e2d\u4e0d\u540c\u7406\u8bba\u4e0e\u65b9\u6cd5\u5404\u5177\u4f18\u52bf\u548c\u5c40\u9650\u3002\u901a\u8fc7\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u8fd9\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u53ef\u4ee5\u5398\u6e05\u8d1d\u53f6\u65af\u89c2\u70b9\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u53ca\u5176\u4e0e\u8ba4\u77e5\u79d1\u5b66\u5173\u7cfb\u4e2d\u7684\u4f5c\u7528\uff0c\u63a8\u8fdb\u7406\u8bba\u53d1\u5c55\u548c\u65b9\u6cd5\u6574\u5408\u3002", "method": "\u672c\u6587\u91c7\u7528\u7406\u8bba\u5206\u6790\u548c\u6982\u5ff5\u6bd4\u8f83\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u4ecb\u7ecd\u4e86\u8d1d\u53f6\u65af\u65b9\u6cd5\uff08\u5177\u6709\u8ba4\u77e5\u79d1\u5b66\u5171\u9e23\u7684\u7406\u8bba\u5438\u5f15\u529b\uff09\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff08\u57fa\u4e8e\u89c6\u89c9\u901a\u8def\u7ed3\u6784\u4e14\u5df2\u53d6\u5f97\u4ea7\u4e1a\u7ea7\u6210\u529f\uff09\uff0c\u5e76\u5728\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u6846\u67b6\u4e0b\u8bc4\u8ff0\u4e8c\u8005\u4f18\u52a3\u3001\u76f8\u5173\u6027\u53ca\u4e92\u8865\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5404\u6709\u957f\u77ed\uff0c\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u80fd\u591f\u7edf\u4e00\u63cf\u8ff0\u4e8c\u8005\uff0c\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5206\u6790BDT\u672c\u8eab\u7684\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u4e86\u4e24\u7c7b\u65b9\u6cd5\u7ed3\u5408\u7684\u65b0\u65b9\u5411\u3002", "conclusion": "\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u4e3a\u7406\u89e3\u548c\u6574\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e3b\u6d41\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7ed3\u5408\u8d1d\u53f6\u65af\u548c\u6df1\u5ea6\u5b66\u4e60\u4ee5\u6784\u5efa\u66f4\u4e30\u5bcc\u8ba1\u7b97\u6846\u67b6\u7684\u7814\u7a76\u8def\u5f84\u3002"}}
{"id": "2602.00846", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00846", "abs": "https://arxiv.org/abs/2602.00846", "authors": ["Zicheng Kong", "Dehua Ma", "Zhenbo Xu", "Alven Yang", "Yiwei Ru", "Haoran Wang", "Zixuan Zhou", "Fuqing Bie", "Liuyu Xiang", "Huijia Wu", "Jian Zhao", "Zhaofeng He"], "title": "Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \\textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \\textbf{text, image, video, and audio}. At the core of our approach is \\textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \\emph{reconcile and filter} preferences while providing a modality-aware \\emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\\% on ShareGPT-V) and audio (66.8\\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Omni-RRM\uff0c\u8fd9\u662f\u9996\u4e2a\u540c\u65f6\u8986\u76d6\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u7684\u5f00\u6e90\u3001\u591a\u7ef4\u5ea6\u7ed3\u6784\u5316\u8bc4\u6d4b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\u548c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5bf9\u9f50\u4e0e\u8bc4\u4ef7\u80fd\u529b\uff0c\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u5747\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u5b9e\u9645\u4e2d\u53d7\u9650\u4e8e\u7c97\u7cd9\u7684\u5956\u52b1\u6a21\u578b\uff08RMs\uff09\uff0c\u5c24\u5176\u7f3a\u4e4f\u80fd\u5168\u9762\u8bc4\u4ef7\u591a\u6a21\u6001\u8f93\u51fa\u7684\u65b9\u6cd5\u3002\u76ee\u524d\u4e3b\u6d41\u505a\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u89c6\u89c9\u3001\u6807\u6ce8\u6602\u8d35\u3001\u4e14\u53cd\u9988\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86MMLMs\u7684\u6027\u80fd\u63d0\u5347\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7ed3\u6784\u5316\u3001\u89e3\u91ca\u6027\u5f3a\u7684\u65b0\u578b\u5956\u52b1\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51faOmni-RRM\uff0c\u57fa\u4e8e\u81ea\u52a8\u5316\u7ba1\u7ebf\u6784\u5efa\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\uff09\u5019\u9009\u54cd\u5e94\u5bf9\uff0c\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7\u5f3a\u5927\u7684\u6559\u5e08\u6a21\u578b\u81ea\u52a8\u751f\u6210\u3001\u7b5b\u9009\u548c\u89e3\u91ca\u504f\u597d\uff0c\u5b9e\u73b0\u4ee5\u8bc4\u6d4b\u89c4\u7a0b\u4e3a\u57fa\u7840\u7684\u7ed3\u6784\u5316\u8bad\u7ec3\u3002\u6a21\u578b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u662f\u6709\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u589e\u5f3a\u6a21\u578b\u533a\u5206\u7ec6\u5fae\u5dee\u5f02\u7684\u80fd\u529b\u3002", "result": "Omni-RRM\u5728\u89c6\u9891\uff08ShareGPT-V\uff0c80.2%\u6b63\u786e\u7387\uff09\u3001\u97f3\u9891\uff08Audio-HH-RLHF\uff0c66.8%\uff09\u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6210\u7ee9\uff0c\u5e76\u5728\u56fe\u50cf\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534717.7%\uff0c\u8fd8\u80fd\u8fc1\u79fb\u5230\u6587\u672c\u504f\u597d\u8bc4\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u5b83\u5f00\u6e90RM\u3002", "conclusion": "Omni-RRM\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u4e0e\u5bf9\u9f50\u80fd\u529b\uff0c\u4e0d\u4ec5\u51c6\u786e\u5ea6\u9ad8\u4e14\u65b9\u6cd5\u901a\u7528\uff0c\u6446\u8131\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5e76\u4ee5\u5176\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u8bbe\u8ba1\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01536", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01536", "abs": "https://arxiv.org/abs/2602.01536", "authors": ["Shuai Liu", "Siheng Ren", "Xiaoyao Zhu", "Quanmin Liang", "Zefeng Li", "Qiang Li", "Xin Hu", "Kai Huang"], "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning", "comment": null, "summary": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\uff08UniDWM\uff09\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8868\u793a\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e76\u5728\u8f68\u8ff9\u89c4\u5212\u30014D\u91cd\u5efa\u53ca\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u590d\u6742\u7684\u9a7e\u9a76\u73af\u5883\u4e2d\uff0c\u89c4\u5212\u7cfb\u7edf\u9700\u8981\u7efc\u5408\u7406\u89e3\u573a\u666f\u7684\u51e0\u4f55\u7ed3\u6784\u3001\u5916\u89c2\u548c\u52a8\u6001\u53d8\u5316\u3002\u76ee\u524d\u7f3a\u4e4f\u80fd\u591f\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u51b3\u7b56\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u5bf9\u573a\u666f\u8fdb\u884c\u5efa\u6a21\u4e0e\u63a8\u7406\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86UniDWM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u8054\u5408\u91cd\u5efa\u8def\u5f84\u5b66\u4e60\u573a\u666f\u7ed3\u6784\uff08\u5305\u62ec\u51e0\u4f55\u4e0e\u89c6\u89c9\u7eb9\u7406\uff09\uff0c\u540c\u65f6\u4f7f\u7528\u6761\u4ef6\u6269\u6563Transformer\u8fdb\u884c\u52a8\u6001\u4e16\u754c\u7684\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5f15\u5165\u4e86\u7ed3\u6784\u548c\u52a8\u6001\u611f\u77e5\u7684\u6f5c\u5728\u7a7a\u95f4\u4f5c\u4e3a\u7269\u7406\u57fa\u7840\u7684\u72b6\u6001\u7a7a\u95f4\uff0c\u7edf\u4e00\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u3002\u7406\u8bba\u4e0a\uff0cUniDWM\u88ab\u89e3\u91ca\u4e3aVAE\u7684\u53d8\u4f53\uff0c\u652f\u6301\u5176\u591a\u7ef4\u8868\u793a\u5b66\u4e60\u3002", "result": "UniDWM\u5728\u8f68\u8ff9\u89c4\u5212\u30014D\u573a\u666f\u91cd\u5efa\u548c\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u4e16\u754c\u8868\u793a\u4e3a\u5b9e\u73b0\u7edf\u4e00\u81ea\u52a8\u9a7e\u9a76\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0cUniDWM\u5728\u591a\u4e2a\u5173\u952e\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u7406\u5ff5\u7684\u6709\u6548\u6027\u548c\u524d\u666f\u3002"}}
{"id": "2602.00292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00292", "abs": "https://arxiv.org/abs/2602.00292", "authors": ["Rory Driscoll", "Alexandros Christoforos", "Chadbourne Davis"], "title": "LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification", "comment": null, "summary": "While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LogicGaze\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u6d4b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u8bc1\u636e\u57fa\u7840\u4e0a\u7684\u63a8\u7406\u94fe\u53ef\u9760\u6027\uff0c\u63ed\u793a\u4e86VLM\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u6613\u9519\u6027\u3002", "motivation": "\u73b0\u6709VLM\u867d\u7136\u80fd\u591f\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u5176\u63a8\u7406\u662f\u5426\u771f\u6b63\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\uff0c\u800c\u975e\u8bed\u8a00\u5e7b\u60f3\uff0c\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u4e3a\u63d0\u5347VLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e13\u95e8\u5de5\u5177\u68c0\u9a8c\u6a21\u578b\u201c\u843d\u5730\u201d\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86LogicGaze\u57fa\u51c6\uff0c\u4eceShareGPT4Video\uff084\u4e07\u6bb5\u89c6\u9891\uff09\u4e0eFlickr30k\u56fe\u50cf\u96c6\u91c7\u6837\uff0c\u8bbe\u8ba1\u5e26\u6709\u89c6\u89c9\u77db\u76fe\u4f46\u8bed\u8a00\u8868\u9762\u5408\u7406\u7684\u56e0\u679c\u94fe\u6270\u52a8\uff0c\u8981\u6c42\u6a21\u578b\u4e00\u4e00\u9a8c\u8bc1\u63a8\u7406\u94fe\u6bcf\u4e00\u6b65\u662f\u5426\u4e0e\u771f\u5b9e\u89c6\u89c9\u8f93\u5165\u76f8\u7b26\u3002\u8bc4\u4ef7\u5206\u4e3a\u4e09\u6b65\uff1a\u56e0\u679c\u94fe\u9a8c\u8bc1\u3001\u53d9\u4e8b\u5408\u6210\u53ca\u6270\u52a8\u62d2\u7edd\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u5f53\u524d\u4e3b\u6d41VLM\uff08\u5982Qwen2.5-VL-72B\uff09\u5728\u9a8c\u8bc1\u63a8\u7406\u94fe\u3001\u62d2\u7edd\u89c6\u89c9\u77db\u76fe\u6270\u52a8\u7b49\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6613\u51fa\u73b0\u201c\u5e7b\u89c9\u201d\u9519\u8bef\u3002", "conclusion": "LogicGaze\u63ed\u793a\u5e76\u91cf\u5316\u4e86VLM\u5728\u591a\u6a21\u6001\u63a8\u7406\u843d\u5730\u4e2d\u7684\u91cd\u5927\u6311\u6218\uff0c\u63a8\u52a8\u672a\u6765VLM\u671d\u5411\u66f4\u5065\u58ee\u3001\u53ef\u4fe1\u8d56\u7684\u65b9\u5411\u53d1\u5c55\u3002\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\uff0c\u4fc3\u8fdb\u884c\u4e1a\u7814\u7a76\u3002"}}
{"id": "2602.00848", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00848", "abs": "https://arxiv.org/abs/2602.00848", "authors": ["Ziwei Gong", "Yanda Chen", "Julia Hirschberg", "Chen Zhao", "He He", "Zhou Yu", "Kathleen Mckeown"], "title": "Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation", "comment": null, "summary": "Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Factuality-Controlled Generation(FCG)\u6846\u67b6\uff0c\u4f7f\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u56de\u7b54\u7684\u771f\u5b9e\u5ea6\u4e0e\u4fe1\u606f\u91cf\u5e73\u8861\uff0c\u5e76\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u4e0d\u540c\u5e94\u7528\u5bf9\u8bed\u8a00\u6a21\u578b\u56de\u7b54\u7684\u771f\u5b9e\u6027(factuality)\u548c\u4fe1\u606f\u91cf(informativeness)\u6709\u4e0d\u540c\u7684\u9700\u6c42\uff0c\u4f46\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u7075\u6d3b\u5e73\u8861\u4e24\u8005\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u63a7\u7684\u751f\u6210\u6846\u67b6\u3002", "method": "\u63d0\u51faFCG\u6846\u67b6\uff0c\u5141\u8bb8\u7528\u6237\u5728\u63d0\u95ee\u65f6\u6307\u5b9a\u5bf9\u771f\u5b9e\u6027\u7684\u9700\u6c42\uff1b\u5e76\u63d0\u51fa\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u63d0\u5347\u6a21\u578b\u6839\u636e\u7ea6\u675f\u7ed9\u51fa\u7b26\u5408\u8981\u6c42\u7684\u7b54\u6848\u3002", "result": "\u7ecf\u8fc7\u5408\u6210\u6570\u636e\u7684FCG\u4efb\u52a1\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u5347\u5728\u9075\u5b88\u771f\u5b9e\u6027\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u4ecd\u4fdd\u6301\u8f83\u9ad8\u7684\u4fe1\u606f\u91cf\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e0b\u7684FCG\u6846\u67b6\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6839\u636e\u6307\u4ee4\u7075\u6d3b\u8c03\u6574\u7b54\u6848\u7684\u771f\u5b9e\u6027\u548c\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42\u3002"}}
{"id": "2602.01632", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01632", "abs": "https://arxiv.org/abs/2602.01632", "authors": ["Chuizheng Kong", "Yunho Cho", "Wonsuhk Jung", "Idris Wibowo", "Parth Shinde", "Sundhar Vinodh-Sangeetha", "Long Kiu Chung", "Zhenyang Chen", "Andrew Mattei", "Advaith Nidumukkala", "Alexander Elias", "Danfei Xu", "Taylor Higgins", "Shreyas Kousik"], "title": "A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation", "comment": "Project page at https://sew-mimic.com/", "summary": "Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u4f53\u52a8\u4f5c\u91cd\u5b9a\u5411\u65b9\u6cd5SEW-Mimic\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u65b9\u5411\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ed\u5f0f\u51e0\u4f55\u89e3\u7b97\u6cd5\u3002\u76f8\u6bd4\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u66f4\u5feb\u66f4\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u5927\u591a\u65707\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u63d0\u5347\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5e76\u5bf9\u7b56\u7565\u5b66\u4e60\u6709\u5e2e\u52a9\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u52a8\u4f5c\u91cd\u5b9a\u5411\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u52a8\u4f5c\u5ef6\u8fdf\u6216\u4e0d\u81ea\u7136\u7b49\u95ee\u9898\uff0c\u800c\u4e14\u53ea\u80fd\u5c06\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u76f4\u63a5\u6620\u5c04\u5230\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5c06\u52a8\u4f5c\u91cd\u5b9a\u5411\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0a\u3001\u4e0b\u81c2\u65b9\u5411\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u80a9\u3001\u8098\u3001\u8155\uff08SEW\uff09\u5173\u952e\u70b9\u5bf9\u673a\u5668\u4eba\u5404\u5173\u8282\u65b9\u5411\u8fdb\u884c\u51e0\u4f55\u5bf9\u9f50\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u901f\uff083kHz\uff09\u95ed\u5f0f\u89e3\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5173\u952e\u70b9\u6765\u6e90\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEW-Mimic\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5148\u5bfc\u7528\u6237\u6d4b\u8bd5\u663e\u793a\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u5b8c\u6210\u7387\u3002\u7528\u8be5\u65b9\u6cd5\u6536\u96c6\u7684\u6570\u636e\u66f4\u5e73\u6ed1\uff0c\u6709\u5229\u4e8e\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u63d0\u9ad8\u5168\u8eab\u91cd\u5b9a\u5411\u901f\u5ea6\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002\u786c\u4ef6\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "SEW-Mimic\u662f\u5b9e\u73b0\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4eba\u5f62\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u63a7\u7684\u57fa\u7840\u6280\u672f\u6a21\u5757\uff0c\u517c\u5177\u9ad8\u6548\u3001\u51c6\u786e\u548c\u5e7f\u6cdb\u9002\u5e94\u6027\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00309", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00309", "abs": "https://arxiv.org/abs/2602.00309", "authors": ["Samuel Church", "Joshua D. Warner", "Danyal Maqbool", "Xin Tie", "Junjie Hu", "Meghan G. Lubner", "Tyler J. Bradshaw"], "title": "Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation", "comment": null, "summary": "The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAM2CT\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528PACS\u7cfb\u7edf\u4e2d\u5df2\u6709\u7684\u7bad\u5934\u548c\u7ebf\u6bb5\u7b49\u7a00\u758f\u6ce8\u91ca\uff0c\u5b9e\u73b0CT\u5f71\u50cf\u4e0a\u76843D\u81ea\u52a8\u5206\u5272\uff0c\u6269\u5c55\u5206\u5272\u6570\u636e\u83b7\u53d6\u7684\u65b9\u5f0f\u3002", "motivation": "\u9ad8\u8d28\u91cf3D\u5206\u5272\u6570\u636e\u5bf9\u4e8e\u6784\u5efaCT\u5f71\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u5206\u5272\u5de5\u4f5c\u91cf\u5927\u3001\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u5b9e\u4e34\u5e8a\u5de5\u4f5c\u4e2d\u5df2\u5b58\u5728\u5927\u91cf\u7b80\u5355\u7684\u6ce8\u91ca\u4fe1\u606f\uff08\u5982\u7bad\u5934\u3001\u7ebf\u6bb5\uff09\u88ab\u7cfb\u7edf\u5b58\u50a8\uff0c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u65b9\u6cd5\u9ad8\u6548\u8f6c\u5316\u5df2\u6709\u7a00\u758f\u6ce8\u91ca\u4e3a3D\u5206\u5272\u6807\u7b7e\uff0c\u4ee5\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6837\u672c\u91cf\u3002", "method": "\u63d0\u51fa\u4e86SAM2CT\u6a21\u578b\uff0c\u57fa\u4e8e\u5df2\u6709\u7684SAM2\u5206\u5272\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u63d0\u793a\u7f16\u7801\u5668\u4ee5\u652f\u6301\u7bad\u5934\u548c\u7ebf\u6bb5\u8f93\u5165\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u9002\u7528\u4e8e3D\u533b\u5b66\u5f71\u50cf\u7684\u8bb0\u5fc6\u7f16\u7801\u7b56\u7565\uff08MCM\uff09\u3002\u8be5\u65b9\u6cd5\u80fd\u5c06PACS\u4e2d\u7684GSPS\uff08\u7a00\u758f\u6ce8\u91ca\uff09\u7ed3\u5408CT\u4f53\u79ef\u6570\u636e\uff0c\u63d0\u793a\u6a21\u578b\u751f\u62103D\u5206\u5272\u3002\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u5b9e\u9645\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "result": "\u5728\u516c\u5f00\u75c5\u7076\u5206\u5272\u6d4b\u8bd5\u4e2d\uff0cSAM2CT\u7528\u7bad\u5934\u548c\u7ebf\u6bb5\u63d0\u793a\u5206\u522b\u53d6\u5f970.649\u548c0.757\u7684Dice\u7cfb\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002\u5e94\u7528\u572860\u4f8b\u4e34\u5e8a\u5b9e\u9645GSPS\u6ce8\u91ca\u4e0a\uff0c87%\u7684\u81ea\u52a8\u5206\u5272\u7ed3\u679c\u88ab\u653e\u5c04\u79d1\u533b\u751f\u8ba4\u4e3a\u53ef\u63a5\u53d7\u6216\u4ec5\u9700\u5c0f\u8c03\u6574\uff0c\u5e76\u5c55\u793a\u4e86\u90e8\u5206\u6025\u8bca\u573a\u666f\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "SAM2CT\u901a\u8fc7\u6709\u6548\u5229\u7528\u5386\u53f2\u7a00\u758f\u6ce8\u91ca\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a213D CT\u5206\u5272\u6807\u7b7e\u81ea\u52a8\u6784\u5efa\u7684\u53ef\u80fd\uff0c\u5bf9\u63d0\u5347\u533b\u5b66\u5f71\u50cfAI\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u7684\u6548\u7387\u4e0e\u89c4\u6a21\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00857", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00857", "abs": "https://arxiv.org/abs/2602.00857", "authors": ["Manveer Singh Tamber", "Hosna Oyarhoseini", "Jimmy Lin"], "title": "Unifying Adversarial Robustness and Training Across Text Scoring Models", "comment": null, "summary": "Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.", "AI": {"tldr": "\u672c\u8bba\u6587\u805a\u7126\u4e8e\u7edf\u4e00\u6587\u672c\u8bc4\u5206\u7c7b\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u6cd5\uff0c\u63d0\u51fa\u8de8\u6a21\u578b\u89d2\u8272\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u8ba9\u6a21\u578b\u5728\u591a\u79cd\u653b\u51fb\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u7814\u7a76\u8f83\u4e3a\u5206\u6563\uff0c\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u53ca\u653b\u51fb\u7684\u7814\u7a76\u65b9\u6cd5\u5404\u5f02\uff0c\u5c1a\u7f3a\u4e4f\u7edf\u4e00\u89c6\u89d2\uff1b\u6587\u672c\u8bc4\u5206\u7c7b\u6a21\u578b\u7684\u5bf9\u6297\u6613\u53d7\u5ffd\u89c6\uff0c\u800c\u5176\u8bc4\u5206\u5931\u8d25\u662f\u53ef\u76f4\u63a5\u68c0\u9a8c\u7684\uff0c\u56e0\u800c\u6709\u5fc5\u8981\u9488\u5bf9\u8fd9\u4e00\u7c7b\u522b\u63d0\u51fa\u901a\u7528\u5bf9\u6297\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u6587\u672c\u8bc4\u5206\u6a21\u578b\uff08\u5305\u62ec\u7a20\u5bc6\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u548c\u5956\u52b1\u6a21\u578b\uff09\u7684\u591a\u79cd\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5c06\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u4e92\u8865\u7ed3\u5408\u4ee5\u63d0\u5347\u6574\u4f53\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u5c06\u653b\u51fb\u548c\u8bad\u7ec3\u7b56\u7565\u6a2a\u8de8\u591a\u79cd\u6a21\u578b\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u5f0f\u666e\u904d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff1b\u800c\u4f5c\u8005\u63d0\u51fa\u7684\u591a\u65b9\u6cd5\u7ec4\u5408\u5bf9\u6297\u8bad\u7ec3\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u591a\u6837\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u8fd8\u63d0\u9ad8\u4e86\u6838\u5fc3\u4efb\u52a1\u8868\u73b0\u3002\u5728RLHF\u80cc\u666f\u4e0b\uff0c\u7ecf\u8fc7\u5bf9\u6297\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u80fd\u663e\u8457\u51cf\u7f13reward hacking\u95ee\u9898\uff0c\u5e76\u52a9\u529b\u8bad\u7ec3\u66f4\u5951\u5408\u4eba\u7c7b\u504f\u597d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u6587\u4e2d\u65b9\u6cd5\u6709\u6548\u7edf\u4e00\u4e86\u6587\u672c\u8bc4\u5206\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u8bc1\u660e\u901a\u8fc7\u591a\u6837\u7684\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\u7ec4\u5408\u80fd\u5b9e\u73b0\u5f3a\u9c81\u68d2\u6027\u548c\u66f4\u4f18\u6a21\u578b\u6548\u679c\uff0c\u5e76\u5bf9\u5b9e\u9645LLM\u5bf9\u9f50\u4e0e\u7a33\u5065\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u653e\u4f9b\u793e\u533a\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2602.01662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01662", "abs": "https://arxiv.org/abs/2602.01662", "authors": ["Pengyuan Guo", "Zhonghao Mai", "Zhengtong Xu", "Kaidi Zhang", "Heng Zhang", "Zichen Miao", "Arash Ajoudani", "Zachary Kingston", "Qiang Qiu", "Yu She"], "title": "AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act", "comment": null, "summary": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AgenticLab\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6a21\u578b\u65e0\u5173\u578b\u5e73\u53f0\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u5b9e\u7269\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8bc4\u4f30\u6700\u65b0\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee3\u7406\u7684\u901a\u7528\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u548c\u63a8\u7406\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u3001\u95ed\u73af\u3001\u5f00\u653e\u4e16\u754c\u64cd\u4f5c\u4e2d\u80fd\u529b\u5c1a\u672a\u660e\u786e\uff0c\u4e14\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u4e0d\u53ef\u6bd4\u3001\u4f9d\u8d56\u6a21\u62df\u548c\u7279\u6b8a\u73af\u5883\uff0c\u7f3a\u4e4f\u901a\u7528\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u6d4b\u5e73\u53f0\u3002", "method": "\u6784\u5efa\u4e86AgenticLab\u5e73\u53f0\uff0c\u5305\u62ec\u611f\u77e5\u3001\u4efb\u52a1\u5206\u89e3\u3001\u5728\u7ebf\u9a8c\u8bc1\u548c\u91cd\u89c4\u5212\u7684\u95ed\u73af\u6d41\u7a0b\uff0c\u5e76\u5728\u771f\u5b9e\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\uff0c\u4f7f\u7528\u8be5\u5e73\u53f0\u7cfb\u7edf\u6027\u8bc4\u6d4b\u4e86\u591a\u79cd\u5148\u8fdbVLM\u4ee3\u7406\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u79bb\u7ebf\u89c6\u89c9-\u8bed\u8a00\u6d4b\u8bd5\u65e0\u6cd5\u53cd\u6620\u7684\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u591a\u6b65\u4e00\u81f4\u6027\u5d29\u6e83\u3001\u7269\u4f53\u906e\u6321/\u573a\u666f\u53d8\u5316\u4e0b\u7684\u5b9a\u4f4d\u5931\u8d25\uff0c\u4ee5\u53ca\u7a7a\u95f4\u63a8\u7406\u4e0d\u8db3\u5bfc\u81f4\u64cd\u4f5c\u4e0d\u53ef\u9760\u3002", "conclusion": "AgenticLab\u80fd\u591f\u652f\u6301\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u901a\u7528\u667a\u80fd\u8bc4\u6d4b\uff0c\u5c06\u5f00\u6e90\u5168\u5957\u8f6f\u786c\u4ef6\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u901a\u7528\u673a\u5668\u4eba\u4ee3\u7406\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2602.00314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00314", "abs": "https://arxiv.org/abs/2602.00314", "authors": ["Apostol Vassilev", "Munawar Hasan", "Edward Griffor", "Honglan Jin", "Pavel Piliptchak", "Mahima Arora", "Thoshitha Gamage"], "title": "On the Assessment of Sensitivity of Autonomous Vehicle Perception", "comment": "21 pages, 17 figures", "summary": "The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.", "AI": {"tldr": "\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u7cfb\u7edf\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u6613\u53d7\u5e72\u6270\uff0c\u5bfc\u81f4\u8bc6\u522b\u4e0e\u68c0\u6d4b\u5931\u8bef\u3002\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u611f\u77e5\u6a21\u578b\u5728\u5404\u79cd\u590d\u6742\u9a7e\u9a76\u60c5\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u8861\u91cf\u4e0e\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u9700\u8981\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u7684\u611f\u77e5\u7cfb\u7edf\u786e\u4fdd\u5b89\u5168\u51b3\u7b56\uff0c\u4f46\u771f\u5b9e\u73af\u5883\u548c\u5bf9\u6297\u56e0\u7d20\u5e38\u5bfc\u81f4\u611f\u77e5\u6570\u636e\u5931\u6548\uff0c\u56e0\u6b64\u4e9f\u9700\u7814\u7a76\u611f\u77e5\u7cfb\u7edf\u5728\u4e0d\u540c\u5e72\u6270\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u4e0e\u6539\u8fdb\u7b56\u7565\u3002", "method": "\u672c\u6587\u5229\u7528\u6a21\u578b\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u91cf\u5316\u611f\u77e5\u6a21\u578b\u5728\u4e0d\u540c\u6076\u52a3\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u654f\u611f\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002\u8fd8\u5f15\u5165\u57fa\u4e8e\u8f66\u8f86\u5728\u4e0d\u540c\u8def\u51b5\u4e0b\u7684\u505c\u8f66\u8ddd\u79bb\u4f5c\u4e3a\u611f\u77e5\u6027\u80fd\u8bc4\u4f30\u6807\u51c6\uff0c\u9009\u7528YOLO\u7cfb\u5217\u3001DETR\u53caRT-DETR\u7b49\u4e94\u79cd\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u5b9e\u6d4b\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u53ca\u5b9e\u9645\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u5f31\u5149\u73af\u5883\uff08\u5982\u96fe\u3001\u4f4e\u9633\u89d2\u7b49\uff09\u5bf9\u611f\u77e5\u6a21\u578b\u5f71\u54cd\u6700\u5927\uff0c\u5bf9\u6297\u573a\u666f\u5982\u76ee\u6807\u906e\u6321\u3001\u6076\u52a3\u5929\u6c14\u7ec4\u5408\u7b49\u4e5f\u663e\u8457\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u7387\u3002\u4e14\u4e0e\u76ee\u6807\u7684\u8ddd\u79bb\u8d8a\u8fdc\uff0c\u611f\u77e5\u80fd\u529b\u4e0b\u964d\u8d8a\u660e\u663e\u3002", "conclusion": "\u611f\u77e5\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u6709\u9650\uff0c\u9700\u9488\u5bf9\u591a\u5143\u5e72\u6270\u60c5\u666f\u63d0\u5347\u6a21\u578b\u8868\u73b0\u548c\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u672a\u6765\u7814\u7a76\u53ef\u5173\u6ce8\u591a\u6a21\u578b\u96c6\u6210\u548c\u73af\u5883\u81ea\u9002\u5e94\u6539\u8fdb\u3002"}}
{"id": "2602.00881", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00881", "abs": "https://arxiv.org/abs/2602.00881", "authors": ["Shounak Paul", "Raghav Dogra", "Pawan Goyal", "Saptarshi Ghosh"], "title": "ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople", "comment": "9 Pages of Main, 1 page of Limitations and Ethics Statement, 11 Pages of Appendix, Accepted for Publication at EACL 2026 (Findings)", "summary": "Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u666e\u901a\u4eba\u63d0\u95ee\u7684\u6cd5\u5f8b\u6761\u6587\u8bc6\u522b\uff08LSI\uff09\u6570\u636e\u96c6ILSiC\uff0c\u5e76\u5728\u4e0a\u9762\u8fdb\u884c\u4e86\u6a21\u578b\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7eaf\u7528\u6cd5\u9662\u5224\u51b3\u8bad\u7ec3\u7684\u6a21\u578b\u5bf9\u666e\u901a\u4eba\u95ee\u9898\u8bc6\u522b\u6548\u679c\u8f83\u5dee\uff0c\u4f46\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8fc1\u79fb\u5b66\u4e60\u6709\u5e2e\u52a9\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u6761\u6587\u8bc6\u522b\u591a\u4ee5\u6cd5\u9662\u5224\u51b3\u4e8b\u5b9e\u4e3a\u8f93\u5165\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u666e\u901a\u4eba\u3001\u975e\u4e13\u4e1a\u4eba\u58eb\u63d0\u51fa\u7684\u95ee\u9898\u5f62\u5f0f\u66f4\u4e3a\u975e\u6b63\u5f0f\u3002\u73b0\u6709\u57fa\u4e8e\u666e\u901a\u4eba\u95ee\u9898\u7684\u6570\u636e\u96c6\u548c\u5bf9\u6b64\u7684\u7814\u7a76\u5f88\u6709\u9650\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6784\u5efa\u5e76\u7814\u7a76\u66f4\u8d34\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u6570\u636e\u548c\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86ILSiC\u6570\u636e\u96c6\uff0c\u5305\u542b500\u591a\u4e2a\u5370\u5ea6\u6cd5\u5f8b\u6cd5\u89c4\u7684\u666e\u901a\u4eba\u63d0\u95ee\u548c\u76f8\u5e94\u6cd5\u9662\u5224\u51b3\uff0c\u5e76\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u63a8\u7406\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6709\u76d1\u7763\u5fae\u8c03\u7b49\u591a\u79cd\u65b9\u6cd5\u7684\u5b9e\u9a8c\u8bc1\u660e\u3002\u8fdb\u4e00\u6b65\uff0c\u5bf9\u4e0d\u540c\u7c7b\u522b\u548c\u6cd5\u89c4\u9891\u7387\u8fdb\u884c\u4e86\u7ec6\u81f4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4ec5\u7528\u6cd5\u9662\u5224\u51b3\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u666e\u901a\u4eba\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5c06\u6cd5\u9662\u6570\u636e\u8fc1\u79fb\u5230\u666e\u901a\u4eba\u6570\u636e\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6709\u76ca\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u95ee\u9898\u7c7b\u522b\u548c\u6cd5\u89c4\u51fa\u73b0\u9891\u7387\u5bf9\u8bc6\u522b\u6548\u679c\u6709\u660e\u663e\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5efa\u7acb\u9762\u5411\u666e\u901a\u4eba\u95ee\u9898\u7684\u6cd5\u5f8b\u6761\u6587\u8bc6\u522b\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u5728\u6a21\u578b\u5f00\u53d1\u4e2d\u5e94\u5145\u5206\u8003\u8651\u5b9e\u9645\u7528\u6237\u95ee\u9898\u7684\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u7b49\u591a\u6837\u5316\u65b9\u6cd5\u53ef\u4e3a\u8be5\u4efb\u52a1\u5e26\u6765\u65b0\u7684\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2602.01679", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01679", "abs": "https://arxiv.org/abs/2602.01679", "authors": ["Raghavasimhan Sankaranarayanan", "Paul Stuart", "Nicholas Ahn", "Arno Sungarian", "Yash Chitalia"], "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications", "comment": "7 pages, 9 figures, 2026 International Symposium on Medical Robotics", "summary": "The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u81ea\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5916\u79d1\u624b\u672f\u5668\u68b0\u7684\u81ea\u52a8\u5206\u62e3\u4e0e\u89c4\u8303\u88c5\u76d8\uff0c\u91cd\u70b9\u89e3\u51b3SPD\u90e8\u95e8\u4eba\u5de5\u7ec4\u76d8\u7684\u4f4e\u6548\u548c\u9ad8\u5dee\u9519\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u76d8\u7cbe\u51c6\u5ea6\u5e76\u51cf\u5c11\u5668\u68b0\u78b0\u649e\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u5668\u68b0\u7684\u4eba\u5de5\u7ec4\u76d8\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u5b58\u5728\u4ea4\u53c9\u6c61\u67d3\u548c\u5668\u68b0\u635f\u574f\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u81ea\u52a8\u5316\u3001\u51c6\u786e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u624b\u672f\u51c6\u5907\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f5c\u8005\u90e8\u7f72\u4e86\u4e00\u5957\u878d\u5408YOLO12\u68c0\u6d4b\u5668\u548c\u7ea7\u8054ResNet\u5206\u7c7b\u5668\u7684\u6df7\u5408\u611f\u77e5\u7b97\u6cd5\uff0c\u8f85\u4ee5\u6807\u6ce8\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u5b9e\u73b0\u5668\u68b0\u8bc6\u522b\u3002\u5168\u7cfb\u7edf\u5305\u62ec\u6821\u51c6\u89c6\u89c9\u6a21\u5757\u30016\u81ea\u7531\u5ea6\u5de5\u4e1a\u673a\u68b0\u81c2\u3001\u5b9a\u5236\u53cc\u7535\u78c1\u5438\u6301\u5668\u548c3D\u6253\u5370\u9694\u677f\u88c5\u7f6e\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u9a71\u52a8\u7684\u7269\u7406\u9694\u79bb\u88c5\u76d8\u7b97\u6cd5\u51cf\u5c11\u8fd0\u8f93\u4e2d\u7684\u5668\u68b0\u78b0\u649e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5668\u68b0\u8bc6\u522b\u548c\u5f52\u7c7b\u4e0a\u5177\u6709\u9ad8\u51c6\u786e\u7387\uff0c\u7ec4\u76d8\u8fc7\u7a0b\u4e2d\u5668\u68b0\u95f4\u7684\u78b0\u649e\u6b21\u6570\u660e\u663e\u4f4e\u4e8e\u4eba\u5de5\u7ec4\u76d8\uff0c\u4e14\u5dee\u5f02\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aSPD\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u5f84\uff0c\u521d\u6b65\u5b9e\u73b0\u4e86\u4e8b\u52a1\u81ea\u52a8\u5316\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u624b\u672f\u51c6\u5907\u7684\u4e00\u81f4\u6027\u3001\u5b89\u5168\u6027\u5e76\u7f29\u77ed\u5904\u7406\u65f6\u95f4\u3002"}}
{"id": "2602.00340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00340", "abs": "https://arxiv.org/abs/2602.00340", "authors": ["Alexandros Christoforos", "Sarah Jenkins", "Michael Brown", "Tuan Pham", "David Chen"], "title": "Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception", "comment": null, "summary": "This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684SynerNet\u534f\u540c\u795e\u7ecf\u4f53\u7f51\u7edc\u6846\u67b6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u9047\u5230\u5206\u5e03\u5916(OOD)\u6982\u5ff5\u65f6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u9000\u5316\u95ee\u9898\u3002\u901a\u8fc7\u56db\u4e2a\u4e13\u7528\u8ba1\u7b97\u5355\u5143\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u4fe1\u606f\u7684\u9ad8\u6548\u4ea4\u6d41\u4e0e\u7ea0\u504f\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u5728\u6781\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u73af\u5883\u4e0b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5206\u5e03\u5916\u65b0\u6982\u5ff5\u65f6\uff0c\u5e38\u56e0\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u9000\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u5982\u4f55\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u77e5\u5206\u5e03\u7684\u9002\u5e94\u529b\u548c\u63a8\u7406\u80fd\u529b\uff0c\u662f\u8de8\u6a21\u6001\u7406\u89e3\u9886\u57df\u7684\u5173\u952e\u96be\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u5305\u542b\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u4e0a\u4e0b\u6587\u3001\u540d\u8bcd\u5d4c\u5165\u3001\u5168\u5c40\u534f\u8c03\u56db\u4e2a\u4e13\u7528\u8ba1\u7b97\u4f53\u7684\u591a\u667a\u80fd\u4f53\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4fe1\u606f\u4f20\u9012\u534f\u8bae\u534f\u540c\u6821\u6b63\u6a21\u6001\u5dee\u5f02\u3002\u5177\u4f53\u8d21\u732e\u5305\u62ec\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6f5c\u5728\u7a7a\u95f4\u7684\u547d\u540d\u83b7\u53d6\u6846\u67b6\u3001\u7528\u4e8e\u63d0\u5347\u5c0f\u6837\u672c\u9002\u5e94\u80fd\u529b\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u4ea4\u6362\u7b97\u6cd5\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u52a8\u6001\u5747\u8861\u673a\u5236\u3002", "result": "\u5728VISTA-Beyond\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0cSynerNet\u5728\u6781\u5c11\u6837\u672c\u4e0e\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5168\u9762\u63d0\u5347\uff0c\u591a\u4e2a\u9886\u57df\u4efb\u52a1\u7684\u7cbe\u5ea6\u63d0\u9ad8\u4e861.2%\u81f35.4%\u3002", "conclusion": "SynerNet\u4e3aVLMs\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u6982\u5ff5\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u5206\u5e03\u5916\u6982\u5ff5\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u4e0e\u63a8\u7406\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00887", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00887", "abs": "https://arxiv.org/abs/2602.00887", "authors": ["Gaurav Srivastava", "Aafiya Hussain", "Chi Wang", "Yingyan Celine Lin", "Xuan Wang"], "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents", "comment": null, "summary": "Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86effGen\uff0c\u4e00\u4e2a\u4e13\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4f18\u5316\u7684\u672c\u5730\u90e8\u7f72\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5177\u6709\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u548c\u5b89\u5168\u7b49\u4f18\u52bf\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u5927\u6a21\u578bAPI\uff0c\u5b58\u5728\u9ad8\u6602\u7684token\u6210\u672c\u53ca\u9690\u79c1\u98ce\u9669\uff0c\u5236\u7ea6\u4e86\u5176\u5728\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u9762\u5411SLM\u7684\u3001\u9ad8\u6548\u53ef\u672c\u5730\u90e8\u7f72\u7684\u6cdb\u7528\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "effGen \u6846\u67b6\u5305\u542b\u56db\u5927\u6280\u672f\u6838\u5fc3\uff1a1\uff09\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u63d0\u793a\u4f18\u5316\u5de5\u5177\u8c03\u7528\uff1b2\uff09\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5206\u89e3\u4e0e\u8c03\u5ea6\uff08\u5e76\u884c\u6216\u4e32\u884c\uff09\uff1b3\uff09\u57fa\u4e8e\u4e94\u56e0\u7d20\u7684\u590d\u6742\u5ea6\u8def\u7531\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u9884\u6267\u884c\u51b3\u7b56\uff1b4\uff09\u7edf\u4e00\u5185\u5b58\u7cfb\u7edf\u6574\u5408\u77ed\u65f6\u3001\u957f\u65f6\u4e0e\u5411\u91cf\u5b58\u50a8\u3002\u540c\u65f6\u7edf\u4e00\u591a\u79cd\u667a\u80fd\u4f53\u534f\u8bae\uff0c\u5b9e\u73b0\u534f\u8bae\u95f4\u901a\u4fe1\u3002", "result": "\u572813\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ceffGen\u5728\u6210\u529f\u7387\u3001\u6267\u884c\u901f\u5ea6\u53ca\u5185\u5b58\u6d88\u8017\u4e0a\u5747\u4f18\u4e8eLangChain\u3001AutoGen\u548cSmolagents\u3002\u5177\u4f53\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u4f18\u5316\u5bf9SLM\u63d0\u5347\u66f4\u5927\uff0c\u590d\u6742\u5ea6\u8def\u7531\u5bf9\u5927\u6a21\u578b\u5e2e\u52a9\u66f4\u5927\uff0c\u4e24\u8005\u7ed3\u5408\u5168\u90e8\u6a21\u578b\u5747\u6709\u63d0\u5347\u3002", "conclusion": "effGen\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6210\u672c\u4e0e\u95e8\u69db\uff0c\u53ef\u5b89\u5168\u672c\u5730\u90e8\u7f72\u5e76\u517c\u5bb9\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u4e3a\u7814\u7a76\u4e0e\u5546\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u5e73\u53f0\u3002"}}
{"id": "2602.01693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01693", "abs": "https://arxiv.org/abs/2602.01693", "authors": ["Kewei Hu", "Michael Zhang", "Wei Ying", "Tianhao Liu", "Guoqiang Hao", "Zimeng Li", "Wanchan Yu", "Jiajian Jing", "Fangwen Chen", "Hanwen Kang"], "title": "GSR: Learning Structured Reasoning for Embodied Manipulation", "comment": null, "summary": "Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGSR\uff08Grounded Scene-graph Reasoning\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u573a\u666f\u56fe\u6765\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u3001\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u591a\u9879\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u7ef4\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u56e0\u679c\u4f9d\u8d56\u53ca\u76ee\u6807\u7ea6\u675f\u7684\u957f\u65f6\u5e8f\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4efb\u52a1\u63a8\u7406\u9690\u542b\u4e8e\u9ad8\u7ef4\u8868\u5f81\uff0c\u96be\u4ee5\u533a\u5206\u4efb\u52a1\u7ed3\u6784\u4e0e\u611f\u77e5\u53d8\u5316\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u4efb\u52a1\u7ed3\u6784\u4e0e\u611f\u77e5\u4fe1\u606f\u6df7\u6dc6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGSR\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u73af\u5883\u72b6\u6001\u5efa\u6a21\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5e76\u5728\u5176\u4e0a\u9010\u6b65\u63a8\u7406\u5bf9\u8c61\u72b6\u6001\u4e0e\u7a7a\u95f4\u5173\u7cfb\uff0c\u663e\u5f0f\u5206\u6790\u52a8\u4f5c\u7684\u524d\u7f6e\u6761\u4ef6\u3001\u540e\u679c\u53ca\u76ee\u6807\u8fbe\u6210\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86Manip-Cognition-1.6M\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8054\u5408\u76d1\u7763\u5bf9\u4e16\u754c\u7406\u89e3\u3001\u52a8\u4f5c\u89c4\u5212\u548c\u76ee\u6807\u89e3\u91ca\u7684\u5b66\u4e60\u3002", "result": "\u5728RLBench\u3001LIBERO\u3001GSR-benchmark\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u6d4b\u663e\u793a\uff0cGSR\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u957f\u65f6\u5e8f\u4efb\u52a1\u5b8c\u6210\u7387\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u660e\u786e\u7684\u4e16\u754c\u72b6\u6001\u8868\u793a\uff08\u5982\u573a\u666f\u56fe\uff09\u4e3a\u53ef\u6269\u5c55\u5316\u673a\u5668\u4eba\u63a8\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff0cGSR\u65b9\u6cd5\u53ef\u663e\u8457\u63a8\u52a8\u673a\u5668\u4eba\u957f\u65f6\u5e8f\u4efb\u52a1\u7684\u63a8\u7406\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.00344", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00344", "abs": "https://arxiv.org/abs/2602.00344", "authors": ["Beidi Zhao", "Wenlong Deng", "Xinting Liao", "Yushu Li", "Nazim Shaikh", "Yao Nie", "Xiaoxiao Li"], "title": "When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs", "comment": "18 pages, 10 figures", "summary": "While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.", "AI": {"tldr": "RAG\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5e38\u5e38\u56e0\u4e3a\u6ce8\u610f\u529b\u504f\u5411\u68c0\u7d22\u6587\u672c\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u53d1\u73b0\u65b0\u5931\u6548\u6a21\u5f0f\u201c\u6ce8\u610f\u529b\u5206\u6563\u201d\uff0c\u63d0\u51fa\u8bad\u7ec3\u5916\u5e72\u9884\u65b9\u6cd5MAD-RAG\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684RAG\u65b9\u6cd5\u5728\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5bb9\u6613\u56e0\u68c0\u7d22\u5185\u5bb9\u5206\u914d\u7ed9\u56fe\u7247\u7684\u6ce8\u610f\u529b\u4e0d\u8db3\u800c\u51fa\u9519\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u5ffd\u89c6\u4e86\u68c0\u7d22\u6587\u672c\u4f1a\u5bfc\u81f4\u5bf9\u4e8e\u539f\u672c\u5c31\u80fd\u56de\u7b54\u7684\u95ee\u9898\u51fa\u73b0\u9519\u8bef\u7684\u65b0\u95ee\u9898\u3002", "method": "\u63d0\u51faMAD-RAG\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u91cd\u95ee\u9898\u8bbe\u5b9a\uff0c\u5c06\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u4e0a\u4e0b\u6587\u6574\u5408\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u6df7\u5408\u673a\u5236\u4fdd\u7559\u7531\u56fe\u50cf\u5f15\u5bfc\u7684\u8bc1\u636e\u3002\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\u3002", "result": "MAD-RAG\u65b9\u6cd5\u5728OK-VQA, E-VQA\u548cInfoSeek\u6570\u636e\u96c6\u7684\u591a\u4e2a\u6a21\u578b\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u63d0\u5347\u6700\u9ad8\u8fbe4.76%\u30019.20%\u30016.18%\uff1b\u80fd\u591f\u7ea0\u6b6374.68%\u7684RAG\u5931\u6548\u6848\u4f8b\uff0c\u4e14\u51e0\u4e4e\u4e0d\u589e\u52a0\u8ba1\u7b97\u91cf\u3002", "conclusion": "MAD-RAG\u6709\u6548\u7f13\u89e3\u4e86RAG\u5728LVLMs\u77e5\u8bc6\u578bVQA\u4e2d\u7684\u201c\u6ce8\u610f\u529b\u5206\u6563\u201d\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u51c6\u786e\u7387\uff0c\u662f\u4f4e\u6210\u672c\u9ad8\u56de\u62a5\u7684\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2602.00913", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00913", "abs": "https://arxiv.org/abs/2602.00913", "authors": ["V\u00edctor Yeste", "Paolo Rosso"], "title": "Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts", "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 42 pages, 4 figures", "summary": "Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\\rightarrow$HO$\\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4eba\u7c7b\u4ef7\u503c\u89c2\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u662f\u5426\u76f4\u63a5\u5229\u7528Schwartz HO\u9ad8\u9636\u7ed3\u6784\u80fd\u591f\u63d0\u5347\u53e5\u5b50\u7ea7\u591a\u6807\u7b7e\u5206\u7c7b\u7684\u6548\u679c\u3002\u7ed3\u679c\u53d1\u73b0\u5f3a\u884c\u65bd\u52a0HO\u5c42\u7ea7\u7ed3\u6784\u4e0d\u4ec5\u65e0\u76ca\uff0c\u751a\u81f3\u6709\u5bb3\uff1b\u800c\u53c2\u6570\u8c03\u4f18\u4e0e\u8f7b\u91cf\u7ea7\u6a21\u578b\u96c6\u6210\u5e26\u6765\u4e86\u66f4\u5b9e\u9645\u7684\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u7528Schwartz\u4ef7\u503c\u89c2\u4f53\u7cfb\uff08\u53ca\u5176HO\u9ad8\u9636\u7c7b\u522b\uff09\u4f5c\u591a\u6807\u7b7e\u5206\u7c7b\u7684\u6807\u7b7e\u96c6\u662f\u53e5\u5b50\u7ea7\u4eba\u7c7b\u4ef7\u503c\u89c2\u68c0\u6d4b\u4e2d\u7684\u5e38\u7528\u6a21\u5f0f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695aHO\u5c42\u7ea7\u7ed3\u6784\u662f\u5426\u80fd\u4e3a\u4efb\u52a1\u672c\u8eab\u63d0\u4f9b\u6709\u6548\u5e2e\u52a9\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u5176\u5b9e\u9645\u4f5c\u7528\u3002", "method": "\u4f5c\u8005\u5728\u5355\u5f208GB GPU\u7684\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u4f7f\u7528ValueEval'24/ValuesML\u7b49\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\uff081\uff09\u76f4\u63a5\u76d1\u7763\u7684transformer\u6a21\u578b\uff0c\uff082\uff09\u7528\u5f3a\u5236mask\u5b9e\u73b0HO\u5230\u7ec6\u7c92\u5ea6\u503c\u7684\u7ba1\u9053\uff0c\uff083\uff09Presence\u2192HO\u2192values\u7ea7\u8054\u4f53\u7cfb\u3002\u540c\u65f6\u5c1d\u8bd5\u5404\u79cd\u4f4e\u6210\u672c\u6539\u8fdb\uff08\u5982\u8bcd\u5178\u3001\u4e0a\u4e0b\u6587\u6269\u5c55\u3001\u4e3b\u9898\u7279\u5f81\uff09\u3001\u6807\u7b7e\u5206\u9608\u8c03\u4f18\u3001\u4e2d\u5c0f\u89c4\u6a21LLM\uff08\u226410B\uff09\u3001QLoRA\u548c\u7b80\u5355\u96c6\u6210\u7b49\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cHO\u9ad8\u9636\u7c7b\u522b\u4ece\u5355\u53e5\u4e2d\u53ef\u4ee5\u5b66\u4e60\u5230\uff0c\u6700\u5bb9\u6613\u7684\u4e00\u5bf9\u4e8c\u5206\u5bf9\u53ef\u8fbeMacro-F1\u7ea60.58\u3002\u4f46\u786c\u6027HO\u7ed3\u6784\uff08\u5f3a\u5236mask\uff09\u5e26\u6765\u53e0\u52a0\u8bef\u5dee\u548c\u53ec\u56de\u4e0b\u964d\uff0c\u53cd\u800c\u591a\u6570\u60c5\u51b5\u4e0b\u635f\u5bb3\u6700\u7ec8\u6027\u80fd\u3002\u6807\u7b7e\u9608\u503c\u8c03\u4f18\u53ef\u663e\u8457\u63d0\u5347\u5f97\u5206\uff08\u6700\u9ad8+0.05 F1\uff09\uff0c\u5c0f\u578btransformer\u96c6\u6210\u4e5f\u80fd\u7a33\u5b9a\u589e\u76ca\uff08\u6700\u9ad8+0.02 F1\uff09\u3002\u5c0f\u578bLLM\u4f5c\u4e3a\u5355\u4e00\u6a21\u578b\u6548\u679c\u4e0d\u53ca\u76d1\u7763encoder\uff0c\u4f46\u5728\u5f02\u6784\u96c6\u6210\u4e2d\u80fd\u8865\u5145\u90e8\u5206\u9519\u8bef\u3002", "conclusion": "Schwartz HO\u7ed3\u6784\u5728\u63cf\u8ff0\u4e0a\u6709\u610f\u4e49\uff0c\u4f46\u5728\u53e5\u5b50\u7ea7\u4eba\u7c7b\u4ef7\u503c\u89c2\u68c0\u6d4b\u4e2d\uff0c\u5f3a\u5236\u6027\u5730\u5c42\u7ea7\u7ea6\u675f\u4f1a\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\u3002\u66f4\u4f18\u7684\u63d0\u5347\u65b9\u5f0f\u662f\u5bf9\u6807\u7b7e\u505a\u7ec6\u7c92\u5ea6\u9608\u503c\u8c03\u4f18\u548c\u5229\u7528\u8f7b\u91cf\u6a21\u578b\u96c6\u6210\u3002"}}
{"id": "2602.01700", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01700", "abs": "https://arxiv.org/abs/2602.01700", "authors": ["Ruoyu Wang", "Xuchen Liu", "Zongzhou Wu", "Zixuan Guo", "Wendi Ding", "Ben M. Chen"], "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels", "comment": "8 pages, 10 figures", "summary": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Tilt-Ropter\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u503e\u8f6c\u65cb\u7ffc\u548c\u88ab\u52a8\u8f6e\u5b50\u7684\u5168\u65b0\u6df7\u5408\u7a7a\u5730\u4e24\u7528\u65e0\u4eba\u673a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u7684\u591a\u6a21\u5f0f\u79fb\u52a8\u3002\u91c7\u7528\u5168\u9a71\u52a8\u8bbe\u8ba1\u548c\u5148\u8fdb\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u52a8\u6027\u4e0e\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u6df7\u5408\u7a7a\u5730\u65e0\u4eba\u673a\u5927\u591a\u4e3a\u6b20\u9a71\u52a8\u7ed3\u6784\uff0c\u5bfc\u81f4\u79fb\u52a8\u548c\u73af\u5883\u9002\u5e94\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5728\u5730\u9762\u6a21\u5f0f\u4e0b\u80fd\u8017\u8f83\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u673a\u52a8\u6027\u3001\u80fd\u6548\u548c\u73af\u5883\u9002\u5e94\u6027\u7684\u65e0\u4eba\u673a\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u957f\u65f6\u4efb\u52a1\u548c\u80fd\u6e90\u53d7\u9650\u73af\u5883\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5168\u9a71\u52a8Tilt-Ropter\u65e0\u4eba\u673a\uff0c\u7ed3\u5408\u4e86\u503e\u8f6c\u65cb\u7ffc\u548c\u88ab\u52a8\u8f6e\u5b50\uff1b\u5f00\u53d1\u4e86\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08NMPC\uff09\u53ca\u63a7\u5236\u5206\u914d\u6a21\u5757\uff0c\u5b9e\u73b0\u8fd0\u52a8\u6a21\u5f0f\u95f4\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u80fd\u6548\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u65f6\u5916\u529b\u4f30\u8ba1\u7b97\u6cd5\u4ee5\u63d0\u5347\u5730\u9762\u63a5\u89e6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\uff0c\u5305\u62ec\u7a7a\u5730\u8f6c\u6362\u548c\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u98de\u884c\u548c\u5730\u9762\u6a21\u5f0f\u4e0b\u90fd\u6709\u4f4e\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5730\u9762\u79fb\u52a8\u6a21\u5f0f\u4e0b\u52a8\u529b\u6d88\u8017\u51cf\u5c1192.8%\u3002", "conclusion": "Tilt-Ropter\u5728\u591a\u6a21\u5f0f\u9ad8\u80fd\u6548\u79fb\u52a8\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6781\u5927\u63d0\u5347\u4e86\u7eed\u822a\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u80fd\u91cf\u53d7\u9650\u7684\u957f\u671f\u4efb\u52a1\u3002"}}
{"id": "2602.00347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00347", "abs": "https://arxiv.org/abs/2602.00347", "authors": ["Chongyu Qu", "Zhengyi Lu", "Yuxiang Lai", "Thomas Z. Li", "Junchao Zhu", "Junlin Guo", "Juming Xiong", "Yanfan Zhu", "Yuechen Yang", "Allen J. Luna", "Kim L. Sandler", "Bennett A. Landman", "Yuankai Huo"], "title": "AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning", "comment": null, "summary": "Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdaFuse\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u4e3a\u6bcf\u4f4d\u60a3\u8005\u9009\u62e9\u5408\u9002\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u80ba\u764c\u98ce\u9669\u9884\u6d4b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5bf9\u6240\u6709\u6a21\u6001\u4e00\u89c6\u540c\u4ec1\u6216\u5b66\u4e60\u6743\u91cd\uff0c\u4f46\u672a\u8003\u8651\u5bf9\u4e8e\u4e2a\u4f53\u60a3\u8005\u662f\u5426\u6240\u6709\u6a21\u6001\u90fd\u9700\u8981\u4f7f\u7528\u3002\u4e3a\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u60a3\u8005\u4e2a\u4f53\u5316\u6570\u636e\u9009\u62e9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AdaFuse\u6846\u67b6\uff0c\u5c06\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u9010\u6b65\u51b3\u5b9a\u662f\u5426\u5f15\u5165\u65b0\u6a21\u6001\u6216\u57fa\u4e8e\u5df2\u6709\u4fe1\u606f\u76f4\u63a5\u9884\u6d4b\uff1b\u8fd9\u6837\u53ef\u6839\u636e\u5df2\u89c2\u6d4b\u6a21\u6001\u81ea\u9002\u5e94\u9009\u62e9\u5e76\u63d0\u524d\u7ec8\u6b62\u65e0\u9700\u5904\u7406\u6240\u6709\u6a21\u6001\u3002", "result": "\u5728\u56fd\u5bb6\u80ba\u764c\u7b5b\u67e5\u8bd5\u9a8c\uff08NLST\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdaFuse\u83b7\u5f97\u4e86\u6700\u9ad8AUC(0.762)\uff0c\u4f18\u4e8e\u6700\u4f73\u5355\u6a21\u6001(0.732)\u3001\u56fa\u5b9a\u878d\u5408\u7b56\u7565(0.759)\u53ca\u81ea\u9002\u5e94\u878d\u5408\u57fa\u7ebf\uff08\u5982DynMM 0.754\u3001MoE 0.742\uff09\uff0c\u4e14\u6bd4\u5168\u90e8\u4e09\u6a21\u6001\u65b9\u6cd5\u8ba1\u7b97\u91cf\u4f4e\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u652f\u6301\u533b\u7597\u5f71\u50cf\u4e2d\u4e2a\u6027\u5316\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u56e0\u4eba\u5236\u5b9c\u7684\u8bca\u65ad\u6d41\u7a0b\uff0c\u63d0\u5347\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.00914", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00914", "abs": "https://arxiv.org/abs/2602.00914", "authors": ["V\u00edctor Yeste", "Rodrigo Rivas-Ar\u00e9valo"], "title": "A Baseline Multimodal Approach to Emotion Recognition in Conversations", "comment": "10 pages", "summary": "We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4f1a\u8bdd\u60c5\u611f\u8bc6\u522b\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u548c\u8bed\u97f3\u7279\u5f81\uff0c\u5728Friends\u5267\u96c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6613\u590d\u73b0\u7684\u57fa\u51c6\u5b9e\u73b0\uff0c\u5e76\u62a5\u544a\u4e86\u57fa\u7ebf\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4f1a\u8bdd\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u5bf9\u591a\u6a21\u6001\u65b9\u6cd5\u65e5\u76ca\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7b80\u5355\u3001\u6613\u7528\u7684\u57fa\u7ebf\u5b9e\u73b0\u6765\u8f85\u52a9\u7814\u7a76\u548c\u516c\u5e73\u6bd4\u8f83\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e2a\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u57fa\u4e8etransformer\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff1b2\uff09\u81ea\u76d1\u7763\u8bed\u97f3\u7279\u5f81\u8868\u5f81\u6a21\u578b\uff1b3\uff09\u901a\u8fc7\u540e\u671f\u878d\u5408\u6280\u672f\u5c06\u4e8c\u8005\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u5728SemEval-2024 Task 3 Friends\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6709\u9650\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5728\u90e8\u5206\u573a\u666f\u4e0b\u591a\u6a21\u6001\u878d\u5408\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u57fa\u7ebf\u914d\u7f6e\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u4eca\u540e\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u548c\u66f4\u4e25\u683c\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2602.01731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01731", "abs": "https://arxiv.org/abs/2602.01731", "authors": ["Jiwoo Hwang", "Taegeun Yang", "Jeil Jeong", "Minsung Yoon", "Sung-Eui Yoon"], "title": "Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion", "comment": "8 pages, 7 figures, Accepted to ICRA 2026, Webpage: https://jiw0o.github.io/cura-ppo/", "summary": "Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CURA-PPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u78b0\u649e\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u4f20\u611f\u5668\u88ab\u906e\u6321\u573a\u666f\u4e0b\u66f4\u5b89\u5168\u6709\u6548\u7684\u975e\u6293\u53d6\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u975e\u6293\u53d6\u64cd\u4f5c\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u7684\u89c6\u91ce\u906e\u6321\uff0c\u5bfc\u81f4 occlusion \u533a\u57df\u5bb9\u6613\u53d1\u751f\u78b0\u649e\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u5904\u7406\u906e\u6321\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u96be\u4ee5\u5b89\u5168\u5bfc\u822a\u4e0e\u64cd\u4f5c\u3002", "method": "\u4f5c\u8005\u63d0\u51faCURA-PPO\u6846\u67b6\uff0c\u5728\u589e\u5f3a\u5b66\u4e60\u4e2d\u663e\u5f0f\u5efa\u6a21\u5bf9\u78b0\u649e\u98ce\u9669\u7684\u5206\u5e03\u9884\u6d4b\uff0c\u7ed3\u5408\u98ce\u9669\u4e0e\u4e0d\u786e\u5b9a\u5ea6\u5f15\u5bfc\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u540c\u65f6\u5229\u7528\u7f6e\u4fe1\u5ea6\u5730\u56fe\u8868\u5f81\u89c2\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u9f13\u52b1\u673a\u5668\u4eba\u4e3b\u52a8\u611f\u77e5\u3001\u8fb9\u64cd\u4f5c\u8fb9\u6536\u96c6\u4fe1\u606f\u89e3\u51b3\u906e\u6321\u95ee\u9898\u3002", "result": "\u5728\u4e0d\u540c\u7269\u4f53\u5c3a\u5bf8\u4e0e\u969c\u788d\u5e03\u7f6e\u7684\u5b9e\u9a8c\u4e2d\uff0cCURA-PPO\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u63d0\u53473\u500d\u6210\u529f\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u5904\u7406\u906e\u6321\u7684\u667a\u80fd\u884c\u4e3a\u3002", "conclusion": "CURA-PPO\u4e3a\u4ec5\u4f9d\u8d56\u673a\u8f7d\u611f\u77e5\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u906e\u6321\u73af\u5883\u4e0b\u7684\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u6548\u6027\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u667a\u80fd\u64cd\u4f5c\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00348", "abs": "https://arxiv.org/abs/2602.00348", "authors": ["Zhengyi Lu", "Ming Lu", "Chongyu Qu", "Junchao Zhu", "Junlin Guo", "Marilyn Lionts", "Yanfan Zhu", "Yuechen Yang", "Tianyuan Yao", "Jayasai Rajagopal", "Bennett Allan Landman", "Xiao Wang", "Xinqiang Yan", "Yuankai Huo"], "title": "MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI", "comment": null, "summary": "Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff08MASC\uff09\uff0c\u540c\u65f6\u89e3\u51b3MRI\u91d1\u5c5e\u4f2a\u5f71\u53bb\u9664\u548c\u52a0\u901f\u91c7\u96c6\u95ee\u9898\uff0c\u5b9e\u73b0\u4f2a\u5f71\u611f\u77e5\u7684k\u7a7a\u95f4\u91c7\u6837\u4e0e\u4f2a\u5f71\u6821\u6b63\u7684\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u91d1\u5c5e\u690d\u5165\u7269\u4f1a\u5bfc\u81f4MRI\u6210\u50cf\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\uff0c\u5f71\u54cd\u8bca\u65ad\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u4f2a\u5f71\u53bb\u9664\u548cMRI\u52a0\u901f\u91c7\u96c6\u5206\u5f00\u5904\u7406\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u517c\u987e\u5f71\u50cf\u8d28\u91cf\u548c\u91c7\u96c6\u6548\u7387\u3002", "method": "\u4f5c\u8005\u63d0\u51faMASC\u6846\u67b6\uff0c\u5c06\u4e3b\u52a8MRI\u91c7\u96c6\u5efa\u6a21\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u91c7\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6839\u636e\u7ecf\u8fc7U-Net\u4f2a\u5f71\u6821\u6b63\u7684\u6b20\u91c7\u6837\u91cd\u5efa\u7ed3\u679c\uff0c\u9009\u62e9k\u7a7a\u95f4\u91c7\u6837\u7ebf\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6848\uff0c\u4f7f\u91c7\u96c6\u7b56\u7565\u4e0e\u4f2a\u5f71\u53bb\u9664\u7f51\u7edc\u534f\u540c\u8fdb\u5316\u3002\u4e3a\u5b9e\u73b0\u6709\u76d1\u7763\u8bad\u7ec3\uff0c\u4f5c\u8005\u57fa\u4e8e\u7269\u7406\u4eff\u771f\u6784\u9020\u4e86\u5e26/\u4e0d\u5e26\u91d1\u5c5e\u690d\u5165\u7269\u7684MRI\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u53ef\u76f4\u63a5\u76d1\u7763\u4f2a\u5f71\u53bb\u9664\u4e0e\u91c7\u96c6\u7b56\u7565\u5b66\u4e60\u3002", "result": "MASC\u7684\u91c7\u6837\u7b56\u7565\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\uff0c\u7aef\u5230\u7aef\u534f\u540c\u4f18\u5316\u63d0\u5347\u4e86\u4f2a\u5f71\u6821\u6b63\u6027\u80fd\u3002\u8de8\u6570\u636e\u96c6\uff08FastMRI\uff09\u5b9e\u9a8c\u8bc1\u660eMASC\u5728\u771f\u5b9e\u4e34\u5e8aMRI\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "MASC\u5b9e\u73b0\u4e86MRI\u91d1\u5c5e\u4f2a\u5f71\u53bb\u9664\u4e0e\u52a0\u901f\u91c7\u96c6\u7684\u8054\u5408\u4f18\u5316\uff0c\u5927\u5e45\u63d0\u5347\u91cd\u5efa\u6548\u679c\uff0c\u65b9\u6cd5\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00945", "abs": "https://arxiv.org/abs/2602.00945", "authors": ["Anusa Saha", "Tanmay Joshi", "Vinija Jain", "Aman Chadha", "Amitava Das"], "title": "Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs", "comment": null, "summary": "LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.\n  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u8bc6\u522b\u548c\u64cd\u63a7\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0e\u8bed\u8a00\u76f8\u5173\u7684\u7279\u5b9a\u795e\u7ecf\u5143\uff08\u79f0\u4e3a\u201c\u8bed\u8a00\u795e\u7ecf\u5143\u201d\uff09\uff0c\u5b9e\u73b0\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed\uff09\u4f5c\u4e3a\u9ed8\u8ba4\u8f93\u51fa\u8bed\u8a00\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u201cNeural FOXP2\u201d\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u79e9\u63a7\u5236\u4e0e\u7a00\u758f\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u5bf9\u6a21\u578b\u8bed\u8a00\u504f\u597d\u673a\u5236\u7684\u5b9a\u5411\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u5176\u4ed6\u8bed\u8a00\u80fd\u529b\u867d\u5b58\u5728\uff0c\u4f46\u503e\u5411\u88ab\u538b\u5236\uff0c\u6a21\u578b\u5bf9\u82f1\u8bed\u6709\u9ed8\u8ba4\u504f\u597d\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u8ba9\u6a21\u578b\u4ee5\u5176\u4ed6\u8bed\u8a00\u4e3a\u4e3b\u8981\u8f93\u51fa\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u591a\u8bed\u8a00\u516c\u5e73\u6027\u53ca\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u201cNeural FOXP2\u201d\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u6b65\uff1a(1)\u5b9a\u4f4d\uff1a\u901a\u8fc7\u8bad\u7ec3\u201c\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u201d(SAE)\u5206\u89e3\u5404\u5c42\u6fc0\u6d3b\uff0c\u8bc6\u522b\u5bf9\u76ee\u6807\u8bed\u8a00\uff08\u5370\u5730\u8bed/\u897f\u73ed\u7259\u8bed\uff09\u9009\u62e9\u6027\u660e\u663e\u7684\u795e\u7ecf\u5143\uff1b(2)\u65b9\u5411\u63d0\u53d6\uff1a\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u7b49\u4f4e\u79e9\u8c31\u5206\u6790\uff0c\u63d0\u53d6\u82f1\u6587\u4e0e\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u663e\u8457\u7684\u795e\u7ecf\u65b9\u5411\u53ca\u9002\u5408\u5e72\u9884\u7684\u7f51\u7edc\u7a97\u53e3\uff1b(3)\u64cd\u63a7\uff1a\u5728\u8fd9\u4e9b\u65b9\u5411\u4e0a\u5bf9\u7279\u5b9a\u8bed\u8a00\u795e\u7ecf\u5143\u65bd\u4ee5\u6fc0\u6d3b\u8c03\u6574\uff0c\u5b9e\u73b0\u6a21\u578b\u8bed\u8a00\u9ed8\u8ba4\u7684\u5207\u6362\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u3001\u7a33\u5b9a\u5730\u5c06\u6a21\u578b\u7684\u4e3b\u8981\u8f93\u51fa\u8bed\u8a00\u4ece\u82f1\u8bed\u8f6c\u4e3a\u5370\u5730\u8bed\u6216\u897f\u73ed\u7259\u8bed\uff0c\u4e14\u5e72\u9884\u8fc7\u7a0b\u5177\u6709\u53ef\u63a7\u6027\u548c\u5b89\u5168\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9009\u5b9a\u65b9\u5411\u4e0e\u795e\u7ecf\u5143\u5e72\u9884\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u6027\u65b9\u6cd5\u5b9a\u5411\u8bc6\u522b\u548c\u64cd\u63a7\u5927\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u795e\u7ecf\u5143\uff0c\u53ef\u4ee5\u5b89\u5168\u4e14\u6709\u6548\u5730\u8bbe\u5b9a\u6a21\u578b\u7684\u9ed8\u8ba4\u8f93\u51fa\u8bed\u8a00\uff0c\u8fd9\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u548c\u53ef\u63a7\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01789", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01789", "abs": "https://arxiv.org/abs/2602.01789", "authors": ["Entong Su", "Tyler Westenbroek", "Anusha Nagabandi", "Abhishek Gupta"], "title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation", "comment": null, "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \\emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aResidual Flow Steering (RFS)\u7684\u65b0\u65b9\u6cd5\uff0c\u9ad8\u6548\u5730\u5bf9\u9884\u8bad\u7ec3\u751f\u6210\u5f0f\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u4e0e\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u6216\u6d41\u5339\u914d\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u867d\u80fd\u8868\u8fbe\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u4f46\u9884\u8bad\u7ec3\u7b56\u7565\u6cdb\u5316\u6709\u9650\u4e14\u9700\u5728\u90e8\u7f72\u65f6\u989d\u5916\u5fae\u8c03\uff0c\u9762\u4e34\u5c40\u90e8\u4fee\u6b63\u6548\u7387\u4f4e\u548c\u9700\u4fdd\u6301\u5168\u5c40\u63a2\u7d22\u80fd\u529b\u7684\u6311\u6218\u3002", "method": "RFS\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6b8b\u5dee\u52a8\u4f5c\u548c\u6f5c\u5728\u566a\u58f0\u5206\u5e03\uff0c\u5728\u9884\u8bad\u7ec3\u6d41\u5339\u914d\u7b56\u7565\u4e0a\u8fdb\u884c\u6b8b\u5dee\u6307\u5bfc\uff0c\u5b9e\u73b0\u5c40\u90e8\u6b8b\u5dee\u4fee\u6b63\u4e0e\u6f5c\u7a7a\u95f4\u8c03\u5236\u7684\u5168\u5c40\u63a2\u7d22\u534f\u540c\uff0c\u65e2\u4fdd\u6301\u539f\u7b56\u7565\u7684\u7ed3\u6784\u4f18\u52bf\uff0c\u53c8\u63d0\u5347\u5fae\u8c03\u901f\u5ea6\u548c\u6570\u636e\u6548\u7387\u3002", "result": "RFS\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5747\u80fd\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u57fa\u653f\u7b56\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u63d0\u5347\u6548\u679c\u3002", "conclusion": "RFS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u5fae\u8c03\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00350", "abs": "https://arxiv.org/abs/2602.00350", "authors": ["Ignacy Kolton", "Kacper Marzol", "Pawe\u0142 Batorski", "Marcin Mazur", "Paul Swoboda", "Przemys\u0142aw Spurek"], "title": "ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models", "comment": null, "summary": "Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u7b56\u7565ReLAPSe\uff0c\u53ef\u4ee5\u9ad8\u6548\u6062\u590d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5df2\u88ab\u201c\u9057\u5fd8\u201d\u7684\u6982\u5ff5\uff0c\u63d0\u9ad8\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u7684\u7ea2\u961f\u6d4b\u8bd5\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u901a\u8fc7\u201c\u673a\u5668\u9057\u5fd8\u201d\u65b9\u6cd5\u5bf9\u6269\u6563\u6a21\u578b\u79fb\u9664\u654f\u611f\u6216\u672a\u6388\u6743\u5185\u5bb9\uff0c\u4f46\u53d1\u73b0\u4ecd\u5b58\u5728\u89c6\u89c9\u4fe1\u606f\u6cc4\u9732\u3002\u73b0\u6709\u5bf9\u6297\u6062\u590d\u624b\u6bb5\u6548\u7387\u4f4e\u6216\u65e0\u6cd5\u76f4\u63a5\u5229\u7528\u6a21\u578b\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5bf9\u9057\u5fd8\u6548\u679c\u7684\u8bc4\u4f30\u4e0e\u653b\u9632\u5bf9\u6297\u80fd\u529b\u3002", "method": "\u63d0\u51faReLAPSe\u6846\u67b6\uff0c\u5c06\u201c\u6062\u590d\u88ab\u9057\u5fd8\u6982\u5ff5\u201d\u95ee\u9898\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u3002\u901a\u8fc7\u2018\u53ef\u9a8c\u8bc1\u5956\u52b1\u2019\u5f3a\u5316\u5b66\u4e60\u65b9\u5f0f\uff0c\u4ee5\u6269\u6563\u6a21\u578b\u81ea\u6709\u7684\u566a\u58f0\u9884\u6d4b\u635f\u5931\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u6307\u5bfc\u667a\u80fd\u4f53\u5b66\u4e60\u5168\u7403\u6027\u7684\u6062\u590d\u7b56\u7565\uff0c\u6709\u522b\u4e8e\u8fc7\u53bb\u9010\u5b9e\u4f8b\u4f18\u5316\u65b9\u6cd5\u3002", "result": "ReLAPSe\u5728\u591a\u79cd\u4e3b\u6d41\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u4e0b\uff0c\u663e\u8457\u9ad8\u6548\u5730\u6062\u590d\u4e86\u7ec6\u7c92\u5ea6\u8eab\u4efd\u4e0e\u98ce\u683c\u4fe1\u606f\uff0c\u4e14\u5b9e\u73b0\u4e86\u8fd1\u5b9e\u65f6\u7684\u653b\u51fb\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9010\u5b9e\u4f8b\u4f18\u5316\u8f6c\u53d8\u4e3a\u5168\u5c40\u7b56\u7565\u5b66\u4e60\uff0cReLAPSe\u63d0\u5347\u4e86\u6062\u590d\u88ab\u9057\u5fd8\u5185\u5bb9\u7684\u6548\u7387\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5b89\u5168\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u5229\u5668\uff0c\u4e5f\u63ed\u793a\u4e86\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6848\u7684\u6f5c\u5728\u5b89\u5168\u9690\u60a3\u3002"}}
{"id": "2602.00970", "categories": ["cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00970", "abs": "https://arxiv.org/abs/2602.00970", "authors": ["Saaduddin Mahmud", "Eugene Bagdasarian", "Shlomo Zilberstein"], "title": "Verification Required: The Impact of Information Credibility on AI Persuasion", "comment": "19 pages, 5 figures", "summary": "Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u4e00\u4e2a\u65b0\u6a21\u578bMixTalk\uff0c\u4ee5\u6a21\u62df\u548c\u5206\u6790LLM\u4e4b\u95f4\u5177\u6709\u6982\u7387\u6027\u53ef\u4fe1\u5ea6\u7684\u4fe1\u606f\u4ea4\u6d41\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u63a5\u6536\u7aef\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u800c\u73b0\u5b9e\u73af\u5883\u4e2d\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\u4ecb\u4e8e\u5b8c\u5168\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u4e4b\u95f4\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u6709\u6548\u8986\u76d6\u8fd9\u7c7b\u6982\u7387\u6027\u53ef\u4fe1\u7684\u6c9f\u901a\u573a\u666f\u3002\u4f5c\u8005\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6218\u7565\u6027\u6c9f\u901a\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u8fdb\u5c55\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MixTalk\u6c9f\u901a\u535a\u5f08\uff1a\u4e00\u4e2a\u53d1\u9001\u8005\u667a\u80fd\u4f53\u7ed3\u5408\u53ef\u9a8c\u8bc1\u4e0e\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4fe1\u606f\u8fdb\u884c\u4f20\u9012\uff0c\u63a5\u6536\u8005\u667a\u80fd\u4f53\u5219\u5728\u6709\u9650\u9a8c\u8bc1\u9884\u7b97\u4e0b\uff0c\u5bf9\u4fe1\u606f\u9009\u62e9\u6027\u9a8c\u8bc1\u5e76\u636e\u4ee5\u63a8\u65ad\u4e8b\u5b9e\u3002\u4f5c\u8005\u5229\u7528LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u5728\u591a\u79cd\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e0b\u5f00\u5c55\u5927\u89c4\u6a21\u6bd4\u8d5b\u3002\u540c\u65f6\u63d0\u51fa\u4e86Tournament Oracle Policy Distillation\uff08TOPD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u4e92\u52a8\u8bb0\u5f55\u4e2d\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u7528\u4e8e\u63a8\u7406\u9636\u6bb5\u4ee5\u63d0\u5347\u63a5\u6536\u65b9\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e3b\u6d41LLM\u5728\u4fe1\u606f\u6982\u7387\u53ef\u4fe1\u5ea6\u63a8\u7406\u548c\u7b56\u7565\u6027\u6c9f\u901a\u4e2d\u7684\u8868\u73b0\u6709\u4f18\u52bf\u4e5f\u6709\u4e0d\u8db3\u3002TOPD\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a5\u6536\u65b9\u5bf9\u529d\u670d\u6027\u4fe1\u606f\u7684\u9c81\u68d2\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u672a\u84b8\u998f\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e30\u5bcc\u4e86\u6218\u7565\u6027\u6c9f\u901a\u667a\u80fd\u4f53\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u771f\u5b9e\u73af\u5883\u90e8\u7f72LLM agent\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u63d0\u51fa\u7684TOPD\u65b9\u6cd5\u5bf9\u63d0\u9ad8\u6c9f\u901a\u9c81\u68d2\u6027\u5177\u6709\u5b9e\u9645\u5e94\u7528\u610f\u4e49\u3002"}}
{"id": "2602.01811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01811", "abs": "https://arxiv.org/abs/2602.01811", "authors": ["Wentao Zhang", "Aolan Sun", "Wentao Mo", "Xiaoyang Qu", "Yuxin Zheng", "Jianzong Wang"], "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models", "comment": "Accepted to 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u7ea0\u6b63\u63a7\u5236\u6846\u67b6VLA-SCT\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u667a\u80fd\u4f53\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u5b8c\u6210\u7387\u3002", "motivation": "\u76ee\u524dVLA\u6a21\u578b\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b58\u5728\uff1a1\uff09\u52a8\u4f5c\u8f93\u51fa\u4e0e\u76ee\u6807\u7a7a\u95f4\u4f4d\u7f6e\u7cbe\u5ea6\u4e0d\u8db3\u5bfc\u81f4\u6293\u53d6\u5931\u8d25\uff1b2\uff09\u7f3a\u4e4f\u5b8c\u6210\u5224\u65ad\uff0c\u6613\u51fa\u73b0\u5197\u4f59\u52a8\u4f5c\u6216\u8d85\u65f6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u63d0\u51faVLA-SCT\u6846\u67b6\uff0c\u5c06\u6570\u636e\u9a71\u52a8\u7684\u52a8\u4f5c\u4f18\u5316\u548c\u6709\u6761\u4ef6\u7684\u4efb\u52a1\u7ec8\u6b62\u903b\u8f91\u7ed3\u5408\uff0c\u5f62\u6210\u81ea\u7ea0\u6b63\u95ed\u73af\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e2d\uff0cVLA-SCT\u6846\u67b6\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u7ec6\u81f4\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u4e0e\u4efb\u52a1\u5b8c\u6210\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VLA-SCT\u65b9\u6cd5\u589e\u5f3a\u4e86VLA\u667a\u80fd\u4f53\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5176\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2602.00381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00381", "abs": "https://arxiv.org/abs/2602.00381", "authors": ["Kezia Minni", "Qiang Zhang", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Image-Caption Rating from Comparative Judgments", "comment": null, "summary": "Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $\u03c1$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u6bd4\u8f83\u6027\u5b66\u4e60\u6a21\u578b\u6765\u8bc4\u4f30\u56fe\u50cf\u63cf\u8ff0\u4e0e\u56fe\u7247\u7684\u5339\u914d\u5ea6\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u76f4\u63a5\u8bc4\u5206\u56de\u5f52\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u968f\u7740\u6570\u636e\u589e\u591a\u8868\u73b0\u4e0d\u65ad\u63d0\u5347\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4eba\u5de5\u4e3a\u56fe\u50cf\u548c\u63cf\u8ff0\u914d\u5bf9\u51c6\u786e\u6027\u8bc4\u5206\u65e2\u8017\u65f6\u53c8\u4e3b\u89c2\uff0c\u4f46\u4e24\u4e24\u6bd4\u8f83\u66f4\u5bb9\u6613\uff0c\u4eba\u7c7b\u5224\u522b\u4e5f\u66f4\u4e00\u81f4\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u8fd9\u4e00\u70b9\uff0c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3001\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528VICR\u6570\u636e\u96c6\uff0c\u501f\u52a9ResNet-50\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0cMiniLM\u63d0\u53d6\u63cf\u8ff0\u7279\u5f81\uff0c\u5206\u522b\u8bad\u7ec3\u4f20\u7edf\u56de\u5f52\u6a21\u578b\uff08\u7528\u4eba\u5de5\u5206\u6570\u62df\u5408\uff09\u4e0e\u6bd4\u8f83\u6027\u5b66\u4e60\u6a21\u578b\uff08\u62df\u5408\u6bd4\u8f83\u5224\u65ad\uff09\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8fdb\u884c\u6392\u5e8f\u8bc4\u4f30\u3002\u53e6\u8fd8\u8fdb\u884c\u4e86\u5c0f\u89c4\u6a21\u7684\u4eba\u7c7b\u8bc4\u6d4b\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u7edd\u5bf9\u8bc4\u5206\u4e0e\u6210\u5bf9\u6bd4\u8f83\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "result": "\u56de\u5f52\u6a21\u578b\u5728Pearson\u548cSpearman\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u4f46\u6bd4\u8f83\u6027\u5b66\u4e60\u6a21\u578b\u968f\u6570\u636e\u91cf\u589e\u52a0\u6301\u7eed\u63d0\u9ad8\uff0c\u9010\u6e10\u63a5\u8fd1\u56de\u5f52\u57fa\u7ebf\u3002\u800c\u4e14\u4eba\u5de5\u5bf9\u6bd4\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u76f4\u63a5\u8bc4\u5206\uff0c\u6bd4\u8f83\u6027\u6807\u6ce8\u901f\u5ea6\u66f4\u5feb\uff0c\u4eba\u7c7b\u6807\u6ce8\u8005\u95f4\u66f4\u4e00\u81f4\u3002", "conclusion": "\u6bd4\u8f83\u6027\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u504f\u597d\uff0c\u4e14\u53ef\u5927\u5e45\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002\u4e24\u4e24\u6bd4\u8f83\u6bd4\u7edd\u5bf9\u8bc4\u5206\u66f4\u9ad8\u6548\uff0c\u53ef\u4f5c\u4e3a\u56fe\u50cf\u2014\u63cf\u8ff0\u5339\u914d\u4efb\u52a1\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00977", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00977", "abs": "https://arxiv.org/abs/2602.00977", "authors": ["Pengyue Yang", "Jiawen Wen", "Haolin Jin", "Linghan Huang", "Huaming Chen", "Ling Chen"], "title": "Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals", "comment": "Accepted at The ACM Web Conference 2026 (WWW 2026)", "summary": "Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u7f6e\u4fe1\u5ea6\uff08Structural Confidence\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5927\u6a21\u578b\u6700\u540e\u4e00\u5c42\u9690\u85cf\u6001\u8f68\u8ff9\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u8f93\u51fa\u6b63\u786e\u6027\u7684\u9884\u6d4b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8de8\u591a\u4e2a\u9ad8\u793e\u4f1a\u6210\u672c\u9886\u57df\u548c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5f3a\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u591a\u6b21\u91c7\u6837\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6982\u7387\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u591a\u6837\u672c\u4e00\u81f4\u6027\uff09\u5728\u5206\u5e03\u6f02\u79fb\u3001\u4e13\u4e1a\u6587\u672c\u548c\u7b97\u529b\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u7a33\u5065\uff0c\u8feb\u5207\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684Structural Confidence\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u8f6c\u6362\u63d0\u53d6\u6700\u540e\u4e00\u5c42\u9690\u85cf\u72b6\u6001\u5728\u65f6\u95f4\u8f74\u4e0a\u7684\u9891\u8c31\u3001\u5c40\u90e8\u53d8\u5316\u3001\u5168\u5c40\u5f62\u72b6\u7b49\u7ed3\u6784\u6027\u63cf\u8ff0\u7b26\uff0c\u6355\u6349\u5185\u90e8\u7a33\u5b9a\u6027\u4fe1\u606f\uff0c\u5b9e\u73b0\u5355\u6b21\u6b63\u5411\u63a8\u7406\u5373\u53ef\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "result": "\u5728FEVER\u3001SciFact\u3001WikiBio-hallucination\u548cTruthfulQA\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7ed3\u6784\u5316\u7f6e\u4fe1\u5ea6\u65b9\u6cd5\u5728AUROC\u548cAUPR\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u4e3b\u6d41\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u6784\u5316\u7f6e\u4fe1\u5ea6\u6846\u67b6\u5728\u65e0\u9700\u591a\u6b21\u91c7\u6837\u548c\u8f85\u52a9\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u793e\u4f1a\u5f71\u54cd\u5927\u4e14\u8d44\u6e90\u53d7\u9650\u7684\u5927\u6a21\u578b\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.01834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01834", "abs": "https://arxiv.org/abs/2602.01834", "authors": ["Siqi Wen", "Shu Yang", "Shaopeng Fu", "Jingfeng Zhang", "Lijie Hu", "Di Wang"], "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models", "comment": null, "summary": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u5b57\u5178\u5b66\u4e60\u7684\u63a8\u7406\u65f6\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u884c\u4e3a\uff08VLA\uff09\u6a21\u578b\u5728\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u62e6\u622a\u6f5c\u5728\u7684\u5371\u9669\u884c\u4e3a\u3002", "motivation": "VLA\u6a21\u578b\u80fd\u57fa\u4e8e\u591a\u6a21\u6001\u4fe1\u606f\u6267\u884c\u5b9e\u9645\u884c\u4e3a\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u4e5f\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff1a\u653b\u51fb\u8005\u53ef\u80fd\u8bf1\u5bfc\u6a21\u578b\u6267\u884c\u5371\u9669\u52a8\u4f5c\u3002\u73b0\u6709\u9632\u62a4\u624b\u6bb5\u4ecb\u5165\u65f6\u673a\u6216\u6a21\u6001\u4e0d\u5f53\uff0c\u4e0d\u80fd\u6839\u672c\u89e3\u51b3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u3001\u66f4\u6709\u6548\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7a00\u758f\u3001\u53ef\u89e3\u91ca\u7684\u5b57\u5178\uff0c\u5206\u6790\u6a21\u578b\u9690\u85cf\u6fc0\u6d3b\uff0c\u8bc6\u522b\u51fa\u4e0e\u6709\u5bb3\u6982\u5ff5\u76f8\u5173\u7684\u65b9\u5411\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f9d\u636e\u9608\u503c\u8fdb\u884c\u5e72\u9884\uff0c\u963b\u65ad\u5371\u9669\u6fc0\u6d3b\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u5b89\u5168\u9632\u63a7\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5bf9\u539f\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cdVLA\u6a21\u578b\u3002", "result": "\u5728Libero-Harm\u3001BadRobot\u3001RoboPair\u548cIS-Bench\u7b49\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e70%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u5728 embodied systems \u4e0a\u5229\u7528\u6982\u5ff5\u89e3\u91ca\u8fdb\u884c\u63a8\u7406\u65f6\u5b89\u5168\u9632\u63a7\uff0c\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u5b89\u5168\u6027\uff0c\u5e76\u4e14\u65b9\u6cd5\u7b80\u5355\u6613\u96c6\u6210\uff0c\u5bf9\u6a21\u578b\u65e0\u4fb5\u5165\uff0c\u662f\u5b9e\u73b0\u53ef\u9760VLA\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.00385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00385", "abs": "https://arxiv.org/abs/2602.00385", "authors": ["Bsher Karbouj", "Adam Michael Altenbuchner", "Joerg Krueger"], "title": "Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects", "comment": null, "summary": "Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u5e76\u5206\u6790\u4e86YOLOv5\u548cFaster R-CNN\u4e24\u79cd\u4e3b\u6d41\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u63a2\u8ba8\u5176\u4f18\u7f3a\u70b9\u5e76\u7ed9\u51fa\u9002\u7528\u6027\u5efa\u8bae\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u76ee\u6807\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u4e3b\u6d41\u68c0\u6d4b\u6a21\u578b\u5728\u7279\u5b9a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u6307\u5bfc\u6709\u9650\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u6bd4\u8f83\u8fd9\u4e9b\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u4e86YOLOv5\uff08\u4e00\u9636\u6bb5\u68c0\u6d4b\u5668\uff09\u4e0eFaster R-CNN\uff08\u4e8c\u9636\u6bb5\u68c0\u6d4b\u5668\uff09\u4e24\u79cd\u6a21\u578b\uff0c\u5728\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7mAP\u3001\u53ec\u56de\u7387\u548c\u63a8\u7406\u901f\u5ea6\u7b49\u591a\u9879\u6307\u6807\u8fdb\u884c\u5b9e\u9a8c\u5bf9\u6bd4\u5206\u6790\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u7f6e\u4fe1\u5ea6\u9608\u503c\u548c\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cYOLOv5\u5728mAP\u3001\u53ec\u56de\u7387\u53ca\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u9002\u5408\u5927\u89c4\u6a21\u3001\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff1b\u800cFaster R-CNN\u5728\u5c0f\u578b\u3001\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u68c0\u6d4b\u53ca\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u4e8c\u8005\u5404\u6709\u5343\u79cb\uff0cYOLOv5\u9002\u7528\u4e8e\u5bf9\u901f\u5ea6\u548c\u5927\u8303\u56f4\u76ee\u6807\u68c0\u6d4b\u8981\u6c42\u9ad8\u7684\u5e94\u7528\uff0c\u800cFaster R-CNN\u9002\u5408\u5bf9\u5c0f\u7269\u4f53\u68c0\u6d4b\u548c\u6076\u52a3\u73af\u5883\u8981\u6c42\u8f83\u9ad8\u7684\u573a\u666f\u3002\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9009\u62e9\u5408\u9002\u7684\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u5efa\u8bae\u3002"}}
{"id": "2602.00981", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00981", "abs": "https://arxiv.org/abs/2602.00981", "authors": ["Yutong Song", "Shiva Shrestha", "Chenhan Lyu", "Elahe Khatibi", "Pengfei Zhang", "Honghui Xu", "Nikil Dutt", "Amir Rahmani"], "title": "MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA", "comment": null, "summary": "Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.", "AI": {"tldr": "MedSpeak\u662f\u4e00\u4e2a\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u56fe\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684ASR\u7ea0\u9519\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u53e3\u8bed\u95ee\u7b54\u7cfb\u7edf\u4e2d\u533b\u5b66\u672f\u8bed\u7684\u8bc6\u522b\u548c\u6574\u4f53\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u53e3\u8bed\u95ee\u7b54\u7cfb\u7edf\u4f9d\u8d56\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6280\u672f\uff0c\u4f46\u5728\u533b\u5b66\u9886\u57df\uff0cASR\u5bf9\u4e13\u4e1a\u672f\u8bed\u7684\u8bc6\u522b\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u5f71\u54cd\u540e\u7eed\u95ee\u7b54\u7684\u6548\u679c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u5f15\u5165\u533b\u5b66\u77e5\u8bc6\u8fdb\u884c\u8f85\u52a9\u3002", "method": "\u63d0\u51fa\u540d\u4e3aMedSpeak\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u53d1\u97f3\u4fe1\u606f\uff0c\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bf9ASR\u8f93\u51fa\u7684\u9519\u8bef\u533b\u5b66\u672f\u8bed\u8fdb\u884c\u7ea0\u6b63\uff0c\u4ece\u800c\u4f18\u5316\u540e\u7eed\u7684\u95ee\u7b54\u4efb\u52a1\u3002", "result": "\u5728\u591a\u9879\u533b\u5b66\u95ee\u7b54\u8bc4\u6d4b\u57fa\u51c6\u4e0a\uff0cMedSpeak\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u672f\u8bed\u8bc6\u522b\u51c6\u786e\u6027\u548c\u6574\u4f53SQA\u7cfb\u7edf\u6548\u679c\uff0c\u5728\u4e1a\u5185\u8fbe\u5230\u9886\u5148\u8868\u73b0\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8f85\u52a9\u7684\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u8865\u8db3ASR\u5728\u533b\u5b66\u9886\u57df\u7684\u5c40\u9650\uff0cMedSpeak\u76ee\u524d\u662f\u533b\u7597\u53e3\u8bed\u95ee\u7b54\u9886\u57df\u7684\u6700\u65b0\u6700\u4f18\u89e3\u3002"}}
{"id": "2602.01860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01860", "abs": "https://arxiv.org/abs/2602.01860", "authors": ["Filip Nov\u00e1k", "Mat\u011bj Petrl\u00edk", "Matej Novosad", "Parakh M. Gupta", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach", "comment": "Visit our webpage for more details: https://mrs.fel.cvut.cz/papers/vision-only-uav-state-estimation", "summary": "Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u76f8\u673a\u548cIMU\u7684\u9ad8\u901f\u65e0\u4eba\u673a\u81ea\u4e3b\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u6f02\u79fb\u8865\u507f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u65e0GNSS\u3001\u73af\u5883\u590d\u6742\u65f6\u7684\u9ad8\u7cbe\u5ea6\u3001\u5f3a\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u4f9d\u8d56\u66f4\u590d\u6742\u786c\u4ef6\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u5728\u6a21\u62df\u4e0e\u771f\u5b9e\u6d4b\u8bd5\u53ca\u56fd\u9645\u7ade\u8d5b\u4e2d\u53d6\u5f97\u51fa\u8272\u6210\u7ee9\u3002", "motivation": "\u76ee\u524d\u5728\u590d\u6742\u3001\u65e0GNSS\u73af\u5883\u4e0b\u9ad8\u901f\u98de\u884c\u7684\u65e0\u4eba\u673a\u9700\u8981\u5feb\u901f\u3001\u51c6\u786e\u3001\u9c81\u68d2\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u590d\u6742\u7684\u786c\u4ef6\uff08\u5982\u53cc\u76ee\u3001\u6fc0\u5149\uff09\uff0c\u8981\u4e48VIO\u6f02\u79fb\u672a\u80fd\u6821\u6b63\uff0c\u72b6\u6001\u4f30\u8ba1\u5bb9\u6613\u5931\u51c6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5355\u76eeRGB\u76f8\u673a\u3001IMU\u53ca\u57fa\u4e8e\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u6807\u5fd7\u7269\u76f8\u673a\u6d4b\u91cf\u7cfb\u7edf\u7684\u6570\u636e\u878d\u5408\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u5b66\u6f02\u79fb\u6a21\u578b\u5728\u7ebf\u4f30\u8ba1\u5e76\u77eb\u6b63VIO\u6240\u6709\u72b6\u6001\uff08\u4f4d\u7f6e\u3001\u59ff\u6001\u3001\u7ebf\u901f\u5ea6\u3001\u89d2\u901f\u5ea6\uff09\u7684\u6f02\u79fb\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u3002\u65b9\u6cd5\u7ecf\u8fc71600\u6b21\u4eff\u771f\u548c\u5927\u91cf\u5b9e\u6d4b\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u8865\u507f\u4e86VIO\u72b6\u6001\u6f02\u79fb\uff0c\u5728\u9ad8\u901f\u3001\u5267\u70c8\u673a\u52a8\u4e0b\u4e5f\u80fd\u4fdd\u6301\u7cbe\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u3002\u5b9e\u9645\u6d4b\u8bd5\u548cA2RL\u4e16\u754c\u7ea7\u65e0\u4eba\u673a\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u56e2\u961f\u4ece210\u652f\u961f\u4f0d\u4e2d\u664b\u7ea7\u56db\u5f3a\u83b7\u5956\u3002", "conclusion": "\u672c\u6587\u7684\u65b0\u65b9\u6cd5\u6027\u80fd\u4f18\u5f02\u3001\u786c\u4ef6\u9700\u6c42\u4f4e\uff0c\u80fd\u63d0\u5347\u9ad8\u901f\u65e0\u4eba\u673a\u5728GNSS-denied\u3001\u590d\u6742\u73af\u5883\u4e0b\u7684\u5e94\u7528\u524d\u666f\uff0c\u5728\u4eff\u771f\u3001\u5b9e\u6d4b\u548c\u56fd\u9645\u8d5b\u4e8b\u4e2d\u5747\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2602.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00391", "abs": "https://arxiv.org/abs/2602.00391", "authors": ["Alberto Mario Ceballos-Arroyo", "Shrikanth M. Yadav", "Chu-Hsuan Lin", "Jisoo Kim", "Geoffrey S. Young", "Huaizu Jiang", "Lei Qin"], "title": "Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data", "comment": "16 pages, 8 figures", "summary": "In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u52a8\u60014D-CTA\u5934\u90e8\u626b\u63cf\u81ea\u52a8\u6807\u6ce8\u8111\u8840\u7ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u65f6\u76f8\u51cf\u53bb\u9aa8\u9abc\u548c\u8f6f\u7ec4\u7ec7\uff0c\u589e\u5f3a\u8840\u7ba1\u53ef\u89c6\u5316\uff0c\u5e76\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\uff0c\u4e14\u516c\u5f00\u4ee3\u7801\u4e0e\u6a21\u578b\u3002", "motivation": "\u8111\u8840\u7ba1\u7cbe\u51c6\u5206\u5272\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u8fc7\u7a0b\u8d39\u65f6\u8d39\u529b\u3002\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u5bf9\u6bd4\u5ea6\u76f8\u4f4d\uff0c\u9020\u6210\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u9ad8\u8111\u8840\u7ba1\u6ce8\u91ca\u6548\u7387\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u91cf\u4e0e\u591a\u6837\u6027\u7684\u521b\u65b0\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u5229\u7528\u52a8\u60014D-CTA\u626b\u63cf\u7684\u591a\u4e2a\u65f6\u95f4\u70b9\uff0c\u91c7\u7528\u591a\u65f6\u76f8\u51cf\u6cd5\u6d88\u9664\u9aa8\u9abc\u548c\u8f6f\u7ec4\u7ec7\u4fe1\u53f7\uff0c\u7a81\u663e\u8111\u8840\u7ba1\u7ed3\u6784\uff0c\u964d\u4f4e\u6807\u6ce8\u96be\u5ea6\u3002\u901a\u8fc7\u5bf9\u540c\u4e00\u5e8f\u5217\u591a\u9636\u6bb5\u91c7\u7528\u76f8\u540c\u5206\u5272\u6ce8\u91ca\uff0c\u6709\u6548\u6269\u5145\u4e86\u53ef\u7528\u8bad\u7ec3\u96c6\uff0c\u5e76\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\uff08nnUNet\uff09\u6a21\u578b\u8fdb\u884c\u8840\u7ba1\u5206\u5272\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u65b0\u6570\u636e\u96c6\u5305\u542b110\u4e2a\u8bad\u7ec3\u56fe\u50cf\u548c165\u4e2a\u6d4b\u8bd5\u56fe\u50cf\u3002\u76f8\u8f83\u4e8e\u540c\u7c7b\u6570\u636e\u96c6\uff0c\u7528nnUNet\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5bf9\u8840\u7ba1\u5206\u5272\u7684\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff0cmDC\u6307\u6807\uff1a\u52a8\u81090.846\uff0c\u9759\u81090.957\u3002\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\uff08aDHD\u548ctSens\uff09\u4e5f\u8868\u73b0\u51fa\u66f4\u4f4e\u8bef\u5dee\u548c\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\uff0c\u8868\u660e\u6a21\u578b\u51c6\u786e\u6355\u6349\u8840\u7ba1\u5f62\u6001\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5229\u75284D-CTA\u4e0d\u540c\u65f6\u95f4\u70b9\u6570\u636e\uff0c\u65b0\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u8840\u7ba1\u6807\u6ce8\u548c\u6a21\u578b\u5206\u5272\u7684\u6548\u7387\u53ca\u51c6\u786e\u7387\uff0c\u4e3a\u8111\u8840\u7ba1\u5f71\u50cf\u5206\u5272\u5e26\u6765\u66f4\u5f3a\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e5f\u4e3a\u76f8\u5173\u7814\u7a76\u516c\u5f00\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u5de5\u5177\u3002"}}
{"id": "2602.00983", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00983", "abs": "https://arxiv.org/abs/2602.00983", "authors": ["Batuhan K. Karaman", "Aditya Rawal", "Suhaila Shakiah", "Mohammad Ghavamzadeh", "Mingyi Hong", "Arijit Biswas", "Ruida Zhou"], "title": "DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning", "comment": "This work is accepted to the 29th International Conference on Artificial Intelligence and Statistics (AISTATS) 2026", "summary": "Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bREINFORCE\u65b9\u6cd5DISPO\uff0c\u901a\u8fc7\u5206\u79bb\u6b63\u786e\u548c\u9519\u8bef\u54cd\u5e94\u7684\u526a\u88c1\uff0c\u6539\u5584\u4e86\u5927\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u73af\u5883\u4e0b\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002DISPO\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u7684\u6743\u8861\uff1aPPO\u98ce\u683c\u65b9\u6cd5\u867d\u7136\u7a33\u5b9a\u4f46\u5b66\u4e60\u6162\uff0cREINFORCE\u98ce\u683c\u65b9\u6cd5\u5b66\u4e60\u5feb\u5374\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u65b0\u65b9\u6cd5\u540c\u65f6\u517c\u987e\u4e24\u8005\u4f18\u52bf\u3002", "method": "DISPO\u7b97\u6cd5\u5c06\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u7684\u4e0a\u526a\u88c1\u548c\u4e0b\u526a\u88c1\u5206\u522b\u4f5c\u7528\u4e8e\u6b63\u786e\u548c\u9519\u8bef\u54cd\u5e94\uff0c\u5b9e\u73b0\u4e86\u56db\u79cd\u53ef\u63a7\u7684\u7b56\u7565\u66f4\u65b0\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u8c03\u4f18\u5404\u526a\u88c1\u53c2\u6570\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u77e5\u8bc6\u84b8\u998f\uff0c\u9632\u6b62\u707e\u96be\u6027\u5931\u8d25\u3002", "result": "\u5728AIME'24\u6570\u5b66\u7ade\u8d5b\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDISPO\u83b7\u5f97\u4e86\u6700\u9ad8\u5206\u6570\uff0861.04%\uff09\uff0c\u663e\u8457\u4f18\u4e8eCISPO\uff0855.42%\uff09\u548cDAPO\uff0850.21%\uff09\uff0c\u5176\u5b83\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e5f\u6709\u7c7b\u4f3c\u63d0\u5347\u3002", "conclusion": "DISPO\u5728\u4fdd\u8bc1\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u6570\u5b66\u7b49\u9886\u57df\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1RL\u9886\u57df\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01870", "abs": "https://arxiv.org/abs/2602.01870", "authors": ["Riccardo Andrea Izzo", "Gianluca Bardaro", "Matteo Matteucci"], "title": "BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models", "comment": null, "summary": "Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BTGenBot-2\uff0c\u4e00\u6b3e\u8f7b\u91cf\u7ea7\u5f00\u6e90\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u6811\u3002\u5b83\u652f\u6301\u96f6\u6837\u672c\u751f\u6210\u548c\u81ea\u52a8\u9519\u8bef\u6062\u590d\uff0c\u4e14\u5728\u5404\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u591a\u4f9d\u8d56\u5c01\u95ed\u6e90\u3001\u8ba1\u7b97\u8d44\u6e90\u4e30\u5bcc\u7684LLM\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u4e8e\u7269\u7406\u673a\u5668\u4eba\u3002\u4e14\u7f3a\u4e4f\u7edf\u4e00\u3001\u6613\u7528\u7684\u673a\u5668\u4eba\u4efb\u52a1\u751f\u6210\u8868\u793a\u5f62\u5f0f\uff0c\u9650\u5236\u4e86LLM\u5728\u673a\u5668\u4eba\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u6b3e\u5177\u670910\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u5c0f\u578b\u8bed\u8a00\u6a21\u578bBTGenBot-2\uff0c\u53ef\u76f4\u63a5\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u52a8\u4f5c\u539f\u8bed\u5217\u8868\u6620\u5c04\u4e3aXML\u683c\u5f0f\u7684\u884c\u4e3a\u6811\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u884c\u4e3a\u6811\u751f\u6210\u548c\u63a8\u7406\u65f6\u7684\u9519\u8bef\u6062\u590d\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u9996\u4e2aLLM\u751f\u6210\u884c\u4e3a\u6811\u7684\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d652\u7c7b\u673a\u5668\u4eba\u4efb\u52a1\u3002", "result": "BTGenBot-2\u5728NVIDIA Isaac Sim\u4eff\u771f\u5e73\u53f0\u7684\u5bfc\u822a\u548c\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0c\u5404\u9879\u529f\u80fd\u548c\u975e\u529f\u80fd\u6307\u6807\u5747\u8d85\u8d8a\u4e86GPT-5\u3001Claude Opus 4.1\u7b49\u66f4\u5927\u6a21\u578b\uff0c\u96f6\u6837\u672c\u6a21\u5f0f\u4e0b\u6210\u529f\u7387\u8fbe90.38%\uff0c\u4e00\u6b21\u6f14\u793a\u540e\u6210\u529f\u7387\u8fbe98.07%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb16\u500d\u3002", "conclusion": "BTGenBot-2\u4e3a\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u5f00\u6e90\u884c\u4e3a\u6811\u751f\u6210\u5de5\u5177\u6807\u51c6\u5316\uff0c\u4fc3\u8fdb\u4e86LLM\u6280\u672f\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.00393", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00393", "abs": "https://arxiv.org/abs/2602.00393", "authors": ["Gabriel Bromonschenkel", "Alessandro L. Koerich", "Thiago M. Paix\u00e3o", "Hil\u00e1rio Tomaz Alves de Oliveira"], "title": "Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset", "comment": "Accepted to JBCS. 18 pages, 11 figures", "summary": "Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5df4\u897f\u8461\u8404\u7259\u8bed\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdTransformer\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u5de5\u4e0e\u81ea\u52a8\u7ffb\u8bd1\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u672c\u571f\u548c\u81ea\u52a8\u7ffb\u8bd1\u6570\u636e\u96c6\u5404\u6709\u4f18\u52a3\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3001\u6587\u672c\u4e0e\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u53ca\u504f\u5dee\u8868\u73b0\u6709\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u5927\u591a\u6570\u56fe\u50cf\u63cf\u8ff0\u7814\u7a76\u96c6\u4e2d\u5728\u82f1\u8bed\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u5df4\u897f\u8461\u8404\u7259\u8bed\u56e0\u7f3a\u4e4f\u6570\u636e\u548c\u6a21\u578b\u800c\u53d1\u5c55\u7f13\u6162\u3002\u90e8\u5206\u7814\u7a76\u901a\u8fc7\u81ea\u52a8\u7ffb\u8bd1\u65b9\u5f0f\u521b\u5efa\u6570\u636e\u96c6\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u81ea\u52a8\u7ffb\u8bd1\u4e0e\u672c\u5730\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5df4\u897f\u8461\u8404\u7259\u8bed\u7248Flickr30K\u6570\u636e\u96c6\uff0c\u4e00\u90e8\u5206\u7531\u6bcd\u8bed\u8005\u4eba\u5de5\u6807\u6ce8\uff0c\u4e00\u90e8\u5206\u7531\u82f1\u8bed\u539f\u6587\u81ea\u52a8\u7ffb\u8bd1\u3002\u9009\u7528\u591a\u79cdTransformer\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u4ea4\u53c9\u8bad\u7ec3-\u6d4b\u8bd5\u65b9\u6cd5\u68c0\u9a8c\u81ea\u52a8\u7ffb\u8bd1\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u7528CLIP-Score\u8bc4\u4f30\u56fe\u6587\u5bf9\u9f50\uff0c\u7528\u6ce8\u610f\u529b\u56fe\u5206\u6790\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "Swin-DistilBERTimbau\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6cdb\u5316\u80fd\u529b\u8f83\u5f3a\u3002ViTucano\u5728\u4f20\u7edf\u6587\u672c\u6307\u6807\u4e0a\u8d85\u8fc7\u591a\u8bed\u79cd\u5927\u6a21\u578b\uff0c\u4f46GPT-4\u5bb6\u65cf\u5728CLIP-Score\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8868\u660e\u5176\u56fe\u6587\u5bf9\u9f50\u66f4\u4f18\u3002\u5206\u6790\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6027\u522b\u5224\u65ad\u3001\u7269\u4f53\u8ba1\u6570\u53ca\u7a7a\u95f4\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u672c\u5730\u4eba\u5de5\u6570\u636e\u6bd4\u81ea\u52a8\u7ffb\u8bd1\u5177\u66f4\u9ad8\u4ef7\u503c\uff0c\u4f46\u4e24\u8005\u7ed3\u5408\u53ef\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b\u65b0\u6a21\u578b\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\u56fe\u50cf\u63cf\u8ff0\u4e0a\u5df2\u4f18\u4e8e\u591a\u8bed\u8a00\u5927\u6a21\u578b\u3002\u6ce8\u610f\u529b\u56fe\u5256\u6790\u63ed\u793a\u6a21\u578b\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u672a\u6765\u9700\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4f18\u5316\u6570\u636e\u548c\u6a21\u578b\u3002"}}
{"id": "2602.00986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00986", "abs": "https://arxiv.org/abs/2602.00986", "authors": ["Guowei Xu", "Mert Yuksekgonul", "James Zou"], "title": "Sparse Reward Subsystem in Large Language Models", "comment": null, "summary": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.", "AI": {"tldr": "\u672c\u8bba\u6587\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9690\u85cf\u72b6\u6001\u4e2d\u53d1\u73b0\u4e86\u7c7b\u4f3c\u751f\u7269\u5927\u8111\u5956\u52b1\u7cfb\u7edf\u7684\u7a00\u758f\u5956\u52b1\u5b50\u7cfb\u7edf\uff0c\u5e76\u8bc6\u522b\u51fa\u4e86\u5177\u5907\u4ef7\u503c\u9884\u671f\u529f\u80fd\u7684\u795e\u7ecf\u5143\u53ca\u5176\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u8fd8\u53d1\u73b0\u4e86\u4e0e\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\uff08RPE\uff09\u76f8\u5173\u7684\u795e\u7ecf\u5143\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u548c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u5956\u52b1\u673a\u5236\uff0c\u7279\u522b\u662f\u6a21\u578b\u4e3a\u4f55\u80fd\u4ea7\u751f\u4e0e\u5956\u52b1\u76f8\u5173\u3001\u7c7b\u4eba\u8ba4\u77e5\u7684\u884c\u4e3a\uff0c\u4ee5\u53ca\u627e\u5230\u4e0e\u751f\u7269\u5927\u8111\u5956\u52b1\u7cfb\u7edf\u7684\u5bf9\u7167\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790LLM\u9690\u85cf\u72b6\u6001\uff0c\u6807\u5b9a\u51fa\u4e86\u4ee3\u8868\u671f\u671b\u503c\u7684\u201c\u4ef7\u503c\u795e\u7ecf\u5143\u201d\uff0c\u5e76\u5229\u7528\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u63a8\u7406\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002\u540c\u65f6\u8de8\u6570\u636e\u96c6\u3001\u591a\u79cd\u6a21\u578b\u4e0e\u67b6\u6784\uff0c\u7cfb\u7edf\u6027\u8003\u5bdf\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u7a33\u5065\u6027\u4e0e\u53ef\u8fc1\u79fb\u6027\uff1b\u8fd8\u5bf9\u4ef7\u503c\u9884\u6d4b\u4e0e\u5b9e\u9645\u5956\u52b1\u4e0d\u4e00\u81f4\u7684\u60c5\u5f62\uff0c\u8fdb\u4e00\u6b65\u5b9a\u4f4d\u51fa\u8d1f\u8d23RPE\uff08\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\uff09\u7684\u201c\u591a\u5df4\u80fa\u795e\u7ecf\u5143\u201d\u3002", "result": "\u53d1\u73b0LLM\u5185\u90e8\u786e\u5b9e\u5b58\u5728\u7c7b\u4f3c\u751f\u7269\u5956\u52b1\u7cfb\u7edf\u7684\u4ef7\u503c/\u591a\u5df4\u80fa\u795e\u7ecf\u5143\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u4e0d\u4ec5\u91cd\u8981\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u89c4\u6a21\u4e0b\u8868\u73b0\u51fa\u8f83\u5f3a\u7a33\u5065\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff0c\u4e14\u80fd\u5206\u8fa8\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7c7b\u751f\u7269\u5956\u52b1\u5b50\u7cfb\u7edf\uff0c\u8be5\u5b50\u7cfb\u7edf\u5bf9\u63a8\u7406\u7b49\u9ad8\u7ea7\u8ba4\u77e5\u4efb\u52a1\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u7c7b\u8111\u673a\u5236\u53ca\u5176\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.01880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01880", "abs": "https://arxiv.org/abs/2602.01880", "authors": ["Giulio Antonio Abbo", "Senne Lenaerts", "Tony Belpaeme"], "title": "Multimodal Large Language Models for Real-Time Situated Reasoning", "comment": "Submitted to the interactivity track of the 21st ACM/IEEE International Conference on Human-Robot Interaction on December 2025, accepted January 2026", "summary": "In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bGPT-4o\u4e0eTurtleBot 4\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u5177\u5907\u89c6\u89c9\u8f93\u5165\u5206\u6790\u548c\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u7684\u667a\u80fd\u5bb6\u5c45\u6e05\u626b\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u7406\u89e3\u5bb6\u5ead\u60c5\u5883\u548c\u7528\u6237\u4ef7\u503c\u53d6\u5411\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u5e94\u7528\u4e2d\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u3001\u793e\u4ea4\u89c4\u8303\u548c\u7528\u6237\u504f\u597d\u7684\u6df1\u5165\u7406\u89e3\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u5176\u6839\u636e\u89c6\u89c9\u73af\u5883\u505a\u51fa\u7b26\u5408\u7528\u6237\u4ef7\u503c\u89c2\u4e0e\u5b9e\u9645\u9700\u6c42\u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u5c06GPT-4o\u6a21\u578b\u4e0eTurtleBot 4\u667a\u80fd\u626b\u5730\u673a\u5668\u4eba\u7ed3\u5408\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u8f93\u5165\u8bc4\u4f30\u5bb6\u5ead\u73af\u5883\uff0c\u5e76\u5224\u65ad\u662f\u5426\u9002\u5408\u5f00\u542f\u6e05\u626b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u3001\u5b9e\u65f6\u7684\u4ef7\u503c\u654f\u611f\u578b\u51b3\u7b56\u6d41\u7a0b\u3002", "result": "\u7cfb\u7edf\u80fd\u5728\u771f\u5b9e\u5bb6\u5ead\u73af\u5883\u4e2d\uff0c\u4ec5\u51ed\u6709\u9650\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u63a8\u65ad\u51fa\u60c5\u666f\u548c\u6d89\u53ca\u7684\u4ef7\u503c\u89c2\uff0c\u5305\u62ec\u6e05\u6d01\u5ea6\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u548c\u51b3\u7b56\u7684\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u671b\u63d0\u5347\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u4e0e\u60c5\u5883\u611f\u77e5\u6c34\u5e73\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f9d\u7136\u9762\u4e34\u4e00\u81f4\u6027\u3001\u504f\u89c1\u548c\u5b9e\u65f6\u6027\u7b49\u6311\u6218\u3002"}}
{"id": "2602.00394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00394", "abs": "https://arxiv.org/abs/2602.00394", "authors": ["Manoj Reddy Bethi", "Sai Rupa Jhade", "Pravallika Yaganti", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences", "comment": null, "summary": "Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6210\u5bf9\u504f\u597d\u6bd4\u8f83\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u5bf9\u7f8e\u672f\u4f5c\u54c1\u7684\u7f8e\u5b66\u5224\u65ad\uff0c\u4ece\u800c\u964d\u4f4e\u83b7\u53d6\u4eba\u5de5\u6807\u7b7e\u7684\u6210\u672c\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6bd4\u5bf9\u5b66\u4e60\u5728\u65e0\u8bc4\u5206\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u826f\uff0c\u4e14\u6807\u6ce8\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u7f8e\u5b66\u5224\u65ad\u5b58\u5728\u5f88\u5f3a\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u4e14\u83b7\u53d6\u4eba\u5de5\u6807\u7b7e\uff08\u5982\u6253\u5206\uff09\u6210\u672c\u5f88\u9ad8\u3002\u533a\u5206\u5f0f\u5b66\u4e60\uff08\u5373\u57fa\u4e8e\u6210\u5bf9\u6bd4\u8f83\uff09\u53ef\u80fd\u6bd4\u7edd\u5bf9\u6253\u5206\u66f4\u52a0\u9ad8\u6548\u4e14\u7a33\u5b9a\uff0c\u4f46\u5176\u5e94\u7528\u4e0e\u6548\u679c\u8fd8\u6709\u5f85\u9a8c\u8bc1\u3002", "method": "\u4f5c\u8005\u5229\u7528ResNet-50\u63d0\u53d6\u7ed8\u753b\u56fe\u50cf\u6df1\u5ea6\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u548c\u6210\u5bf9\u6bd4\u8f83\u53cc\u5206\u652f\u6a21\u578b\uff0c\u5e76\u7cfb\u7edf\u5bf9\u6bd4\u4e86\u6df1\u5ea6\u6a21\u578b\u4e0e\u4f20\u7edf\u7ebf\u6027\u57fa\u7ebf\u4e4b\u95f4\u7684\u6548\u679c\uff0c\u56de\u5f52\u4e0e\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\u7684\u8868\u73b0\uff0c\u65e0\u8bc4\u5206\u6761\u4ef6\u4e0b\u6027\u80fd\uff0c\u4ee5\u53ca\u4e0d\u540c\u6ce8\u91ca\u65b9\u5f0f\u7684\u6210\u672c\u4e0e\u6548\u7387\u5dee\u5f02\u3002", "result": "\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u5728$R^2$\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe328%\u3002\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\u867d\u65e0\u6cd5\u8bbf\u95ee\u6253\u5206\uff0c\u4f46\u6027\u80fd\u63a5\u8fd1\u56de\u5f52\u6a21\u578b\uff0c\u4f53\u73b0\u4e86\u5176\u5b9e\u7528\u6027\u3002\u4e2a\u4f53\u504f\u597d\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\uff0c\u4f46\u5e73\u5747\u504f\u597d\u8f83\u5bb9\u6613\u3002\u884c\u4e3a\u5b9e\u9a8c\u663e\u793a\u6210\u5bf9\u5224\u65ad\u6bd4\u76f4\u63a5\u8bc4\u5206\u80fd\u8282\u770160%\u7684\u4eba\u529b\u6807\u6ce8\u65f6\u95f4\u3002", "conclusion": "\u6210\u5bf9\u504f\u597d\u6bd4\u8f83\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u80fd\u6709\u6548\u5efa\u6a21\u89c6\u89c9\u7f8e\u5b66\u5224\u65ad\u4e14\u6781\u5927\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\uff0c\u4f46\u4e2a\u4f53\u5316\u504f\u597d\u9884\u6d4b\u4ecd\u5177\u6311\u6218\u3002\u6b64\u65b9\u6cd5\u9002\u5408\u5927\u89c4\u6a21\u7f8e\u5b66\u5efa\u6a21\u5e94\u7528\u3002"}}
{"id": "2602.00996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00996", "abs": "https://arxiv.org/abs/2602.00996", "authors": ["Abhijit Chakraborty", "Ashish Raj Shekhar", "Shiven Agarwal", "Vivek Gupta"], "title": "DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework", "comment": null, "summary": "Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.", "AI": {"tldr": "DeALOG\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u534f\u4f5c\uff0c\u63d0\u5347\u590d\u6742\u591a\u6a21\u6001\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u9700\u8981\u6574\u5408\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u7b49\u591a\u79cd\u4fe1\u606f\u6e90\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u63d0\u5347\u5408\u4f5c\u4e0e\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "method": "DeALOG\u8bbe\u8ba1\u4e86\u591a\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\uff08\u5904\u7406\u8868\u683c\u3001\u4e0a\u4e0b\u6587\u3001\u89c6\u89c9\u3001\u6458\u8981\u548c\u9a8c\u8bc1\uff09\uff0c\u6240\u6709\u4ee3\u7406\u901a\u8fc7\u4e00\u4e2a\u6301\u4e45\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u8fdb\u884c\u4ea4\u4e92\uff0c\u5171\u4eab\u4fe1\u606f\u548c\u68c0\u6d4b\u9519\u8bef\uff0c\u5b9e\u73b0\u65e0\u4e2d\u5fc3\u5316\u534f\u4f5c\u3002", "result": "\u5728FinQA\u3001TAT-QA\u3001CRT-QA\u3001WikiTableQuestions\u3001FeTaQA\u548cMultiModalQA\u7b49\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002\u5206\u6790\u8868\u660e\u5171\u4eab\u65e5\u5fd7\u3001\u667a\u80fd\u4f53\u4e13\u95e8\u5316\u548c\u6821\u9a8c\u673a\u5236\u663e\u8457\u6709\u52a9\u4e8e\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "DeALOG\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u95ee\u7b54\u5904\u7406\uff0c\u4e3a\u591a\u6e90\u4fe1\u606f\u6574\u5408\u4e0e\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.01892", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01892", "abs": "https://arxiv.org/abs/2602.01892", "authors": ["Alexandre Lombard", "Florent Perronnet", "Nicolas Gaud", "Abdeljalil Abbas-Turki"], "title": "Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study", "comment": null, "summary": "This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8def\u5f84\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8f6e\u57fa\u7ebf\u4e0a\u52a8\u6001\u63d2\u503c\u63a7\u5236\u70b9\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u6a2a\u5411\u63a7\u5236\uff0c\u5e76\u7ed3\u5408\u524d\u8f74\u548c\u540e\u8f74\u63a7\u5236\u5668\u8fde\u7eed\u5207\u6362\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4e8e\u524d\u8f74\u6216\u540e\u8f74\u7684\u8def\u5f84\u8ddf\u8e2a\u65b9\u6cd5\u5728\u4e0d\u540c\u9a7e\u9a76\u573a\u666f\uff08\u5982\u4f4e\u901f\u3001\u5012\u8f66\u7b49\uff09\u4e0b\u9002\u5e94\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u8f68\u8ff9\u504f\u5dee\u6216\u63a7\u5236\u4e0d\u8fde\u7eed\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u66f4\u5e73\u6ed1\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u4e86\u524d\u8f74Stanley\u63a7\u5236\u5668\u4e0e\u57fa\u4e8e\u66f2\u7387\u7684\u540e\u8f74\u63a7\u5236\u5668\u8fdb\u884c\u91cd\u5fc3\u63d2\u503c\uff08barycentric blending\uff09\uff0c\u52a8\u6001\u8ba1\u7b97\u8f6e\u57fa\u7ebf\u4e0a\u4efb\u610f\u63a7\u5236\u70b9\u7684\u6a2a\u5411\u547d\u4ee4\uff0c\u52a9\u4e8e\u5b9e\u73b0\u524d\u540e\u8f74\u63a7\u5236\u7684\u65e0\u7f1d\u5207\u6362\u3002\u7eb5\u5411\u63a7\u5236\u5219\u4f9d\u8d56\u57fa\u4e8e\u865a\u62df\u8f68\u8ff9\u8fb9\u754c\u548c\u5c04\u7ebf\u8ffd\u8e2a\u7684\u7b56\u7565\uff0c\u6839\u636e\u524d\u65b9\u51e0\u4f55\u7ea6\u675f\u8c03\u6574\u901f\u5ea6\u3002\u6574\u4f53\u65b9\u6848\u5728\u4eff\u771f\u548c\u771f\u5b9e\u8f66\u8f86\uff08\u914d\u5907GPS-RTK\u3001\u96f7\u8fbe\u3001\u91cc\u7a0b\u8ba1\u4e0eIMU\uff09\u4e0a\u7edf\u4e00\u5b9e\u73b0\u548c\u9a8c\u8bc1\u3002", "result": "\u5728\u95ed\u73af\u8ddf\u8e2a\u548c\u5012\u8f66\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3001\u66f4\u5e73\u6ed1\u7684\u8f6c\u5411\u4ee5\u53ca\u76f8\u6bd4\u56fa\u5b9a\u63a7\u5236\u70b9\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u52a8\u6001\u63d2\u503c\u63a7\u5236\u70b9\u548c\u521b\u65b0\u7684\u7eb5\u5411\u63a7\u5236\u7b56\u7565\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u51c6\u786e\u548c\u7075\u6d3b\u7684\u8def\u5f84\u8ddf\u8e2a\u80fd\u529b\uff0c\u6709\u671b\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9a7e\u9a76\u8868\u73b0\u3002"}}
{"id": "2602.00395", "categories": ["cs.CV", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00395", "abs": "https://arxiv.org/abs/2602.00395", "authors": ["Roger Hsiao", "Yuchen Fang", "Xiangru Huang", "Ruilong Li", "Hesam Rabeti", "Zan Gojcic", "Javad Lavaei", "James Demmel", "Sophia Shao"], "title": "3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting", "comment": null, "summary": "We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (H\u00f6llein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D Gaussian Splatting\u573a\u666f\u8bad\u7ec3\u4e8c\u9636\u4f18\u5316\u56683DGS$^2$-TR\uff0c\u901a\u8fc7\u5bf9Hessian\u77e9\u9635\u5bf9\u89d2\u7ebf\u7684\u8fd1\u4f3c\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u9ad8\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u5e76\u6781\u5927\u5730\u538b\u7f29\u4e86\u8bad\u7ec3\u4ee3\u4ef7\u3002", "motivation": "3D Gaussian Splatting\u9886\u57df\u4e3b\u6d41\u7684\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\u8981\u4e48\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u8981\u4e48\u8ba1\u7b97\u590d\u6742\u5ea6\u592a\u5927\uff0c\u4e0d\u5229\u4e8e\u5b9e\u9645\u573a\u666f\u8bad\u7ec3\u9700\u6c42\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4e00\u79cd\u9ad8\u6548\u7b80\u6d01\u7684\u65b9\u6cd5\u5e73\u8861\u4f18\u5316\u80fd\u529b\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u63a8\u52a8\u66f4\u5927\u578b\u573a\u666f\u4e0e\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u7528Hutchinson\u6280\u5de7\u8fd1\u4f3cHessian\u5bf9\u89d2\u7ebf\uff0c\u4ece\u800c\u4e0d\u7528\u5927\u89c4\u6a21\u7684\u663e\u5f0f\u66f2\u7387\u8868\u793a\uff0c\u5b9e\u73b0O(n)\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8eHellinger\u8ddd\u79bb\u7684\u9010\u53c2\u6570\u4fe1\u8d56\u57df\u6b63\u5219\uff0c\u4ee5\u89e3\u51b33DGS\u6e32\u67d3\u8fc7\u7a0b\u7684\u5f3a\u975e\u7ebf\u6027\uff0c\u63d0\u5347\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u5b8c\u5168\u76f8\u540c\u7684\u53c2\u6570\u521d\u59cb\u5316\u4e0b\uff0c3DGS$^2$-TR\u65e0\u9700\u70b9\u4e91\u52a0\u5bc6\u4fbf\u80fd\u4ee5ADAM\u4e00\u534a\u7684\u8bad\u7ec3\u8fed\u4ee3\u6570\u83b7\u5f97\u66f4\u4f73\u91cd\u5efa\u7ed3\u679c\uff0c\u5e76\u4e14\u5728GPU\u663e\u5b58\u5f00\u9500\u4e0a\u8fdc\u4f4e\u4e8e\u4f20\u7edf\u4e8c\u9636\u65b9\u6cd5\uff083DGS-LM\uff09\uff0c\u63a5\u8fd1ADAM\u3002", "conclusion": "3DGS$^2$-TR\u5728\u4fdd\u6301\u9ad8\u91cd\u5efa\u54c1\u8d28\u7684\u540c\u65f6\uff0c\u5927\u5e45\u5ea6\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u4e3a\u5904\u7406\u66f4\u5927\u89c4\u6a21\u573a\u666f\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00998", "abs": "https://arxiv.org/abs/2602.00998", "authors": ["Zhikun Xu", "Xiaodong Yu", "Ben Zhou", "Jiang Liu", "Jialian Wu", "Ze Wang", "Ximeng Sun", "Hao Chen", "Zicheng Liu"], "title": "Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning", "comment": null, "summary": "Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7ecf\u5e38\u9519\u8bef\u5e94\u7528\u5f15\u7406\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86RULES\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u90e8\u5206\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5224\u65ad\u5f15\u7406\u9002\u7528\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u51fa\u73b0\u5f15\u7528\u5f15\u7406\u65f6\u4e0d\u68c0\u67e5\u524d\u63d0\u6761\u4ef6\uff0c\u4ece\u800c\u4ea7\u751f\u63a8\u7406\u9519\u8bef\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u786e\u4fdd\u6a21\u578b\u5728\u63a8\u7406\u65f6\u80fd\u6b63\u786e\u5224\u65ad\u5f15\u7406\u7684\u9002\u7528\u6027\u3002", "method": "\u4f5c\u8005\u5c06\u5f15\u7406\u5224\u65ad\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5bf9\u5019\u9009\u5f15\u7406\u5206\u522b\u8f93\u51fa\u524d\u63d0\u68c0\u67e5\u548c\u7ed3\u8bba\u5b9e\u7528\u6027\u68c0\u67e5\uff0c\u518d\u7ed3\u5408\u4e24\u8005\u51b3\u5b9a\u5f15\u7406\u662f\u5426\u53ef\u7528\u3002\u63d0\u51fa\u7684RULES\u65b9\u6cd5\u91c7\u7528\u4e24\u6bb5\u5f0f\u8f93\u51fa\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u90e8\u5206\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u635f\u5931\u6765\u5bf9\u9519\u8bef\u90e8\u5206\u8fdb\u884c\u60e9\u7f5a\u3002\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u522b\u6db5\u76d6\u81ea\u7136\u8bed\u8a00\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u8bed\u6599\uff0c\u5e76\u5728\u591a\u4e2aLLM\u4e0a\u8bc4\u4f30\u9c81\u68d2\u6027\u53ca\u7aef\u5230\u7aef\u6548\u679c\u3002", "result": "\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u5355\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u6bd4\u8f83\uff0cRULES\u65b9\u6848\u5728\u539f\u57df\u3001\u7834\u574f\u9002\u7528\u6027\u7684\u6270\u52a8\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u63d0\u5347\uff0c\u5728\u7aef\u5230\u7aef\u4efb\u52a1\u4e0a\u4ea6\u8fbe\u5230\u540c\u7b49\u6216\u7565\u6709\u63d0\u5347\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u6bb5\u5f0f\u7ed3\u6784\u548c\u90e8\u5206\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u9c81\u68d2\u6027\u5747\u4e0d\u53ef\u6216\u7f3a\u3002", "conclusion": "RULES\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6b63\u786e\u5224\u65ad\u5f15\u7406\u9002\u7528\u6027\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u63a8\u7406\u8d28\u91cf\uff0c\u5176\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u6a21\u5757\u5316\u76d1\u7763\u65b9\u5f0f\u5bf9\u4e8e\u6a21\u578b\u9c81\u68d2\u6027\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2602.01899", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01899", "abs": "https://arxiv.org/abs/2602.01899", "authors": ["Ozgur Erkent"], "title": "Multi-Task Learning for Robot Perception with Imbalanced Data", "comment": "16 pages", "summary": "Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5728\u90e8\u5206\u4efb\u52a1\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\u65f6\u4ecd\u80fd\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4efb\u52a1\u95f4\u76f8\u4e92\u4f5c\u7528\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5f71\u54cd\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5b58\u5728\u8d44\u6e90\u6709\u9650\u3001\u90e8\u5206\u4efb\u52a1\u6807\u7b7e\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u6807\u7b7e\u6570\u91cf\u4e0d\u5e73\u8861\u65f6\u8868\u73b0\u53d7\u9650\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u5e94\u5bf9\u7f3a\u6807\u7b7e\u4e0e\u6570\u636e\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u53ef\u5728\u90e8\u5206\u4efb\u52a1\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e\u60c5\u5883\u4e0b\u5b9e\u73b0\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u901a\u8fc7\u5c06\u90e8\u5206\u4efb\u52a1\u7684\u8f93\u51fa\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u8bad\u7ec3\u6559\u5e08\u7f51\u7edc\uff0c\u63d0\u5347\u5176\u4ed6\u4efb\u52a1\u8868\u73b0\u3002\u5e76\u5bf9\u4efb\u52a1\u95f4\u6027\u80fd\u63d0\u5347\u7684\u673a\u5236\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8eNYUDv2\u548cCityscapes\u6570\u636e\u96c6\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4e0e\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e0a\u68c0\u9a8c\u4e86\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u91cf\u8f83\u5c0f\u65f6\u4f9d\u7136\u83b7\u5f97\u4f18\u5316\u6548\u679c\u3002\u9a8c\u8bc1\u4e86\u901a\u8fc7\u4f7f\u7528\u67d0\u4e9b\u4efb\u52a1\u8f93\u51fa\u53ef\u4ee5\u63d0\u5347\u5176\u4ed6\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u90e8\u5206\u4efb\u52a1\u65e0\u6807\u7b7e\u65f6\uff0c\u4ecd\u53ef\u4ee5\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u5b66\u4e60\u673a\u5236\u63d0\u5347\u591a\u4efb\u52a1\u7cfb\u7edf\u8868\u73b0\uff0c\u4e14\u4efb\u52a1\u95f4\u5177\u6709\u534f\u540c\u589e\u76ca\u3002\u56e0\u6b64\u63d0\u51fa\u7684\u65b9\u6cd5\u5bf9\u8d44\u6e90\u6709\u9650\u6216\u96be\u4ee5\u6807\u6ce8\u6570\u636e\u7684\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2602.00414", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00414", "abs": "https://arxiv.org/abs/2602.00414", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Aldair Ernesto Gongora", "Ruben Glatt", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "title": "Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure", "comment": null, "summary": "Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ba1\u9053\uff0c\u5c06\u5b9e\u9a8c\u5ba4\u5b89\u5168\u573a\u666f\u7684\u6587\u672c\u63cf\u8ff0\u8f6c\u5316\u4e3a\u53ef\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8bc4\u4ef7\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u63a2\u7d22\u4e86VLMs\u5728\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u4e2d\u7684\u8868\u73b0\uff1b\u521b\u65b0\u5730\u5229\u7528\u573a\u666f\u56fe\u4f18\u5316VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u5e38\u56e0\u5c0f\u5c0f\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u5f15\u53d1\u4e25\u91cd\u4f24\u5bb3\uff0c\u73b0\u6709\u6301\u7eed\u6027\u5b89\u5168\u76d1\u63a7\u53d7\u9650\u4e8e\u4eba\u529b\uff0c\u800cVLMs\u5177\u5907\u81ea\u52a8\u76d1\u63a7\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u89c6\u89c9\u6570\u636e\u8bc4\u6d4b\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u6761\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u6587\u672c\u573a\u666f\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u573a\u666f\u56fe\u5e76\u7528\u56fe\u50cf\u751f\u6210\u6a21\u578b\u6e32\u67d3\u56fe\u7247\uff0c\u4ece\u800c\u5f97\u5230(\u56fe\u50cf-\u573a\u666f\u56fe-\u771f\u503c\uff09\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u79cdVLMs\u57fa\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5b89\u5168\u98ce\u9669\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u4e0a\u4e0b\u6587\u5bf9\u9f50\u540e\u5904\u7406\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u7eaf\u89c6\u89c9\u573a\u666f\u4e0b\u8868\u73b0\u3002", "result": "\u5728\u5305\u542b1207\u4e2a\u6837\u672c\u3001362\u79cd\u573a\u666f\u7684\u6570\u636e\u96c6\u4e0a\uff0cVLMs\u82e5\u7528\u573a\u666f\u56fe\uff08\u6587\u672c\u7ed3\u6784\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ec5\u7528\u89c6\u89c9\u4fe1\u606f\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u6240\u63d0\u7684\u57fa\u4e8e\u573a\u666f\u56fe\u5bf9\u9f50\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u5728\u7eaf\u89c6\u89c9\u4efb\u52a1\u7684\u98ce\u9669\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "VLMs\u5f53\u524d\u96be\u4ee5\u76f4\u63a5\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5173\u7cfb\u7528\u4e8e\u590d\u6742\u573a\u666f\u63a8\u7406\uff0c\u9700\u901a\u8fc7\u573a\u666f\u56fe\u5f15\u5bfc\u5b9e\u73b0\u80fd\u529b\u5ef6\u4f38\uff0c\u8be5\u65b9\u6cd5\u80fd\u52a9\u529b\u81ea\u52a8\u5316\u3001\u7cbe\u51c6\u7684\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u573a\u666f\u5e94\u7528\u3002"}}
{"id": "2602.01007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01007", "abs": "https://arxiv.org/abs/2602.01007", "authors": ["Zishuo Bao", "Jiaqi Leng", "Junxiong Wang", "Bowen Peng", "Yucheng Lu"], "title": "Distilling Token-Trained Models into Byte-Level Models", "comment": "17 pages, 3 figures, 13 tables", "summary": "Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5df2\u6709\u7684\u57fa\u4e8etoken\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\uff08BLM\uff09\uff0c\u907f\u514d\u4e86\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u800c\u9700\u8017\u8d39\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ea\u9700\u7ea61250\u4ebf\u5b57\u8282\u7684\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684BLM\u867d\u7136\u5177\u6709\u4f18\u79c0\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u9700\u8981\u4ece\u5934\u8bad\u7ec3\uff0c\u5f00\u9500\u6781\u5927\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u66f4\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u5c06\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684token\u7ea7\u6a21\u578b\u8f6c\u4e3aBLM\uff0c\u5145\u5206\u5229\u7528\u73b0\u6709\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5f0f\u84b8\u998f\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u9010\u6b65\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u901a\u8fc7\u5bf9\u9f50token\u6a21\u578b\u4e0e\u5b57\u8282\u8868\u793a\u7684\u5d4c\u5165\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u5728\u5b57\u8282\u7a7a\u95f4\u7684\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u5b57\u8282\u7ea7\u751f\u6210\u3002", "result": "\u5728\u5305\u62ecLlama\u3001Qwen\u548cOLMo\u7b49\u591a\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u7528\u7ea61250\u4ebf\u5b57\u8282\u6570\u636e\u8bad\u7ec3\u51fa\u7684BLM\u57fa\u672c\u4fdd\u7559\u4e86\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8be5\u65b9\u6cd5\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u5c06token\u7ea7\u5927\u6a21\u578b\u8f6c\u6362\u6210\u529f\u80fd\u5b8c\u5907\u7684BLM\uff0c\u4e3a\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u7528\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01916", "abs": "https://arxiv.org/abs/2602.01916", "authors": ["Keyu Chen", "Wenchao Sun", "Hao Cheng", "Zheng Fu", "Sifa Zheng"], "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning", "comment": "Accepted by ICRA 2026", "summary": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u901a\u4eff\u771f\u65b9\u6cd5ForSim\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u7684\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7684\u95ed\u73af\u4eff\u771f\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4e2d\u3002", "motivation": "\u5f53\u524d\u4ea4\u901a\u4eff\u771f\u5728\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\u4e2d\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u6709\uff1a1\uff09\u4eff\u771f\u4e2d\u7531\u4e8e\u91c7\u7528\u5f00\u653e\u5f0f\u6a21\u4eff\u5b66\u4e60\u5bfc\u81f4\u534f\u53d8\u91cf\u79fb\u4f4d\u95ee\u9898\uff1b2\uff09\u4eff\u771f\u573a\u666f\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u4ea4\u901a\u4e2d\u7684\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u4eff\u771f\u4e92\u52a8\u6027\u5dee\uff0c\u964d\u4f4e\u4e86\u4eff\u771f\u7684\u771f\u5b9e\u6027\u548c\u7528\u5904\u3002", "method": "\u63d0\u51faForSim\uff0c\u4e00\u79cd\u9010\u6b65\u95ed\u73af\u524d\u5411\u4eff\u771f\u8303\u5f0f\u3002\u6bcf\u4e2a\u4eff\u771f\u6b65\u957f\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u4f20\u64ad\u6700\u63a5\u8fd1\u53c2\u8003\u8f68\u8ff9\u7684\u5019\u9009\u8f68\u8ff9\uff0c\u4fdd\u8bc1\u591a\u6a21\u6001\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u3002\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u5219\u4f9d\u8d56\u4e00\u6b65\u9884\u6d4b\u65b9\u6cd5\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u4e4b\u95f4\u4ea4\u4e92\u6027\u66f4\u5f3a\u7684\u52a8\u6001\u6f14\u5316\u3002ForSim\u4e0e\u73b0\u6709RIFT\u4eff\u771f\u6846\u67b6\u7ed3\u5408\uff0c\u5bf9\u4ea4\u901a\u7b56\u7565\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cForSim\u96c6\u6210\u5230RIFT\u6846\u67b6\u540e\uff0c\u5728\u5b89\u5168\u6027\u63d0\u5347\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6548\u7387\u3001\u771f\u5b9e\u5ea6\u548c\u8212\u9002\u5ea6\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u975e\u95ed\u73af\u3001\u591a\u6a21\u6001\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "conclusion": "\u95ed\u73af\u591a\u6a21\u6001\u4ea4\u4e92\u5efa\u6a21\u5bf9\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u7684\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002ForSim\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u9ad8\u4fdd\u771f\u5ea6\u3001\u66f4\u53ef\u9760\u7684\u4ea4\u901a\u4eff\u771f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u66f4\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.00420", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00420", "abs": "https://arxiv.org/abs/2602.00420", "authors": ["Yihang Chen", "Zhao Xu", "Youyuan Jiang", "Tianle Zheng", "Cho-Jui Hsieh"], "title": "Text is All You Need for Vision-Language Model Jailbreaking", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Text-DJ\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u6587\u672c\u62c6\u5206\u4e3a\u5b50\u67e5\u8be2\u3001\u6df7\u6742\u5e72\u6270\u67e5\u8be2\u5e76\u8f6c\u5316\u4e3a\u56fe\u7247\uff0c\u7ed5\u8fc7\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5b89\u5168\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8d8a\u72f1\u3002", "motivation": "\u867d\u7136LVLMs\u914d\u5907\u4e86\u5b89\u5168\u63aa\u65bd\uff0c\u4f46\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5bf9\u663e\u5f0f\u6587\u5b57\u6216\u5173\u8054\u573a\u666f\u7684\u68c0\u6d4b\u3002\u8bba\u6587\u52a8\u673a\u662f\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u7684OCR\u80fd\u529b\u7ed5\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u5c42\u9762\u7684\u5b89\u5168\u9632\u62a4\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u6b65\u7684Text-DJ\u653b\u51fb\u6d41\u7a0b\uff1a1\uff09\u5c06\u6709\u5bb3\u95ee\u9898\u62c6\u6210\u591a\u4e2a\u66f4\u6e29\u548c\u7684\u5b50\u95ee\u9898\uff1b2\uff09\u52a0\u5165\u4e00\u6279\u5185\u5bb9\u65e0\u5173\u7684\u5e72\u6270\u95ee\u9898\uff1b3\uff09\u5c06\u6240\u6709\u5b50\u95ee\u9898\u548c\u5e72\u6270\u95ee\u9898\u4ee5\u56fe\u50cf\u7f51\u683c\u5f62\u5f0f\u8f93\u5165\u6a21\u578b\uff0c\u5176\u4e2d\u5b50\u95ee\u9898\u4f4d\u4e8e\u4e2d\u95f4\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ed5\u8fc7\u5f53\u524d\u4e3b\u6d41LVLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u4f7f\u5176\u8f93\u51fa\u539f\u672c\u88ab\u7981\u6b62\u7684\u5185\u5bb9\u3002", "conclusion": "\u66b4\u9732\u51faLVLMs\u7684OCR\u901a\u9053\u5728\u5904\u7406\u5206\u6563\u548c\u591a\u56fe\u8f93\u5165\u65f6\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u9488\u5bf9\u5206\u5272\u7684\u591a\u6a21\u6001\u8f93\u5165\u8bbe\u8ba1\u65b0\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2602.01015", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01015", "abs": "https://arxiv.org/abs/2602.01015", "authors": ["Conrad Borchers", "Jill-J\u00eann Vie", "Roger Azevedo"], "title": "Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident", "comment": "Manuscript under review", "summary": "Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.", "AI": {"tldr": "LLMs\u5728AI\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u4e0d\u80fd\u5b8c\u5168\u51c6\u786e\u6a21\u62df\u65b0\u624b\u7684\u63a8\u7406\u4e0e\u5143\u8ba4\u77e5\uff0c\u5176\u63a8\u7406\u6bd4\u771f\u5b9e\u5b66\u751f\u66f4\u8fde\u8d2f\u3001\u5197\u957f\u548c\u4e00\u81f4\uff0c\u4f4e\u4f30\u4e86\u4eba\u7c7b\u5b66\u4e60\u7684\u788e\u7247\u5316\u7279\u70b9\u3002", "motivation": "\u968f\u7740LLM\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6559\u80b2\u8f85\u5bfc\uff0c\u7406\u89e3\u5176\u662f\u5426\u80fd\u771f\u5b9e\u53cd\u6620\u65b0\u624b\u5b66\u4e60\u8005\u7684\u601d\u7ef4\u8fc7\u7a0b\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u73b0\u6709\u8bc4\u4f30\u8fc7\u4e8e\u4fa7\u91cd\u89e3\u9898\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u4eba\u7684\u4e0d\u5b8c\u5168\u63a8\u7406\u7279\u6027\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u4e86630\u6761\u5316\u5b66\u591a\u6b65\u95ee\u9898\u7684\u201c\u60f3\u58f0\u8bf4\u51fa\u201d\u8bed\u6599\uff0c\u7ed3\u5408\u5b66\u751f\u4f7f\u7528\u63d0\u793a\u548c\u5c1d\u8bd5\u7b49\u89e3\u9898\u65e5\u5fd7\uff0c\u5c06LLM\uff08\u5982GPT-4.1\uff09\u5728\u4e0d\u540c\u63d0\u793a\u573a\u666f\u4e0b\u751f\u6210\u7684\u63a8\u7406\u4e0e\u771f\u5b9e\u5b66\u751f\u7684\u8868\u8fbe\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u5b66\u751f\u9010\u6b65\u89e3\u9898\u8868\u73b0\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "GPT-4.1\u5728\u751f\u6210\u63a8\u7406\u65f6\uff0c\u6bd4\u771f\u5b9e\u5b66\u751f\u66f4\u8fde\u8d2f\u3001\u5570\u55e6\u4e14\u8868\u8fbe\u591a\u6837\u6027\u8f83\u4f4e\uff0c\u5c24\u4ee5\u63d0\u793a\u4fe1\u606f\u66f4\u4e30\u5bcc\u65f6\u4e3a\u751a\u3002\u540c\u65f6\uff0c\u6a21\u578b\u666e\u904d\u9ad8\u4f30\u4e86\u5b66\u751f\u8868\u73b0\uff0c\u6ca1\u6709\u53cd\u6620\u51fa\u65b0\u624b\u5e38\u89c1\u7684\u788e\u7247\u5316\u548c\u6613\u53d8\u6027\u3002", "conclusion": "LLMs\u96be\u4ee5\u5fe0\u5b9e\u6a21\u62df\u5b66\u4e60\u8005\u7684\u63a8\u7406\u548c\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8fd9\u79cd\u5dee\u5f02\u6e90\u81ea\u5176\u8bad\u7ec3\u6570\u636e\u66f4\u504f\u5411\u4e13\u5bb6\u7b54\u6848\u4e14\u7f3a\u4e4f\u60c5\u611f\u548c\u8bb0\u5fc6\u9650\u5236\u7684\u8868\u8fbe\u3002\u8bc4\u4f30\u6846\u67b6\u6709\u52a9\u4e8e\u672a\u6765\u66f4\u597d\u5730\u8bbe\u8ba1\u652f\u6301\u65b0\u624b\u5b66\u4e60\u4e0e\u81ea\u6211\u8c03\u8282\u7684\u81ea\u9002\u5e94AI\u7cfb\u7edf\u3002"}}
{"id": "2602.01930", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01930", "abs": "https://arxiv.org/abs/2602.01930", "authors": ["Felix Igelbrink", "Lennart Niecksch", "Marian Renz", "Martin G\u00fcnther", "Martin Atzmueller"], "title": "LIEREx: Language-Image Embeddings for Robotic Exploration", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in KI - K\u00fcnstliche Intelligenz, and is available online at https://doi.org/10.1007/s13218-026-00902-6", "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u7ed3\u54083D\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u90e8\u5206\u672a\u77e5\u73af\u5883\u4e0b\u76ee\u6807\u5bfc\u5411\u63a2\u7d22\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u4e49\u5730\u56fe\u4f9d\u8d56\u56fa\u5b9a\u7684\u7b26\u53f7\u8bcd\u6c47\uff0c\u96be\u4ee5\u5904\u7406\u9884\u5148\u672a\u5b9a\u4e49\u7684\u65b0\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e0b\u5e94\u7528\u53d7\u9650\u3002", "method": "\u91c7\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\uff0c\u5c06\u7269\u4f53\u8868\u793a\u4e3a\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\uff08open-set mapping\uff09\uff0c\u5e76\u4e0e\u73b0\u6709\u76843D\u8bed\u4e49\u573a\u666f\u56fe\u7ed3\u5408\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u76ee\u6807\u7269\u4f53\u7684\u81ea\u4e3b\u63a2\u7d22\u3002", "result": "\u96c6\u6210\u65b9\u6848\uff08LIEREx\uff09\u80fd\u8ba9\u81ea\u4e3b\u4f53\u5728\u90e8\u5206\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u57fa\u4e8e\u76ee\u6807\u7684\u63a2\u7d22\u4efb\u52a1\uff0c\u65e0\u9700\u4f9d\u8d56\u56fa\u5b9a\u6807\u7b7e\u3002", "conclusion": "\u7ed3\u5408VLFMs\u4e0e3D\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u80fd\u591f\u63d0\u5347\u673a\u5668\u4eba\u8bed\u4e49\u7406\u89e3\u4e0e\u5f00\u653e\u96c6\u63a2\u7d22\u80fd\u529b\uff0c\u62d3\u5c55\u4e86\u673a\u5668\u4eba\u5728\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e2d\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.00440", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00440", "abs": "https://arxiv.org/abs/2602.00440", "authors": ["Anugunj Naman", "Gaibo Zhang", "Ayushman Singh", "Yaguang Zhang"], "title": "DISK: Dynamic Inference SKipping for World Models", "comment": null, "summary": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.", "AI": {"tldr": "\u63d0\u51faDISK\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8df3\u8dc3\u51b3\u7b56\u52a0\u901f\u89c6\u9891\u548c\u8f68\u8ff9\u6269\u6563\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u80fd\u6709\u6548\u63d0\u5347\u957f\u65f6\u5e8f\u63a8\u7406\u6548\u7387\u4e14\u4fdd\u6301\u9884\u6d4b\u8d28\u91cf\u3002", "motivation": "\u957f\u65f6\u5e8f\u89c6\u9891\u4e0e\u81ea\u8f66\u8f68\u8ff9\u751f\u6210\uff08\u4e16\u754c\u5efa\u6a21\uff09\u8fc7\u7a0b\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u63a8\u7406\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u901f\u5ea6\u6162\uff0c\u9650\u5236\u5176\u5b9e\u7528\u6027\u3002\u4f5c\u8005\u8bd5\u56fe\u63d0\u51fa\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94\u6027\u8df3\u8dc3\u7b56\u7565\u663e\u8457\u52a0\u901f\u63a8\u7406\u6d41\u7a0b\uff0c\u964d\u4f4e\u90e8\u7f72\u548c\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u51faDISK\u65b9\u6cd5\uff0c\u6838\u5fc3\u4e3a\u4e24\u4e2a\u8026\u5408\u7684\u6269\u6563\u53d8\u6362\u5668\uff08\u5206\u522b\u7528\u4e8e\u89c6\u9891\u548c\u8f68\u8ff9\uff09\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u63a7\u5236\u5668\u5b9e\u73b0\u591a\u6a21\u6001\u8de8\u6a21\u8df3\u8dc3\u51b3\u7b56\uff0c\u5e76\u7ed3\u5408\u9ad8\u9636\u6f5c\u5728\u5dee\u5206\u8df3\u8dc3\u6d4b\u8bd5\u3001\u63a7\u5236\u5668\u7edf\u8ba1\u8de8\u65f6\u5e8f\u4f20\u64ad\uff0c\u63d0\u5347\u957f\u65f6\u5e8f\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u65e0\u9700\u518d\u8bad\u7ec3\u539f\u6a21\u578b\u3002", "result": "\u57281500\u7ec4NuPlan\u548cNuScenes\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u57fa\u4e8eNVIDIA L40S GPU\uff0cDISK\u5728\u4fdd\u8bc1\u9884\u6d4b\u7cbe\u5ea6\uff08L2\u8bef\u5dee\uff09\u3001\u89c6\u89c9\u8d28\u91cf\uff08FID/FVD\uff09\u548c\u4e0b\u6e38\u6027\u80fd\uff08PDMS\u6307\u6807\uff09\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8f68\u8ff9\u6269\u65632\u500d\u63d0\u901f\u3001\u89c6\u9891\u6269\u65631.6\u500d\u63d0\u901f\u3002", "conclusion": "DISK\u80fd\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u52a0\u901f\u591a\u6a21\u6001\u6269\u6563\u4e16\u754c\u6a21\u578b\u7684\u957f\u65f6\u5e8f\u63a8\u7406\uff0c\u517c\u987e\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01030", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.01030", "abs": "https://arxiv.org/abs/2602.01030", "authors": ["Sheng-Lun Wei", "Yu-Ling Liao", "Yen-Hua Chang", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations", "comment": "Accepted as a long findings paper at EACL 2026", "summary": "This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $\u03ba$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u591a\u8bed\u79cd\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u8bed\u97f3\u504f\u89c1\uff0c\u5e76\u6784\u5efa\u4e86BiasInEar\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u97f3\u6761\u4ef6\u4e0b\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u878d\u5408\u8fdb\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u3001\u53e3\u97f3\u3001\u6027\u522b\u7b49\u7ef4\u5ea6\u4e0a\u7684\u516c\u5e73\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u591a\u8bed\u79cd\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u8bed\u97f3\u504f\u89c1\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86BiasInEar\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u8986\u76d6\u82f1\u8bed\u3001\u4e2d\u6587\u548c\u97e9\u8bed\uff0c\u4e14\u6309\u6027\u522b\u548c\u53e3\u97f3\u5747\u8861\uff0c\u603b\u8ba170.8\u5c0f\u65f6\u300111200\u9053\u9898\uff1b\u91c7\u7528\u51c6\u786e\u7387\u3001\u71b5\u3001APES\u548cFleiss' \u03ba\u7b49\u56db\u79cd\u6307\u6807\uff0c\u8003\u5bdf\u4e5d\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u5728\u8bed\u8a00\u3001\u6027\u522b\u3001\u53e3\u97f3\u3001\u9009\u9879\u987a\u5e8f\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff0cMLLMs\u5bf9\u6027\u522b\u7b49\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u8f83\u4e3a\u9c81\u68d2\uff0c\u4f46\u5bf9\u8bed\u8a00\u548c\u9009\u9879\u987a\u5e8f\u654f\u611f\uff0c\u8868\u660e\u8bed\u97f3\u4f1a\u653e\u5927\u7ed3\u6784\u6027\u504f\u89c1\u3002\u6b64\u5916\uff0c\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u4e0e\u63a8\u7406\u7b56\u7565\u4f1a\u663e\u8457\u5f71\u54cd\u5176\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u97f3\u878d\u5408\u5927\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u6587\u672c\u4e0e\u8bed\u97f3\u8f93\u5165\u95f4\u6a21\u578b\u8868\u73b0\uff0c\u4fc3\u8fdb\u540e\u7eedMLLMs\u516c\u5e73\u6027\u7814\u7a76\u3002\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01939", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01939", "abs": "https://arxiv.org/abs/2602.01939", "authors": ["Yuxin He", "Ruihao Zhang", "Tianao Shen", "Cheng Liu", "Qiang Nie"], "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy", "comment": "ICRA 2026", "summary": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEFM-10\u7684\u65b0\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u63a2\u7d22\u6027\u4e0e\u4e13\u6ce8\u6027\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u81c2\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\uff08BAP\uff09\uff0c\u7ed3\u5408\u4e3b\u52a8\u89c6\u89c9\u4e0e\u529b\u611f\u77e5\uff0c\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u53d7\u906e\u6321\u73af\u5883\u4e0b\u7684\u64cd\u4f5c\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u4e3b\u6444\u50cf\u5934\u901a\u5e38\u5b89\u88c5\u5728\u201c\u5934\u90e8\u201d\uff0c\u5728\u6267\u884c\u64cd\u4f5c\u4efb\u52a1\u65f6\u66f4\u9891\u7e41\u5730\u9762\u4e34\u89c6\u89c9\u906e\u6321\uff0c\u5f53\u524d\u65b9\u6cd5\u5e94\u5bf9\u906e\u6321\u4fe1\u606f\u4e0d\u8db3\u6709\u5c40\u9650\uff0c\u6fc0\u53d1\u4e86\u4f5c\u8005\u63a2\u7d22\u66f4\u4e3b\u52a8\u7684\u4fe1\u606f\u6536\u96c6\u65b9\u5f0f\u4ee5\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u201c\u63a2\u7d22\u6027\u4e0e\u4e13\u6ce8\u6027\u64cd\u4f5c\u201d\u95ee\u9898\uff08EFM\uff09\uff0c\u5236\u5b9a\u4e86\u5305\u542b10\u4e2a\u4efb\u52a1\u30014\u5927\u7c7b\u522b\u7684EFM-10\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u201c\u53cc\u81c2\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\u201d\uff08BAP\uff09\uff1bBAP\u5229\u7528\u4e00\u53ea\u673a\u68b0\u81c2\u4e3b\u52a8\u8c03\u6574\u89c6\u89c9\u89c6\u89d2\uff0c\u53e6\u4e00\u53ea\u673a\u68b0\u81c2\u8fdb\u884c\u529b\u611f\u77e5\u64cd\u4f5c\u3002\u57fa\u4e8e\u6b64\u7b56\u7565\uff0c\u6536\u96c6\u4e86BAPData\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u9a8c\u8bc1BAP\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBAP\u7b56\u7565\u5728EFM-10\u57fa\u51c6\u4efb\u52a1\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u89c6\u89c9\u906e\u6321\u7b49\u590d\u6742\u73af\u5883\u4e0b\u7684\u4fe1\u606f\u6536\u96c6\u548c\u64cd\u4f5c\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "EFM-10\u57fa\u51c6\u4e0eBAP\u7b56\u7565\u4e3a\u673a\u5668\u4eba\u5728\u4e3b\u52a8\u89c6\u89d2\u4e0e\u529b\u611f\u77e5\u4e0b\uff0c\u63d0\u9ad8\u5bf9\u906e\u6321\u7b49\u6311\u6218\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u548c\u57fa\u7840\uff0c\u671f\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.00450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00450", "abs": "https://arxiv.org/abs/2602.00450", "authors": ["Ethan Anderson", "Justin Silva", "Kyle Zheng", "Sameer Pusegaonkar", "Yizhou Wang", "Zheng Tang", "Sujit Biswas"], "title": "Model Optimization for Multi-Camera 3D Detection and Tracking", "comment": null, "summary": "Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86Sparse4D\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6444\u50cf\u5934\u4e0b\u7684\u591a\u76ee\u68073D\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u5b9e\u9a8c\u5206\u6790\u5176\u5728\u5e27\u7387\u964d\u4f4e\u3001\u91cf\u5316\u3001\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u53ca\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4e0b\u7684\u8868\u73b0\u4e0e\u6743\u8861\u3002", "motivation": "\u5ba4\u5185\u73af\u5883\u4e0b\uff0c\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u9700\u5728\u89c6\u89d2\u5f02\u6784\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5e27\u7387\u964d\u4f4e\u3001\u53c2\u6570\u91cf\u5316\u4e0e\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u517c\u987e\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "Sparse4D\u4e3a\u57fa\u4e8equery\u7684\u65f6\u7a7a3D\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6846\u67b6\uff0c\u91c7\u7528\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u5230\u7edf\u4e00\u4e16\u754c\u5750\u6807\uff0c\u901a\u8fc7\u5b9e\u4f8b\u8bb0\u5fc6\u5b9e\u73b0\u7a00\u758f\u5bf9\u8c61\u67e5\u8be2\u4f20\u64ad\u3002\u8bba\u6587\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5e27\u7387\u4e0b\u8c03\u3001\u540e\u8bad\u7ec3\u91cf\u5316\uff08INT8/FP8\uff09\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u3001\u4ee5\u53caTransformer Engine\u6df7\u5408\u7cbe\u5ea6\u5fae\u8c03\u7b49\u591a\u79cd\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8861\u91cf\u6307\u6807AvgTrackDur\u7528\u4e8e\u5ea6\u91cf\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002", "result": "Sparse4D\u5728\u4e2d\u7b49\u5e27\u7387\u964d\u4f4e\u65f6\u8868\u73b0\u7a33\u5b9a\uff0c2 FPS\u4ee5\u4e0b\u65f6\u8eab\u4efd\u5173\u8054\u5927\u5e45\u4e0b\u964d\uff1b\u9aa8\u5e72\u4e0e\u9888\u90e8\u7684\u9009\u62e9\u6027\u91cf\u5316\u5728\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u95f4\u8868\u73b0\u6700\u4f73\uff0c\u6ce8\u610f\u529b\u76f8\u5173\u6a21\u5757\u5bf9\u4f4e\u7cbe\u5ea6\u654f\u611f\uff1bWILDTRACK\u6570\u636e\u96c6\u4e0a\u4f4e\u5e27\u7387\u9884\u8bad\u7ec3\u5e26\u6765\u663e\u8457\u96f6\u6837\u672c\u63d0\u5347\uff0c\u5c11\u91cf\u5fae\u8c03\u589e\u76ca\u6709\u9650\uff1bTransformer Engine\u6df7\u5408\u7cbe\u5ea6\u80fd\u63d0\u5347\u5ef6\u8fdf\u4e0e\u6269\u5c55\u6027\uff0c\u4f46\u4f1a\u5f71\u54cd\u8eab\u4efd\u4f20\u64ad\u7a33\u5b9a\u6027\u3002", "conclusion": "Sparse4D\u53ef\u5728\u591a\u6444\u50cf\u5934\u5ba4\u5185\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u517c\u987e\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u5bf9\u91cf\u5316\u3001\u7cbe\u5ea6\u548c\u6570\u636e\u6cdb\u5316\u6709\u8f83\u597d\u9002\u5e94\u6027\uff0c\u4f46\u9700\u5173\u6ce8\u6781\u4f4e\u5e27\u7387\u548c\u6df7\u5408\u7cbe\u5ea6\u5bf9\u8eab\u4efd\u5173\u8054\u7a33\u5b9a\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u672a\u6765\u9700\u7ed3\u5408\u7a33\u5b9a\u6027\u9a8c\u8bc1\u4ee5\u63d0\u5347\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01063", "abs": "https://arxiv.org/abs/2602.01063", "authors": ["Bin Han", "Deuksin Kwon", "Jonathan Gratch"], "title": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u76f8\u540c\u7684\u4eba\u683c\u8bbe\u5b9a\u5728\u4e0d\u540c\u5bf9\u8bdd\u573a\u666f\u4e0b\uff0cLLM\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u8bed\u8a00\u3001\u884c\u4e3a\u548c\u60c5\u7eea\u7279\u5f81\uff0c\u63ed\u793a\u4e86\u4eba\u683c\u8868\u8fbe\u7684\u60c5\u5883\u654f\u611f\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u660e\u786e\u7684\u4eba\u683c\u63d0\u793a\u8bcd\u8fdb\u884c\u4e2a\u6027\u5316\u8bbe\u5b9a\uff0c\u4f46\u5b9e\u9645\u7684\u4eba\u683c\u884c\u4e3a\u8868\u73b0\u4f1a\u56e0\u5177\u4f53\u573a\u666f\u800c\u6709\u6240\u4e0d\u540c\u3002\u7406\u89e3\u8fd9\u79cd\u53d8\u5f02\u7684\u539f\u56e0\u548c\u8868\u73b0\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u5bf9\u8bdd\u578b\u4eba\u5de5\u667a\u80fd\u7684\u8bbe\u8ba1\u3002", "method": "\u4f5c\u8005\u8bbe\u5b9a\u7edf\u4e00\u7684\u4eba\u683c\u63d0\u793a\u8bcd\uff0c\u5e76\u5728\u5bd2\u6684\u3001\u8c08\u5224\u3001\u5c0f\u7ec4\u51b3\u7b56\u4e0e\u5171\u60c5\u56db\u79cd\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0c\u7cfb\u7edf\u5206\u6790LLM\u7684\u8bed\u8a00\u3001\u884c\u4e3a\u548c\u60c5\u7eea\u8f93\u51fa\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u60c5\u5883\u7ebf\u7d22\u663e\u8457\u5f71\u54cd\u4e86LLM\u7684\u4eba\u683c\u8868\u8fbe\u548c\u60c5\u7eea\u57fa\u8c03\uff1b\u76f8\u540c\u7684\u4eba\u683c\u901a\u8fc7\u4e0d\u540c\u7684\u793e\u4f1a\u548c\u60c5\u611f\u4efb\u52a1\u8868\u73b0\u51fa\u884c\u4e3a\u548c\u8bed\u8a00\u7684\u53d8\u5316\u3002", "conclusion": "LLM\u7684\u4eba\u683c\u8868\u8fbe\u4e0d\u662f\u56fa\u5b9a\u6216\u4e0d\u4e00\u81f4\u7684\uff0c\u800c\u662f\u6839\u636e\u4e0d\u540c\u60c5\u5883\u505a\u51fa\u4eba\u7c7b\u822c\u7684\u7075\u6d3b\u9002\u5e94\u3002\u8fd9\u4e00\u53d1\u73b0\u7b26\u5408\u5168\u4eba\u683c\u7406\u8bba\uff0c\u63d0\u793aAI\u53ef\u5b9e\u73b0\u66f4\u793e\u4f1a\u5316\u3001\u60c5\u611f\u5316\u7684\u4e92\u52a8\u8868\u73b0\u3002"}}
{"id": "2602.01948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01948", "abs": "https://arxiv.org/abs/2602.01948", "authors": ["Patrick Frank", "Christian Friedrich"], "title": "A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications", "comment": "17 pages, 14 figures, submitted to Robotics and Computer-Integrated Manufacturing (RCIM)", "summary": "Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5b8f-\u5fae\u64cd\u63a7\u7cfb\u7edf\u7684\u63a7\u5236\u67b6\u6784\uff0c\u5c06\u5b8f\u64cd\u63a7\u5668\u4e3b\u52a8\u7eb3\u5165\u4ea4\u4e92\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u63a7\u5236\u5e26\u5bbd\uff0c\u5e76\u901a\u8fc7\u591a\u9879\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5b8f-\u5fae\u64cd\u63a7\u7cfb\u7edf\u4e2d\uff0c\u5b8f\u64cd\u63a7\u5668\u53ea\u8d1f\u8d23\u4f4d\u7f6e\u63a7\u5236\uff0c\u5fae\u64cd\u63a7\u5668\u8d1f\u8d23\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u5bfc\u81f4\u4ea4\u4e92\u63a7\u5236\u5e26\u5bbd\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u52a8\u6001\u4efb\u52a1\u9700\u6c42\u3002\u4f5c\u8005\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u5e26\u5bbd\u74f6\u9888\uff0c\u63d0\u9ad8\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u63a7\u5236\u67b6\u6784\uff0c\u4f7f\u5b8f\u64cd\u63a7\u5668\u53c2\u4e0e\u4e3b\u52a8\u4ea4\u4e92\u63a7\u5236\uff0c\u5e76\u5f15\u5165\u4ee3\u7406\u6a21\u578b\uff08surrogate models\uff09\u7b80\u5316\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u4fbf\u4e8e\u5bf9\u786c\u4ef6\u53d8\u5316\u8fdb\u884c\u9002\u5e94\u3002\u901a\u8fc7\u4e0e\u4e3b\u4ece\u5f0f\uff08leader-follower\uff09\u548c\u4f20\u7edf\u529b\u63a7\u5236\u65b9\u6cd5\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u65b9\u6848\u6548\u679c\u3002", "result": "\u65b0\u67b6\u6784\u4e0b\uff0c\u63a7\u5236\u5e26\u5bbd\u63d0\u5347\u81f3\u4e3b\u4ece\u6cd52.1\u500d\u3001\u4f20\u7edf\u673a\u5668\u4eba\u529b\u63a7\u523612.5\u500d\uff0c\u5728\u78b0\u649e\u3001\u529b\u8ddf\u8e2a\u3001\u88c5\u914d\u4efb\u52a1\u7b49\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7eb3\u5165\u5b8f\u64cd\u63a7\u5668\u53c2\u4e0e\u4ea4\u4e92\u63a7\u5236\uff0c\u53ef\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u63a7\u5236\u5e26\u5bbd\u4e0e\u52a8\u6001\u6027\u80fd\uff0c\u65b0\u65b9\u6cd5\u9002\u5e94\u6027\u5f3a\uff0c\u9a8c\u8bc1\u6709\u6548\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u5de5\u4e1a\u4efb\u52a1\u3002"}}
{"id": "2602.00462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00462", "abs": "https://arxiv.org/abs/2602.00462", "authors": ["Benno Krojer", "Shravan Nayak", "Oscar Ma\u00f1as", "Vaibhav Adlakha", "Desmond Elliott", "Siva Reddy", "Marius Mosbach"], "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs", "comment": null, "summary": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u65b9\u6cd5LatentLens\uff0c\u53ef\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9token\u7684\u9690\u7a7a\u95f4\u8868\u793a\u6620\u5c04\u4e3a\u53ef\u8bfb\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u589e\u5f3a\u4e86\u89c6\u89c9token\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u901a\u8fc7\u7b80\u5355\u53d8\u6362\u5c31\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\u89c6\u89c9token\uff0c\u7136\u800c\u8fd9\u4e9b\u89c6\u89c9token\u5728\u6a21\u578b\u5404\u5c42\u4e2d\u7f16\u7801\u4e86\u4ec0\u4e48\u8bed\u4e49\u76ee\u524d\u7f3a\u4e4f\u89e3\u91ca\u5de5\u5177\u3002\u7814\u7a76\u52a8\u56e0\u5728\u4e8e\u63d0\u5347VLM\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u66f4\u597d\u5730\u7406\u89e3\u89c6\u89c9\u4e0e\u8bed\u8a00\u8868\u793a\u7684\u5bf9\u9f50\u60c5\u51b5\u3002", "method": "\u63d0\u51faLatentLens\u65b9\u6cd5\uff1a\u5c06\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u7f16\u7801\u4e3a\u5e26\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684token\u8868\u5f81\uff0c\u8ba1\u7b97\u89c6\u89c9token\u548c\u6240\u6709\u6587\u672ctoken\u8868\u5f81\u7684\u76f8\u4f3c\u5ea6\uff0c\u901a\u8fc7top-k\u6700\u8fd1\u90bb\u6587\u672ctoken\u4e3a\u6bcf\u4e2a\u89c6\u89c9token\u751f\u6210\u8bed\u4e49\u63cf\u8ff0\u3002\u5b9e\u9a8c\u6db5\u76d610\u4e2a\u4e0d\u540c\u7684VLM\uff0c\u5e76\u4e0e\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5e38\u7528\u7684LogitLens\u65b9\u6cd5\u4f4e\u4f30\u4e86\u89c6\u89c9token\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u800cLatentLens\u80fd\u8ba9\u5927\u591a\u6570\u89c6\u89c9token\u5728\u6240\u6709\u6a21\u578b\u3001\u6240\u6709\u5c42\u6b21\u90fd\u6613\u4e8e\u89e3\u91ca\u3002\u751f\u6210\u7684\u63cf\u8ff0\u66f4\u5177\u8bed\u4e49\u6027\u548c\u7ec6\u7c92\u5ea6\u3002", "conclusion": "LatentLens\u6781\u5927\u63d0\u5347\u4e86VLM\u89c6\u89c9token\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u66f4\u7d27\u5bc6\u5bf9\u9f50\u5173\u7cfb\uff0c\u5e76\u4e3a\u5206\u6790\u6a21\u578b\u9690\u8868\u793a\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01064", "abs": "https://arxiv.org/abs/2602.01064", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuo Yang", "Chu Yuan Zhang", "Jianhua Tao"], "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs", "comment": null, "summary": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \\textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u77e5\u8bc6\u7eaf\u5316\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6559\u5e08LLM\u7684\u89e3\u91ca\uff0c\u89e3\u51b3\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u4e2d\u5b58\u5728\u7684\u77e5\u8bc6\u51b2\u7a81\u548c\u8d44\u6e90\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u79cd\u7eaf\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u91c7\u7528\u591a\u6559\u5e08\u6a21\u578b\u65f6\uff0c\u5bb9\u6613\u5bfc\u81f4\u77e5\u8bc6\u51b2\u7a81\uff0c\u5e76\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u8f83\u9ad8\uff0c\u9650\u5236\u4e86\u9ad8\u6027\u80fd\u5c0f\u6a21\u578b\u7684\u843d\u5730\u4e0e\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u591a\u6559\u5e08\u84b8\u998f\u7684\u6548\u679c\u4e0e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u201c\u77e5\u8bc6\u7eaf\u5316\u201d\u6982\u5ff5\uff0c\u5c06\u591a\u4e2a\u6559\u5e08LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u878d\u5408\u4e3a\u5355\u4e00\u7684\u6709\u6548\u89e3\u91ca\uff0c\u6765\u51cf\u5c11\u77e5\u8bc6\u51b2\u7a81\u3002\u8fdb\u4e00\u6b65\uff0c\u8bbe\u8ba1\u4e86\u4e94\u79cd\u4e0d\u540c\u89c6\u89d2\u7684\u7eaf\u5316\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5305\u62ec\u57fa\u4e8e\u8def\u7531\u7684\u6280\u672f\u7b49\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u7eaf\u5316\u65b9\u6cd5\u63d0\u5347\u4e86\u84b8\u998f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u77e5\u8bc6\u51b2\u7a81\u3002\u5176\u4e2d\uff0c\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u77e5\u8bc6\u7eaf\u5316\u6280\u672f\u80fd\u4f18\u5316\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u521b\u65b0\u7eaf\u5316\u6280\u672f\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.02006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02006", "abs": "https://arxiv.org/abs/2602.02006", "authors": ["Thomas Jantos", "Giulio Delama", "Stephan Weiss", "Jan Steinbrener"], "title": "Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements", "comment": "Accepted for publication at ICRA 2026, Vienna, Austria", "summary": "Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.\n  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u57fa\u4e8eAI\u7684\u7269\u4f53\u76f8\u5bf9\u72b6\u6001\u6d4b\u91cf\u65b9\u7a0b\uff0c\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u57fa\u4e8eAI\u7684\u7269\u4f53\u8bc6\u522b\u4e0e6\u81ea\u7531\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u8d8a\u6765\u8d8a\u5e38\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5f02\u5e38\u503c\u5904\u7406\u5f71\u54cd\u6700\u7ec8\u5b9a\u4f4d\u6548\u679c\u3002\u5982\u4f55\u6709\u6548\u878d\u5408DNN\u6d4b\u91cf\u7ed3\u679c\u81f3\u6ee4\u6ce2\u5668\u6846\u67b6\uff0c\u5e76\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\uff0c\u662f\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u4f5c\u8005\u5c06AI\u63a8\u65ad\u5f97\u5230\u7684\u7269\u4f53\u76f8\u5bf96-DoF\u4f4d\u59ff\uff0c\u76f4\u63a5\u4f5c\u4e3a\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u89c2\u6d4b\u91cf\uff0c\u5e76\u5bf9\u6d4b\u91cf\u65b9\u7a0b\u8fdb\u884c\u4e86\u91cd\u6784\uff0c\u4f7f\u4f4d\u7f6e\u4e0e\u65cb\u8f6c\u4fe1\u606f\u53ef\u89e3\u8026\uff0c\u4ece\u800c\u51cf\u5f31\u65cb\u8f6c\u6d4b\u91cf\u8bef\u5dee\u7684\u5f71\u54cd\u5e76\u652f\u6301\u90e8\u5206\u89c2\u6d4b\u62d2\u7edd\u3002\u6b64\u5916\uff0c\u5c06\u539f\u672c\u56fa\u5b9a\u7684\u6d4b\u91cf\u534f\u65b9\u5dee\u77e9\u9635\u66ff\u6362\u4e3aDNN\u9884\u6d4b\u7684\u5185\u7980\u4e0d\u786e\u5b9a\u6027\uff08aleatoric uncertainty\uff09\uff0c\u4ee5\u52a8\u6001\u53cd\u6620\u6d4b\u91cf\u4fe1\u5fc3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u91cd\u6784\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6ee4\u6ce2\u5668\u5728\u591a\u76ee\u6807\u573a\u666f\u4e0b\u7684\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u8fd8\u63d0\u9ad8\u4e86\u5bf9\u5f02\u5e38\u503c\u548c\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u3002\u89e3\u8026\u64cd\u4f5c\u548c\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5747\u5e26\u6765\u663e\u8457\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u878d\u5408AI\u6d4b\u91cf\u7684\u89c2\u6d4b\u65b9\u7a0b\u91cd\u6784\u548c\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\uff0c\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u786e\u5b9e\u6709\u6548\uff0c\u5efa\u8bae\u76f8\u5173\u4efb\u52a1\u91c7\u7528\u8be5\u65b9\u6cd5\u63d0\u5347\u5b9a\u4f4d\u9c81\u68d2\u6027\u4e0e\u5b9e\u65f6\u6027\u3002"}}
{"id": "2602.00463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00463", "abs": "https://arxiv.org/abs/2602.00463", "authors": ["Xin Zhang", "Shen Chen", "Jiale Zhou", "Lei Li"], "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting", "comment": "Accepted to ICASSP2026", "summary": "Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSGS\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6587\u672c\u9ad8\u8d28\u91cf\u751f\u6210\u5168\u666f3D\u573a\u666f\uff0c\u660e\u663e\u63d0\u5347\u4e86\u7ec6\u8282\u548c\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c89\u6d78\u5f0f\u5e94\u7528\uff08VR\u3001AR\u3001\u6e38\u620f\uff09\u8feb\u5207\u9700\u8981\u80fd\u4ece\u6587\u672c\u751f\u6210\u771f\u5b9e3D\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u53d7\u9650\u4e8e3D-\u6587\u672c\u6570\u636e\u7a00\u7f3a\u4e0e\u591a\u89c6\u89d2\u878d\u5408\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u573a\u666f\u8fc7\u4e8e\u7b80\u5355\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u9ad8\u4e00\u81f4\u6027\u7684\u6587\u672c\u9a71\u52a83D\u573a\u666f\u751f\u6210\u3002", "method": "PSGS\u6846\u67b6\u5305\u542b\u4e24\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u901a\u8fc7\u4e24\u5c42\u4f18\u5316\u67b6\u6784\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u5168\u666f\u56fe\uff0c\u5148\u7531\u5e03\u5c40\u63a8\u7406\u5c42\u5c06\u6587\u672c\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u7a7a\u95f4\u5173\u7cfb\uff0c\u518d\u7528\u81ea\u4f18\u5316\u5c42\u7ed3\u5408\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u53cd\u9988\u53cd\u590d\u63d0\u5347\u753b\u9762\u7ec6\u8282\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u5168\u666f\u6ed1\u52a8\u673a\u5236\uff0c\u901a\u8fc7\u6709\u91cd\u53e0\u91c7\u6837\u83b7\u5f97\u5168\u5c40\u4e00\u81f4\u76843D Gaussian Splatting\u70b9\u4e91\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u6df1\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u589e\u5f3a\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPSGS\u5728\u5168\u666f\u751f\u6210\u548c3D\u573a\u666f\u6548\u679c\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5185\u5bb9\u66f4\u7ec6\u81f4\u3001\u66f4\u5177\u5438\u5f15\u529b\u3002", "conclusion": "PSGS\u4e3a\u6587\u672c\u52303D\u9ad8\u8d28\u91cf\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u4fc3\u8fdb\u6c89\u6d78\u5f0f\u5185\u5bb9\u751f\u6210\u7684\u76f8\u5173\u5e94\u7528\u9886\u57df\u3002"}}
{"id": "2602.01068", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01068", "abs": "https://arxiv.org/abs/2602.01068", "authors": ["Chaoqun Cui", "Shijing Wang", "Liangbin Huang", "Qingqing Gu", "Zhaolong Huang", "Xiao Zeng", "Wenji Mao"], "title": "From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization", "comment": "Accepted to ICLR 2026", "summary": "The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u5a92\u4f53\u5b57\u5e55\u7ffb\u8bd1\u7684\u5b9a\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86ALPO\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7ffb\u8bd1\u504f\u597d\u5bf9\u9f50\u548c\u8868\u8fbe\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5df2\u5e7f\u6cdb\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u5782\u76f4\u9886\u57df\uff08\u5982\u5b57\u5e55\u7ffb\u8bd1\uff09\u9762\u4e34\u8868\u8fbe\u4e0d\u591f\u751f\u52a8\u3001\u7ec6\u8282\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u63d0\u5347LLMs\u5bf9\u7279\u5b9a\u57df\u5982\u5b57\u5e55\u7ffb\u8bd1\u7684\u5b9a\u5236\u5316\u548c\u8868\u73b0\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u591a\u8bed\u5411\u5b57\u5e55\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u5c40\u90e8\u504f\u597d\u4f18\u5316\uff08ALPO\uff09\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7ffb\u8bd1LLM\u5728\u8868\u8fbe\u6027\u548c\u7ec6\u7c92\u5ea6\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u5316\u3002\u5e76\u9a8c\u8bc1LLM\u4f5c\u4e3a\u7ffb\u8bd1\u5956\u52b1\u6a21\u578b\u548c\u8bc4\u4f30\u5668\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f15\u5165ALPO\u7684\u7ffb\u8bd1LLM\u5728\u591a\u7ef4\u5ea6\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u80fd\u6709\u6548\u9002\u5e94\u4e0d\u540c\u9886\u57df\u4e0b\u76f4\u8bd1\u4e0e\u610f\u8bd1\u7684\u9700\u6c42\u3002", "conclusion": "ALPO\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86LLM\u5728\u89c6\u89c9\u5a92\u4f53\u5b57\u5e55\u9886\u57df\u7684\u5b9a\u5236\u5316\u7ffb\u8bd1\u80fd\u529b\uff0c\u4e3a\u5782\u76f4\u9886\u57df\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02026", "abs": "https://arxiv.org/abs/2602.02026", "authors": ["Zhenwei Niu", "Xiaoyi Chen", "Jiayu Hu", "Zhaoyang Liu", "Xiaozu Ju"], "title": "Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp", "comment": null, "summary": "We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5b9e\u65f6\u6469\u64e6\u529b\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u6293\u53d6\u63a7\u5236\u7684\u6e29\u548c\u578b\u673a\u5668\u4eba\u6293\u53d6\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u7528\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0\u65e2\u7a33\u56fa\u53c8\u4e0d\u635f\u574f\u7269\u4f53\u7684\u6e29\u548c\u6293\u53d6\u662f\u4e00\u5927\u96be\u9898\uff0c\u4e3b\u8981\u53d7\u6469\u64e6\u529b\u53d8\u5316\u548c\u63a7\u5236\u7cbe\u5ea6\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u611f\u77e5\u548c\u63a7\u5236\u65b9\u6cd5\u63d0\u5347\u6293\u53d6\u67d4\u987a\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u91c7\u7528\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\u5b9e\u65f6\u4f30\u8ba1\u7269\u4f53\u7684\u6469\u64e6\u7cfb\u6570\uff0c\u5e76\u5c06\u4f30\u7b97\u7ed3\u679c\u65e0\u7f1d\u96c6\u6210\u8fdb\u4e00\u4e2a\u53ef\u52a8\u6001\u8c03\u8282\u6293\u53d6\u529b\u7684\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u95ed\u73af\u8026\u5408\u3002\u4f20\u611f\u5668\u611f\u77e5\u4e0e\u63a7\u5236\u5668\u8c03\u8282\u534f\u540c\u5de5\u4f5c\uff0c\u4e0d\u65ad\u4f9d\u636e\u6700\u65b0\u53cd\u9988\u8c03\u6574\u6293\u529b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u673a\u5668\u4eba\u6293\u53d6\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u54cd\u5e94\u6027\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u6548\u7387\uff0c\u8868\u660e\u5176\u53ef\u7a33\u5b9a\u6267\u884c\u6e29\u548c\u6293\u53d6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u7684\u7a33\u5065\u6027\u4e0e\u67d4\u987a\u6027\uff0c\u4e3a\u6e29\u548c\u578b\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u800c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00470", "abs": "https://arxiv.org/abs/2602.00470", "authors": ["Pengyu Chen", "Fangzheng Lyu", "Sicheng Wang", "Cuizhen Wang"], "title": "ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation", "comment": null, "summary": "Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6811\u51a0\u5206\u5272\u6846\u67b6\uff08ZS-TreeSeg\uff09\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u5206\u5272\u548c\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\uff0c\u901a\u8fc7\u5bf9\u6811\u51a0\u5efa\u6a21\u4e3a\u661f\u5f62\u51f8\u5bf9\u8c61\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u6811\u51a0\u5b9e\u4f8b\u5206\u5272\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u51a0\u5c42\u5bc6\u5ea6\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u73b0\u6709\u7684\u6811\u51a0\u5206\u5272\u65b9\u6cd5\u5728\u5bc6\u96c6\u3001\u91cd\u53e0\u51a0\u5c42\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u4e14\u6cdb\u5316\u6027\u6709\u9650\uff0c\u901a\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982Segment Anything\uff09\u53c8\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5206\u5272\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u8bad\u7ec3\u4ee3\u4ef7\u4f4e\u4e14\u80fd\u66f4\u597d\u6cdb\u5316\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faZS-TreeSeg\u96f6\u6837\u672c\u5206\u5272\u6846\u67b6\uff0c\u5c06\u6811\u51a0\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\u8f6c\u5316\u4e3a\u6210\u719f\u7684\u51a0\u5c42\u8bed\u4e49\u5206\u5272\u548c\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\u7684\u9002\u914d\u3002\u5177\u4f53\u5730\uff0c\u7528Cellpose-SAM\u65b9\u6cd5\uff0c\u5c06\u6811\u51a0\u5efa\u6a21\u4e3a\u661f\u5f62\u51f8\u5bf9\u8c61\uff0c\u5e76\u5229\u7528\u62d3\u6251\u6d41\u573a\u4e0e\u5411\u91cf\u6536\u655b\u6027\uff0c\u5b9e\u73b0\u91cd\u53e0\u6811\u51a0\u5b9e\u4f8b\u7684\u6570\u5b66\u5206\u79bb\u3002", "result": "\u5728NEON\u548cBAMFOREST\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u68c0\u9a8c\uff0c\u8868\u660e\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u4f20\u611f\u5668\u7c7b\u578b\u548c\u51a0\u5c42\u5bc6\u5ea6\u4e0b\u90fd\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u5206\u5272\u6548\u679c\u3002", "conclusion": "ZS-TreeSeg\u65b9\u6cd5\u4e3a\u6811\u51a0\u5b9e\u4f8b\u5206\u5272\u548c\u6807\u7b7e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4f4e\u6210\u672c\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.01070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01070", "abs": "https://arxiv.org/abs/2602.01070", "authors": ["Ahsan Bilal", "Ahmed Mohsin", "Muhammad Umer", "Ali Subhan", "Hassan Rizwan", "Ayesha Mohsin", "Dean Hougen"], "title": "What If We Allocate Test-Time Compute Adaptively?", "comment": null, "summary": "Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u52a8\u6001\u5f15\u5bfc\u63a8\u7406\u8def\u5f84\u7684\u751f\u6210\u548c\u9009\u62e9\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u63a8\u7406\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u65b9\u5f0f\uff0c\u5982\u5747\u5300\u5206\u914d\u548c\u56fa\u5b9a\u91c7\u6837\uff0c\u4e0d\u591f\u9ad8\u6548\u4e14\u7f3a\u4e4f\u9488\u5bf9\u6027\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u6216\u9ad8\u96be\u5ea6\u57fa\u51c6\u4e0a\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u9ad8\u6548\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u591a\u6b21\u8fed\u4ee3\u751f\u6210\u548c\u9009\u62e9\u63a8\u7406\u8f68\u8ff9\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u667a\u80fd\u4f53\u53ef\u80fd\u4f1a\u751f\u6210\u9ad8\u5c42\u6b21\u8ba1\u5212\uff0c\u9009\u62e9\u63a8\u7406\u5de5\u5177\u548c\u8ba1\u7b97\u7b56\u7565\uff0c\u5e76\u751f\u6210\u5019\u9009\u63a8\u7406\u8def\u5f84\u3002\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u4f5c\u4e3a\u7edf\u4e00\u63a7\u5236\u4fe1\u53f7\uff0c\u5728\u8fed\u4ee3\u5185\u805a\u5408\u6b65\u7ea7\u5206\u6570\u8fdb\u884c\u526a\u679d\u548c\u6269\u5c55\uff0c\u5728\u8fed\u4ee3\u95f4\u7528\u805a\u5408\u7684\u8f68\u8ff9\u5956\u52b1\u9009\u62e9\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728\u591a\u9879\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u8f83\u4f20\u7edf\u76f4\u63a5\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u65b9\u5f0f\u6709\u663e\u8457\u63d0\u5347\u3002\u5728MATH-500\u3001AIME24\u548cAMO-Bench\u7b49\u9ad8\u96be\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53d6\u5f97\u4e86\u8f83\u5927\u6027\u80fd\u589e\u76ca\uff0c\u5c24\u5176\u5728\u66f4\u590d\u6742\u7684\u4efb\u52a1\u4e0a\u6709\u6570\u500d\u63d0\u5347\u3002", "conclusion": "\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u5206\u914d\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u96be\u5ea6\u63a8\u7406\u4efb\u52a1\u7684\u7cbe\u5ea6\u5e76\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d6a\u8d39\uff0c\u6709\u6548\u5c06\u8ba1\u7b97\u96c6\u4e2d\u5728\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u8def\u5f84\u4e0a\u3002"}}
{"id": "2602.02035", "categories": ["cs.RO", "cs.AI", "cs.IT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02035", "abs": "https://arxiv.org/abs/2602.02035", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "title": "Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization", "comment": "Accepted at the 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria. 9 pages, 4 figures, 6 tables", "summary": "Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u4e0e\u5411\u91cf\u91cf\u5316\u7684\u65b0\u578b\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u5173\u952e\u4fe1\u606f\u7684\u6709\u6548\u538b\u7f29\u548c\u52a8\u6001\u901a\u4fe1\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d7\u9650\u5e26\u5bbd\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5e38\u5e38\u53d7\u9650\u4e8e\u5e26\u5bbd\uff0c\u5bfc\u81f4\u901a\u4fe1\u4e0d\u7545\uff0c\u5f71\u54cd\u534f\u4f5c\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u538b\u7f29\u548c\u52a8\u6001\u901a\u4fe1\u51b3\u7b56\u65b9\u9762\u666e\u904d\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7406\u8bba\u57fa\u7840\u66f4\u5f3a\u7684\u901a\u4fe1\u673a\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7406\u8bba\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5b9e\u73b0\u901a\u4fe1\u6d88\u606f\u7684\u538b\u7f29\u4e0e\u79bb\u6563\u5316\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u95e8\u63a7\u901a\u4fe1\u673a\u5236\uff0c\u6839\u636e\u73af\u5883\u8bed\u5883\u548c\u667a\u80fd\u4f53\u72b6\u6001\u52a8\u6001\u5224\u5b9a\u662f\u5426\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u8bba\u4f18\u5316\u65b9\u6cd5\u786e\u4fdd\u4fdd\u7559\u4efb\u52a1\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5728\u590d\u6742\u534f\u8c03\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e41.4%\u5e26\u5bbd\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u76f8\u8f83\u65e0\u901a\u4fe1\u57fa\u7ebf\u63d0\u5347\u6027\u80fd181.8%\u3002\u5728\u8986\u76d6\u6210\u529f\u7387-\u5e26\u5bbd\u66f2\u7ebf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u9762\u79ef\u6307\u6807\u4e0a\uff0c\u4ee50.198\u663e\u8457\u4f18\u4e8e\u4e0b\u4e00\u4e2a\u6700\u4f73\u65b9\u6cd50.142\uff0c\u663e\u793a\u4e86\u5728\u6240\u6709\u5e26\u5bbd-\u6027\u80fd\u6743\u8861\u4e0b\u7684\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u65b9\u6cd5\u7406\u8bba\u57fa\u7840\u624e\u5b9e\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u901a\u4fe1\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5e26\u5bbd\u53d7\u9650\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\u96c6\u7fa4\u3001\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u548c\u5206\u5e03\u5f0f\u4f20\u611f\u7f51\u7edc\uff09\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2602.00484", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00484", "abs": "https://arxiv.org/abs/2602.00484", "authors": ["Rong-Lin Jian", "Ming-Chi Luo", "Chen-Wei Huang", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association", "comment": "Winner Solution of SoccerTrack in ACM Multimedia 2025 Workshop MMSports", "summary": "Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTATrack\u7684\u591a\u76ee\u6807\u8ffd\u8e2a\u6846\u67b6\uff0c\u4e13\u95e8\u5e94\u5bf9\u4f53\u80b2\u8fd0\u52a8\u4e2d\u9c7c\u773c\u6444\u50cf\u5934\u4e0b\u7684\u8ffd\u8e2a\u96be\u9898\uff0c\u5e76\u5728SoccerTrack Challenge 2025\u4e2d\u83b7\u5f97\u51a0\u519b\u3002", "motivation": "\u4f53\u80b2\u8fd0\u52a8\u591a\u76ee\u6807\u8ffd\u8e2a\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u9c7c\u773c\u6444\u50cf\u5934\u5bfc\u81f4\u56fe\u50cf\u51e0\u4f55\u7578\u53d8\u548c\u5c3a\u5ea6\u53d8\u5316\u6781\u5927\u7684\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u51fa\u73b0\u76ee\u6807\u906e\u6321\u3001\u8eab\u4efd\u5207\u6362\u548c\u8ffd\u8e2a\u788e\u7247\u5316\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u8ffd\u8e2a\u65b9\u6cd5\u3002", "method": "GTATrack\u7531\u4e24\u5927\u6838\u5fc3\u6a21\u5757\u7ec4\u6210\uff1a\u4e00\u4e2a\u57fa\u4e8eDeep Expansion IoU\uff08Deep-EIoU\uff09\u7684\u8fd0\u52a8\u65e0\u5173\u5728\u7ebf\u5173\u8054\u6a21\u5757\uff0c\u7528\u4e8e\u77ed\u671f\u5185\u7684\u76ee\u6807\u5339\u914d\uff1b\u4e00\u4e2a\u5168\u7403\u8f68\u8ff9\u5173\u8054\uff08GTA\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u66f4\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684\u76ee\u6807\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u8fd8\u5f15\u5165\u4e86\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u63d0\u9ad8\u68c0\u6d4b\u5668\u5728\u5c0f\u76ee\u6807\u548c\u7578\u53d8\u76ee\u6807\u4e0a\u7684\u53ec\u56de\u7387\u3002", "result": "GTATrack\u5728\u9c7c\u773c\u6444\u50cf\u5934\u4e0b\u7684\u8db3\u7403\u591a\u76ee\u6807\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\uff0c\u83b7\u5f97HOTA\u8bc4\u52060.60\uff0c\u4ee5\u53ca\u663e\u8457\u964d\u4f4e\u7684\u8bef\u62a5\u6570\uff08982\uff09\uff0c\u5728SoccerTrack Challenge 2025\u6bd4\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "GTATrack\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u5173\u8054\u548c\u5168\u5c40\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u5207\u6362\u3001\u906e\u6321\u4ee5\u53ca\u8ffd\u8e2a\u788e\u7247\u5316\u7b49\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u9c7c\u773c\u89c6\u9891\u4e0b\u7684\u591a\u76ee\u6807\u8ffd\u8e2a\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2602.01116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01116", "abs": "https://arxiv.org/abs/2602.01116", "authors": ["Wenxuan Zhang", "Yuan-Hao Jiang", "Changyong Qi", "Rui Jia", "Yonghe Wu"], "title": "Logic-Oriented Retriever Enhancement via Contrastive Learning", "comment": "accepted by icassp 2026", "summary": "Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLORE\u7684\u65b0\u578b\u68c0\u7d22\u5668\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u6fc0\u6d3b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u5206\u6790\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u68c0\u7d22\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u6027\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\u5373\u53ef\u63d0\u5347\u53ec\u56de\u4e0e\u751f\u6210\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u68c0\u7d22\u5668\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u6027\u800c\u96be\u4ee5\u5904\u7406\u6d89\u53ca\u590d\u6742\u903b\u8f91\u5173\u7cfb\u7684\u67e5\u8be2\u3002\u672c\u6587\u65e8\u5728\u6fc0\u53d1\u6a21\u578b\u5185\u5728\u7684\u903b\u8f91\u5206\u6790\u80fd\u529b\uff0c\u63d0\u5347\u5176\u5728\u8fd9\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faLORE\uff08Logic ORiented Retriever Enhancement\uff09\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5f15\u5bfc\u6a21\u578b\u8868\u793a\u5411\u7b26\u5408\u903b\u8f91\u7ed3\u6784\u7684\u8bc1\u636e\u9760\u62e2\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u3002LORE\u4e0d\u9700\u8981\u989d\u5916\u76d1\u7763\u3001\u5916\u90e8\u8d44\u6e90\u6216\u9884\u68c0\u7d22\u5206\u6790\uff0c\u5e76\u53ef\u76f4\u63a5\u517c\u5bb9\u73b0\u6709\u7d22\u5f15\u3002", "result": "LORE\u5728\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\u548c\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u8868\u73b0\u4e0a\u5747\u6709\u663e\u8457\u7684\u63d0\u5347\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u6570\u636e\u96c6\u5747\u9002\u7528\u3002", "conclusion": "LORE\u6709\u6548\u6316\u6398\u548c\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u80fd\u529b\uff0c\u5728\u65e0\u9700\u989d\u5916\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u68c0\u7d22\u4e0e\u751f\u6210\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u6027\u3002"}}
{"id": "2602.02038", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02038", "abs": "https://arxiv.org/abs/2602.02038", "authors": ["Etienne M\u00e9nager", "Justin Carpentier"], "title": "Frictional Contact Solving for Material Point Method", "comment": null, "summary": "Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u9690\u5f0f\u7269\u8d28\u70b9\u6cd5\uff08MPM\uff09\u7684\u6469\u64e6\u63a5\u89e6\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u9c81\u68d2\u7684\u6469\u64e6\u63a5\u89e6\u5904\u7406\uff0c\u63d0\u5347\u4e86\u7269\u7406\u4eff\u771f\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7269\u8d28\u70b9\u6cd5\u5728\u5904\u7406\u542b\u6469\u64e6\u63a5\u89e6\u65f6\u5b58\u5728\u74f6\u9888\uff0c\u5982\u63a5\u89e6\u70b9\u68c0\u6d4b\u548c\u6469\u64e6\u5b9a\u5f8b\uff08\u975e\u7a7f\u900f\u3001\u5e93\u4ed1\u6469\u64e6\u3001\u6700\u5927\u8017\u6563\u539f\u7406\uff09\u6267\u884c\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u4eff\u771f\u6548\u679c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u5173\u952e\u96be\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9488\u5bf9\u9690\u5f0fMPM\u7684\u6469\u64e6\u63a5\u89e6\u6d41\u7a0b\uff1a\u78b0\u649e\u68c0\u6d4b\u9636\u6bb5\u5229\u7528\u7c92\u5b50\u4e2d\u5fc3\u51e0\u4f55\u5143\u4ef6\u5b9a\u4f4d\u63a5\u89e6\u70b9\uff1b\u63a5\u89e6\u5206\u89e3\u9636\u6bb5\u5c06\u6469\u64e6\u63a5\u89e6\u5efa\u6a21\u4e3a\u63a5\u89e6\u51b2\u91cf\u7684\u975e\u7ebf\u6027\u4e92\u8865\u95ee\u9898\uff08NCP\uff09\uff1b\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u6c42\u89e3\uff0c\u5e76\u91cd\u7528MPM\u5185\u672c\u8eab\u7684\u7ebf\u6027\u5316\u6a21\u5757\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e0e\u6570\u503c\u7a33\u5b9a\u3002\u65b9\u6848\u53ef\u65e0\u7f1d\u96c6\u6210\u4e8eMPM\uff0c\u4e14\u4e0d\u4f9d\u8d56\u5177\u4f53\u7684\u6750\u6599\u6a21\u578b\u3001\u63d2\u503c\u51fd\u6570\u6216\u8f6c\u79fb\u65b9\u6848\u3002", "result": "\u65b9\u6cd5\u5728\u4e03\u4e2a\u4ee3\u8868\u6027\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u573a\u666f\u6db5\u76d6\u5f39\u6027\u4e0e\u5f39\u5851\u6027\u53cd\u5e94\u3001\u5404\u7c7b\u590d\u6742\u53d8\u5f62\u51e0\u4f55\u4f53\u4ee5\u53ca\u591a\u79cd\u63a5\u89e6\u5de5\u51b5\u3002\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u51c6\u786e\u7684\u63a5\u89e6\u5b9a\u4f4d\u548c\u53ef\u9760\u7684\u6469\u64e6\u5904\u7406\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u517c\u5177\u7cbe\u5ea6\u3001\u53ef\u9760\u6027\u53ca\u901a\u7528\u6027\uff0c\u662f\u9762\u5411\u673a\u5668\u4eba\u53ca\u76f8\u5173\u9886\u57df\u57fa\u4e8eMPM\u4eff\u771f\u7684\u5b9e\u7528\u6469\u64e6\u63a5\u89e6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00489", "abs": "https://arxiv.org/abs/2602.00489", "authors": ["Sicong Zang", "Tao Sun", "Cairong Yan"], "title": "Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level", "comment": "Source codes are coming soon", "summary": "Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8349\u56fe\u9010\u7b14\u7f16\u8f91\u7684\u65b9\u6cd5\uff08SketchMod\uff09\uff0c\u901a\u8fc7\u5bf9\u6e90\u7b14\u753b\u8fdb\u884c\u53d8\u6362\uff08\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u5e73\u79fb\uff09\uff0c\u4f7f\u5176\u5728\u76ee\u6807\u8349\u56fe\u4e2d\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u7075\u6d3b\u7684\u79fb\u690d\u548c\u66ff\u6362\uff0c\u63d0\u5347\u7f16\u8f91\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u8349\u56fe\u7f16\u8f91\u65b9\u6cd5\u4ec5\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u6e90\u7b14\u753b\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u6e90\u7b14\u753b\u5728\u5927\u5c0f\u548c\u65b9\u5411\u4e0a\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u5408\u6210\u7ed3\u679c\u4e0d\u81ea\u7136\u6216\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002\u4e9f\u9700\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u6e90\u7b14\u753b\u5c5e\u6027\uff0c\u4f7f\u7f16\u8f91\u540e\u7684\u7ed3\u679c\u7b26\u5408\u76ee\u6807\u8349\u56fe\u7684\u6574\u4f53\u98ce\u683c\u548c\u5e03\u5c40\u3002", "method": "\u4f5c\u8005\u63d0\u51faSketchMod\uff0c\u901a\u8fc7\u5b66\u4e60\u76ee\u6807\u8349\u56fe\u7684\u683c\u5c40\uff0c\u81ea\u52a8\u63a8\u65ad\u6e90\u7b14\u753b\u5230\u76ee\u6807\u7684\u4e09\u4e2a\u5173\u952e\u504f\u79fb\u5c5e\u6027\uff1a\u6bd4\u4f8b\uff08scale\uff09\u3001\u65b9\u5411\uff08orientation\uff09\u548c\u4f4d\u7f6e\uff08position\uff09\u3002\u5177\u4f53\u505a\u6cd5\u5305\u62ec1\uff09\u8c03\u6574\u6bd4\u4f8b\u4ee5\u5339\u914d\u7a7a\u95f4\u5c3a\u5bf8\u30012\uff09\u65cb\u8f6c\u4ee5\u5bf9\u9f50\u5c40\u90e8\u51e0\u4f55\u30013\uff09\u5e73\u79fb\u4ee5\u5951\u5408\u8bed\u4e49\u5e03\u5c40\uff0c\u5e76\u5f00\u653e\u5bf9\u7b14\u753b\u5c5e\u6027\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSketchMod\u5728\u8349\u56fe\u9010\u7b14\u7f16\u8f91\u4efb\u52a1\u4e2d\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u80fd\u5b9e\u73b0\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\uff0c\u64cd\u4f5c\u7ed3\u679c\u66f4\u52a0\u81ea\u7136\u3001\u8bed\u4e49\u4e0e\u76ee\u6807\u4e00\u81f4\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "SketchMod\u4e3a\u8349\u56fe\u9010\u7b14\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u8fc7\u7a0b\u7684\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u548c\u6700\u7ec8\u7684\u89c6\u89c9\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u5e26\u6765\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01119", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01119", "abs": "https://arxiv.org/abs/2602.01119", "authors": ["Konstantin Chernyshev", "Ekaterina Artemova", "Viacheslav Zhukov", "Maksim Nerush", "Mariia Fedorova", "Iryna Repik", "Olga Shapovalova", "Aleksey Sukhorosov", "Vladimir Dobrovolskii", "Natalia Mikhailova", "Sergei Tilga"], "title": "Tendem: A Hybrid AI+Human Platform", "comment": null, "summary": "Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.", "AI": {"tldr": "Tendem\u662f\u4e00\u4e2a\u7ed3\u5408AI\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u80fd\u5728\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\uff0c\u7ecf\u9a8c\u8bc1\u4f18\u4e8e\u5355\u4e00AI\u6216\u4eba\u5de5\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u9047\u5230\u590d\u6742\u4efb\u52a1\u6216\u51fa\u9519\u65f6\u8868\u73b0\u6709\u9650\uff0c\u5b8c\u5168\u4f9d\u8d56\u4eba\u7c7b\u64cd\u4f5c\u53c8\u6548\u7387\u4f4e\uff0c\u5982\u4f55\u534f\u540cAI\u4e0e\u4eba\u7c7b\u4ee5\u53d6\u5f97\u66f4\u4f18\u8f93\u51fa\uff0c\u662f\u8be5\u7814\u7a76\u5173\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Tendem\u7cfb\u7edf\uff0cAI\u8d1f\u8d23\u7ed3\u6784\u5316\u4e0e\u91cd\u590d\u6027\u5de5\u4f5c\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728\u6a21\u578b\u51fa\u9519\u6216\u9700\u7ed3\u679c\u9a8c\u8bc1\u65f6\u4ecb\u5165\uff0c\u5e76\u5728\u4ea4\u4ed8\u524d\u7edf\u4e00\u8d28\u68c0\u3002\u5bf994\u4e2a\u5b9e\u9645\u4efb\u52a1\u8fdb\u884c\u8bc4\u6d4b\uff0c\u5e76\u4e0eUpwork\u7684AI-only\u53cahuman-only\u6d41\u7a0b\u5bf9\u6bd4\u3002", "result": "Tendem\u5728\u8f93\u51fa\u8d28\u91cf\u4e0e\u4ea4\u4ed8\u901f\u5ea6\u4e0a\u5747\u9ad8\u4e8eAI-only\u53cahuman-only\u6d41\u7a0b\uff0c\u8fd0\u7ef4\u6210\u672c\u4e0ehuman-only\u76f8\u5f53\u3002Tendem\u7684AI Agent\u5728\u7b2c\u4e09\u65b9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u63a5\u8fd1\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u7279\u522b\u662f\u5728\u7f51\u9875\u6d4f\u89c8\u3001\u5de5\u5177\u4f7f\u7528\u4e0e\u524d\u6cbf\u77e5\u8bc6\u63a8\u65ad\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7AI\u4e0e\u4eba\u7c7b\u7684\u534f\u540c\uff0cTendem\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u4efb\u52a1\u6267\u884c\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u667a\u80fd\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02142", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02142", "abs": "https://arxiv.org/abs/2602.02142", "authors": ["Ruiteng Zhao", "Wenshuo Wang", "Yicheng Ma", "Xiaocong Li", "Francis E. H. Tay", "Marcelo H. Ang", "Haiyue Zhu"], "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation", "comment": null, "summary": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7269\u7406\u529b\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u529b\u611f\u77e5\u7684VLA\uff08\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff09\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u529b\u84b8\u998f\u6a21\u5757\u201d\u4ece\u89c6\u89c9\u4e0e\u72b6\u6001\u4e2d\u63d0\u53d6\u529b\u4fe1\u53f7\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u590d\u6742\u64cd\u4f5c\u80fd\u529b\uff0c\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528\u4f20\u611f\u5668\u7684\u65b9\u6848\u3002", "motivation": "\u4f20\u7edfVLA\u7cfb\u7edf\u4e2d\uff0c\u529b\u611f\u77e5\u4f9d\u8d56\u6602\u8d35\u4e14\u6613\u635f\u7684\u529b/\u529b\u77e9\u4f20\u611f\u5668\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u6210\u672c\u548c\u9002\u7528\u6027\u4e0a\u7684\u63a8\u5e7f\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63a2\u7d22\u4e0d\u4f9d\u8d56\u7269\u7406\u4f20\u611f\u5668\u7684\u529b\u611f\u77e5\u65b9\u6cd5\uff0c\u4ece\u800c\u964d\u4f4e\u786c\u4ef6\u8981\u6c42\uff0c\u62d3\u5c55\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51fa\u529b\u84b8\u998f\u6a21\u5757\uff08FDM\uff09\uff1a\u901a\u8fc7\u5b66\u4e60\u673a\u5236\uff0c\u5c06\u89c6\u89c9\u89c2\u6d4b\u548c\u673a\u5668\u4eba\u72b6\u6001\u4e0e\u529b\u4fe1\u53f7\u7a7a\u95f4\u5bf9\u9f50\uff0c\u751f\u6210\u529btoken\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u5c06\u8be5token\u6ce8\u5165\u9884\u8bad\u7ec3\u7684VLM\uff0c\u5b9e\u73b0\u5177\u5907\u529b\u611f\u77e5\u63a8\u7406\u800c\u4e0d\u5e72\u6270\u5176\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff1b\u8fd8\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7684\u5148\u9a8c\u878d\u5408\uff0c\u63d0\u5347\u611f\u77e5-\u52a8\u4f5c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u7269\u7406\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6848\u7684\u201c\u84b8\u998f\u529btoken\u201d\u4e0d\u4ec5\u4f18\u4e8e\u76f4\u63a5\u7684\u4f20\u611f\u5668\u529b\u6d4b\u91cf\uff0c\u8fd8\u4f18\u4e8e\u5176\u4ed6\u5bf9\u6bd4\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65e0\u4f20\u611f\u5668\u529b\u611f\u77e5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "FD-VLA\u80fd\u591f\u4ee5\u66f4\u4f4e\u6210\u672c\u3001\u66f4\u9ad8\u6cdb\u5316\u6027\u4e3a\u673a\u5668\u4eba\u590d\u6742\u4efb\u52a1\u5e26\u6765\u51c6\u786e\u7684\u529b\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u4fc3\u8fdb\u4e86\u65e0\u529b\u4f20\u611f\u5668\u673a\u5668\u4eba\u5728\u5b9e\u9645\u64cd\u4f5c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.00490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00490", "abs": "https://arxiv.org/abs/2602.00490", "authors": ["Chia-Ming Lee", "Yu-Hao Ho", "Yu-Fan Lin", "Jen-Wei Lee", "Li-Wei Kang", "Chih-Chung Hsu"], "title": "HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion", "comment": "Accepted by ICASSP 2026", "summary": "Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u65e2\u63d0\u9ad8\u4e86\u6548\u7387\u53c8\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u516c\u5e03\u4e86\u4ee3\u7801\u3002", "motivation": "\u5f53\u524d\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u611f\u53d7\u91ce\u6709\u9650\u3001\u5149\u8c31\u5197\u4f59\u548c\u81ea\u6ce8\u610f\u529b\u7684\u9ad8\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u6548\u679c\u548c\u6548\u7387\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u7a7a\u95f4-\u5149\u8c31\u5bc6\u96c6\u76f8\u5173\u7f51\u7edc\uff08HSSDCT\uff09\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5206\u5c42\u5bc6\u96c6\u6b8b\u5deeTransformer\u6a21\u5757\uff08HDRTB\uff09\uff0c\u901a\u8fc7\u6269\u5c55\u7a97\u53e3\u548c\u5bc6\u96c6\u6b8b\u5dee\u8fde\u63a5\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\uff1b2\uff09\u7a7a\u95f4-\u5149\u8c31\u76f8\u5173\u5c42\uff08SSCL\uff09\uff0c\u5c06\u7a7a\u95f4\u548c\u5149\u8c31\u4f9d\u8d56\u56e0\u5b50\u5316\uff0c\u5c06\u81ea\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u964d\u4e3a\u7ebf\u6027\uff0c\u5e76\u7f13\u89e3\u5149\u8c31\u5197\u4f59\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u66f4\u4f18\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6c34\u5e73\u3002", "conclusion": "HSSDCT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01125", "abs": "https://arxiv.org/abs/2602.01125", "authors": ["Jichu Li", "Yilun Zhong", "Zhiting Li", "Feng Zhou", "Quyu Kong"], "title": "Long-range Modeling and Processing of Multimodal Event Sequences", "comment": null, "summary": "Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u65f6\u5e8f\u70b9\u8fc7\u7a0b\uff08TPP\uff09\u7ed3\u5408\uff0c\u6269\u5c55\u5230\u591a\u6a21\u6001\uff08\u5305\u62ec\u89c6\u89c9\uff09\u4fe1\u606f\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u538b\u7f29\u673a\u5236\u6539\u5584\u957f\u4e0a\u4e0b\u6587\u6587\u672c\u751f\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6587\u672c\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfTPP\u5df2\u5f00\u59cb\u6269\u5c55\u5904\u7406\u6587\u672c\u4fe1\u606f\uff0c\u4f46\u9762\u5bf9\u591a\u6a21\u6001\uff08\u5982\u89c6\u89c9+\u6587\u672c\uff09\u6570\u636e\u65f6\uff0c\u5e8f\u5217\u957f\u5ea6\u5927\u5e45\u589e\u52a0\uff0c\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u96be\u4ee5\u6709\u6548\u751f\u6210\u9700\u8981\u957f\u7a0b\u4f9d\u8d56\u7406\u89e3\u7684\u8fde\u8d2f\u6587\u672c\u63cf\u8ff0\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u4e0b\u7684\u957f\u5e8f\u5217\u5904\u7406\u548c\u5185\u5bb9\u751f\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u81f3\u89c6\u89c9\u6a21\u6001\u7684LLM-TPP\u6846\u67b6\uff0c\u5c06\u6587\u672c\u751f\u6210\u3001\u65f6\u95f4\u9884\u6d4b\u548c\u7c7b\u578b\u9884\u6d4b\u7ed3\u5408\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u65f6\u95f4\u76f8\u4f3c\u6027\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u538b\u7f29\u673a\u5236\uff0c\u538b\u7f29\u957f\u5e8f\u5217\u540c\u65f6\u4fdd\u6301\u5173\u952e\u4e8b\u4ef6\u6a21\u5f0f\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728\u538b\u7f29\u5e8f\u5217\u4e0a\u9884\u8bad\u7ec3\uff0c\u4e4b\u540e\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u5305\u62ec\u6709\u6311\u6218\u6027\u7684DanmakuTPP-QA\u57fa\u51c6\u5728\u5185\u7684\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u7387\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u65b9\u9762\u5747\u8d85\u8fc7\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001TPP\u6846\u67b6\u548c\u538b\u7f29\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u4e8b\u4ef6\u5e8f\u5217\u7684\u5efa\u6a21\u548c\u6587\u672c\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u4e8b\u4ef6\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02181", "abs": "https://arxiv.org/abs/2602.02181", "authors": ["Elad Siman Tov", "Nili E. Krausz"], "title": "Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls", "comment": "Submitted to 2026 IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)", "summary": "Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u548c\u5e94\u7528\u808c\u8089\u534f\u8c03\u89c4\u5f8b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5047\u80a2\u6b65\u6001\u7684\u80fd\u91cf\u6548\u7387\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0b\u80a2\u622a\u80a2\u8005\u6b65\u884c\u65f6\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u5de5\u5177\u7bb1\u8f85\u52a9\u7814\u7a76\u3002", "motivation": "\u52a8\u529b\u5047\u80a2\u867d\u80fd\u63d0\u4f9b\u80fd\u91cf\uff0c\u4f46\u51cf\u5c11\u622a\u80a2\u8005\u6b65\u884c\u4ee3\u8c22\u6210\u672c\u4f9d\u7136\u56f0\u96be\u3002\u5df2\u6709\u6b65\u6001\u534f\u8c03\u89c4\u5f8b\uff08ISC\uff09\u5728\u5065\u5eb7\u4eba\u6b65\u6001\u4e2d\u4e0e\u80fd\u91cf\u652f\u51fa\u76f8\u5173\uff0c\u4f46\u5728\u5047\u80a2\u5e94\u7528\u4e2d\u5206\u6790\u4e0d\u8db3\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u6316\u6398\u5176\u673a\u5236\u53ca\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u7528\u4e8e\u5206\u6790\u4e0b\u80a2\u4e09\u7ef4\u8fd0\u52a8\u5b66\u6570\u636e\u7684ISC\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u636e\u6b64\u6269\u5c55\u5230\u57fa\u4e8e\u529b\u77e9\u7684\u534f\u8c03\u89c4\u5f8b\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u5065\u5eb7\u4eba\u548c\u4e0d\u540c\u5047\u80a2\u7528\u6237\u7684\u6b65\u6001ISC\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u7ea6\u675f\u9884\u6d4b\u53ef\u8865\u507f\u88ab\u52a8\u8db3\u90e8\u5f71\u54cd\u7684\u80eb\u9aa8\u89d2\u5ea6\u548c\u529b\u77e9\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u867d\u7136\u65e0\u8bba\u91c7\u7528\u52a8\u529b\u6216\u88ab\u52a8\u5047\u80a2\uff0c\u6b65\u6001\u4e3b\u6210\u5206\uff08\u9ad8\u7a0b\u89d2\uff09\u5747\u4fdd\u6301\u5171\u9762\uff0c\u4f46\u57fa\u4e8e\u529b\u77e9\u7684\u534f\u8c03\u6027\u5728\u5047\u80a2\u4f7f\u7528\u8005\u4e2d\u4e0b\u964d\u3002\u901a\u8fc7ISC\u7ea6\u675f\u53ef\u9884\u6d4b\u5e76\u90e8\u5206\u8865\u507f\u88ab\u52a8\u5047\u80a2\u5bfc\u81f4\u7684\u6b65\u6001\u53d8\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u4e0e\u5de5\u5177\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u4f18\u5316\u622a\u80a2\u8005\u6b65\u6001\u534f\u8c03\uff0c\u4e3a\u52a8\u529b\u5047\u80a2\u7684\u63a7\u5236\u6539\u8fdb\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5e76\u652f\u6301\u540e\u7eed\u5bf9\u6b65\u884c\u795e\u7ecf\u63a7\u5236\u673a\u5236\u7684\u7814\u7a76\u3002"}}
{"id": "2602.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00504", "abs": "https://arxiv.org/abs/2602.00504", "authors": ["Jiahe Wu", "Bing Cao", "Qilong Wang", "Qinghua Hu", "Dongdong Li", "Pengfei Zhu"], "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RGBX-R1\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u5728RGB\u4ee5\u5916\u89c6\u89c9\u6a21\u6001\uff08\u5982\u7ea2\u5916\u3001\u6df1\u5ea6\u3001\u4e8b\u4ef6\u6570\u636e\uff09\u4e0b\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u7684RGBX-Grounding\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524dMLLM\u4e3b\u8981\u9884\u8bad\u7ec3\u4e8eRGB\u6a21\u6001\uff0c\u5bfc\u81f4\u5176\u5728\u7ea2\u5916\u3001\u6df1\u5ea6\u3001\u4e8b\u4ef6\u6570\u636e\u7b49\u975eRGB\u89c6\u89c9\u6a21\u6001\u4e0a\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u8fd9\u4e9b\u6a21\u6001\u5728\u590d\u6742\u573a\u666f\u4e0b\u6781\u4e3a\u91cd\u8981\u3002\u4e3a\u63d0\u5347\u6a21\u578b\u5bf9\u591a\u79cd\u89c6\u89c9\u6a21\u6001\u7684\u8ba4\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u9700\u6709\u65b0\u7684\u65b9\u6cd5\u548c\u8bc4\u4ef7\u4f53\u7cfb\u3002", "method": "\u63d0\u51faRGBX-R1\u6846\u67b6\uff0c\u5f15\u5165UAV\uff08Understand-Associate-Validate\uff09\u63d0\u793a\u7b56\u7565\u5f62\u6210\u89c6\u89c9\u6a21\u6001\u7684\u601d\u7ef4\u94fe\uff08VM-CoT\uff09\uff0c\u4ee5\u8fc1\u79fbRGB\u80fd\u529b\u5230\u5176\u4ed6\u6a21\u6001\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1aCS-SFT\uff08\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\uff09\u9636\u6bb5\u5229\u7528VM-CoT\u8fdb\u884c\u57fa\u672c\u6a21\u6001\u8ba4\u77e5\u8bad\u7ec3\uff1bST-RFT\uff08\u65f6\u7a7a\u5f3a\u5316\u5fae\u8c03\uff09\u9636\u6bb5\u7528\u57fa\u4e8eMuST\u5956\u52b1\u7684\u65f6\u7a7a\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6784\u5efa\u4e86\u9996\u4e2aRGBX-Grounding\u57fa\u51c6\uff0c\u5e76\u5728\u4e09\u9879RGBX\u5b9a\u4f4d\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u8d85\u8fc7\u57fa\u7ebf22.71%\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u7a7a\u95f4\u611f\u77e5\u4e0a\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "RGBX-R1\u663e\u8457\u63d0\u5347\u4e86MLLM\u5728\u591a\u6a21\u6001\u89c6\u89c9\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u975eRGB\u6570\u636e\u573a\u666f\u4e0b\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u6269\u5c55\u548c\u590d\u6742\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.01132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01132", "abs": "https://arxiv.org/abs/2602.01132", "authors": ["Abhilekh Borah", "Shubhra Ghosh", "Kedar Joshi", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation", "comment": "19 pages, 6 figures", "summary": "Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9762\u5bf9\u903b\u8f91\u7b49\u4ef7\u4f46\u8868\u8ff0\u88ab\u6df7\u6dc6\u7684\u95ee\u9898\u65f6\uff0c\u8868\u73b0\u5927\u5e45\u4e0b\u964d\uff0c\u663e\u793a\u7406\u89e3\u6df1\u5ea6\u4e0d\u8db3\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u8868\u8ff0\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u540c\u6837\u903b\u8f91\u4f46\u88ab\u91cd\u65b0\u8868\u8ff0\u6216\u6df7\u6dc6\u7684\u95ee\u9898\u8868\u73b0\u8f83\u5dee\u3002\u7814\u7a76\u8be5\u5f31\u70b9\u6709\u52a9\u4e8e\u63a8\u52a8\u6a21\u578b\u7684\u771f\u6b63\u7406\u89e3\u80fd\u529b\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86Logifus\u903b\u8f91\u6df7\u6dc6\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86LogiQAte\u8bca\u65ad\u57fa\u51c6\uff0c\u6db5\u76d6\u56db\u7c7b\u63a8\u7406\u4efb\u52a1\uff08\u6a21\u7cca\u5316\u4e00\u9636\u903b\u8f91\u3001\u8840\u7f18\u5173\u7cfb\u3001\u6570\u5b57\u5e8f\u5217\u3001\u65b9\u5411\u611f\uff09\uff0c\u5e76\u5bf9\u516d\u79cd\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u6d4b\u3002", "result": "\u6df7\u6dc6\u5316\u4efb\u52a1\u5bfc\u81f4\u4e3b\u6d41\u6a21\u578b\u96f6\u6837\u672c\u63a8\u7406\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff1aGPT-4o\u4e0b\u964d47%\u3001GPT-5\u4e0b\u964d27%\u3001o4-mini\u4e0b\u964d22%\u3002", "conclusion": "\u5f53\u524d\u5927\u6a21\u578b\u66f4\u591a\u662f\u5bf9\u8868\u9762\u5f62\u5f0f\u8fdb\u884c\u89e3\u6790\uff0c\u7f3a\u4e4f\u5bf9\u6df1\u5c42\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u6709\u5fc5\u8981\u6784\u5efa\u771f\u6b63\u7406\u89e3\u548c\u4fdd\u6301\u8bed\u4e49\u7684\u6a21\u578b\u3002"}}
{"id": "2602.02236", "categories": ["cs.RO", "cs.LG", "cs.NE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02236", "abs": "https://arxiv.org/abs/2602.02236", "authors": ["Julian Lemmel", "Felix Resch", "M\u00f3nika Farsang", "Ramin Hasani", "Daniela Rus", "Radu Grosu"], "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL", "comment": null, "summary": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5b9e\u65f6\u9012\u5f52\u5f3a\u5316\u5b66\u4e60\uff08RTRRL\uff09\u5bf9\u5df2\u9884\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u5728\u7ebf\u5fae\u8c03\uff0c\u63d0\u5347\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8f85\u52a9\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4eff\u771f\u548c\u771f\u5b9e\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u63a7\u5236\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5bb9\u6613\u56e0\u73af\u5883\u53d8\u5316\u6216\u7cfb\u7edf\u6f02\u79fb\u800c\u6027\u80fd\u8fc5\u901f\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u578b\u63a7\u5236\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002\u4e9f\u9700\u4e00\u79cd\u53ef\u4ee5\u4f7f\u7b56\u7565\u9002\u5e94\u65b0\u73af\u5883\u53d8\u5316\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u751f\u7269\u5b66\u4e0a\u53ef\u884c\u7684\u5b9e\u65f6\u9012\u5f52\u5f3a\u5316\u5b66\u4e60\uff08RTRRL\uff09\u7b97\u6cd5\uff0c\u5bf9\u9884\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u5728\u7ebf\u5fae\u8c03\uff0c\u540c\u65f6\u5f15\u5165\u6db2\u6001-\u963b\u6297\u7535\u5bb9\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08LRLC RNN\uff09\uff0c\u589e\u5f3a\u7cfb\u7edf\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u80fd\u529b\u3002\u65b9\u6cd5\u5728CarRacing\u4eff\u771f\u73af\u5883\u4ee5\u53ca\u914d\u5907\u4e8b\u4ef6\u6444\u50cf\u5934\u7684RoboRacer\u771f\u5b9e\u8f66\u9053\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7RTRRL\u4e0eLRLC RNN\u7684\u7ed3\u5408\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u81ea\u4e3b\u4f53\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u9ad8\u6548\u5728\u7ebf\u5fae\u8c03\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u5747\u63d0\u5347\u4e86\u63a7\u5236\u8868\u73b0\uff0c\u8bc1\u660e\u8be5\u95ed\u73af\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\u5728\u52a8\u6001\u6216\u73b0\u5b9e\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u5b66\u4e60\u578b\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00505", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00505", "abs": "https://arxiv.org/abs/2602.00505", "authors": ["Jingrui Zhang", "Feng Liang", "Yong Zhang", "Wei Wang", "Runhao Zeng", "Xiping Hu"], "title": "Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models", "comment": null, "summary": "With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SparseCut\uff0c\u4e00\u79cd\u6539\u8fdb\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u80fd\u529b\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u6377\u5f84\u63d0\u5347\u591a\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u7684\u6574\u5408\u6548\u7387\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u6269\u5c55\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u7b49\u8de8\u6a21\u6001\u77e5\u8bc6\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u5c24\u5176\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u635f\u5931\uff0c\u5f71\u54cd\u6a21\u578b\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51faSparseCut\u67b6\u6784\uff0c\u5728\u8de8\u6a21\u6001\u7f16\u7801\u5668\u4e0eLLM\u95f4\u5f15\u5165\u7a00\u758f\u6377\u5f84\u8fde\u63a5\uff0c\u5b9e\u73b0\u591a\u5c42\u6b21\uff08\u9ad8\u3001\u4e2d\u3001\u4f4e\u5c42\uff09\u89c6\u89c9\u7279\u5f81\u9ad8\u6548\u878d\u5408\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u9ad8\u6548\u591a\u7c92\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5728\u8f93\u9001\u524d\u878d\u5408\u89c6\u89c9\u7279\u5f81\uff0c\u4ee5\u4fdd\u62a4\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u8f93\u5165\u957f\u5ea6\u548c\u7b97\u529b\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSparseCut\u53ef\u660e\u663e\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u591a\u4e2a\u4e3b\u6d41\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5177\u5907\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u7684\u57fa\u7840\u8bed\u8a00\u5927\u6a21\u578b\u3002", "conclusion": "SparseCut\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u5c42\u6b21\u8bed\u4e49\u878d\u5408\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e2\u63d0\u5347\u4e86\u6a21\u578b\u7406\u89e3\u80fd\u529b\uff0c\u4e5f\u63a7\u5236\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u4f18\u8d28\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01161", "abs": "https://arxiv.org/abs/2602.01161", "authors": ["Reem I. Masoud", "Chen Feng", "Shunta Asano", "Saied Alshahrani", "Philip Colin Treleaven", "Miguel R. D. Rodrigues"], "title": "Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models", "comment": null, "summary": "The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6587\u5316\u9002\u5e94\u5fae\u8c03\u7684\u6570\u636e\u96c6\u5728\u8bed\u8a00\u5b66\u5c5e\u6027\u4e0a\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u7279\u5f81\u4e0e\u6a21\u578b\u6587\u5316\u8868\u73b0\u7684\u5173\u8054\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u68c0\u9a8c\u63ed\u793a\u4e0d\u540c\u5c5e\u6027\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u5927\u6a21\u578b\u5728\u5168\u7403\u8303\u56f4\u5185\u90e8\u7f72\uff0c\u4f46\u5176\u5728\u6587\u5316\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u51fa\u73b0\u6587\u5316\u9519\u4f4d\u7684\u95ee\u9898\uff0c\u800c\u5fae\u8c03\u6570\u636e\u96c6\u7684\u5177\u4f53\u8bed\u8a00\u5b66\u7279\u5f81\u4e0e\u6587\u5316\u5bf9\u9f50\u6548\u679c\u4e4b\u95f4\u7684\u5173\u7cfb\u8fd8\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793a\u5fae\u8c03\u6570\u636e\u96c6\u7684\u54ea\u4e9b\u5c5e\u6027\u5bf9\u6587\u5316\u9002\u5e94\u8d77\u5173\u952e\u4f5c\u7528\u3002", "method": "\u4f5c\u8005\u4ece\u6570\u636e\u96c6\u89d2\u5ea6\u51fa\u53d1\uff0c\u5bf9\u963f\u62c9\u4f2f\u8bed\u3001\u4e2d\u6587\u548c\u65e5\u8bed\u7684\u5fae\u8c03\u6570\u636e\u96c6\u5206\u522b\u8ba1\u7b97\u591a\u79cd\u8f7b\u91cf\u7ea7\u8bed\u8a00\u3001\u8bed\u4e49\u548c\u7ed3\u6784\u6307\u6807\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u5728\u6bcf\u79cd\u8bed\u8a00\u5185\u90e8\u964d\u7ef4\u63d0\u53d6\u4e3b\u8981\u53d8\u5316\u7ef4\u5ea6\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6570\u636e\u5b50\u96c6\u5bf9LLaMA\u3001Mistral\u3001DeepSeek\u4e09\u5927\u6a21\u578b\u5bb6\u65cf\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7528\u6587\u5316\u77e5\u8bc6\u3001\u4ef7\u503c\u89c2\u548c\u89c4\u8303\u57fa\u51c6\u96c6\u8bc4\u6d4b\u5176\u8868\u73b0\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u5206\u6790\u548c\u53d7\u63a7\u5b9e\u9a8c\u6570\u636e\u5b50\u96c6\u5e72\u9884\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5404\u4e3b\u6210\u5206\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u4e3b\u6210\u5206\uff08\u5982\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u8bcd\u6c47\u548c\u53e5\u6cd5\u591a\u6837\u6027\u3001\u8bcd\u6c47/\u7ed3\u6784\u4e30\u5bcc\u6027\uff09\u4e0e\u6a21\u578b\u4e0b\u6e38\u7684\u6587\u5316\u8868\u73b0\u6709\u76f8\u5173\u6027\uff0c\u4f46\u8fd9\u79cd\u76f8\u5173\u6027\u9ad8\u5ea6\u4f9d\u8d56\u6240\u7528\u6a21\u578b\u3002\u8bcd\u6c47\u4e3b\u5bfc\u7684\u6210\u5206\uff08PC3\uff09\u8868\u73b0\u8f83\u4e3a\u7a33\u5065\uff0c\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u548c\u57fa\u51c6\u4e2d\u5e26\u6765\u4e00\u81f4\u63d0\u5347\uff0c\u800c\u6ce8\u91cd\u8bed\u4e49\u8fde\u8d2f\u6027\u6216\u591a\u6837\u6027\u6781\u7aef\u7684\u6210\u5206\uff08PC1-PC2\uff09\u6548\u679c\u5f80\u5f80\u4e2d\u6027\u751a\u81f3\u8d1f\u9762\u3002", "conclusion": "\u5fae\u8c03\u6570\u636e\u96c6\u5728\u8bcd\u6c47\u548c\u7ed3\u6784\u4e30\u5bcc\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\u53ef\u4ee5\u66f4\u7a33\u5065\u63d0\u5347\u6a21\u578b\u6587\u5316\u9002\u5e94\u8868\u73b0\uff0c\u800c\u8fc7\u5ea6\u8ffd\u6c42\u8bed\u4e49\u6216\u591a\u6837\u6027\u5e76\u4e0d\u4e00\u5b9a\u6709\u5229\u3002\u6570\u636e\u96c6\u5c5e\u6027\u4e0e\u6a21\u578b\u7684\u4e92\u52a8\u9700\u7ed3\u5408\u5177\u4f53\u6a21\u578b\u8bc4\u4f30\uff0c\u5f3a\u8c03\u4e86\u66f4\u7ec6\u81f4\u3001\u5b9a\u5236\u5316\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\u5bf9\u6587\u5316\u9002\u5e94\u6027\u5927\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.02269", "categories": ["cs.RO", "cs.AI", "cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02269", "abs": "https://arxiv.org/abs/2602.02269", "authors": ["Jon \u0160kerlj", "Seongjin Bien", "Abdeldjallil Naceri", "Sami Haddadin"], "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eROS2\u7684\u591a\u673a\u5668\u4eba\u63a7\u5236\u5f00\u6e90\u6846\u67b6multipanda_ros2\uff0c\u4fa7\u91cd\u9ad8\u5b9e\u65f6\u6027\u548c\u7cbe\u786e\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5e73\u6ed1\u63a7\u5236\u5668\u5207\u6362\u3001\u9ad8\u7cbe\u5ea6\u4eff\u771f\u4e0e\u73b0\u5b9e\u73af\u5883\u7684\u4e00\u81f4\u6027\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u96be\u4ee5\u5b9e\u73b0\u9ad8\u9891\u5b9e\u65f6\uff081kHz\uff09\u591a\u81c2\u534f\u4f5c\uff0c\u4e14\u4eff\u771f\u4e0e\u5b9e\u9645\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e0b\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86multipanda_ros2\u67b6\u6784\uff0c\u5229\u7528ros2 control\u63a5\u53e3\u7edf\u4e00\u591a\u673a\u5668\u4eba\u63a7\u5236\uff1b\u63d0\u51facontrollet-feature\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5c06\u63a7\u5236\u5668\u5207\u6362\u5ef6\u8fdf\u538b\u7f29\u52302ms\u4ee5\u5185\uff1b\u96c6\u6210\u9ad8\u4fdd\u771fMuJoCo\u4eff\u771f\uff0c\u5b9e\u73b0\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u76f8\u5173\u7684\u5b9a\u91cf\u8bc4\u4f30\uff1b\u91c7\u7528\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u4f18\u5316\u7269\u7406\u4e00\u81f4\u6027\uff0c\u63d0\u5347sim2real\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7cfb\u7edf\u53ef\u5728\u5355\u8fdb\u7a0b\u4e0b\u7a33\u5b9a\u63a7\u5236\u591a\u53f0Franka\u673a\u5668\u4eba\uff0c1kHz\u5b9e\u65f6\u5faa\u73af\u4e0b\u5ef6\u8fdf\u6781\u4f4e\uff0c\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u63a7\u5236\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002\u901a\u8fc7\u60ef\u6027\u53c2\u6570\u8fa8\u8bc6\uff0c\u5b9e\u73b0\u4e86\u529b\u548c\u529b\u77e9\u7684\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "multipanda_ros2\u4e3a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u53ca\u9ad8\u96be\u4ea4\u4e92\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u53ef\u9760\u3001\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u4eff\u771f\u4e0e\u5b9e\u9645\u7684\u6027\u80fd\u5dee\uff0c\u5e76\u9002\u7528\u4e8e\u9ad8\u7ea7\u673a\u5668\u4eba\u7814\u7a76\u3002"}}
{"id": "2602.00508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00508", "abs": "https://arxiv.org/abs/2602.00508", "authors": ["Min Shi", "Xiaohui Zeng", "Jiannan Huang", "Yin Cui", "Francesco Ferroni", "Jialuo Li", "Shubham Pachori", "Zhaoshuo Li", "Yogesh Balaji", "Haoxiang Wang", "Tsung-Yi Lin", "Xiao Fu", "Yue Zhao", "Chieh-Yun Chen", "Ming-Yu Liu", "Humphrey Shi"], "title": "DuoGen: Towards General Purpose Interleaved Multimodal Generation", "comment": "Technical Report. Project Page: https://research.nvidia.com/labs/dir/duetgen/", "summary": "Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.", "AI": {"tldr": "DuoGen\u662f\u4e00\u79cd\u901a\u7528\u7684\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u65b0\u9896\u67b6\u6784\uff0c\u5728\u6587\u672c\u4e0e\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u548c\u6a21\u578b\u57fa\u5ea7\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5728\u901a\u7528\u6307\u4ee4\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u548c\u56fe\u50cf\u3002", "method": "\u4f5c\u8005\u63d0\u51faDuoGen\uff0c\u5305\u62ec\u4e09\u5927\u521b\u65b0\uff1a\uff081\uff09\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7f51\u7ad9\u4fe1\u606f\u548c\u5408\u6210\u4efb\u52a1\uff1b\uff082\uff09\u67b6\u6784\u4e0a\u7ed3\u5408\u4e86\u5f3a\u89c6\u89c9\u7406\u89e3\u7684\u591a\u6a21\u6001LLM\u548c\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u80fd\u529b\uff0c\u91c7\u7528\u89e3\u8026\u4e24\u9636\u6bb5\u65b9\u6cd5\u5148\u8c03\u4f18MLLM\u518d\u5bf9\u9f50DiT\u4ee5\u751f\u6210\u4ea4\u9519\u56fe\u6587\u5e8f\u5217\uff1b\uff083\uff09\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "\u5728\u516c\u5f00\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u4e0a\uff0cDuoGen\u5728\u6587\u672c\u8d28\u91cf\u3001\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u56fe\u6587\u4e00\u81f4\u6027\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "conclusion": "DuoGen\u901a\u8fc7\u7cfb\u7edf\u7684\u6570\u636e\u3001\u67b6\u6784\u548c\u8bc4\u6d4b\u521b\u65b0\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u5e7f\u6cdb\u573a\u666f\u4e0b\u7684\u7edf\u4e00\u751f\u6210\u4efb\u52a1\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01162", "abs": "https://arxiv.org/abs/2602.01162", "authors": ["Nipuna Abeykoon", "Ashen Weerathunga", "Pubudu Wijesinghe", "Parameswari Krishnamurthy"], "title": "Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages", "comment": null, "summary": "Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7c7b\u578b\u5b66\u7684\u6846\u67b6\uff0c\u65e0\u9700\u5e73\u884c\u8bed\u6599\u6216\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u5373\u53ef\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7684\u7ed3\u6784\u5408\u89c4\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4ee5\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e3a\u4e3b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\uff0c\u5bb9\u6613\u5e26\u5165\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u8868\u8fbe\u548c\u7ed3\u6784\u504f\u89c1\uff0c\u9020\u6210\u7ed3\u6784\u4e0d\u7b26\uff0c\u4e14\u96be\u4ee5\u7528\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\u7684\u6846\u67b6\uff1a1) \u901a\u7528\u5143\u8bed\u8a00\u6846\u67b6\uff08UMF\uff09\uff0c\u901a\u8fc716\u4e2a\u7c7b\u578b\u5b66\u7ef4\u5ea6\u53ca\u52a0\u6743\u5dee\u5f02\u5f97\u5206\u4e3a\u4e0d\u540c\u8bed\u8a00\u5efa\u7acb\u7ed3\u6784\u5316\u6863\u6848\uff1b2) \u8ba1\u7b97\u5f15\u64ce\uff0c\u901a\u8fc7\u751f\u6210\u9636\u6bb5\u7684\u8bed\u8a00\u6b67\u4e49\u6d88\u89e3\u4e0e\u9009\u62e9\u9636\u6bb5\u7684\u7c7b\u578b\u5b66\u5408\u89c4\u8bc4\u5206\uff0c\u5bf9\u5019\u9009\u7ffb\u8bd1\u8fdb\u884c\u5904\u7406\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5e73\u884c\u8bed\u6599\u548c\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u652f\u6301\u4efb\u4f55\u80fd\u751f\u6210\u591a\u5019\u9009\u7684\u5927\u6a21\u578b\u3002", "result": "\u5728\u4e5d\u4e2a\u8bed\u5bf9\u3001341\u4e2a\u5305\u542b\u4e0d\u540c\u5f62\u6001\u548c\u53e5\u6cd5\u73b0\u8c61\u7684\u82f1\u8bed\u53e5\u5b50\u4e0a\u8bc4\u6d4b\uff0c\u5e72\u9884\u7387\u4e0e\u82f1\u8bed\u7c7b\u578b\u8ddd\u79bb\u6b63\u76f8\u5173\u3002\u5bf9\u4e0d\u540c\u7c7b\u578b\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e72\u9884\u7cbe\u51c6\u7387\u5206\u522b\u8fbe48.16%\u300128.15%\u300186.26%\u3002", "conclusion": "\u8fd9\u4e00\u6846\u67b6\u65e0\u9700\u989d\u5916\u5e73\u884c\u8bed\u6599\u548c\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ed3\u6784\u5408\u89c4\u6027\u548c\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5177\u5907\u843d\u5730\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.02331", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02331", "abs": "https://arxiv.org/abs/2602.02331", "authors": ["Shaoting Zhu", "Baijun Ye", "Jiaxuan Wang", "Jiakang Chen", "Ziwen Zhuang", "Linzhan Mou", "Runhan Huang", "Hang Zhao"], "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour", "comment": "Project Page: https://ttt-parkour.github.io/", "summary": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u5feb\u901f\u8bad\u7ec3\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u5e76\u901a\u8fc7\u6781\u4e3a\u590d\u6742\u3001\u4ece\u672a\u89c1\u8fc7\u7684\u5730\u5f62\uff0c\u5b9e\u73b0\u52a8\u6001\u8dd1\u9177\u3002\u6838\u5fc3\u6d41\u7a0b\u5305\u62ec\u5148\u5728\u865a\u62df\u591a\u6837\u5730\u5f62\u9884\u8bad\u7ec3\uff0c\u518d\u901a\u8fc7RGB-D\u91cd\u5efa\u7684\u771f\u5b9e\u5730\u5f62\u5feb\u901f\u5fae\u8c03\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u901a\u7528\u7684\u673a\u5668\u4eba\u884c\u8d70\u7b56\u7565\u867d\u7136\u80fd\u5e94\u5bf9\u591a\u6837\u5730\u5f62\uff0c\u4f46\u9762\u5bf9\u590d\u6742\u751a\u81f3\u5168\u65b0\u73af\u5883\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u4e3a\u6269\u5c55\u673a\u5668\u4eba\u5728\u6781\u7aef\u5730\u5f62\u7684\u9002\u5e94\u80fd\u529b\uff0c\u63d0\u9ad8\u5b9e\u9645\u843d\u5730\u6548\u80fd\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u3002", "method": "\u91c7\u7528Real-to-Sim-to-Real\u8bad\u7ec3\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u591a\u6837\u5730\u5f62\u4e0a\u79bb\u7ebf\u9884\u8bad\u7ec3\u7b56\u7565\uff1b2\uff09\u9488\u5bf9\u771f\u5b9e\u65b0\u5730\u5f62\uff0c\u901a\u8fc7RGB-D\u6570\u636e\u91cd\u5efa\u9ad8\u4fdd\u771f\u5730\u5f62\u7f51\u683c\uff0c\u5728\u7ebf\u9ad8\u901f\u5fae\u8c03\u7b56\u7565\uff08\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0cTTT\uff09\u3002\u6574\u4e2a\u91c7\u96c6-\u91cd\u5efa-\u5fae\u8c03\u6d41\u7a0b10\u5206\u949f\u5185\u5b8c\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7TTT\u5fae\u8c03\u540e\u7684\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u5728\u590d\u6742\u969c\u788d\uff08\u6954\u5f62\u7269\u3001\u67f1\u6869\u3001\u7bb1\u4f53\u3001\u68af\u5f62\u3001\u7a84\u6881\uff09\u4e0a\u5b9e\u73b0\u9c81\u68d2\u901a\u884c\uff0c\u7b56\u7565\u5177\u6709\u5f88\u597d\u7684\u96f6\u6837\u672c\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TTT-Parkour\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u590d\u6742\u65b0\u5730\u5f62\u4e2d\u5b8c\u6210\u9ad8\u96be\u5ea6\u52a8\u6001\u79fb\u52a8\u7684\u80fd\u529b\uff0c\u4e3a\u62d3\u5c55\u673a\u5668\u4eba\u9002\u5e94\u73b0\u5b9e\u6781\u7aef\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.00516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00516", "abs": "https://arxiv.org/abs/2602.00516", "authors": ["Kunal Mahatha", "Jose Dolz", "Christian Desrosiers"], "title": "SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation", "comment": null, "summary": "We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65e0\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u5272\u4efb\u52a1\u5efa\u6a21\u4e3a\u6269\u6563\u8bf1\u5bfc\u7684\u4eb2\u548c\u56fe\u4e0a\u7684\u968f\u673a\u6d41\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u4fee\u526a\u7684\u9a6c\u5c14\u53ef\u592b\u4f20\u64ad\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u8fb9\u754c\u9510\u5ea6\u548cmask\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u53d6\u5f97\u4e86SOTA\u96f6\u6837\u672c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u5206\u5272\u65b9\u6cd5\u666e\u904d\u57fa\u4e8e\u5149\u8c31\u56fe\u5212\u5206\uff08\u4f9d\u8d56\u6269\u6563\u5f97\u5230\u7684\u4eb2\u548c\u529b\uff09\uff0c\u5b58\u5728\u9700\u9884\u5148\u8bbe\u5b9a\u805a\u7c7b\u6570\u3001\u8fb9\u754c\u8fc7\u5ea6\u5e73\u6ed1\u3001\u5bf9\u566a\u58f0\u548c\u591a\u5cf0\u4eb2\u548c\u5206\u5e03\u654f\u611f\u7b49\u95ee\u9898\uff0c\u540c\u65f6\u5ffd\u89c6\u4e86\u5bf9\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u7684\u5efa\u6a21\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e9b\u5f0a\u7aef\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6\u4eb2\u548c\u7ed3\u6784\u5efa\u6a21\u548c\u4fe1\u606f\u4f20\u64ad\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u5c06\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5efa\u6a21\u4e3a\u6269\u6563\u8bf1\u5bfc\u4eb2\u548c\u56fe\u4e0a\u7684\u968f\u673a\u6d41\u5e73\u8861\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u7ed3\u5408\u5c40\u90e8\u90bb\u57df\uff0c\u63d0\u53d6\u7a00\u758f\u800c\u8868\u73b0\u529b\u5f3a\u7684\u4eb2\u548c\u7ed3\u6784\uff1b2\uff09\u8bbe\u8ba1\u5e26\u6709\u81ea\u9002\u5e94\u4fee\u526a\u7b56\u7565\u7684\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684Markov\u4f20\u64ad\u673a\u5236\uff0c\u6291\u5236\u4e0d\u53ef\u9760\u8f6c\u79fb\u3001\u589e\u5f3a\u786e\u4fe1\u4eb2\u548c\u8def\u5f84\u3002", "result": "\u57287\u4e2a\u4e3b\u6d41\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u96f6\u6837\u672c\u5206\u5272\u6027\u80fd\u8d85\u8d8a\u6b64\u524d\u6240\u6709\u5149\u8c31\u805a\u7c7b\u7c7b\u65b9\u6cd5\uff0c\u5206\u5272\u8fb9\u754c\u66f4\u9510\u5229\uff0c\u533a\u57df\u66f4\u8fde\u8d2f\uff0cmask\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u4e86\u4f20\u7edf\u5149\u8c31\u5206\u5272\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u6ce8\u91cd\u5c40\u90e8\u90bb\u57df\u4e0e\u5168\u5c40\u6269\u6563\u7684\u7ed3\u5408\u3001\u53ca\u968f\u673a\u6e38\u8d70\u4e2d\u5173\u952e\u8def\u5f84\u5f3a\u5316\uff0c\u5e26\u6765\u4e86\u8bad\u7ec3\u81ea\u7531\u5206\u5272\u65b9\u6cd5\u7684\u65b0\u6027\u80fd\u4e0a\u9650\u3002"}}
{"id": "2602.01169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01169", "abs": "https://arxiv.org/abs/2602.01169", "authors": ["Shahem Sultan", "Shahem Fadi", "Yousef Melhim", "Ibrahim Alsarraj", "Besher Hassan"], "title": "PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues", "comment": "8 pages, 5 figures", "summary": "This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPedagoSense\u7684\u7cfb\u7edf\uff0c\u53ef\u4ee5\u68c0\u6d4b\u548c\u63a8\u8350\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b26\u5408\u7b56\u7565\u7684\u5bf9\u8bdd\u56de\u590d\uff0c\u4ece\u800c\u63d0\u5347\u5e08\u751f\u5bf9\u8bdd\u7684\u6559\u5b66\u4e92\u52a8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u7cfb\u7edf\u96be\u4ee5\u81ea\u52a8\u68c0\u6d4b\u548c\u63a8\u8350\u6709\u6548\u7684\u6559\u5b66\u7b56\u7565\uff0c\u56e0\u6b64\u96be\u4ee5\u63d0\u5347\u5e08\u751f\u4ea4\u6d41\u7684\u6559\u5b66\u6210\u6548\u548c\u4e2a\u6027\u5316\u6c34\u5e73\u3002", "method": "PedagoSense\u7cfb\u7edf\u9996\u5148\u901a\u8fc7\u4e8c\u5206\u7c7b\u6a21\u578b\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u662f\u5426\u5b58\u5728\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u8fdb\u4e00\u6b65\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4ee5\u8bc6\u522b\u5177\u4f53\u7b56\u7565\u7c7b\u578b\u3002\u540c\u65f6\uff0c\u7cfb\u7edf\u7ed3\u5408\u5bf9\u8bdd\u8bed\u5883\u63a8\u8350\u5408\u9002\u7684\u6559\u5b66\u7b56\u7565\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0e\u8be5\u7b56\u7565\u4e00\u81f4\u7684\u56de\u590d\u3002", "result": "\u5728\u4eba\u7c7b\u6807\u6ce8\u7684\u5e08\u751f\u5bf9\u8bdd\u6570\u636e\u53ca\u6269\u5145\u7684\u975e\u6559\u5b66\u5bf9\u8bdd\u6570\u636e\u4e0a\uff0c\u7cfb\u7edf\u5728\u6559\u5b66\u7b56\u7565\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u6570\u636e\u589e\u5f3a\u65f6\u83b7\u5f97\u6301\u7eed\u63d0\u5347\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7b56\u7565\u5206\u7c7b\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "PedagoSense\u6210\u529f\u7ed3\u5408\u6559\u5b66\u7406\u8bba\u4e0eLLM\u5bf9\u8bdd\u751f\u6210\uff0c\u4e3a\u81ea\u9002\u5e94\u6559\u80b2\u6280\u672f\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6559\u5b66\u7b56\u7565\u652f\u6301\uff0c\u63a8\u52a8\u667a\u80fd\u6559\u80b2\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.02389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02389", "abs": "https://arxiv.org/abs/2602.02389", "authors": ["Marina Ruediger", "Ashis G. Banerjee"], "title": "Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures", "comment": "This paper will appear in the proceedings of the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408SLAM\u6570\u636e\u7684\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u4efb\u52a1\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u5148\u9a8c\u51e0\u4f55\u77e5\u8bc6\u4e0b\u7684\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u68c0\u6d4b\u901a\u5e38\u7f3a\u4e4f\u5148\u9a8c\u7684\u7cbe\u786e\u51e0\u4f55\u4fe1\u606f\uff0c\u4f20\u7edf\u4efb\u52a1\u5206\u914d\u548c\u8def\u5f84\u89c4\u5212\u65b9\u5f0f\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u8986\u76d6\u68c0\u6d4b\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u590d\u6742\u6216\u672a\u77e5\u73af\u5883\u3002\u4e3a\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u81ea\u4e3b\u68c0\u6d4b\u80fd\u529b\uff0c\u6025\u9700\u65b0\u7684\u4efb\u52a1\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u591a\u673a\u5668\u4ebaSLAM\u83b7\u53d6\u7684\u73af\u5883\u7f51\u683c\u4fe1\u606f\uff0c\u7ed3\u5408\u786c\u4ef6\u53c2\u6570\u4e0e\u73af\u5883\u6761\u4ef6\uff0c\u81ea\u52a8\u751f\u6210\u68c0\u6d4b\u4efb\u52a1\u3002\u91c7\u7528\u9884\u671f\u5173\u952e\u70b9\u5f97\u5206\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u526a\u679d\u65b9\u6cd5\u8fdb\u884c\u4efb\u52a1\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u6c34\u4e0b\u5b9e\u5730\u6d4b\u8bd5\u8bc4\u4f30\u7b97\u6cd5\u6548\u679c\uff0c\u518d\u4e0eVoronoi\u5206\u533a\u548c\u725b\u8015\uff08boustrophedon\uff09\u65b9\u5f0f\u8fdb\u884c\u68c0\u6d4b\u8986\u76d6\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u672a\u63d0\u524d\u83b7\u5f97\u73af\u5883\u51e0\u4f55\u4fe1\u606f\u60c5\u51b5\u4e0b\uff0c\u80fd\u81ea\u52a8\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u79d1\u5b66\u5206\u914d\u68c0\u6d4b\u4efb\u52a1\u3002\u4e0e\u4f20\u7edf\u5206\u533a\u6cd5\u76f8\u6bd4\uff0c\u8986\u76d6\u8303\u56f4\u66f4\u4f18\u4e14\u66f4\u6613\u5173\u6ce8\u53ef\u80fd\u51fa\u73b0\u7f3a\u9677\u7684\u4f4d\u7f6e\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u73af\u5883\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u7684\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u68c0\u6d4b\u4efb\u52a1\u5206\u914d\uff0c\u63d0\u5347\u4e86\u81ea\u9002\u5e94\u80fd\u529b\u548c\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u548c\u672a\u77e5\u73af\u5883\u3002"}}
{"id": "2602.00522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00522", "abs": "https://arxiv.org/abs/2602.00522", "authors": ["Chaoran Xu", "Chengkan Lv", "Qiyu Chen", "Feng Zhang", "Zhengtao Zhang"], "title": "MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval", "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5MRAD\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5316\u7684\u8bb0\u5fc6\u68c0\u7d22\u66ff\u4ee3\u4ee5\u5f80\u7684\u6a21\u578b\u62df\u5408\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u8de8\u9886\u57df\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5e38\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u590d\u6742\u7684\u63d0\u793a\u5b66\u4e60\u6216\u62df\u5408\u6280\u5de7\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u63a8\u7406\u8fc7\u7a0b\u8017\u65f6\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u4e14\u8de8\u9886\u57df\u7a33\u5b9a\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "MRAD\u65b9\u6cd5\u57fa\u4e8e\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5229\u7528\u8f85\u52a9\u6570\u636e\u6784\u5efa\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u7684\u4e24\u5c42\u7279\u5f81\u8bb0\u5fc6\u5e93\uff0c\u901a\u8fc7\u68c0\u7d22\u7279\u5f81-\u6807\u7b7e\u5bf9\u76f4\u63a5\u83b7\u53d6\u5f02\u5e38\u5f97\u5206\u3002\u63d0\u51fa\u4e09\u79cd\u5b9e\u73b0\uff1a\u65e0\u8bad\u7ec3\u7684MRAD-TF\u3001\u901a\u8fc7\u7ebf\u6027\u5c42\u5fae\u8c03\u68c0\u7d22\u5ea6\u91cf\u7684MRAD-FT\u3001\u5728\u8bed\u8a00\u63d0\u793a\u4e2d\u52a8\u6001\u6ce8\u5165\u5148\u9a8c\u504f\u7f6e\u589e\u5f3a\u6cdb\u5316\u7684MRAD-CLIP\u3002", "result": "MRAD\u6846\u67b6\u572816\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u6027\u80fd\uff0c\u65e0\u8bba\u5728\u65e0\u8bad\u7ec3\u8fd8\u662f\u5fae\u8c03\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MRAD\u8bc1\u660e\u5145\u5206\u5229\u7528\u539f\u59cb\u6570\u636e\u7684\u7ecf\u9a8c\u5206\u5e03\uff08\u8bb0\u5fc6\u5e93\u68c0\u7d22\uff09\u800c\u975e\u4ec5\u9760\u6a21\u578b\u62df\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01170", "abs": "https://arxiv.org/abs/2602.01170", "authors": ["Besher Hassan", "Ibrahim Alsarraj", "Musaab Hasan", "Yousef Melhim", "Shahem Fadi", "Shahem Sultan"], "title": "EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech", "comment": "10 pages, 3 figures", "summary": "This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86EmoAra\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u60c5\u611f\u4fdd\u6301\u8de8\u8bed\u8a00\u53e3\u8bed\u4ea4\u6d41\u7cfb\u7edf\uff0c\u4e3b\u8981\u5e94\u7528\u4e8e\u94f6\u884c\u5ba2\u6237\u670d\u52a1\u9886\u57df\uff0c\u5b9e\u73b0\u82f1\u8bed\u8bed\u97f3\u5230\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u7684\u60c5\u611f\u4fdd\u6301\u4f20\u9012\u3002", "motivation": "\u94f6\u884c\u5ba2\u6237\u670d\u52a1\u4e2d\uff0c\u51c6\u786e\u4f20\u9012\u7528\u6237\u7684\u60c5\u611f\u5bf9\u4e8e\u670d\u52a1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8de8\u8bed\u8a00\u8bed\u97f3\u7cfb\u7edf\u5f80\u5f80\u5ffd\u7565\u4e86\u60c5\u611f\u4f20\u9012\uff0c\u5bfc\u81f4\u4fe1\u606f\u548c\u670d\u52a1\u4f53\u9a8c\u7684\u635f\u5931\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u4fdd\u6301\u60c5\u611f\u7684\u8de8\u8bed\u8a00\u4ea4\u6d41\u7cfb\u7edf\u3002", "method": "EmoAra\u6574\u5408\u4e86\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08CNN\u5206\u7c7b\u5668\uff09\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08Whisper\uff09\u3001\u673a\u5668\u7ffb\u8bd1\uff08\u5fae\u8c03MarianMT\uff09\u3001\u6587\u672c\u8f6c\u8bed\u97f3\uff08MMS-TTS-Ara\uff09\u591a\u4e2a\u6a21\u5757\uff0c\u5b9e\u73b0\u82f1\u8bed\u8bed\u97f3\u8f93\u5165\u5230\u4fdd\u7559\u60c5\u611f\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u8f93\u51fa\u3002", "result": "\u60c5\u611f\u8bc6\u522bF1\u503c\u8fbe\u523094%\uff0c\u7ffb\u8bd1BLEU\u4e3a56\uff0cBERTScore F1\u4e3a88.7%\u3002\u5728\u4eba\u7c7b\u94f6\u884c\u9886\u57df\u7ffb\u8bd1\u8bc4\u4ef7\u4e2d\u5e73\u5747\u5f97\u5206\u4e3a81%\u3002", "conclusion": "EmoAra\u80fd\u591f\u5728\u8de8\u8bed\u8a00\u53e3\u8bed\u4ea4\u6d41\u4e2d\u6709\u6548\u4fdd\u6301\u60c5\u611f\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u94f6\u884c\u5ba2\u6237\u670d\u52a1\u4e2d\u60c5\u611f\u4f20\u9012\u7684\u6548\u679c\u3002\u5b9e\u73b0\u7ec6\u8282\u548c\u8d44\u6e90\u5df2\u7ecf\u5728GitHub\u516c\u5f00\u3002"}}
{"id": "2602.02396", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02396", "abs": "https://arxiv.org/abs/2602.02396", "authors": ["Amisha Bhaskar", "Pratap Tokekar", "Stefano Di Cairano", "Alexander Schperberg"], "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning", "comment": "10 pages main text and 4 figures, and 11 pages appendix and 10 figures, total 21 pages and 14 figures", "summary": "Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u9ad8\u6548\u591a\u611f\u77e5\u4eff\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u6279\u91cf\u5168\u5c40\u62d2\u7edd\u91c7\u6837\u7684IMLE\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u751f\u6210\u5668\u548c\u591a\u6a21\u6001\u611f\u77e5\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9891\u7387\u3001\u4f4e\u5ef6\u8fdf\u4e14\u8868\u73b0\u4f18\u5f02\u7684\u76ee\u6807\u64cd\u4f5c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\u3001\u6d41\u5339\u914d\u3001IMLE\uff09\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u6355\u6349\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u3001\u5b9e\u65f6\u9ad8\u9891\u63a7\u5236\u548c\u878d\u5408\u591a\u4f20\u611f\u8f93\u5165\u7684\u5168\u90e8\u9700\u6c42\u3002", "method": "\u63d0\u51faPRISM\u7b56\u7565\uff0c\u57fa\u4e8e\u6279\u91cf\u5168\u5c40\u62d2\u7edd\u91c7\u6837\u53d8\u4f53\u7684IMLE\uff0c\u7ed3\u5408\u65f6\u95f4\u591a\u611f\u77e5\u7f16\u7801\u5668\uff08\u878d\u5408RGB\u3001\u6df1\u5ea6\u3001\u89e6\u89c9\u3001\u97f3\u9891\u548c\u672c\u4f53\u611f\u77e5\uff09\u4e0ePerformer\u67b6\u6784\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u5355\u6b65\u63a8\u7406\u3002", "result": "\u5728Unitree Go2\u3001UR5\u7b49\u786c\u4ef6\u4e0a\uff0cPRISM\u5728\u590d\u6742\u7269\u7406\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u8d85\u8fc7\u6700\u5148\u8fdb\u6269\u6563\u7b56\u756510%-25%\uff0c\u4e14\u4ee530-50Hz\u9ad8\u9891\u7387\u95ed\u73af\u63a7\u5236\u3002\u5728CALVIN\u7b49\u5927\u578b\u4eff\u771f\u57fa\u51c6\u4e0a\uff0cPRISM\u6bd4\u6269\u6563\u3001\u6d41\u5339\u914d\u7b56\u7565\u5206\u522b\u63d0\u9ad8\u7ea625%\u300120%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5c06\u8f68\u8ff9\u6296\u52a8\u51cf\u5c1120-50\u500d\u3002", "conclusion": "PRISM\u80fd\u4ee5\u66f4\u5feb\u901f\u5ea6\u3001\u66f4\u51c6\u8868\u73b0\u4ee5\u53ca\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u8986\u76d6\uff0c\u5728\u673a\u5668\u4eba\u4eff\u4eba\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6269\u6563\u5f0f\u6a21\u578b\uff0c\u907f\u514d\u4e86\u8fed\u4ee3\u91c7\u6837\u5e26\u6765\u7684\u9ad8\u5ef6\u8fdf\u3002"}}
{"id": "2602.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00523", "abs": "https://arxiv.org/abs/2602.00523", "authors": ["Yujia Tong", "Tian Zhang", "Yunyang Wan", "Kaiwei Lin", "Jingling Yuan", "Chuang Hu"], "title": "SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\\times$ for Qwen2.5-VL-72B.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u51b3\u7b56\u6811\u7ed3\u6784\u7684\u63a8\u6d4b\u5f0f\u89e3\u7801\u52a0\u901f\u65b9\u6cd5SAGE\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u66f4\u6709\u6548\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u5f0f\u89e3\u7801\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u6811\u7ed3\u6784\uff0c\u4e0d\u80fd\u81ea\u9002\u5e94\u5e94\u5bf9\u4e0d\u540c\u751f\u6210\u6b65\u9aa4\u4e0a\u7684\u9884\u6d4b\u96be\u5ea6\uff0c\u5bfc\u81f4\u63a5\u7eb3\u957f\u5ea6\u548c\u52a0\u901f\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "SAGE\u6846\u67b6\u5229\u7528\u8f93\u51fa\u71b5\u4f5c\u4e3a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u52a8\u6001\u8c03\u8282\u63a8\u6d4b\u6811\u7ed3\u6784\uff1a\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u7528\u66f4\u6df1\u66f4\u7a84\u7684\u6811\u4ee5\u52a0\u5927\u63a8\u6d4b\u6df1\u5ea6\uff0c\u7f6e\u4fe1\u5ea6\u4f4e\u5219\u7528\u66f4\u6d45\u66f4\u5bbd\u7684\u6811\u6765\u589e\u52a0\u63a2\u7d22\u591a\u6837\u6027\u3002", "result": "SAGE\u63d0\u9ad8\u4e86\u5e73\u5747\u63a5\u7eb3\u957f\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u7ebf\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8fdc\u8d85\u9759\u6001\u6811\u65b9\u6cd5\u7684\u63a8\u7406\u52a0\u901f\u3002\u5b9e\u9a8c\u4e2d\uff0c\u65e0\u8f93\u51fa\u8d28\u91cf\u635f\u5931\u4e0b\uff0cLLaVA-OneVision-72B\u52a0\u901f\u8fbe3.36\u500d\uff0cQwen2.5-VL-72B\u52a0\u901f\u8fbe3.18\u500d\u3002", "conclusion": "SAGE\u53ef\u5b9e\u73b0\u63a8\u6d4b\u89e3\u7801\u901f\u5ea6\u7684\u663e\u8457\u63d0\u5347\uff0c\u65e0\u9700\u727a\u7272\u8f93\u51fa\u8d28\u91cf\uff0c\u5c55\u73b0\u4e86\u6309\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u63a8\u6d4b\u7ed3\u6784\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01193", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01193", "abs": "https://arxiv.org/abs/2602.01193", "authors": ["Shashini Nilukshi", "Deshan Sumanathilaka"], "title": "Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation", "comment": "2 figures, 2 Tables, Accepted at IEEE TIC 2026", "summary": "This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bcd\u4e49\u6d88\u6b67\uff08VWSD\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f3a\u8c03\u591a\u6a21\u6001\u624b\u6bb5\u5728\u8bcd\u4e49\u6d88\u6b67\u4e0a\u7684\u63d0\u5347\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e3b\u8981\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u8bcd\u4e49\u6d88\u6b67\u4ec5\u5229\u7528\u6587\u672c\u548c\u8bcd\u5178\u8d44\u6e90\uff0c\u96be\u4ee5\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u4efb\u52a1\u4e2d\u7684\u8bcd\u4e49\u6b67\u4e49\u95ee\u9898\u3002VWSD \u5f15\u5165\u89c6\u89c9\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u5224\u522b\u8bcd\u4e49\uff0c\u6ee1\u8db3\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u5e94\u7528\u9700\u6c42\u3002", "method": "\u68b3\u7406\u4e862016\u81f32025\u5e74VWSD\u9886\u57df\u7684\u4ee3\u8868\u6027\u7814\u7a76\uff0c\u5305\u62ec\u57fa\u4e8e\u7279\u5f81\u3001\u56fe\u7ed3\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff1b\u91cd\u70b9\u5206\u6790\u4e86CLIP\u7b49\u5bf9\u6bd4\u6a21\u578b\u3001\u6269\u6563\u5f0f\u6587\u672c\u751f\u6210\u3001LLM\u652f\u6301\u4e0b\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u53caprompt\u5de5\u7a0b\u3001\u591a\u8bed\u79cd\u9002\u5e94\u548c\u6a21\u578b\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5b9a\u91cf\u7ed3\u679c\u663e\u793a\uff0cCLIP\u5fae\u8c03\u6a21\u578b\u53ca\u6574\u5408LLM\u7684VWSD\u7cfb\u7edf\u5728\u51c6\u786e\u7387\uff08MRR\uff09\u4e0a\u8f83\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u53476-8\uff05\u3002", "conclusion": "VWSD\u5df2\u53d6\u5f97\u5b9e\u8d28\u8fdb\u5c55\uff0c\u4f46\u591a\u8bed\u79cd\u6570\u636e\u532e\u4e4f\u3001\u4e0a\u4e0b\u6587\u5c40\u9650\u3001\u6a21\u578b\u504f\u89c1\u53ca\u8bc4\u6d4b\u4f53\u7cfb\u4e0d\u8db3\u7b49\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002\u672a\u6765\u7814\u7a76\u5c06\u7740\u91cd\u5728CLIP\u5bf9\u9f50\u3001\u6269\u6563\u751f\u6210\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u878d\u5408\uff0c\u5b9e\u73b0\u66f4\u5f3a\u3001\u5177\u60c5\u5883\u611f\u77e5\u3001\u591a\u8bed\u79cd\u7684\u6d88\u6b67\u80fd\u529b\u3002"}}
{"id": "2602.02402", "categories": ["cs.RO", "cs.AI", "cs.CV", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.02402", "abs": "https://arxiv.org/abs/2602.02402", "authors": ["Mu Huang", "Hui Wang", "Kerui Ren", "Linning Xu", "Yunsong Zhou", "Mulin Yu", "Bo Dai", "Jiangmiao Pang"], "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation", "comment": "Project page: https://city-super.github.io/SoMA/", "summary": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.", "AI": {"tldr": "SoMA\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6591\u70b9\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8f6f\u4f53\u7269\u4f53\u64cd\u7eb5\u4eff\u771f\uff0c\u63d0\u9ad8\u4e86\u4eff\u771f\u7cbe\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u7a33\u5b9a\u5904\u7406\u8bf8\u5982\u957f\u65f6\u95f4\u5e03\u6599\u6298\u53e0\u7b49\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u8f6f\u4f53\u7269\u4f53\u4eff\u771f\u5668\u53d7\u9650\u4e8e\u9884\u8bbe\u7269\u7406\u6216\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u7f3a\u4e4f\u673a\u5668\u4eba\u6761\u4ef6\u63a7\u5236\uff0c\u5bfc\u81f4\u4eff\u771f\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSoMA\u6a21\u62df\u5668\uff0c\u5c06\u53ef\u53d8\u5f62\u52a8\u6001\u3001\u73af\u5883\u529b\u4e0e\u673a\u5668\u4eba\u5173\u8282\u52a8\u4f5c\u8026\u5408\u4e8e\u7edf\u4e00\u7684\u6f5c\u5728\u795e\u7ecf\u7a7a\u95f4\uff0c\u901a\u8fc73D\u9ad8\u65af\u6591\u70b9\u5efa\u6a21\u5b66\u4e60\u4ea4\u4e92\uff0c\u65e0\u9700\u9884\u8bbe\u7269\u7406\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4eff\u771f\u3002", "result": "SoMA\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u7684\u8f6c\u5f55\u4eff\u771f\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u63d0\u534720%\uff0c\u80fd\u591f\u7a33\u5b9a\u9ad8\u6548\u5730\u4eff\u771f\u8bf8\u5982\u957f\u671f\u5e03\u6599\u6298\u53e0\u7b49\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "SoMA\u4e3a\u8f6f\u4f53\u64cd\u7eb5\u5e26\u6765\u66f4\u9ad8\u7684\u771f\u5b9e\u611f\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u590d\u6742\u89e6\u7269\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u6709\u529b\u7684\u4eff\u771f\u652f\u6301\u3002"}}
{"id": "2602.00531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00531", "abs": "https://arxiv.org/abs/2602.00531", "authors": ["Tianyi Zhang", "Antoine Simoulin", "Kai Li", "Sana Lakdawala", "Shiqing Yu", "Arpit Mittal", "Hongyu Fu", "Yu Lin"], "title": "Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment", "comment": null, "summary": "Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u68c0\u6d4b\u6846\u67b6(VLDet)\uff0c\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u91d1\u5b57\u5854\u548c\u5f15\u5165\u65b0\u7684\u5bf9\u9f50\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u8868\u73b0\u3002\u65b0\u65b9\u6cd5\u5728COCO2017\u548cLVIS\u4e0a\u68c0\u6d4b\u65b0\u7c7b\u522b\u7684\u8868\u73b0\u5747\u8d85\u8fc7\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u53ea\u80fd\u68c0\u6d4b\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5e0c\u671b\u8ba9\u6a21\u578b\u80fd\u8bc6\u522b\u8bad\u7ec3\u96c6\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u7c7b\u522b\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u5bf9\u9f50\u6216\u4e3b\u5e72\u7f51\u7edc\u9002\u5e94\u65b9\u9762\u6548\u679c\u6709\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51faVisual-Language Detection\uff08VLDet\uff09\u6846\u67b6\uff0c1) \u91cd\u65b0\u8bbe\u8ba1\u7279\u5f81\u91d1\u5b57\u5854\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff1b2) \u63d0\u51faVL-PUB\u6a21\u5757\uff0c\u6709\u6548\u5229\u7528CLIP\u89c6\u89c9-\u8bed\u8a00\u77e5\u8bc6\u5e76\u4f7f\u4e3b\u5e72\u7f51\u7edc\u9002\u5e94\u68c0\u6d4b\u4efb\u52a1\uff1b3) \u63d0\u51faSigRPN\u6a21\u5757\uff0c\u901a\u8fc7sigmoid-based anchor-text\u5bf9\u6bd4\u635f\u5931\u4ee5\u63d0\u5347\u65b0\u7c7b\u522b\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728COCO2017\u65b0\u7c7b\u522b\u4e0a\u83b7\u5f9758.7 AP\uff0c\u5728LVIS\u65b0\u7c7b\u522b\u4e0a\u83b7\u5f9724.8 AP\uff0c\u5206\u522b\u6bd4\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u9ad827.6%\u548c6.9%\u3002\u5728\u5e38\u89c4\u5c01\u95ed\u7c7b\u522b\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4e5f\u6709\u8f83\u597d\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u8868\u73b0\u3002", "conclusion": "VLDet\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\u5bf9\u9f50\u4e0e\u68c0\u6d4b\u5206\u652f\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u65b0\u7c7b\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u671b\u62d3\u5c55\u76ee\u6807\u68c0\u6d4b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.01203", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01203", "abs": "https://arxiv.org/abs/2602.01203", "authors": ["Zizhuo Fu", "Wenxuan Zeng", "Runsheng Wang", "Meng Li"], "title": "Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse", "comment": null, "summary": "Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u6a21\u578b\u6ce8\u610f\u529b\u5c42\u4e2d\u51fa\u73b0\u7684 attention sink \u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e sink-aware \u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u8f85\u52a9\u635f\u5931\u6765\u7f13\u89e3 head collapse\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u5bb9\u6613\u51fa\u73b0\u6ce8\u610f\u529b\u8fc7\u5ea6\u96c6\u4e2d\u4e8e\u9996\u4e2a token \u7684\u73b0\u8c61\uff08attention sink\uff09\uff0c\u5bfc\u81f4\u4ec5\u90e8\u5206\u6ce8\u610f\u529b\u5934\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u771f\u6b63\u5de5\u4f5c\uff08head collapse\uff09\uff0c\u800c\u76ee\u524d\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u6210\u56e0\u548c\u4e0d\u540c\u673a\u5236\u95f4\u7684\u8054\u7cfb\u7f3a\u5c11\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0 Vanilla Attention \u548c Sink Attention \u81ea\u7136\u6784\u6210\u4e86\u4e00\u79cd\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7684\u63a5\u5165\u65b9\u5f0f\uff0c\u4ece\u800c\u89e3\u91ca head collapse \u7684\u51fa\u73b0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86 sink-aware \u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5728\u6ce8\u610f\u529b\u5c42\u5f15\u5165\u4e86\u8f85\u52a9\u7684\u8d1f\u8f7d\u5747\u8861\u635f\u5931\u4ee5\u5e73\u8861\u5404\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u5404\u6ce8\u610f\u529b\u5934\u7684\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u63d0\u5347\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\u4e0b\uff08\u5305\u62ecVanilla\u3001Sink\u3001Gated Attention\uff09\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5c42\u4e2d\u4f9d\u7136\u6f5c\u5728\u7684 MoE \u7ed3\u6784\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76 attention sink \u548c head collapse \u95ee\u9898\u3001\u6539\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2602.02411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02411", "abs": "https://arxiv.org/abs/2602.02411", "authors": ["Hanwen Ren", "Junyong Kim", "Aathman Tharmasanthiran", "Ahmed H. Qureshi"], "title": "Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces", "comment": null, "summary": "Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u9ad8\u6548\u91cd\u6392\u7269\u4f53\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u91cd\u6392\u95ee\u9898\u7ecf\u5e38\u9762\u4e34\u975e\u5355\u8c03\u6027\uff08\u7269\u4f53\u95f4\u76f8\u4e92\u963b\u6321\uff09\uff0c\u9700\u8981\u590d\u6742\u7684\u5408\u4f5c\u548c\u4e2d\u95f4\u79fb\u52a8\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5355\u8c03\u60c5\u5f62\uff0c\u7f3a\u4e4f\u5bf9\u5177\u6709\u5e7f\u6cdb\u5b9e\u9645\u610f\u4e49\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3001\u975e\u5355\u8c03\u73af\u5883\u95ee\u9898\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e2d\u592e\u63a7\u5236\u7684\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08CAM-MCTS\uff09\u6846\u67b6\uff0c\u5c06\u4e2d\u592e\u4efb\u52a1\u5206\u914d\uff08\u5404\u673a\u5668\u4eba\u4e92\u77e5\u8ba1\u5212\uff0c\u76f8\u4e92\u914d\u5408\u4f18\u5316\u5168\u5c40\u6548\u7387\uff09\u4e0e\u5f02\u6b65\u6267\u884c\uff08\u901a\u8fc7\u4e00\u6b65\u524d\u77bb\u6210\u672c\u4f30\u8ba1\uff0c\u673a\u5668\u4eba\u65e0\u9700\u7b49\u5f85\u4ed6\u4eba\u800c\u80fd\u81ea\u4e3b\u627f\u62c5\u65b0\u4efb\u52a1\uff09\u7ed3\u5408\uff0c\u6709\u6548\u51cf\u5c11\u673a\u5668\u4eba\u7a7a\u95f2\u4e0e\u7b49\u5f85\u65f6\u95f4\u3002", "result": "\u5728\u5404\u79cd\u590d\u6742\u3001\u6df7\u4e71\u7684\u5355\u8c03\u4e0e\u975e\u5355\u8c03\u4efb\u52a1\u4e2d\uff0cCAM-MCTS\u5728makespan\u6307\u6807\u4e0a\u4f18\u4e8e\u5f3a\u5bf9\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u3002\u540c\u65f6\uff0c\u5728\u771f\u5b9e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u591a\u914d\u7f6e\u4e0b\u6d4b\u8bd5\uff0c\u8868\u73b0\u51fa\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "CAM-MCTS\u80fd\u591f\u9ad8\u6548\u3001\u9c81\u68d2\u5730\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7269\u4f53\u91cd\u6392\u95ee\u9898\uff0c\u660e\u663e\u964d\u4f4e\u4efb\u52a1\u603b\u65f6\u95f4\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00536", "abs": "https://arxiv.org/abs/2602.00536", "authors": ["Yifan Zhang", "Qian Chen", "Yi Liu", "Wengen Li", "Jihong Guan"], "title": "SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal", "comment": null, "summary": "Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u6269\u6563\u6a21\u578b\uff08SADER\uff09\uff0c\u7528\u4e8e\u591a\u65f6\u76f8\u9065\u611f\u4e91\u53bb\u9664\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5e38\u56e0\u4e91\u8986\u76d6\u800c\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u540e\u7eed\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e91\u53bb\u9664\u65b9\u6cd5\u5728\u91c7\u6837\u6548\u7387\u548c\u7ed3\u6784\u3001\u65f6\u5e8f\u4fe1\u606f\u5229\u7528\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u96be\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u6539\u8fdb\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SADER\uff0c\u5305\u62ec\u4e00\u4e2a\u591a\u65f6\u76f8\u6761\u4ef6\u6269\u6563\u7f51\u7edc\uff08MTCDN\uff09\uff0c\u80fd\u901a\u8fc7\u65f6\u5e8f\u878d\u5408\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u5145\u5206\u6355\u6349\u591a\u65f6\u76f8\u3001\u591a\u6a21\u6001\u5173\u8054\uff1b\u5f15\u5165\u4e91\u611f\u77e5\u6ce8\u610f\u529b\u635f\u5931\u51fd\u6570\uff0c\u6839\u636e\u4e91\u539a\u5ea6\u4e0e\u4eae\u5ea6\u7a81\u663e\u4e91\u4e3b\u5bfc\u533a\u57df\uff1b\u5e76\u4e3a\u8fde\u7eed\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u91cd\u590d\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u5229\u7528\u5f15\u5bfc\u6821\u6b63\u65b9\u5f0f\u8fed\u4ee3\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5728\u591a\u4e2a\u591a\u65f6\u76f8\u9065\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSADER\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u4e91\u53bb\u9664\u65b9\u6cd5\uff0c\u6548\u679c\u66f4\u597d\u3001\u66f4\u7a33\u5b9a\u3002", "conclusion": "SADER\u6a21\u578b\u80fd\u591f\u66f4\u9ad8\u6548\u5e76\u5145\u5206\u5229\u7528\u7ed3\u6784\u4e0e\u65f6\u5e8f\u4fe1\u606f\uff0c\u63d0\u5347\u9065\u611f\u4e91\u53bb\u9664\u7684\u6548\u679c\uff0c\u6709\u671b\u63a8\u52a8\u76f8\u5173\u4e0b\u6e38\u9065\u611f\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01204", "abs": "https://arxiv.org/abs/2602.01204", "authors": ["Xuqin Zhang", "Quan He", "Zhenrui Zheng", "Zongzhang Zhang", "Xu He", "Dong Li"], "title": "ASTER: Agentic Scaling with Tool-integrated Extended Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6ASTER\uff0c\u901a\u8fc7\u6539\u8fdb\u521d\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u591a\u8f6e\u8c03\u7528\u5916\u90e8\u5de5\u5177\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8fbe\u5230\u6570\u5b66\u9886\u57df\u7684\u6700\u65b0\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u5927\u6a21\u578b\u591a\u8f6e\u5de5\u5177\u63a8\u7406\u9762\u4e34\u2018\u4ea4\u4e92\u5d29\u6e83\u2019\u95ee\u9898\uff0c\u5373\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5185\u90e8\u63a8\u7406\u800c\u975e\u6709\u6548\u3001\u591a\u8f6e\u5730\u8c03\u7528\u5916\u90e8\u5de5\u5177\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u627e\u5230\u66f4\u4f18\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u521d\u59cb\u2018\u51b7\u542f\u52a8\u2019\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u9636\u6bb5\u884c\u4e3a\u8bbe\u5b9a\u3001\u51b7\u542f\u52a8\u8f68\u8ff9\u7684\u4ea4\u4e92\u5bc6\u5ea6\u3001\u4ee5\u53caRL\u8bad\u7ec3\u9884\u7b97\u5bf9\u5927\u6a21\u578b\u63a8\u7406\u4e0e\u6cdb\u5316\u7684\u5f71\u54cd\u3002\u63d0\u51faASTER\u6846\u67b6\uff0c\u5728\u51b7\u542f\u52a8\u9636\u6bb5\u4f18\u5148\u9009\u53d6\u4ea4\u4e92\u5bc6\u5ea6\u9ad8\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c\u5927\u5927\u52a0\u5f3a\u6a21\u578b\u5916\u90e8\u5de5\u5177\u53c2\u4e0e\u7684\u4e60\u60ef\u3002", "result": "\u53d1\u73b0\u4ec5\u4f7f\u75284000\u6761\u9ad8\u4ea4\u4e92\u5bc6\u5ea6\u4e13\u5bb6\u8f68\u8ff9\u5373\u53ef\u5e26\u6765\u6700\u4f18\u7684\u540e\u7eed\u8868\u73b0\u3002\u57fa\u4e8e\u6b64\u8bad\u7ec3\u7684ASTER-4B\u6a21\u578b\uff0c\u5728AIME 2025\u7b49\u6570\u5b66\u9ad8\u96be\u5ea6\u57fa\u51c6\u4e0a\u8fbe\u523090%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u4e3b\u6d41\u5f00\u6e90\u5927\u6a21\u578b\u3002", "conclusion": "ASTER\u5c55\u793a\u4e86\u5c0f\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u51b7\u542f\u52a8\u6570\u636e\u5bf9\u5927\u6a21\u578bRL\u8bad\u7ec3\u6548\u679c\u7684\u5de8\u5927\u4fc3\u8fdb\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u5927\u6a21\u578b\u591a\u8f6e\u5de5\u5177\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u53d1\u5c55\u8def\u5f84\u3002"}}
{"id": "2602.02430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02430", "abs": "https://arxiv.org/abs/2602.02430", "authors": ["Pierre-Yves Lajoie", "Benjamin Ramtoula", "Daniele De Martini", "Giovanni Beltrame"], "title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM", "comment": null, "summary": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u57fa\u7840\u6a21\u578b\u63d0\u5347\u591a\u673a\u5668\u4eba\u53bb\u4e2d\u5fc3\u5316SLAM\u56de\u73af\u68c0\u6d4b\u7684\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5927\u89c6\u89d2\u53d8\u5316\u60c5\u51b5\u4e0b\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u591a\u673a\u5668\u4ebaSLAM\u9762\u4e34\u7531\u4e8e\u4e0d\u540c\u89c6\u89d2\u5bfc\u81f4\u7684\u5730\u56fe\u91cd\u53e0\u96be\u4ee5\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u6fc0\u53d1\u4e86\u4f5c\u8005\u5229\u7528\u65b0\u5174\u76843D\u57fa\u7840\u6a21\u578b\u6765\u589e\u8fdb\u4e0d\u540c\u673a\u5668\u4eba\u7684\u534f\u4f5c\u4e0e\u6570\u636e\u878d\u5408\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5c06\u5f3a\u5927\u76843D\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u73b0\u6709C-SLAM\u7ba1\u9053\u4e2d\uff0c\u901a\u8fc7\u5355\u76ee\u56fe\u50cf\u5bf9\u53ef\u9760\u4f30\u7b97\u673a\u5668\u4eba\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u5e76\u63d0\u51fa\u5f3a\u5065\u7684\u79bb\u7fa4\u70b9\u5254\u9664\u65b9\u6cd5\u548c\u9ad8\u6548\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\u7b97\u6cd5\u6765\u6d88\u9664\u5c3a\u5ea6\u6b67\u4e49\u3002", "result": "\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u5728\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7cbe\u5ea6\u3001\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u7b49\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u660e\u663e\u63d0\u5347\uff0c\u5c24\u5176\u9002\u5408\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6848\u5c55\u793a\u4e863D\u57fa\u7840\u6a21\u578b\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4ebaSLAM\u91cc\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u63a8\u52a8\u5927\u89c4\u6a21\u3001\u534f\u4f5c\u5f0f\u673a\u5668\u4eba\u5730\u56fe\u6784\u5efa\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00542", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00542", "abs": "https://arxiv.org/abs/2602.00542", "authors": ["Mohammad Saeid", "Amir Salarpour", "Pedram MohajerAnsari", "Mert D. Pes\u00e9"], "title": "NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation", "comment": "Accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV 2026)", "summary": "We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods", "AI": {"tldr": "NPNet\u662f\u4e00\u79cd\u5b8c\u5168\u65e0\u53c2\u6570\u5316\u76843D\u70b9\u4e91\u5206\u7c7b\u4e0e\u5206\u5272\u65b9\u6cd5\uff0c\u65e0\u9700\u5b66\u4e60\u6743\u91cd\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9ad8\u65af-\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u8868\u73b0\uff0c\u7279\u522b\u9002\u5408\u5c0f\u6837\u672c\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u67093D\u70b9\u4e91\u5206\u6790\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\uff0c\u9700\u8981\u5927\u91cf\u53c2\u6570\u548c\u8bad\u7ec3\uff1b\u800c\u65e0\u53c2\u6570\u65b9\u6cd5\u4e0d\u9700\u8981\u6743\u91cd\u5b66\u4e60\uff0c\u4f46\u901a\u5e38\u6027\u80fd\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u6743\u91cd\u53c8\u6709\u4f18\u826f\u8868\u73b0\u7684\u65e0\u53c2\u6570\u70b9\u4e91\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u8bb0\u5fc6\u4e0e\u63a8\u7406\u6548\u7387\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u5c0f\u6837\u672c\u60c5\u51b5\u3002", "method": "NPNet\u5b8c\u5168\u57fa\u4e8e\u786e\u5b9a\u6027\u7b97\u5b50\u5982\u6700\u8fdc\u70b9\u91c7\u6837\u3001k\u8fd1\u90bb\u548c\u6c60\u5316\u6765\u63d0\u53d6\u70b9\u7279\u5f81\u3002\u5176\u6838\u5fc3\u65b9\u6cd5\u662f\u81ea\u9002\u5e94\u5e26\u5bbd\u548c\u9ad8\u65af-\u4f59\u5f26\u6df7\u5408\u7684\u9ad8\u65af\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ece\u8f93\u5165\u51e0\u4f55\u7ed3\u6784\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u4e0e\u91c7\u6837\u5bc6\u5ea6\u4fdd\u6301\u7a33\u5b9a\u3002\u5728\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u989d\u5916\u52a0\u5165\u56fa\u5b9a\u9891\u7387\u7684\u5085\u91cc\u53f6\u7279\u5f81\u63d0\u4f9b\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "NPNet\u5728ModelNet40\u3001ModelNet-R\u3001ScanObjectNN\u4e0eShapeNetPart\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u65e0\u53c2\u6570\u65b9\u6cd5\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5728ModelNet40\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u3002\u540c\u65f6\uff0c\u5176\u5b58\u50a8\u9700\u6c42\u548c\u63a8\u7406\u65f6\u95f4\u4f18\u4e8e\u4ee5\u5f80\u7684\u65e0\u53c2\u6570\u65b9\u6cd5\u3002", "conclusion": "NPNet\u65e0\u9700\u4efb\u4f55\u5b66\u4e60\u6743\u91cd\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u4f4d\u7f6e\u7f16\u7801\uff0c\u517c\u987e\u4e86\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u975e\u53c2\u6570\u53163D\u70b9\u4e91\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01208", "abs": "https://arxiv.org/abs/2602.01208", "authors": ["Kai Zhang", "Jiayi Liao", "Chengpeng Li", "Ziyuan Xie", "Sihang Li", "Xiang Wang"], "title": "Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling", "comment": null, "summary": "Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \\textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\\% over Pass@1 and 22.70\\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86Chronos\uff0c\u4e00\u79cd\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u7684\u8f7b\u91cf\u5316\u6295\u7968\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684Test-Time Scaling\u65b9\u6cd5\uff08\u5982\u591a\u6570\u6295\u7968\u3001\u542f\u53d1\u5f0f\u6253\u5206\uff09\u5bf9\u63a8\u7406\u8f68\u8ff9\u6216token\u4e00\u89c6\u540c\u4ec1\uff0c\u5bb9\u6613\u53d7\u5230\u8d28\u91cf\u6ce2\u52a8\u548c\u5c40\u90e8\u903b\u8f91\u5931\u8bef\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8f68\u8ff9\u8d28\u91cf\u8bc4\u4f30\u673a\u5236\u3002", "method": "Chronos\u65b9\u6cd5\u5c06\u6bcf\u6761\u63a8\u7406\u8f68\u8ff9\u89c6\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u5b66\u4e60token\u6982\u7387\u7684\u8f68\u8ff9\u7279\u5f81\uff0c\u5bf9\u4e0d\u540c\u8f68\u8ff9\u8d4b\u4e88\u8d28\u91cf\u5206\u6570\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u6295\u7968\u673a\u5236\u6765\u63d0\u5347\u5224\u51b3\u7684\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7b80\u5355\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\u3002", "result": "\u5927\u91cf\u5728\u540c\u57df\u548c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e0b\u8bc4\u4f30\u8868\u660e\uff0cChronos\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5747\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002\u5728HMMT25\u4efb\u52a1\u4e0a\uff0c\u91c7\u7528Qwen3-4B-Thinking-2507\u6a21\u578b\u7684Chronos@128\u65b9\u6848\u76f8\u6bd4Pass@1\u63d0\u5347\u4e8634.21%\uff0c\u76f8\u6bd4Maj@128\u63d0\u5347\u4e8622.70%\u3002", "conclusion": "Chronos\u80fd\u591f\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u7a33\u5b9a\u663e\u8457\u63d0\u5347TTS\u8303\u5f0f\u4e0b\u5927\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u7387\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u54c1\u8d28\u589e\u5f3a\u5de5\u5177\u3002"}}
{"id": "2602.02454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02454", "abs": "https://arxiv.org/abs/2602.02454", "authors": ["Ansh Kumar Sharma", "Yixiang Sun", "Ninghao Lu", "Yunzhe Zhang", "Jiarao Liu", "Sherry Yang"], "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model", "comment": "https://world-gymnast.github.io/", "summary": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u4e16\u754c\u6a21\u578b\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5World-Gymnast\uff0c\u901a\u8fc7\u5728\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u7b56\u7565\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u4e86\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8868\u73b0\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u4e0e\u8f6f\u4ef6\u4eff\u771f\u663e\u8457\u4f18\u8d8a\u3002", "motivation": "\u673a\u5668\u4eba\u4f9d\u8d56\u7269\u7406\u4e92\u52a8\u8fdb\u884c\u5b66\u4e60\u6210\u672c\u9ad8\u6602\uff0c\u800c\u76ee\u524d\u4e3b\u6d41\u7684\u4e13\u5bb6\u793a\u8303\u76d1\u7763\u5fae\u8c03\u548c\u4eff\u771f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u90fd\u5b58\u5728\u6570\u636e\u91cf\u4e0d\u8db3\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u843d\u5dee\u7684\u95ee\u9898\u3002\u8fd1\u671f\u4e16\u754c\u6a21\u578b\u7684\u51fa\u73b0\u8ba9\u57fa\u4e8e\u771f\u5b9e\u89c6\u9891\u6570\u636e\u7684\u865a\u62df\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\uff0c\u662f\u5426\u80fd\u66f4\u6709\u6548\u63d0\u5347\u771f\u5b9e\u673a\u5668\u4eba\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u63d0\u51fa\u4e86World-Gymnast\uff1a\u5148\u7528\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u2014\u52a8\u4f5c\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u53ef\u63a7\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u5728\u8be5\u6a21\u578b\u4e0a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u89c6\u89c9\u2014\u8bed\u8a00\u2014\u52a8\u4f5c\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9\u2014\u8bed\u8a00\u6a21\u578b\u5bf9\u751f\u6210\u7684\u89c6\u9891\u8fdb\u884c\u5956\u52b1\u8bc4\u4f30\uff1b\u8fc7\u7a0b\u4e2d\u8fd8\u80fd\u57fa\u4e8e\u591a\u6837\u5316\u8bed\u8a00\u6307\u4ee4\u3001\u4e0d\u540c\u573a\u666f\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5e76\u652f\u6301\u7b56\u7565\u4e0e\u4e16\u754c\u6a21\u578b\u7684\u5728\u7ebf\u534f\u540c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728Bridge\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0cWorld-Gymnast\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe18\u500d\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u4eff\u771f\u5f3a\u5316\u5b66\u4e60\u63d0\u53472\u500d\uff0c\u5e76\u5c55\u73b0\u4e86\u5bf9\u4e8e\u65b0\u573a\u666f\u3001\u65b0\u6307\u4ee4\u548c\u5728\u7ebf\u8fed\u4ee3\u4f18\u5316\u7684\u5f3a\u5927\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u9a71\u52a8\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728\u5176\u4e0a\u4e91\u7aef\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u6709\u671b\u6253\u7834\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u74f6\u9888\uff0c\u4e3a\u5b9e\u73b0\u4eba\u4eba\u53ef\u7528\u7684\u5bb6\u5ead\u673a\u5668\u4eba\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.00559", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00559", "abs": "https://arxiv.org/abs/2602.00559", "authors": ["Wenbin Xing", "Quanxing Zha", "Lizheng Zu", "Mengran Li", "Ming Li", "Junchi Yan"], "title": "Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models", "comment": null, "summary": "Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., \"All are correct\" and \"None of the above\") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OmniVCHall\u57fa\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u6d4b\u89c6\u9891\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u591a\u79cd\u89c6\u9891\u5e7b\u89c9\uff08\u5c24\u5176\u662f\u590d\u5408\u578b\u5e7b\u89c9\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faTriCD\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5e94\u5bf9\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5e7b\u89c9\u7814\u7a76\u4e3b\u8981\u805a\u7126\u4e8e\u5355\u4e00\u9519\u8bef\u7c7b\u578b\uff0c\u5ffd\u7565\u4e86\u7531\u591a\u79cd\u7a7a\u95f4\u548c\u65f6\u95f4\u56e0\u7d20\u4ea4\u4e92\u9020\u6210\u7684\u590d\u5408\u578b\u5e7b\u89c9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u548c\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u4e0e\u7f13\u89e3\u6b64\u7c7b\u95ee\u9898\u3002", "method": "1\uff09\u8bbe\u8ba1OmniVCHall\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u89c6\u9891\u9886\u57df\u3001\u521b\u65b0\u5e7b\u89c9\u7c7b\u578b\u4e0e\u7ec6\u81f4\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u5bf9\u6297\u6027\u7b54\u6848\u9632\u6b62\u5957\u8def\u63a8\u7406\uff1b2\uff09\u63d0\u51faTriCD\uff1a\u5305\u542b\u4e09\u8def\u5f84\u6821\u51c6\u673a\u5236\u7684\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6270\u52a8\u751f\u6210\u8d1f\u6837\u672c\u3001\u663e\u8457\u6027\u589e\u5f3a\u6a21\u5757\u5f3a\u5316\u89c6\u89c9\u8bc1\u636e\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u572839\u4e2a\u4e3b\u6d41VLLMs\u4e0a\u8bc4\u6d4b\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u6a21\u578b\u4ea6\u5728\u590d\u5408\u5e7b\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u5927\u5e45\u4e0b\u964d\u3002\u5e94\u7528TriCD\u5904\u7406\u540e\uff0c\u5728\u4e24\u7c7b\u4e3b\u6d41\u67b6\u6784\u4e0b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u534710%\u4ee5\u4e0a\u3002", "conclusion": "OmniVCHall\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u89c6\u9891\u5e7b\u89c9\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6d4b\u8bd5\u6807\u51c6\uff0cTriCD\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u5408\u5e7b\u89c9\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u6570\u636e\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.01227", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01227", "abs": "https://arxiv.org/abs/2602.01227", "authors": ["Zhanming Shen", "Zeyu Qin", "Jiaqi Hu", "Wentao Ye", "Hao Chen", "Xiaomeng Hu", "Haokai Xu", "Gang Chen", "Yi R. Fung", "Haobo Wang"], "title": "Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority", "comment": null, "summary": "The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u201cToken Priority\u201d\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u5efa\u6a21\u4e0e\u4eba\u7c7b\u771f\u5b9e\u6548\u7528\u5bf9\u9f50\u7684\u6838\u5fc3\u6865\u6881\uff0c\u5c06\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u91cd\u5851\u8fc7\u7a0b\uff0c\u5e76\u57fa\u4e8e\u6b64\u5bf9\u8fd1\u671f\u5bf9\u9f50\u6280\u672f\u8fdb\u884c\u7edf\u4e00\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u901a\u5e38\u4ee5\u7ec6\u7c92\u5ea6\uff08\u5982\u9010token\u81ea\u56de\u5f52\uff09\u7684\u65b9\u5f0f\u751f\u6210\u6587\u672c\uff0c\u4f46\u76d1\u7763\u4fe1\u53f7\u5f80\u5f80\u7c97\u7cd9/\u7edf\u4e00\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u771f\u6b63\u5bf9\u9f50\u4eba\u7c7b\u671f\u671b\u3002\u8fd9\u4e00\u7c92\u5ea6\u4e0d\u5339\u914d\u9650\u5236\u4e86\u6a21\u578b\u80fd\u529b\u63d0\u5347\u3002\u8bba\u6587\u63d0\u51fa\u7528Token Priority\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5c06SFT\u89c6\u4e3a\u6570\u636e\u5206\u5e03\u91cd\u5851\uff0c\u4f9d\u636eToken Priority\u601d\u60f3\u91cd\u6784\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5305\u62ec\u533a\u5206\u6b63\u4f18\u5148\uff08\u7528\u4e8e\u8fc7\u6ee4\u566a\u58f0token\uff09\u548c\u6709\u7b26\u53f7\u4f18\u5148\uff08\u7528\u4e8e\u6d88\u9664\u6709\u5bb3token\uff09\u3002\u636e\u6b64\u5bf9\u73b0\u6709\u76f8\u5173\u65b9\u6cd5\u8fdb\u884c\u7406\u8bba\u7edf\u4e00\u548c\u5206\u7c7b\u5206\u6790\u3002", "result": "\u901a\u8fc7Token Priority\u89c6\u89d2\uff0c\u5c06\u73b0\u6709\u5bf9\u9f50\u4e0e\u5206\u5e03\u4fee\u6b63\u65b9\u6cd5\u5206\u4e3a\u4e24\u5927\u7c7b\uff0c\u7cfb\u7edf\u68b3\u7406\u5e76\u5bf9\u5df2\u53d6\u5f97\u7684\u7a81\u7834\u548c\u5b58\u5728\u95ee\u9898\u8fdb\u884c\u68b3\u7406\uff0c\u5c55\u793a\u8be5\u7406\u8bba\u5f3a\u5927\u7684\u89e3\u91ca\u548c\u6307\u5bfc\u80fd\u529b\u3002", "conclusion": "Token Priority\u4e3a\u673a\u5236\u8bbe\u8ba1\u548c\u76d1\u7763\u4fe1\u53f7\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u53ef\u7cbe\u51c6\u6355\u6349token\u7ea7\u7684\u5bf9\u9f50\u9700\u6c42\u3002\u672a\u6765\u5e94\u8fdb\u4e00\u6b65\u5b8c\u5584token\u6743\u91cd\u5206\u914d\u3001\u8bc4\u4ef7\u4f53\u7cfb\u53ca\u89e3\u51b3\u6807\u7b7e\u566a\u58f0\u7b49\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.02456", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02456", "abs": "https://arxiv.org/abs/2602.02456", "authors": ["Albert Gassol Puigjaner", "Angelos Zacharia", "Kostas Alexis"], "title": "Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning", "comment": "ICRA 2026, 8 pages", "summary": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u5206\u5c423D\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u7279\u5f81\u548c\u5bf9\u8c61\u5173\u7cfb\u63a8\u7406\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d4b\u80fd\u673a\u5668\u4eba\u5bf9\u590d\u6742\u73af\u5883\u8fdb\u884c\u7406\u89e3\u548c\u4efb\u52a1\u63a8\u7406\u3002", "motivation": "\u4f20\u7edfSLAM\u53ca\u5176\u6269\u5c55\u4ec5\u80fd\u5b9e\u73b0\u5ea6\u91cf\u91cd\u5efa\u548c\u57fa\u672c\u8bed\u4e49\u6620\u5c04\uff0c\u96be\u4ee5\u6355\u6349\u66f4\u9ad8\u5c42\u6b21\u7684\u573a\u666f\u7ed3\u6784\u548c\u5bf9\u8c61\u95f4\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u4ee3\u7406\u5bf9\u73af\u5883\u7684\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u5206\u5c42\u76843D\u573a\u666f\u56fe\uff0c\u6574\u5408\u591a\u5c42\u7ea7\u7684\u5f00\u653e\u8bcd\u6c47\u7279\u5f81\u5e76\u652f\u6301\u5bf9\u8c61\u5173\u7cfb\u63a8\u7406\u3002\u65b9\u6cd5\u5229\u7528VLM\u63a8\u65ad\u8bed\u4e49\u5173\u7cfb\uff0c\u7ed3\u5408LLM\u4e0eVLM\u7684\u63a8\u7406\u6a21\u5757\uff0c\u4ece\u800c\u66f4\u597d\u5730\u89e3\u91ca\u573a\u666f\u4fe1\u606f\u5e76\u8d4b\u80fd\u4efb\u52a1\u63a8\u7406\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u90e8\u7f72\u4e8e\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5728\u591a\u4e2a\u73af\u5883\u548c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u573a\u666f\u7406\u89e3\u548c\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7ed3\u54083D\u573a\u666f\u56fe\u3001VLM\u548cLLM\u80fd\u591f\u5927\u5e45\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u590d\u6742\u573a\u666f\u7ed3\u6784\u548c\u4efb\u52a1\u7684\u7406\u89e3\u3001\u63a8\u7406\u4e0e\u4ea4\u4e92\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u4f53\u81ea\u4e3b\u5bfc\u822a\u548c\u73af\u5883\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u6b21\u7684\u8868\u8fbe\u548c\u80fd\u529b\u3002"}}
{"id": "2602.00570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00570", "abs": "https://arxiv.org/abs/2602.00570", "authors": ["Xingyu Luo", "Yidong Cai", "Jie Liu", "Jie Tang", "Gangshan Wu", "Limin Wang"], "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates", "comment": null, "summary": "Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGLAD\u7684\u751f\u6210\u5f0f\u8bed\u8a00\u8f85\u52a9\u89c6\u89c9\u8ffd\u8e2a\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u591a\u6a21\u6001\u751f\u6210\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8bed\u4e49\u56fe\u50cf\u4e0b\u7684\u8ffd\u8e2a\u6027\u80fd\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u8ffd\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\u7b49\u4f4e\u8bed\u4e49\u56fe\u50cf\u65f6\uff0c\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u53d7\u9650\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u4e0e\u56fe\u50cf\u7279\u5f81\u878d\u5408\u7684\u6548\u679c\u4e0d\u4f73\uff0c\u6025\u9700\u66f4\u6709\u6548\u7684\u878d\u5408\u7b56\u7565\u3002", "method": "\u63d0\u51faGLAD\u6a21\u578b\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u7684\u6587\u672c\u4e0e\u6a21\u677f\u56fe\u50cf\u591a\u6a21\u6001\u878d\u5408\uff0c\u589e\u5f3a\u4e8c\u8005\u517c\u5bb9\u6027\uff0c\u5e76\u63d0\u5347\u56fe\u50cf\u8bed\u4e49\u4fe1\u606f\uff0c\u5e2e\u52a9\u6062\u590d\u6a21\u7cca\u3001\u8bed\u4e49\u4e0d\u660e\u7684\u6a21\u677f\u56fe\u50cf\u3002", "result": "GLAD\u65b9\u6cd5\u5728\u591a\u9879\u516c\u5f00\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u65b0\u6700\u597d\uff08state-of-the-art\uff09\u7684\u8ffd\u8e2a\u8868\u73b0\uff0c\u540c\u65f6\u5177\u6709\u8f83\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u751f\u6210\u5f0f\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8bed\u4e49\u56fe\u50cf\u7684\u89c6\u89c9-\u8bed\u8a00\u8ffd\u8e2a\u6548\u679c\uff0c\u5bf9\u672a\u6765\u76f8\u5173\u4efb\u52a1\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2602.01239", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01239", "abs": "https://arxiv.org/abs/2602.01239", "authors": ["Jamshid Mozafari", "Hamed Zamani", "Guido Zuccon", "Adam Jatowt"], "title": "Inferential Question Answering", "comment": "Proceedings of the ACM Web Conference 2026 (WWW 2026)", "summary": "Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7c7b\u65b0\u7684\u95ee\u7b54\u4efb\u52a1\u2014\u2014\u63a8\u7406\u578b\u95ee\u7b54\uff08Inferential QA\uff09\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u4ec5\u63d0\u4f9b\u7ebf\u7d22\u7684\u6587\u672c\u63a8\u65ad\u7b54\u6848\uff0c\u5e76\u6784\u5efa\u4e86QUIT\u6570\u636e\u96c6\u4ee5\u7814\u7a76\u6b64\u95ee\u9898\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u95ee\u7b54\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u4ece\u95f4\u63a5\u8bc1\u636e\u4e2d\u63a8\u7406\u7b54\u6848\u3002", "motivation": "\u867d\u7136\u95ee\u7b54\u7cfb\u7edf\u7814\u7a76\u5e7f\u6cdb\uff0c\u4f46\u5927\u591a\u6570\u5de5\u4f5c\u5047\u8bbe\u7b54\u6848\u53ef\u4ee5\u76f4\u63a5\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u6216\u751f\u6210\uff0c\u5ffd\u7565\u4e86\u9700\u8981\u63a8\u7406\u624d\u80fd\u5f97\u51fa\u7b54\u6848\u7684\u573a\u666f\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a8\u52a8\u95ee\u7b54\u7cfb\u7edf\u4ece\u76f4\u63a5\u62bd\u53d6\u5411\u63a8\u7406\u7406\u89e3\u8f6c\u53d8\u3002", "method": "\u4f5c\u8005\u63d0\u51faInferential QA\u4efb\u52a1\uff0c\u6784\u5efa\u4e86QUIT\u6570\u636e\u96c6\uff08\u5305\u542b7,401\u4e2a\u95ee\u9898\u548c240\u4e07\u6bb5\u6587\u672c\uff09\uff0c\u7531\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u63d0\u793a\u540e\u624b\u5de5\u6807\u6ce8\u76f8\u5173\u6027\u3002\u8bc4\u4f30\u4e86\u4e0d\u540c\u7684\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u4ee5\u53caLLM\u9605\u8bfb\u5668\u5728\u63a8\u7406\u578b\u95ee\u7b54\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5728\u63a8\u7406\u578b\u95ee\u7b54\u4e2d\uff0c\u4f20\u7edf\u68c0\u7d22\u5668\u8868\u73b0\u4e0d\u4f73\uff0c\u91cd\u6392\u5e8f\u5668\u63d0\u5347\u6709\u9650\uff0c\u5fae\u8c03\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u5373\u4f7f\u662f\u5f3a\u8c03\u63a8\u7406\u7684LLM\u4e5f\u672a\u4f18\u4e8e\u5c0f\u578b\u901a\u7528\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u95ee\u7b54\u7cfb\u7edf\u548c\u6a21\u578b\u5c1a\u672a\u5177\u5907\u5229\u7528\u95f4\u63a5\u6587\u672c\u7ebf\u7d22\u8fdb\u884c\u63a8\u7406\u7b54\u9898\u7684\u80fd\u529b\uff0cInferential QA\u4e3a\u66f4\u9ad8\u5c42\u6b21\u7406\u89e3\u548c\u63a8\u7406\u5f00\u542f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.02459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02459", "abs": "https://arxiv.org/abs/2602.02459", "authors": ["Zhiyu Huang", "Yun Zhang", "Johnson Liu", "Rui Song", "Chen Tang", "Jiaqi Ma"], "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments", "comment": null, "summary": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5b9e\u65f6\u63a7\u5236\u3001\u5177\u5907\u5ef6\u8fdf\u611f\u77e5\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578bTIC-VLA\uff0c\u80fd\u6709\u6548\u8865\u507f\u8bed\u4e49\u63a8\u7406\u8fc7\u7a0b\u7684\u5ef6\u8fdf\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u673a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u666e\u904d\u5047\u8bbe\u8bed\u4e49\u63a8\u7406\u4e0e\u52a8\u4f5c\u63a7\u5236\u7684\u65f6\u95f4\u5b8c\u5168\u540c\u6b65\uff0c\u7136\u800c\u5b9e\u9645\u4e2d\uff0c\u8bed\u4e49\u63a8\u7406\u5f80\u5f80\u6bd4\u52a8\u4f5c\u6267\u884c\u5b58\u5728\u672c\u8d28\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u673a\u5668\u4eba\u65e0\u6cd5\u51c6\u786e\u3001\u53ca\u65f6\u5730\u54cd\u5e94\u590d\u6742\u6307\u4ee4\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u56e0\u8bed\u4e49\u5ef6\u8fdf\u5e26\u6765\u7684\u52a8\u4f5c\u6267\u884c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faTIC-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5ef6\u8fdf\u611f\u77e5\u7684\u8bed\u4e49-\u63a7\u5236\u63a5\u53e3\uff0c\u5728\u52a8\u4f5c\u751f\u6210\u65f6\u7efc\u5408\u8003\u8651\u5ef6\u8fdf\u540c\u6e90\u7684\u8bed\u4e49\u72b6\u6001\u3001\u663e\u5f0f\u5ef6\u8fdf\u5143\u6570\u636e\u548c\u5f53\u524d\u89c2\u6d4b\uff0c\u4ece\u800c\u8865\u507f\u63a8\u7406\u65f6\u5ef6\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u5ef6\u8fdf\u4e00\u81f4\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u5f15\u5165\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f7f\u8bad\u7ec3\u4e0e\u5b9e\u9645\u90e8\u7f72\u65f6\u7684\u5f02\u6b65\u573a\u666f\u76f8\u5339\u914d\u3002\u5b9e\u9a8c\u5e73\u53f0\u65b9\u9762\uff0c\u5f00\u53d1\u4e86DynaNav\u4eff\u771f\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cTIC-VLA\u6a21\u578b\u5728\u591a\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u4f9d\u7136\u4fdd\u6301\u4e86\u9c81\u68d2\u800c\u9ad8\u6548\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u5176\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edfVLA\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TIC-VLA\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u7684\u8bed\u4e49-\u63a7\u5236\u8026\u5408\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e0b\u3001\u65f6\u5ef6\u6761\u4ef6\u4e0b\u7684\u6574\u4f53\u63a7\u5236\u9c81\u68d2\u6027\u548c\u6307\u4ee4\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2602.00579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00579", "abs": "https://arxiv.org/abs/2602.00579", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Yanye Lu"], "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration", "comment": "Accepted by ICLR 2026", "summary": "Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u50cf\u590d\u539f\u65b9\u6cd5BDG\uff08Bridging Degradation discrimination and Generation\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u964d\u8d28\u5224\u522b\u4e0e\u751f\u6210\u878d\u5408\u673a\u5236\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u9000\u5316\u4e0e\u590d\u539f\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u56fe\u50cf\u590d\u539f\u4efb\u52a1\u9762\u4e34\u5982\u4f55\u533a\u5206\u591a\u79cd\u56fe\u50cf\u964d\u8d28\u7c7b\u578b\u53ca\u6c34\u5e73\uff0c\u5e76\u636e\u6b64\u6709\u6548\u590d\u539f\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u964d\u8d28\u5224\u522b\u4e0e\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "method": "\u63d0\u51faMAS-GLCM\uff08\u591a\u89d2\u5ea6\u591a\u5c3a\u5ea6\u7070\u5ea6\u5171\u751f\u77e9\u9635\uff09\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u5224\u522b\u964d\u8d28\u7c7b\u578b\u4e0e\u6c34\u5e73\u3002\u5c06\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u751f\u6210\u3001\u6865\u63a5\u548c\u590d\u539f\u4e09\u4e2a\u9636\u6bb5\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\uff0c\u5c06MAS-GLCM\u5224\u522b\u4fe1\u606f\u6ce8\u5165\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u9000\u5316\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u4e0e\u590d\u539f\u80fd\u529b\u3002", "result": "BDG\u65b9\u6cd5\u5728all-in-one\u590d\u539f\u548c\u771f\u5b9e\u8d85\u5206\u8fa8\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u771f\u503c\u4fdd\u771f\u5ea6\u4e0e\u611f\u77e5\u8d28\u91cf\u7684\u53cc\u8d62\uff0c\u8d85\u8d8a\u4e86\u5df2\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u6027\u5730\u7ed3\u5408\u964d\u8d28\u5224\u522b\u4e0e\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u4fe1\u606f\uff0cBDG\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u66f4\u5f3a\u5927\u3001\u6cdb\u5316\u66f4\u597d\u7684\u56fe\u50cf\u590d\u539f\u80fd\u529b\uff0c\u5bf9\u590d\u6742\u9000\u5316\u53ca\u591a\u4efb\u52a1\u573a\u666f\u5177\u6709\u76f4\u63a5\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.01240", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01240", "abs": "https://arxiv.org/abs/2602.01240", "authors": ["Ke Sun", "Guangsheng Bao", "Han Cui", "Yue Zhang"], "title": "Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection", "comment": null, "summary": "Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5DetectRouter\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8f93\u5165\u9009\u62e9\u6700\u5408\u9002\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u7528\u56fa\u5b9a\u4ee3\u7406\u6a21\u578b\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\uff0c\u4f46\u6027\u80fd\u5bf9\u4ee3\u7406\u4e0e\u771f\u5b9e\u6e90\u7684\u5339\u914d\u5ea6\u975e\u5e38\u654f\u611f\uff0c\u6ca1\u6709\u4e00\u6b3e\u4ee3\u7406\u80fd\u901a\u7528\u4e8e\u6240\u6709\u8f93\u5165\u3002\u5982\u4f55\u4e3a\u6bcf\u4e2a\u8f93\u5165\u9009\u62e9\u6700\u4f73\u4ee3\u7406\uff0c\u662f\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faDetectRouter\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u767d\u76d2\u6a21\u578b\u6784\u5efa\u533a\u5206\u6027\u7684\u539f\u578b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u51e0\u4f55\u8ddd\u79bb\u4e0e\u68c0\u6d4b\u5206\u6570\u7684\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5bf9\u9ed1\u76d2\u6e90\u7684\u6cdb\u5316\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9\u6bcf\u4e2a\u8f93\u5165\u8def\u7531\u5230\u6700\u4f73\u68c0\u6d4b\u4ee3\u7406\u3002", "result": "\u5728EvoBench\u548cMAGE\u57fa\u51c6\u4e0a\uff0cDetectRouter\u5728\u591a\u79cd\u68c0\u6d4b\u6807\u51c6\u548c\u6a21\u578b\u5bb6\u65cf\u4e0a\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u4ee3\u7406\u6a21\u578b\uff0cDetectRouter\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8def\u7531\u95ee\u9898\u5728\u751f\u6210\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.02473", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02473", "abs": "https://arxiv.org/abs/2602.02473", "authors": ["Yinhuai Wang", "Qihan Zhao", "Yuen Fui Lau", "Runyi Yu", "Hok Wai Tsui", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang", "Ping Tan"], "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos", "comment": null, "summary": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HumanX\u6846\u67b6\uff0c\u53ef\u5c06\u4eba\u7c7b\u89c6\u9891\u8f6c\u5316\u4e3a\u53ef\u6cdb\u5316\u7684\u7c7b\u4eba\u673a\u5668\u4eba\u4ea4\u4e92\u6280\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6280\u80fd\u8fc1\u79fb\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5236\u9020\u7c7b\u4eba\u673a\u5668\u4eba\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u771f\u5b9e\u591a\u53d8\u73af\u5883\u548c\u4efb\u52a1\u4e00\u76f4\u5f88\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u5230\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\u7a00\u7f3a\u6216\u9700\u7e41\u7410\u5956\u52b1\u8bbe\u8ba1\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u63a8\u5e7f\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u67b6\u6784\u6253\u7834\u8fd9\u4e00\u74f6\u9888\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u66f4\u9ad8\u6548\u5730\u5b66\u4e60\u5e76\u6cdb\u5316\u5404\u79cd\u4ea4\u4e92\u6280\u80fd\u3002", "method": "\u63d0\u51faHumanX\u5168\u6808\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09XGen\u6a21\u5757\uff0c\u80fd\u5c06\u4eba\u7c7b\u89c6\u9891\u8f6c\u6362\u4e3a\u591a\u6837\u3001\u7269\u7406\u5408\u7406\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u5e76\u8fdb\u884c\u6269\u589e\uff1b2\uff09XMimic\u6a21\u5757\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u4eff\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u548c\u6cdb\u5316\u4ea4\u4e92\u6280\u80fd\uff1b\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bbe\u8ba1\u5956\u52b1\u3002", "result": "\u5728\u4e94\u5927\u9886\u57df\uff08\u7bee\u7403\u3001\u8db3\u7403\u3001\u7fbd\u6bdb\u7403\u3001\u642c\u8fd0\u3001\u5bf9\u6297\uff09\u8bc4\u4f30\uff0cHumanX\u5b66\u4f1a\u4e8610\u9879\u590d\u6742\u6280\u80fd\uff0c\u5305\u62ec\u590d\u6742\u7bee\u7403\u52a8\u4f5c\u548c\u4e0e\u4eba\u6301\u7eed\u4f20\u7403\u7b49\uff0c\u8fd9\u4e9b\u6280\u80fd\u53ef\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u5b9e\u4f53\u7c7b\u4eba\u673a\u5668\u4eba\uff0c\u5e76\u4e14\u6cdb\u5316\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad88\u500d\u4ee5\u4e0a\u3002", "conclusion": "HumanX\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u591a\u6837\u3001\u6cdb\u5316\u7684\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6280\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u7684\u9014\u5f84\uff0c\u6709\u671b\u7a81\u7834\u673a\u5668\u4eba\u4ea4\u4e92\u667a\u80fd\u53d1\u5c55\u7684\u74f6\u9888\u3002"}}
{"id": "2602.00583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00583", "abs": "https://arxiv.org/abs/2602.00583", "authors": ["Xiangdong Li", "Ye Lou", "Ao Gao", "Wei Zhang", "Siyang Song"], "title": "MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation", "comment": null, "summary": "The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MAUGen\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u5177\u6709\u7cbe\u786e\u8868\u60c5\u52a8\u4f5c\u5355\u5143\u6807\u7b7e\uff08\u5305\u542b\u51fa\u73b0\u4e0e\u5f3a\u5ea6\uff09\u548c\u8eab\u4efd\u591a\u6837\u7684\u9ad8\u8d28\u91cf\u4eba\u8138\u56fe\u50cf\u3002\u65b0\u65b9\u6cd5\u63d0\u5347\u4e86\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u4e3a\u540e\u7eedAU\u8bc6\u522b\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u4eba\u53e3\u591a\u6837\u4e14\u5e26\u6709\u7cbe\u786e\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u4fe1\u606f\u7684\u4eba\u8138\u56fe\u50cf\u6570\u636e\uff0c\u8fd9\u6210\u4e3a\u5f00\u53d1\u5e7f\u6cdb\u9002\u7528AU\u8bc6\u522b\u7cfb\u7edf\u7684\u74f6\u9888\u3002\u56e0\u6b64\u9700\u8981\u521b\u9020\u65b0\u7684\u751f\u6210\u5f0f\u6846\u67b6\u4ee5\u4e30\u5bcc\u76f8\u5173\u6570\u636e\u8d44\u6e90\u3002", "method": "\u63d0\u51faMAUGen\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e24\u5927\u6a21\u5757\uff1a1\uff09\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6a21\u5757\uff08MRL\uff09\uff0c\u8054\u5408\u8868\u5f81\u6587\u672c\u3001\u8eab\u4efd\u3001\u8868\u60c5\u548cAU\u6807\u7b7e\uff1b2\uff09\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u6807\u7b7e\u751f\u6210\u6a21\u5757\uff08DIG\uff09\uff0c\u53ef\u4ee5\u4ece\u7edf\u4e00\u6f5c\u7a7a\u95f4\u89e3\u7801\u4e3a\u8eab\u4efd\u591a\u6837\u3001\u6807\u7b7e\u4e00\u81f4\u7684\u4eba\u8138\u4e0eAU\u6210\u5bf9\u6837\u672c\u3002", "result": "\u57fa\u4e8eMAUGen\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u5927\u89c4\u6a21\u3001\u591a\u8eab\u4efd\u3001AU\u6ce8\u91ca\u9f50\u5168\u7684\u5408\u6210\u8138\u90e8\u52a8\u4f5c\u5355\u5143\u6570\u636e\u96c6\uff08MIFA\uff09\uff0c\u5e76\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u56fe\u50cf\u8d28\u91cf\u3001\u591a\u6837\u6027\u53ca\u6807\u7b7e\u4e00\u81f4\u6027\u3002", "conclusion": "MAUGen\u4e3a\u9762\u90e8\u8868\u60c5\u52a8\u4f5c\u5355\u5143\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6837\u672c\u4e0d\u8db3\u53ca\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u4eba\u8138\u53caAU\u6807\u7b7e\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AU\u8bc6\u522b\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01244", "abs": "https://arxiv.org/abs/2602.01244", "authors": ["Siwei Wu", "Yizhi Li", "Yuyang Song", "Wei Zhang", "Yang Wang", "Riza Batista-Navarro", "Xian Yang", "Mingjie Tang", "Bryan Dai", "Jian Yang", "Chenghua Lin"], "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments", "comment": "Agentic Trajectory, Agentic Model, Terminal, Code Agent", "summary": "Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\textbf{\\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \\textbf{\\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \\textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, \\textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTerminalTraj\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6267\u884c\u3001\u53ef\u9a8c\u8bc1\u7684\u7ec8\u7aef\u4efb\u52a1\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u7ec8\u7aef\u73af\u5883\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7ec8\u7aef\u4efb\u52a1\u9700\u8981\u6a21\u578b\u7406\u89e3\u548c\u64cd\u4f5c\u590d\u6742\u3001\u591a\u6837\u4e14\u957f\u671f\u7684\u547d\u4ee4\u884c\u64cd\u4f5c\uff0c\u4f46\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\u6536\u96c6\u96be\u5ea6\u5927\u3002\u6311\u6218\u4e3b\u8981\u6709\uff1a\u6bcf\u4e2a\u4efb\u52a1\u9700\u72ec\u7acb\u3001\u9002\u914d\u7684Docker\u73af\u5883\uff08\u4fdd\u8bc1\u53ef\u6267\u884c\u6027\uff09\uff0c\u4e14\u591a\u6837\u8f93\u51fa\u96be\u7edf\u4e00\u9a8c\u8bc1\uff08\u4fdd\u8bc1\u53ef\u9a8c\u8bc1\u6027\uff09\uff0c\u8fd9\u4e9b\u90fd\u5236\u7ea6\u4e86Terminal Agent\u6a21\u578b\u5927\u89c4\u6a21\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u6784\u5efaTerminalTraj\u6d41\u7a0b\uff0c\u5305\u62ec\uff1a1\uff09\u4e25\u9009\u9ad8\u8d28\u91cf\u4ed3\u5e93\uff0c\u81ea\u52a8\u6784\u5efaDocker\u73af\u5883\uff1b2\uff09\u4efb\u52a1\u5b9e\u4f8b\u81ea\u52a8\u4e0eDocker\u73af\u5883\u5bf9\u9f50\u751f\u6210\uff1b3\uff09\u81ea\u52a8\u5408\u6210\u53ef\u6267\u884c\u7684\u8f68\u8ff9\u53ca\u9a8c\u8bc1\u4ee3\u7801\uff0c\u786e\u4fdd\u4ea7\u51fa\u7684\u6570\u636e\u65e2\u53ef\u6267\u884c\u53c8\u53ef\u81ea\u52a8\u9a8c\u8bc1\u3002\u6700\u7ec8\uff0c\u751f\u6210\u4e86\u8986\u76d68\u7c7b\u9886\u57df\u768432K Docker\u955c\u50cf\u548c5\u4e07\u4f59\u6761\u8f68\u8ff9\u3002", "result": "\u57fa\u4e8eTerminalTraj\u6570\u636e\uff0c\u4f7f\u7528Qwen2.5-Coder\u6a21\u578b\u8bad\u7ec3\uff0c\u6a21\u578b\u5728TerminalBench\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u663e\u8457\uff1aTB 1.0\u63d0\u534720%\uff0cTB 2.0\u63d0\u534710%\u3002\u7279\u522b\u662fTerminalTraj-32B\u6a21\u578b\uff0c\u5728\u5c0f\u4e8e100B\u53c2\u6570\u89c4\u6a21\u4e0b\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\uff08TB 1.0\u4e0a\u4e3a35.30%\uff0cTB 2.0\u4e0a\u4e3a22.00%\uff09\uff0c\u4e14\u63a8\u7406\u65f6\u6269\u5c55\u6027\u8868\u73b0\u4f18\u826f\u3002", "conclusion": "TerminalTraj\u6709\u6548\u89e3\u51b3\u4e86\u7ec8\u7aef\u4efb\u52a1\u53ef\u6267\u884c\u6027\u4e0e\u53ef\u9a8c\u8bc1\u6027\u96be\u9898\uff0c\u4e3a\u8bad\u7ec3\u66f4\u5f3a\u7684\u7ec8\u7aef\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u575a\u5b9e\u6570\u636e\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u5927\u6a21\u578b\u5b9e\u9645\u8868\u73b0\u3002\u6240\u6709\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.02481", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02481", "abs": "https://arxiv.org/abs/2602.02481", "authors": ["Brent Yi", "Hongsuk Choi", "Himanshu Gaurav Singh", "Xiaoyu Huang", "Takara E. Truong", "Carmelo Sferrazza", "Yi Ma", "Rocky Duan", "Pieter Abbeel", "Guanya Shi", "Karen Liu", "Angjoo Kanazawa"], "title": "Flow Policy Gradients for Robot Control", "comment": "Project webpage: https://hongsukchoi.github.io/fpo-control", "summary": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eflow matching\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u7b56\u7565\u68af\u5ea6\u5728\u52a8\u4f5c\u5206\u5e03\u9009\u62e9\u4e0a\u7684\u5c40\u9650\uff0c\u5728\u591a\u4e2a\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u548c\u865a\u5b9e\u8f6c\u79fb\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f3c\u7136\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u53d7\u9650\u4e8e\u53ef\u5fae\u5206\u52a8\u4f5c\u4f3c\u7136\uff0c\u9700\u8981\u8f93\u51fa\u7b80\u5355\u5206\u5e03\uff08\u5982\u9ad8\u65af\uff09\uff0c\u9650\u5236\u4e86\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u3002\u8feb\u5207\u9700\u8981\u80fd\u591f\u652f\u6301\u66f4\u590d\u6742\u7b56\u7565\u5206\u5e03\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5728\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u5c06flow matching\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u5e94\u7528\u5230\u673a\u5668\u4eba\u63a7\u5236\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u591a\u79cd\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u8db3\u5f0f\u884c\u8d70\u3001\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u8ddf\u8e2a\u3001\u64cd\u63a7\u4efb\u52a1\u7b49\uff09\u4e2d\u6709\u6548\u8bad\u7ec3\u548c\u5fae\u8c03\u66f4\u5177\u8868\u73b0\u529b\u7684\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u7ed5\u8fc7\u4e86\u52a8\u4f5c\u4f3c\u7136\u8ba1\u7b97\u6b65\u9aa4\u3002\u5e76\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\u548c\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u65f6\u80fd\u591f\u5145\u5206\u5229\u7528flow\u8868\u793a\u8fdb\u884c\u63a2\u7d22\uff0c\u5728\u5fae\u8c03\u65b9\u9762\u8f83\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4ee5\u53ca\u4e24\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u865a\u5b9e\u8f6c\u79fb\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "flow matching\u7b56\u7565\u68af\u5ea6\u7a81\u7834\u4e86\u4f20\u7edf\u4f3c\u7136\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u8bad\u7ec3\u4e0e\u5fae\u8c03\u6548\u679c\uff0c\u4e3a\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.00593", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00593", "abs": "https://arxiv.org/abs/2602.00593", "authors": ["Yifan Jiang", "Cong Zhang", "Bofei Zhang", "Yifan Yang", "Bingzhang Wang", "Yew-Soon Ong"], "title": "From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking", "comment": null, "summary": "Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8be6\u7ec6\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u77e5\u8bc6\u63a8\u7406\u7ed3\u5408\u80fd\u529b\u7684\u65b0\u57fa\u51c6Pix2Fact\u3002\u5b9e\u9a8c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u663e\u793a\u51fa\u73b0\u6709VLMs\u7684\u660e\u663e\u77ed\u677f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u53ea\u80fd\u5355\u72ec\u8bc4\u4f30\u89c6\u89c9\u5b9a\u4f4d\u6216\u63a8\u7406\uff0c\u4f46\u4e0d\u80fd\u6d4b\u8bd5\u4e24\u8005\u534f\u540c\u80fd\u529b\uff0c\u96be\u4ee5\u53cd\u6620\u73b0\u5b9e\u590d\u6742\u573a\u666f\u9700\u6c42\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u57fa\u51c6\u63a8\u52a8\u6a21\u578b\u5728\u7ec6\u81f4\u89c6\u89c9\u7406\u89e3\u4e0e\u591a\u6b65\u903b\u8f91\u63a8\u7406\u7ed3\u5408\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1000\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u7247\u3001\u8986\u76d68\u7c7b\u65e5\u5e38\u573a\u666f\u7684\u65b0VQA\u57fa\u51c6Pix2Fact\u3002\u95ee\u9898\u7531\u9876\u5c16\u535a\u58eb\u4e0e\u4e13\u4e1a\u56e2\u961f\u4e25\u5bc6\u8bbe\u8ba1\uff0c\u8981\u6c42\u89c6\u89c9\u7ec6\u8282\u5b9a\u4f4d\u3001\u5916\u90e8\u77e5\u8bc6\u878d\u5408\u4e0e\u591a\u8df3\u63a8\u7406\uff0c\u5e76\u57289\u4e2a\u4e3b\u6d41VLM\u6a21\u578b\u548c\u4eba\u7c7b\u4e0a\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u6700\u5f3a\u6a21\u578b\u4ec5\u83b7\u5f9724.0%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u800c\u4eba\u7c7b\u51c6\u786e\u7387\u4e3a56%\uff0c\u8868\u660ePix2Fact\u5bf9\u5f53\u524dVLMs\u6784\u6210\u6781\u5927\u6311\u6218\u3002", "conclusion": "Pix2Fact\u5145\u5206\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u7ea7\u89c6\u89c9\u7406\u89e3\u4e0a\u7684\u5c40\u9650\uff0c\u6709\u671b\u6210\u4e3a\u63a8\u52a8\u591a\u6a21\u6001\u667a\u80fd\u4f53\u878d\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u4e0e\u6df1\u5c42\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2602.01246", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01246", "abs": "https://arxiv.org/abs/2602.01246", "authors": ["Jamshid Mozafari", "Seyed Parsa Mousavinasab", "Adam Jatowt"], "title": "PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian", "comment": "Submitted to SIGIR 2026", "summary": "Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PARSE\uff0c\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u6ce2\u65af\u8bed\u7684\u5f00\u653e\u9886\u57df\u63a8\u7406\u578b\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u9898\u578b\u548c\u63a8\u7406\u7c7b\u578b\uff0c\u65e8\u5728\u5f25\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u9ad8\u8d28\u91cfQA\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u63a8\u7406\u578b\u95ee\u7b54\u9886\u57df\u7684\u7814\u7a76\u548c\u8bc4\u6d4b\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u7cfb\u7edf\u6027\u7684\u63a8\u7406\u578b\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u76f8\u5173QA\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e0a\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10800\u9053\u9898\u76ee\u7684PARSE\u57fa\u51c6\uff0c\u6db5\u76d6\u5e03\u5c14\u578b\u3001\u9009\u62e9\u9898\u548c\u4e8b\u5b9e\u578b\u95ee\u7b54\uff0c\u5e76\u63a7\u5236\u751f\u6210\u6d41\u7a0b\u4fdd\u8bc1\u63a8\u7406\u591a\u6837\u6027\u3002\u6570\u636e\u96c6\u901a\u8fc7LLM\u751f\u6210\u4e0e\u4eba\u5de5\u6821\u9a8c\u3001\u591a\u9636\u6bb5\u7b5b\u9009\u3001\u6ce8\u91ca\u53ca\u4e00\u81f4\u6027\u68c0\u67e5\u63d0\u5347\u8bed\u8a00\u4e0e\u4e8b\u5b9e\u8d28\u91cf\u3002\u5bf9\u591a\u8bed\u8a00\u4e0e\u6ce2\u65af\u8bedLLM\u6a21\u578b\u91c7\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff08\u5982\u94fe\u5f0f\u63a8\u7406\u3001\u5c11\u6837\u672c\u5b66\u4e60\uff09\u5f00\u5c55\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u6ce2\u65af\u8bed\u4e0e\u7ed3\u6784\u5316\u7684\u63d0\u793a\u7b56\u7565\uff08\u94fe\u5f0f\u63a8\u7406/\u5c11\u6837\u672c\u5b66\u4e60\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u4e00\u6b65\u5fae\u8c03\uff08fine-tuning\uff09\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u7279\u522b\u9488\u5bf9\u6ce2\u65af\u8bed\u7684\u6a21\u578b\u6548\u679c\u3002", "conclusion": "PARSE\u6709\u6548\u586b\u8865\u4e86\u6ce2\u65af\u8bed\u53ca\u4f4e\u8d44\u6e90\u8bed\u8a00\u63a8\u7406\u578b\u95ee\u7b54\u8bc4\u6d4b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u652f\u6301\u6a21\u578b\u7684\u516c\u5e73\u5bf9\u6bd4\u548c\u5b9e\u7528\u6027\u6539\u8fdb\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7814\u53d1\u4e0e\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.00618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00618", "abs": "https://arxiv.org/abs/2602.00618", "authors": ["Yian Zhao", "Rushi Ye", "Ruochong Zheng", "Zesen Cheng", "Chaoran Feng", "Jiashu Yang", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting", "comment": "ICCV 2025", "summary": "3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \\textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8c03\u8282\u98ce\u683c\u5f3a\u5ea6\u76843D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u201cTune-Your-Style\u201d\uff0c\u5b9e\u73b0\u4e863D\u573a\u666f\u98ce\u683c\u6548\u679c\u7684\u81ea\u5b9a\u4e49\u8c03\u6574\uff0c\u517c\u987e\u4e86\u5185\u5bb9\u4e0e\u98ce\u683c\uff0c\u5e76\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "3D\u98ce\u683c\u8fc1\u79fb\u7684\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u5e73\u88613D\u5185\u5bb9\u672c\u8eab\u548c\u53c2\u8003\u98ce\u683c\u7684\u56fe\u6848\u3001\u8272\u5f69\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7075\u6d3b\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u5bf9\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u7684\u5dee\u5f02\u5316\u9700\u6c42\uff0c\u4ec5\u80fd\u8f93\u51fa\u56fa\u5b9a\u7684\u98ce\u683c\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5177\u5907\u98ce\u683c\u5f3a\u5ea6\u53ef\u8c03\u8282\u6027\u76843D\u98ce\u683c\u8fc1\u79fb\u65b9\u6848\uff0c\u4ee5\u589e\u5f3a\u81ea\u5b9a\u4e49\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u9ad8\u65af\u795e\u7ecf\u5143\u5bf9\u98ce\u683c\u5f3a\u5ea6\u8fdb\u884c\u663e\u5f0f\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u98ce\u683c\u8c03\u8282\u5668\uff0c\u5b9e\u73b0\u4e86\u98ce\u683c\u6ce8\u5165\u7684\u5f3a\u5ea6\u53ef\u8c03\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u53ef\u8c03\u8282\u98ce\u683c\u5316\u5f15\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u98ce\u683c\u5bf9\u9f50\uff0c\u4ece\u6269\u6563\u6a21\u578b\u83b7\u5f97\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u98ce\u683c\u5316\u89c6\u56fe\uff0c\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u6765\u5e73\u8861\u98ce\u683c\u5f15\u5bfc\u548c\u5185\u5bb9\u4fdd\u7559\uff0c\u786e\u4fdd\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u7a33\u5b9a\u9ad8\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u751f\u6210\u89c6\u89c9\u4e0a\u4ee4\u4eba\u6ee1\u610f\u76843D\u98ce\u683c\u5316\u7ed3\u679c\uff0c\u8fd8\u80fd\u6839\u636e\u7528\u6237\u9700\u8981\u7075\u6d3b\u8c03\u8282\u98ce\u683c\u5f3a\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u98ce\u683c\u8fc1\u79fb\u7684\u81ea\u5b9a\u4e49\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684Tune-Your-Style\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e863D\u98ce\u683c\u8fc1\u79fb\u4e2d\u5185\u5bb9\u4e0e\u98ce\u683c\u7684\u7075\u6d3b\u5e73\u8861\uff0c\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u548c\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u4e863DGS\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01274", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01274", "abs": "https://arxiv.org/abs/2602.01274", "authors": ["Situo Zhang", "Yifan Zhang", "Zichen Zhu", "Hankun Wang", "Da Ma", "Danyang Zhang", "Lu Chen", "Kai Yu"], "title": "PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length", "comment": null, "summary": "Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e86Pacer\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574draft\u957f\u5ea6\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86Speculative Decoding\u7684\u6548\u7387\u3002", "motivation": "\u539f\u6709\u7684Speculative Decoding\u65b9\u6cd5\u5728draft\u957f\u5ea6\u56fa\u5b9a\u65f6\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u63a8\u7406\u6b65\u9aa4\u6240\u9700\u7684\u6700\u4f18\u957f\u5ea6\uff0c\u9650\u5236\u4e86\u63a8\u7406\u52a0\u901f\u7684\u6f5c\u529b\u3002", "method": "Pacer\u5728SD\u57fa\u7840\u4e0a\u52a0\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u4e14\u53ef\u8bad\u7ec3\u7684pre-verification\u5c42\uff0c\u5bf9draft token\u5757\u8fdb\u884c\u9884\u9a8c\u8bc1\uff0c\u82e5\u4e0d\u901a\u8fc7\u5219\u963b\u6b62\u8fdb\u4e00\u6b65\u751f\u6210\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574draft\u957f\u5ea6\u3002", "result": "\u5728\u591a\u4e2aSD\u6a21\u578b\u7ec4\u5408\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0cPacer\u6bd4\u4f20\u7edfautoregressive\u89e3\u7801\u6700\u9ad8\u63d0\u901f2.66\u500d\uff0c\u5e76\u5728\u5404\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6SD\uff1b\u7ed3\u5408Ouroboros\u540e\u63d0\u901f\u6700\u9ad8\u53ef\u8fbe3.09\u500d\u3002", "conclusion": "Pacer\u901a\u8fc7\u52a8\u6001draft\u957f\u5ea6\u63a7\u5236\u548c\u9884\u9a8c\u8bc1\u673a\u5236\uff0c\u80fd\u8fdb\u4e00\u6b65\u91ca\u653eSpeculative Decoding\u5728LLM\u63a8\u7406\u52a0\u901f\u4e2d\u7684\u6f5c\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00621", "abs": "https://arxiv.org/abs/2602.00621", "authors": ["Guangtao Lyu", "Xinyi Cheng", "Qi Liu", "Chenghao Xu", "Jiexi Yan", "Muli Yang", "Fen Fang", "Cheng Deng"], "title": "Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering", "comment": null, "summary": "LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91c7\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u89e3\u89c6\u89c9\u5927\u6a21\u578b\uff08LVLMs\uff09\u5185\u90e8\u8868\u5f81\uff0c\u53d1\u73b0\u5e7b\u89c9\u73b0\u8c61\u4e0e\u7279\u5b9a\u795e\u7ecf\u5143\u5f02\u5e38\u6fc0\u6d3b\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u63a7\u5e72\u9884\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u9488\u5bf9\u8f93\u51fa\u5c42\u51cf\u5c11\u5e7b\u89c9\uff0c\u672a\u6df1\u5165\u5206\u6790\u5e7b\u89c9\u4ea7\u751f\u7684\u5185\u90e8\u673a\u5236\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u5185\u90e8\u8868\u5f81\uff0c\u7406\u89e3\u5e76\u4ece\u6e90\u5934\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5c06LVLMs\u7684\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165\u5206\u89e3\u4e3a\u7a00\u758f\u3001\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u5143\u7c7b\u578b\u3002\u5229\u7528\u5bf9\u6bd4\u5206\u6790\uff08CNS\uff09\uff0c\u8bc6\u522b\u548c\u5e72\u9884\u56fe\u50cf\u7279\u5f02\u6027\u795e\u7ecf\u5143\uff0c\u5bf9\u5176\u8fdb\u884c\u9009\u62e9\u6027\u589e\u5f3a\u6216\u6291\u5236\uff0c\u4ee5\u6539\u5584\u89c6\u89c9\u7406\u89e3\u548c\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u63ed\u793a\uff0c\u5e7b\u89c9\u901a\u5e38\u6e90\u81ea\u56fe\u50cf\u7279\u5f02\u6027\u795e\u7ecf\u5143\u7684\u5f02\u5e38\u6fc0\u6d3b\u6216\u5931\u8c03\u3002CNS\u65b9\u6cd5\u53ef\u6709\u6548\u589e\u5f3a\u6709\u7528\u795e\u7ecf\u5143\u3001\u6291\u5236\u566a\u58f0\u6fc0\u6d3b\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6548\u679c\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\uff0c\u4ece\u795e\u7ecf\u5143\u5c42\u9762\u5bf9LVLMs\u8fdb\u884c\u8868\u5f81\u4e0e\u5e72\u9884\uff0c\u53ef\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u591a\u6a21\u6001\u7406\u89e3\uff0c\u4e14\u65b9\u6cd5\u4e0e\u73b0\u6709\u540e\u89e3\u7801\u9636\u6bb5\u6280\u672f\u517c\u5bb9\u3002"}}
{"id": "2602.01313", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01313", "abs": "https://arxiv.org/abs/2602.01313", "authors": ["Chuanrui Hu", "Tong Li", "Xingze Gao", "Hongda Chen", "Dannong Xu", "Yi Bai", "Tianwei Lin", "Xinda Zhao", "Xiaohong Li", "Jiaqi An", "Yunyun Han", "Jian Pei", "Yafeng Deng"], "title": "EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models", "comment": "10 pages, 2 figures, 4 tables", "summary": "Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EverMemBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u957f\u65f6\u8bb0\u5fc6\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u591a\u65b9\u3001\u591a\u4e3b\u9898\u3001\u89d2\u8272\u5206\u660e\u7684\u957f\u5bf9\u8bdd\u573a\u666f\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u590d\u6742\u8bb0\u5fc6\u3001\u63a8\u7406\u548c\u68c0\u7d22\u65b9\u9762\u7684\u91cd\u5927\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u8bc4\u6d4b\u57fa\u51c6\u5927\u591a\u53ea\u5173\u6ce8\u4e24\u4eba\u3001\u5c0f\u8303\u56f4\u4e3b\u9898\u5bf9\u8bdd\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u751f\u6d3b\u590d\u6742\u591a\u53d8\u3001\u4fe1\u606f\u4ea4\u9519\u7684\u957f\u65f6\u5bf9\u8bdd\u9700\u6c42\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u63a8\u52a8\u9002\u7528\u4e8e\u771f\u5b9e\u591a\u65b9\u957f\u671f\u4ea4\u6d41\u573a\u666f\u7684\u8bb0\u5fc6\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86EverMemBench\uff0c\u5305\u542b\u8de8\u4e3b\u9898\u3001\u591a\u89d2\u8272\u3001\u591a\u7ec4\u5bf9\u8bdd\u3001\u8d85\u8fc7\u767e\u4e07tokens\uff0c\u5e76\u968f\u65f6\u95f4\u6f14\u53d8\u3002\u57fa\u51c6\u8986\u76d61000+\u95ee\u7b54\u5bf9\uff0c\u4ece\u7ec6\u7c92\u5ea6\u8bb0\u5fc6\u53ec\u56de\u3001\u8bb0\u5fc6\u89c9\u5bdf\u3001\u7528\u6237\u753b\u50cf\u7406\u89e3\u7b49\u7ef4\u5ea6\u8bc4\u6d4bLLM\u8bb0\u5fc6\u7cfb\u7edf\u3002", "result": "\u8bc4\u6d4b\u663e\u793a\uff1a(1) \u591a\u8df3\u63a8\u7406\u5728\u591a\u65b9\u4f1a\u8bdd\u4e0b\u6781\u96be\uff0c\u5373\u4f7f\u662f\u7406\u60f3\u6a21\u578b\u51c6\u786e\u7387\u4e5f\u4ec526%\uff1b(2) \u65f6\u95f4\u63a8\u7406\u65e0\u6cd5\u4ec5\u51ed\u65f6\u95f4\u6233\uff0c\u9700\u66f4\u590d\u6742\u7684\u8bed\u4e49\u7248\u672c\u7406\u89e3\uff1b(3) \u68c0\u7d22\u73af\u8282\u662f\u5f53\u524d\u8bb0\u5fc6\u80fd\u529b\u77ed\u677f\uff0c\u8bed\u4e49\u68c0\u7d22\u96be\u4ee5\u5173\u8054\u9690\u5f0f\u76f8\u5173\u4f46\u8bed\u4e49\u8de8\u5ea6\u5927\u7684\u4fe1\u606f\u3002", "conclusion": "EverMemBench\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u590d\u6742\u5bf9\u8bdd\u8bb0\u5fc6\u7ed3\u6784\u548c\u68c0\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6311\u6218\u6027\u7684\u6807\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u771f\u5b9e\u573a\u666f\u4e0bLLM\u6df1\u5c42\u8bb0\u5fc6\u7814\u7a76\u3002"}}
{"id": "2602.00627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00627", "abs": "https://arxiv.org/abs/2602.00627", "authors": ["Benxiang Zhai", "Yifang Xu", "Guofeng Zhang", "Yang Li", "Sidan Du"], "title": "FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization", "comment": "Accept by ICANN 2025", "summary": "Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FaceSnap\u65b9\u6cd5\uff0c\u53ef\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u5feb\u901f\u751f\u6210\u9ad8\u4e00\u81f4\u6027\u7684\u4e2a\u6027\u5316\u4eba\u50cf\u56fe\u7247\uff0c\u517c\u987e\u6548\u7387\u548c\u7ec6\u8282\u8fd8\u539f\uff0c\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u9700\u5927\u91cf\u5fae\u8c03\u3001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u8981\u4e48\u8138\u90e8\u7ec6\u8282\u8fd8\u539f\u5ea6\u4e0d\u9ad8\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9ad8\u6548\u3001\u901a\u7528\u4e14\u4fdd\u771f\u7684\u4eba\u50cf\u5b9a\u5236\u751f\u6210\u95ee\u9898\u3002", "method": "\u57fa\u4e8eStable Diffusion\uff0c\u63d0\u51faFaceSnap\u65b9\u6cd5\uff0c\u53ea\u9700\u5355\u5f20\u53c2\u8003\u56fe\u50cf\uff0c\u4e00\u6b65\u63a8\u7406\u5373\u53ef\u5f97\u7ed3\u679c\u3002\u521b\u65b0\u70b9\u5305\u62ec\uff1a\u8bbe\u8ba1\u4e86Facial Attribute Mixer\uff0c\u878d\u5408\u4f4e\u9ad8\u7ea7\u4eba\u8138\u7279\u5f81\uff1b\u5f15\u5165Landmark Predictor\uff0c\u901a\u8fc7\u5730\u6807\u70b9\u5b9e\u73b0\u591a\u6837\u5316\u59ff\u6001\u548c\u7a7a\u95f4\u63a7\u5236\uff1b\u6700\u7ec8\u901a\u8fc7ID-preserving\u6a21\u5757\uff0c\u5c06\u4e0a\u8ff0\u7279\u5f81\u8f93\u5165UNet\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFaceSnap\u5728\u4e2a\u6027\u5316\u4e0e\u5b9a\u5236\u4eba\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u6548\u679c\u6781\u4f73\uff0c\u751f\u6210\u7ed3\u679c\u6bd4\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u66f4\u52a0\u771f\u5b9e\u4e14\u7ec6\u8282\u4e30\u5bcc\u3002", "conclusion": "FaceSnap\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u7e41\u7410\u5fae\u8c03\u5373\u80fd\u9ad8\u6548\u3001\u7cbe\u51c6\u5730\u8fdb\u884c\u4e2a\u6027\u5316\u4eba\u50cf\u751f\u6210\uff0c\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u5e94\u7528\u524d\u666f\uff0c\u5728\u8be5\u9886\u57df\u8d85\u8fc7\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2602.01326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01326", "abs": "https://arxiv.org/abs/2602.01326", "authors": ["Zirui Wu", "Lin Zheng", "Zhihui Xie", "Jiacheng Ye", "Jiahui Gao", "Shansan Gong", "Yansong Feng", "Zhenguo Li", "Wei Bi", "Guorui Zhou", "Lingpeng Kong"], "title": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "comment": "ICLR 2026", "summary": "Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DreamOn\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86DLM\u5728\u751f\u6210\u957f\u5ea6\u52a8\u6001\u53d8\u5316\u5185\u5bb9\u65f6\u53d7\u5230\u4e8b\u5148\u56fa\u5b9amask\u957f\u5ea6\u9650\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u53ef\u53d8\u957f\u5ea6\u7684\u6587\u672c\u8865\u5168\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u8865\u5168\u4efb\u52a1\uff0c\u4f46\u53d7\u9650\u4e8e\u9700\u8981\u9884\u5148\u8bbe\u5b9amask\u957f\u5ea6\uff0c\u8fd9\u5728\u5b9e\u9645\u4ee3\u7801\u8865\u5168\u7b49\u4efb\u52a1\u4e2d\u4e25\u91cd\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u6253\u7834\u8fd9\u4e00\u56fa\u5b9a\u957f\u5ea6\u751f\u6210\u7684\u9650\u5236\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51faDreamOn\u6269\u6563\u6846\u67b6\uff0c\u5728\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u957f\u5ea6\u63a7\u5236\u72b6\u6001\uff0c\u4f7f\u6a21\u578b\u53ef\u4ee5\u81ea\u4e3b\u6269\u5c55\u6216\u6536\u7f29\u8f93\u51fa\u957f\u5ea6\uff0c\u4ec5\u4f9d\u8d56\u81ea\u8eab\u9884\u6d4b\u51b3\u5b9a\u8f93\u51fa\u957f\u5ea6\u3002\u6b64\u673a\u5236\u65e0\u9700\u6539\u53d8\u539f\u6709\u6269\u6563\u6a21\u578b\u7ed3\u6784\uff0c\u53ea\u9700\u5bf9\u8bad\u7ec3\u76ee\u6807\u505a\u5c11\u91cf\u4fee\u6539\u3002\u8be5\u65b9\u6cd5\u88ab\u96c6\u6210\u5230Dream-Coder-7B\u548cDiffuCoder-7B\u4e2d\u3002", "result": "DreamOn\u5728HumanEval-Infilling\u548cSantaCoder-FIM\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u8865\u5168\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5df2\u77e5\u7406\u60f3\u8f93\u51fa\u957f\u5ea6\uff08oracle\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4e0e\u7406\u8bba\u6700\u4f18\u4e00\u81f4\u3002", "conclusion": "DreamOn\u6d88\u9664\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u53d8\u957f\u5ea6\u6587\u672c\u751f\u6210\u4e2d\u7684\u6839\u672c\u969c\u788d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4e3aDLM\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002\u4ee3\u7801\u5df2\u5f00\u653e\u3002"}}
{"id": "2602.00635", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00635", "abs": "https://arxiv.org/abs/2602.00635", "authors": ["Lingsong Wang", "Mancheng Meng", "Ziyan Wu", "Terrence Chen", "Fan Yang", "Dinggang Shen"], "title": "S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning", "comment": null, "summary": "Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86S^3POT\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u9762\u90e8\u56fe\u50cf\u5728\u6709\u906e\u6321\u65f6\u7684\u5206\u5272\u6548\u679c\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4eba\u8138\u751f\u6210\u548c\u81ea\u76d1\u7763\u7a7a\u95f4\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u533a\u57df\u5e38\u88ab\u8bef\u5224\u4e3a\u9762\u90e8\u7ec4\u6210\u90e8\u5206\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u5206\u5272\u65b9\u6cd5\u5728\u9047\u5230\u906e\u6321\u7269\u65f6\u5bb9\u6613\u5c06\u5176\u8bef\u5206\u7c7b\u4e3a\u9762\u90e8\u6210\u5206\uff0c\u4e3b\u8981\u56e0\u4e3a\u906e\u6321\u7269\u7c7b\u578b\u590d\u6742\u5e76\u4e14\u4eba\u5de5\u6807\u6ce8\u906e\u6321\u63a9\u7801\u4ee3\u4ef7\u9ad8\u6602\u3002\u4f5c\u8005\u5e0c\u671b\u65e0\u9700\u6536\u96c6\u6216\u6807\u6ce8\u6240\u6709\u53ef\u80fd\u7684\u906e\u6321\u7c7b\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u6709\u906e\u6321\u4eba\u8138\u7684\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51faS^3POT\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a\u53c2\u8003\u56fe\u50cf\u751f\u6210\uff08RF\uff09\u3001\u7279\u5f81\u589e\u5f3a\uff08FE\uff09\u3001\u63d0\u793a\u9009\u62e9\uff08PS\uff09\u3002\u5229\u7528\u4eba\u8138\u751f\u6210\u5668\u91cd\u5efa\u65e0\u906e\u6321\u7684\u4eba\u8138\u4f5c\u4e3a\u53c2\u7167\uff0c\u6bd4\u8f83\u539f\u56fe\u4e0e\u53c2\u8003\u56fe\u7279\u5f81\u4ee5\u751f\u6210\u7a7a\u95f4\u63d0\u793a\uff0c\u8fdb\u4e00\u6b65\u7528\u8fd9\u4e9b\u63d0\u793a\u6539\u8fdb\u5206\u5272\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5b8c\u6210\u906e\u6321\u5206\u5272\u3002\u6574\u4e2a\u6d41\u7a0b\u4e0d\u4f9d\u8d56\u4e8e\u906e\u6321\u63a9\u7801\u7684\u6807\u6ce8\u3002", "result": "\u5728\u4f5c\u8005\u81ea\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0cS^3POT\u5728\u906e\u6321\u4eba\u8138\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6bcf\u4e2a\u6a21\u5757\u7684\u6709\u6548\u6027\u4e5f\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "S^3POT\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u751f\u6210\u548c\u81ea\u76d1\u7763\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6709\u906e\u6321\u4eba\u8138\u56fe\u50cf\u7684\u5206\u5272\u6548\u679c\uff0c\u65e0\u9700\u8017\u65f6\u7684\u906e\u6321\u63a9\u7801\u6807\u6ce8\uff0c\u672a\u6765\u6709\u671b\u62d3\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.01348", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01348", "abs": "https://arxiv.org/abs/2602.01348", "authors": ["Yu Liu", "Wenxiao Zhang", "Cong Cao", "Fangfang Yuan", "Weizhuo Chen", "Cheng Hu", "Pin Xu", "Yuling Yang", "Kun Peng", "Diandian Guo", "Qiang Sun", "Yanbing Liu", "Jin B. Hong", "Zhiyuan Ma"], "title": "CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering", "comment": null, "summary": "Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRAFT\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u4e2d\u5927\u6a21\u578b\u7684\u63a8\u7406\u53ef\u4fe1\u5ea6\u548c\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u4e2d\uff0c\u73b0\u6709\u7684RAG\u65b9\u6cd5\u5728\u751f\u6210\u56de\u7b54\u65f6\u9762\u4e34\u63a8\u7406\u5d29\u6e83\u3001\u63a8\u7406\u4e0e\u7b54\u6848\u4e0d\u4e00\u81f4\u3001\u8f93\u51fa\u7ed3\u6784\u5931\u63a7\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCRAFT\uff08Calibrated Reasoning with Answer-Faithful Traces\uff09\uff0c\u57fa\u4e8eGroup Relative Policy Optimization (GRPO)\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u786e\u5b9a\u6027\u4e0e\u5224\u522b\u5668\u53cc\u91cd\u5956\u52b1\u673a\u5236\u4f18\u5316\u591a\u8df3\u63a8\u7406\u8fc7\u7a0b\uff0c\u786e\u4fdd\u7ed3\u6784\u548c\u8bed\u4e49\u4e0a\u7684\u5fe0\u5b9e\u6027\u3002\u652f\u6301\u591a\u79cd\u53ef\u63a7\u63a8\u7406\u8f68\u8ff9\uff0c\u4fbf\u4e8e\u5206\u6790\u7ed3\u6784\u4e0e\u5c3a\u5ea6\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cCRAFT\u63d0\u5347\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002CRAFT 7B\u6a21\u578b\u5728\u5404\u79cd\u63a8\u7406\u8f68\u8ff9\u8bbe\u7f6e\u4e0b\uff0c\u8868\u73b0\u63a5\u8fd1\u95ed\u6e90\u5927\u6a21\u578b\u3002", "conclusion": "CRAFT\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u517c\u987e\u4e86\u63a8\u7406\u7684\u7ed3\u6784\u6027\u548c\u8bed\u4e49\u5fe0\u5b9e\u6027\uff0c\u4e3a\u63d0\u5347\u5927\u6a21\u578b\u53ef\u9760\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.00807", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00807", "abs": "https://arxiv.org/abs/2602.00807", "authors": ["Xianzhe Fan", "Shengliang Deng", "Xiaoyang Wu", "Yuxiang Lu", "Zhuoling Li", "Mi Yan", "Yujia Zhang", "Zhizheng Zhang", "He Wang", "Hengshuang Zhao"], "title": "Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds", "comment": null, "summary": "Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5c063D\u4fe1\u606f\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00-\u884c\u4e3a\uff08VLA\uff09\u6a21\u578b\u4ee5\u63d0\u5347\u5176\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u591a\u79cd\u70b9\u4e91\u8f93\u5165\u7684Any3D-VLA\u6846\u67b6\uff0c\u5e76\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u76ee\u524dVLA\u6a21\u578b\u591a\u4ee52D\u56fe\u50cf\u4e3a\u8f93\u5165\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u7406\u89e3\u53d7\u9650\u30023D\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u73b0\u67093D\u6570\u636e\u7a00\u7f3a\u4e14\u5b58\u5728\u9886\u57df\u95f4\u5dee\u5f02\uff0c\u963b\u788d\u4e863D\u89c6\u89c9\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\u3002", "method": "\u63d0\u51faAny3D-VLA\u65b9\u6cd5\uff0c\u5c06\u6a21\u62df\u5668\u3001\u4f20\u611f\u5668\u548c\u6a21\u578b\u4f30\u7b97\u7684\u70b9\u4e91\u7edf\u4e00\u4e3a\u8bad\u7ec3\u8f93\u5165\uff0c\u4ece\u800c\u751f\u6210\u591a\u6837\u5316\u76843D\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u4e0e2D\u7279\u5f81\u878d\u5408\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u7aef\u5230\u7aef\u7ba1\u7ebf\u4ee5\u5b66\u4e60\u9886\u57df\u65e0\u5173\u76843D\u8868\u793a\uff0c\u7f13\u89e3\u6570\u636e\u548c\u9886\u57df\u5dee\u5f02\u5e26\u6765\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u663e\u5f0f\u63d0\u5347\u4e3a\u70b9\u4e91\u80fd\u6709\u6548\u589e\u5f3a\u4e0e2D\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0cAny3D-VLA\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u5747\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u51cf\u8f7b\u4e86\u9886\u57df\u95f4\u5dee\u5f02\u5f71\u54cd\u3002", "conclusion": "\u5f15\u5165\u591a\u79cd\u70b9\u4e91\u4fe1\u606f\u4e0e2D\u7279\u5f81\u878d\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8de8\u73af\u5883\u89c6\u89c9\u7406\u89e3\u548c\u884c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u4e0e\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.00637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00637", "abs": "https://arxiv.org/abs/2602.00637", "authors": ["Vivek Madhavaram", "Vartika Sengar", "Arkadipta De", "Charu Sharma"], "title": "VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning", "comment": "WACV 2026, Project page: https://vivekmadhavaram.github.io/vizor/", "summary": "Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like \"left/right\", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86VIZOR\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u76f4\u63a5\u4ece3D\u573a\u666f\u751f\u6210\u89c6\u89d2\u4e0d\u53d8\u7684\u573a\u666f\u56fe\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u5728\u751f\u6210\u573a\u666f\u56fe\u65f6\u4f9d\u8d56\u591a\u79cd\u8f93\u5165\u4e14\u6613\u53d7\u89c6\u89d2\u5f71\u54cd\uff0c\u7a7a\u95f4\u5173\u7cfb\u5982\u5de6\u53f3\u65b9\u5411\u901a\u5e38\u4e0d\u7a33\u5b9a\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faVIZOR\uff0c\u4e00\u4e2a\u8bad\u7ec3\u81ea\u7531\u3001\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb3D\u573a\u666f\u751f\u6210\u7a20\u5bc6\u4e14\u89c6\u89d2\u4e0d\u53d8\u7684\u573a\u666f\u56fe\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4ee5\u5404\u5bf9\u8c61\u671d\u5411\u4e3a\u53c2\u8003\u5b9a\u4e49\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u63a8\u7406\u5f00\u653e\u8bcd\u6c47\u7684\u7a7a\u95f4\u53ca\u4e34\u8fd1\u5173\u7cfb\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u76ee\u6807\u5b9a\u4f4d\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\uff0cVIZOR\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728Replica\u548cNr3D\u6570\u636e\u96c6\u7684\u96f6\u6837\u672c\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u5347\u4e8622%\u548c4.81%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "VIZOR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89d2\u4f9d\u8d56\u548c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u95ee\u9898\uff0c\u57283D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u4e14\u4e00\u81f4\u7684\u573a\u666f\u56fe\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u540e\u7eed\u76f8\u5173\u4efb\u52a1\u6709\u91cd\u8981\u4fc3\u8fdb\u4f5c\u7528\u3002"}}
{"id": "2602.01362", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01362", "abs": "https://arxiv.org/abs/2602.01362", "authors": ["Yue Liu", "Yuzhong Zhao", "Zheyong Xie", "Qixiang Ye", "Jianbin Jiao", "Yao Hu", "Shaosheng Cao", "Yunfan Liu"], "title": "Balancing Understanding and Generation in Discrete Diffusion Models", "comment": "32 pages, Code is available at https://github.com/MzeroMiko/XDLM", "summary": "In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u79bb\u6563\u751f\u6210\u5efa\u6a21\u4e24\u5927\u4e3b\u6d41\u8303\u5f0f\uff08MDLM\u548cUDLM\uff09\u7684\u65b9\u6cd5XDLM\uff0c\u517c\u5177\u8bed\u4e49\u7406\u89e3\u4e0e\u751f\u6210\u8d28\u91cf\u4f18\u52bf\uff0c\u5e76\u4f18\u5316\u4e86\u5185\u5b58\u4e0e\u63a8\u7406\u6548\u7387\u3002", "motivation": "MDLM\u5728\u8bed\u4e49\u7406\u89e3\u548c\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u751f\u6210\u8d28\u91cf\u7a0d\u900a\uff1b\u76f8\u53cd\uff0cUDLM\u5728\u5c11\u6b65\u751f\u6210\u8d28\u91cf\u4e0a\u9886\u5148\uff0c\u4f46\u7406\u89e3\u6cdb\u5316\u4e0d\u8db3\u3002\u4e24\u8005\u5747\u672a\u5728\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8bbe\u8ba1\u4e00\u79cd\u80fd\u517c\u987e\u4e8c\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u7a33\u566a\u58f0\u6838\u7684XDLM\u65b9\u6cd5\uff0c\u7406\u8bba\u4e0a\u7edf\u4e00\u4e86MDLM\u548cUDLM\uff0c\u4e14\u901a\u8fc7\u4ee3\u6570\u7b80\u5316\u540e\u9a8c\u6982\u7387\u7f13\u89e3\u4e86\u5185\u5b58\u74f6\u9888\u3002XDLM\u53ef\u89c6\u4f5c\u8fd9\u4e24\u7c7b\u8303\u5f0f\u7684\u63a8\u5e7f\uff0c\u5177\u4f53\u53c2\u6570\u8bbe\u5b9a\u4e0b\u53ef\u9000\u5316\u4e3a\u8fd9\u4e24\u8005\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cXDLM\u5728\u7406\u89e3\u80fd\u529b\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff1a\u5728\u96f6\u6837\u672c\u6587\u672c\u4efb\u52a1\u4e0a\u6bd4UDLM\u9ad85.4\u5206\uff0c\u5728\u5c11\u6b65\u56fe\u50cf\u751f\u6210\u4e0a\u4ee5\u66f4\u4f4e\u7684FID\u503c\uff0854.1 vs 80.8\uff09\u8d85\u8fc7MDLM\u3002\u5c06XDLM\u6269\u5c55\u52308B\u53c2\u6570\u7684\u5927\u6a21\u578b\u65f6\uff0c\u4ec5\u752832\u6b65\u5373\u53ef\u8fbe15\u5206MBPP\uff0c\u662f\u57fa\u7ebf\u7684\u4e24\u500d\u3002", "conclusion": "XDLM\u540c\u65f6\u517c\u987e\u4e86\u8bed\u4e49\u7406\u89e3\u4e0e\u751f\u6210\u8d28\u91cf\uff0c\u7406\u8bba\u4e0a\u7edf\u4e00\u4e86MDLM\u548cUDLM\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u548c\u5927\u6a21\u578b\u6269\u5c55\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u9886\u5148\uff0c\u5c55\u793a\u51fa\u4f18\u8d8a\u7684\u957f\u8fdc\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.00810", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00810", "abs": "https://arxiv.org/abs/2602.00810", "authors": ["Ze Huang", "Zhongyang Xiao", "Mingliang Song", "Longan Yang", "Hongyuan Yuan", "Li Sun"], "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization", "comment": null, "summary": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.", "AI": {"tldr": "VVLoc\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b9e\u73b0\u540c\u65f6\u62d3\u6251\u4e0e\u5ea6\u91cf\u5b9a\u4f4d\u7684\u7edf\u4e00\u795e\u7ecf\u7f51\u7edc\uff0c\u652f\u6301\u591a\u76ee\u6444\u50cf\u5934\uff0c\u8d4b\u4e88\u5b9a\u4f4d\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u8bad\u7ec3\u9ad8\u6548\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5206\u522b\u5904\u7406\u62d3\u6251\u5b9a\u4f4d\u4e0e\u5ea6\u91cf\u5b9a\u4f4d\uff0c\u591a\u4e3a\u5355\u76ee\u65b9\u6848\uff0c\u4e14\u4f9d\u8d56\u989d\u5916\u5148\u9a8c\uff08\u59823D\u8bed\u4e49\u6216\u4f4d\u59ff\uff09\uff0c\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u91cf\uff0c\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u4e9f\u987b\u4e00\u79cd\u7edf\u4e00\u3001\u9ad8\u6548\u3001\u53ef\u4fe1\u7684\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51faVVLoc\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u62d3\u6251\u4e0e\u5ea6\u91cf\u5b9a\u4f4d\u8054\u5408\uff0c\u80fd\u591f\u8bc4\u4f30\u89c6\u89c9\u89c2\u6d4b\u4e4b\u95f4\u7684\u5730\u7406\u63a5\u8fd1\uff0c\u5e76\u7528\u5339\u914d\u7b56\u7565\u4f30\u7b97\u76f8\u5bf9\u4f4d\u59ff\uff0c\u8f93\u51fa\u5b9a\u4f4d\u7f6e\u4fe1\u5ea6\u3002\u5176\u53ea\u9700\u89c6\u89c9\u6570\u636e\u5bf9\u548c\u771f\u5b9e\u4f4d\u59ff\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u590d\u6742\u6570\u636e\u3002\u652f\u6301\u591a\u6444\u50cf\u5934\u8f93\u5165\u3002", "result": "VVLoc\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u91c7\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u793a\u4e86\u4e1a\u754c\u9886\u5148\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u65e0\u8bba\u62d3\u6251\u5b9a\u4f4d\u8fd8\u662f\u5ea6\u91cf\u5b9a\u4f4d\u4efb\u52a1\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VVLoc\u6709\u6548\u7edf\u4e00\u4e86\u591a\u6444\u50cf\u5934\u4e0b\u7684\u62d3\u6251\u4e0e\u5ea6\u91cf\u5b9a\u4f4d\uff0c\u5e76\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u8f93\u51fa\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u5177\u5907\u5b9e\u9645\u843d\u5730\u80fd\u529b\u3002"}}
{"id": "2602.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00639", "abs": "https://arxiv.org/abs/2602.00639", "authors": ["Yifang Xu", "Benxiang Zhai", "Chenyu Zhang", "Ming Li", "Yang Li", "Sidan Du"], "title": "Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization", "comment": "Accepted by Information Fusion 2025", "summary": "Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u4eba\u50cf\u5b9a\u5236\u65b9\u6cd5Diff-PC\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u9762\u90e8\u7279\u5f81\u3001\u8868\u60c5\u3001\u59ff\u6001\u548c\u80cc\u666f\u7684\u9ad8\u81ea\u7531\u5ea6\u5b9a\u5236\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4eba\u50cf\u5b9a\u5236\u65b9\u6cd5\u5728\u8eab\u4efd\u7279\u5f81\u4fdd\u7559\u4e0e\u9762\u90e8\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u771f\u5b9e\u8fd8\u539f\u8eab\u4efd\u548c\u7075\u6d3b\u63a7\u5236\u8868\u60c5\u3001\u59ff\u6001\u3002\u4e3a\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u517c\u987eID\u4fdd\u771f\u548c\u4e2a\u6027\u5316\u5b9a\u5236\u3002", "method": "\u63d0\u51faDiff-PC\u6846\u67b6\uff1a\u9996\u5148\u5229\u75283D\u4eba\u8138\u9884\u6d4b\u5668\u91cd\u5efa\u5305\u542b\u53c2\u7167\u8eab\u4efd\u3001\u76ee\u6807\u8868\u60c5\u548c\u59ff\u6001\u76843D\u4eba\u8138\u5148\u9a8c\uff1b\u8bbe\u8ba1ID-Encoder\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u9762\u90e8\u7279\u5f81\uff0c\u63d0\u5347\u7ec6\u8282\u63d0\u53d6\u80fd\u529b\uff1b\u901a\u8fc7ID-Ctrl\u6a21\u5757\u5f15\u5bfcID\u7279\u5f81\u5bf9\u9f50\uff1b\u5f15\u5165ID-Injector\u5f3a\u5316\u8eab\u4efd\u4fdd\u771f\u4e0e\u9762\u90e8\u53ef\u63a7\u6027\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4f5c\u8005\u81ea\u5efa\u7684ID\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u4eba\u8138\u76f8\u4f3c\u5ea6\u548c\u6587\u672c\u5230\u56fe\u50cf\u7684\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cDiff-PC\u5728\u8eab\u4efd\u4fdd\u771f\u3001\u9762\u90e8\u63a7\u5236\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u4e0a\u5747\u8d85\u8d8a\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u517c\u5bb9\u591a\u79cd\u57fa\u7840\u751f\u6210\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Diff-PC\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u50cf\u5b9a\u5236\u4efb\u52a1\u5728\u8eab\u4efd\u8fd8\u539f\u548c\u4e2a\u6027\u5316\u5b9a\u5236\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5bf9\u76f8\u5173\u5b9e\u9645\u5e94\u7528\u5177\u6709\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2602.01378", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01378", "abs": "https://arxiv.org/abs/2602.01378", "authors": ["Poushali Sengupta", "Shashi Raj Pandey", "Sabita Maharjan", "Frank Eliassen"], "title": "Context Dependence and Reliability in Autoregressive Language Models", "comment": null, "summary": "Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRISE\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u89e3\u91ca\u4e2d\u7684\u8f93\u5165\u5f52\u56e0\u95ee\u9898\uff0c\u4f7f\u5176\u66f4\u52a0\u7a33\u5065\u4e0e\u53ef\u4fe1\u3002", "motivation": "\u5f53\u524dLLM\u5e38\u7528\u7684\u89e3\u91ca\u65b9\u6cd5\u5728\u9762\u5bf9\u5197\u4f59\u548c\u91cd\u53e0\u7684\u8bed\u5883\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u8f93\u5165\u7684\u5fae\u5c0f\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u89e3\u91ca\u5927\u5e45\u6ce2\u52a8\uff0c\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5e26\u6765\u5982\u63d0\u793a\u6ce8\u5165\u7b49\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u6025\u9700\u4e00\u79cd\u80fd\u533a\u5206\u771f\u6b63\u5173\u952e\u8bed\u5883\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faRISE\uff08Redundancy-Insensitive Scoring of Explanation\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u5728\u5176\u4ed6\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u72ec\u7279\u5f71\u54cd\uff0c\u51cf\u5f31\u5197\u4f59\u9879\u7684\u5e72\u6270\uff0c\u4ece\u800c\u5f97\u5230\u66f4\u4e3a\u51c6\u786e\u4e0e\u7a33\u5b9a\u7684\u5f52\u56e0\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRISE\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u66f4\u7a33\u5065\u3001\u6e05\u6670\u7684\u8f93\u5165\u5f52\u56e0\u89e3\u91ca\uff0c\u66f4\u597d\u5730\u5f3a\u8c03\u4e86\u6761\u4ef6\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "conclusion": "RISE\u63d0\u5347\u4e86LLM\u8f93\u51fa\u5f52\u56e0\u89e3\u91ca\u7684\u53ef\u63a7\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6709\u52a9\u4e8e\u5728\u5173\u952e\u573a\u666f\u4e0b\u5bf9\u6a21\u578b\u884c\u4e3a\u8fdb\u884c\u6709\u6548\u76d1\u63a7\u548c\u4fe1\u4efb\u8bc4\u4f30\u3002"}}
{"id": "2602.00982", "categories": ["cs.CV", "cs.AI", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00982", "abs": "https://arxiv.org/abs/2602.00982", "authors": ["Phu-Hoa Pham", "Chi-Nguyen Tran", "Dao Sy Duy Minh", "Nguyen Lam Phu Quy", "Huynh Trung Kiet"], "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025", "comment": "15 pages, 8 tables. Technical Report for winning solutions (Track 1 & Track 2) at the NeurIPS 2025 Mouse vs. AI Challenge", "summary": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.", "AI": {"tldr": "\u672c\u8bba\u6587\u603b\u7ed3\u4e86NeurIPS 2025\u201cMouse vs. AI: Robust Visual Foraging Competition\u201d\u4e24\u9879\u8d5b\u9053\u7684\u83b7\u80dc\u65b9\u6cd5\uff0c\u5206\u522b\u9488\u5bf9\u89c6\u89c9\u9c81\u68d2\u6027\u548c\u795e\u7ecf\u5bf9\u9f50\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u7b80\u6d01\u4f46\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u9610\u8ff0\u4e86\u6a21\u578b\u7ed3\u6784\u4e0e\u8868\u73b0\u7684\u5173\u7cfb\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\u5728\u89c6\u89c9\u9c81\u68d2\u6027\u548c\u795e\u7ecf\u5bf9\u9f50\u65b9\u9762\u4e0e\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u4ecd\u6709\u5dee\u8ddd\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5728\u8fd9\u4e9b\u5173\u952e\u6311\u6218\u4e0b\uff0c\u6a21\u578b\u7ed3\u6784\u7684\u590d\u6742\u6027\u5982\u4f55\u5f71\u54cd\u5176\u6cdb\u5316\u80fd\u529b\u548c\u751f\u7269\u5bf9\u9f50\u8868\u73b0\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5177\u751f\u7269\u542f\u53d1\u610f\u4e49\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u7684\u89c6\u89c9\u4ee3\u7406\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "method": "\u5728\u89c6\u89c9\u9c81\u68d2\u6027\u8d5b\u9053\u4e2d\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u6d01\u7684\u4e24\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u95e8\u63a7\u7ebf\u6027\u5355\u5143\u548c\u89c2\u5bdf\u5f52\u4e00\u5316\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002\u5728\u795e\u7ecf\u5bf9\u9f50\u8d5b\u9053\u4e2d\uff0c\u91c7\u752816\u5c42ResNet\u98ce\u683c\u6df1\u5ea6\u6a21\u578b\uff0c\u914d\u5408GLU\u95e8\u63a7\u673a\u5236\u4ee5\u62df\u5408\u751f\u7269\u795e\u7ecf\u54cd\u5e94\u3002\u4ed6\u4eec\u5bf9\u8bad\u7ec3\u5386\u7a0b\u4e2d\u5341\u4e2a\u6a21\u578b\u8282\u70b9\u8fdb\u884c\u4e86\u6027\u80fd\u4e0e\u53c2\u6570\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u548c\u5931\u8d25\u6848\u4f8b\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u4e0d\u540c\u7ed3\u6784\u7684\u4f18\u52a3\u3002", "result": "\u89c6\u89c9\u9c81\u68d2\u6027\u8d5b\u9053\u8f7b\u91cf\u7ea7\u6a21\u578b\u6700\u7ec8\u5f97\u5206\u8fbe\u523095.4%\uff0c\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002\u795e\u7ecf\u5bf9\u9f50\u8d5b\u905316\u5c42\u6df1\u5ea6\u6a21\u578b\u5728\u795e\u7ecf\u9884\u6d4b\u4e0a\u83b7\u5f97top-1\u6027\u80fd\uff08\u53c2\u6570\u91cf\u4e3a1780\u4e07\uff09\u3002\u8bad\u7ec3\u6b65\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u5448\u73b0\u975e\u5355\u8c03\u5173\u7cfb\uff0c\u7ea620\u4e07\u6b65\u65f6\u8868\u73b0\u6700\u4f18\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u7b80\u5316\u7ed3\u6784\u5bf9\u9c81\u68d2\u6027\u6709\u76ca\uff0c\u800c\u6df1\u5ea6\u4e0e\u5bb9\u91cf\u66f4\u9ad8\u7684\u6a21\u578b\u5219\u5728\u795e\u7ecf\u5bf9\u9f50\u65b9\u9762\u66f4\u4f18\u3002", "conclusion": "\u7b80\u6d01\u67b6\u6784\u6709\u52a9\u4e8e\u89c6\u89c9\u9c81\u68d2\u6027\uff0c\u800c\u66f4\u6df1\u3001\u66f4\u5927\u5bb9\u91cf\u7684\u6a21\u578b\u9002\u5408\u795e\u7ecf\u5bf9\u9f50\uff0c\u8fd9\u4e00\u53d1\u73b0\u5bf9\u73b0\u6709\u5173\u4e8e\u6a21\u578b\u590d\u6742\u5ea6\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8bef\u533a\u63d0\u51fa\u8d28\u7591\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u5f3a\u9c81\u68d2\u6027\u548c\u751f\u7269\u5bf9\u9f50\u7684\u89c6\u89c9\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53c2\u8003\u5efa\u8bae\u3002"}}
{"id": "2602.00650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00650", "abs": "https://arxiv.org/abs/2602.00650", "authors": ["Mohammadreza Gholipour Shahraki", "Mehdi Rezaeian", "Mohammad Ghasemzadeh"], "title": "A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Mamba-SAM\u6df7\u5408\u67b6\u6784\uff0c\u5c06SAM\u57fa\u7840\u6a21\u578b\u4e0eMamba\u7cfb\u5217\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7ed3\u5408\uff0c\u63d0\u53473D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u65e2\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u529b\uff0c\u53c8\u517c\u987e\u9002\u5e94\u533b\u7597\u6210\u50cf\u9886\u57df\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u7840\u6a21\u578b\uff08\u5982SAM\uff09\u5728\u533b\u5b66\u5f71\u50cf\u5206\u5272\u573a\u666f\u4e2d\u56e0\u9886\u57df\u5dee\u5f02\u3001\u4e8c\u7ef4\u8bbe\u8ba1\u548c\u5fae\u8c03\u6210\u672c\u9ad8\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u4e9f\u9700\u9ad8\u6548\u4e14\u80fd\u591f\u9002\u5e94\u533b\u5b66\u5f71\u50cf\u4e0d\u540c\u7279\u5f81\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMamba-SAM\u67b6\u6784\uff0c\u5305\u62ec\u53cc\u5206\u652f\u65b9\u5f0f\uff08\u878d\u5408\u51bb\u7ed3\u7684SAM\u7f16\u7801\u5668\u4e0e\u53ef\u8bad\u7ec3\u7684VMamba\u7f16\u7801\u5668\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u96c6\u6210\uff09\u548c\u57fa\u4e8e\u9002\u914d\u5668\u65b9\u5f0f\uff08\u5728\u51bb\u7ed3\u7684SAM ViT\u7f16\u7801\u5668\u4e2d\u63d2\u5165\u8f7b\u91cf\u53163D\u611f\u77e5\u7684TPMamba\u6a21\u5757\uff09\uff0c\u5e76\u5f15\u5165\u591a\u9891\u95e8\u63a7\u5377\u79ef\uff08MFGC\uff09\u63d0\u5347\u7a7a\u95f4-\u9891\u57df\u7684\u7279\u5f81\u8868\u8fbe\u3002", "result": "\u5728ACDC\u5fc3\u810fMRI\u6570\u636e\u96c6\u4e0a\uff0c\u53cc\u5206\u652fMamba-SAM-Base\u6a21\u578bDice\u5747\u503c0.906\uff0c\u90e8\u5206\u6307\u6807\u8d85\u8fc7UNet++\u4e0e\u5176\u4ed6\u57fa\u7ebf\uff1b\u57fa\u4e8eTPMFGC\u7684\u9002\u914d\u5668\u65b9\u6848\u5728\u4fdd\u8bc1\u8f83\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5177\u5907\u66f4\u5feb\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u9ad8\u6548\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u4e0eSSM\u67b6\u6784\u80fd\u517c\u987e\u9884\u8bad\u7ec3\u80fd\u529b\u548c\u9886\u57df\u9002\u5e94\u6027\uff0c\u4e3a3D\u533b\u5b66\u5f71\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.01381", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01381", "abs": "https://arxiv.org/abs/2602.01381", "authors": ["Youheng Zhu", "Yiping Lu"], "title": "On the Power of (Approximate) Reward Models for Inference-Time Scaling", "comment": null, "summary": "Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.\n  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u63a8\u7406\u65f6\u7f29\u653e\uff08inference-time scaling\uff09\u4e2d\u91c7\u7528\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u4e86\u5728\u5956\u52b1\u6a21\u578b\u7684Bellman\u8bef\u5dee\u53d7\u63a7\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u663e\u8457\u63d0\u9ad8SMC\u63a8\u7406\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4f9d\u8d56\u4e8e\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u4e2d\u95f4\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0e\u8ba1\u7b97\u5206\u914d\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ea\u80fd\u83b7\u5f97\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u3002\u4f5c\u8005\u5173\u6ce8\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4ec0\u4e48\u65f6\u5019\u3001\u4e3a\u4ec0\u4e48\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u4e5f\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6548\u7387\uff1f", "method": "\u4f5c\u8005\u4ee5Sequential Monte Carlo (SMC) \u6846\u67b6\u4e3a\u57fa\u7840\uff0c\u5206\u6790\u63a8\u7406\u65f6\u95f4\u7f29\u653e\uff0c\u63d0\u51fa\u4ee5Bellman\u8bef\u5dee\u6765\u8861\u91cf\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684\u597d\u574f\uff0c\u5e76\u63a8\u5bfc\u7406\u8bba\u8fb9\u754c\u3002\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u8bc1\u660e\u8bef\u5dee\u4e3aO(1/T)\u65f6\u4f1a\u5e26\u6765\u672c\u8d28\u6027\u7684\u6548\u7387\u63d0\u5347\u3002", "result": "\u8bc1\u660e\u53ea\u8981\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684Bellman\u8bef\u5dee\u754c\u4e8eO(1/T)\u4e4b\u5185\uff0cSMC\u63a8\u7406\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u53ef\u7531\u6307\u6570\u7ea7\u964d\u4e3a\u591a\u9879\u5f0f\u7ea7\uff0c\u63a8\u7406\u6548\u7387\u5f97\u5230\u6307\u6570\u7ea7\u63d0\u5347\u3002", "conclusion": "\u5373\u4f7f\u53ea\u7528\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\uff0c\u53ea\u8981\u5176Bellman\u8bef\u5dee\u8db3\u591f\u5c0f\uff0c\u4e5f\u53ef\u4ee5\u6781\u5927\u63d0\u5347SMC\u63a8\u7406\u6548\u7387\uff0c\u5bf9\u5927\u6a21\u578b\u63a8\u7406\u4f18\u5316\u5177\u6709\u91cd\u8981\u7406\u8bba\u4ef7\u503c\u3002"}}
{"id": "2602.01158", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01158", "abs": "https://arxiv.org/abs/2602.01158", "authors": ["Daniel Yezid Guarnizo Orjuela", "Leonardo Scappatura", "Veronica Di Gennaro", "Riccardo Andrea Izzo", "Gianluca Bardaro", "Matteo Matteucci"], "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $\u03c0_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5bf9\u4e8e\u4f20\u611f\u5668\u7ea7\u56fe\u50cf\u6270\u52a8\uff08\u5982\u566a\u58f0\u3001\u574f\u70b9\u3001\u955c\u5934\u6c61\u6e0d\uff09\u6781\u4e3a\u8106\u5f31\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u6062\u590d\u6a21\u5757Corruption Restoration Transformer\uff08CRT\uff09\u3002", "motivation": "\u867d\u7136VLA\u6a21\u578b\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u771f\u5b9e\u73af\u5883\u90e8\u7f72\u65f6\u5e38\u5e38\u56e0\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\u4e25\u91cd\u9000\u5316\uff0c\u4e14\u76ee\u524d\u76f8\u5173\u6587\u732e\u591a\u96c6\u4e2d\u5728\u906e\u6321\uff0c\u9c9c\u6709\u7cfb\u7edf\u7814\u7a76\u4f20\u611f\u5668\u7ea7\u56fe\u50cf\u635f\u574f\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u8bc4\u4f30\u4e86VLA\u6a21\u578b\u5bf9\u4e8e\u56fe\u50cf\u635f\u574f\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u6210\u529f\u7387\u6781\u5267\u4e0b\u964d\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51faCRT\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b66\u4e60\u4ece\u53d7\u635f\u56fe\u50cf\u6062\u590d\u5e72\u51c0\u7279\u5f81\uff0c\u8be5\u6a21\u5757\u5373\u63d2\u5373\u7528\uff0c\u65e0\u9700\u5bf9\u5df2\u6709\u6a21\u578b\u8fdb\u884c\u590d\u6742\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff1aVLA\u6a21\u578b\u5728\u672a\u52a0\u4fdd\u62a4\u65f6\u5728\u6807\u51c6\u57fa\u51c6\u4e0b\u6210\u529f\u7387\u53ef\u753190%\u964d\u81f32%\uff1b\u52a0\u88c5CRT\u540e\u80fd\u5728\u4e25\u91cd\u635f\u574f\u4e0b\u5c06\u8868\u73b0\u6062\u590d\u5230\u63a5\u8fd1\u57fa\u7ebf\u6c34\u5e73\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u73b0\u5b9e\u89c6\u89c9\u5e72\u6270\u4e0b\u8106\u5f31\uff0cCRT\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4f20\u611f\u5668\u5931\u771f\u73af\u5883\u4e0b\u7684\u5b9e\u7528\u6027\uff0c\u662f\u5b9e\u73b0\u901a\u7528\u673a\u5668\u4eba\u89c6\u89c9\u63a7\u5236\u91cd\u8981\u7684\u4e00\u6b65\u3002"}}
{"id": "2602.00653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00653", "abs": "https://arxiv.org/abs/2602.00653", "authors": ["Lukas Kuhn", "Giuseppe Serra", "Florian Buettner"], "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment", "comment": null, "summary": "Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5NOVA\uff0c\u4ee5\u7b80\u5316\u548c\u63d0\u5347\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u7684\u6548\u7387\u4e0e\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u591a\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u9700\u8981\u6781\u5927\u7684batch size\u3001\u590d\u6742\u7684\u8d1f\u6837\u672c\u91c7\u6837\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\u4e14\u4e0d\u7a33\u5b9a\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u6548\u7387\u9ad8\u4e14\u7a33\u5b9a\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faNOVA\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u51bb\u7ed3\u7684\u9886\u57df\u7279\u5b9a\u6587\u672c\u7f16\u7801\u5668\uff08\u5982ClinicalBERT\uff09\uff1b2\uff09\u56fe\u50cf\u7ecf\u8fc7\u589e\u5f3a\u540e\uff0c\u89c6\u89c9\u7f16\u7801\u5668\u9884\u6d4b\u6587\u672c\u5d4c\u5165\uff1b3\uff09\u901a\u8fc7SIGReg\uff08Sketched Isotropic Gaussian Regularization\uff09\u5f3a\u5236\u5bf9\u9f50\u4e3a\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\uff0c\u6d88\u9664\u5bf9\u8d1f\u6837\u672c/\u52a8\u91cf\u7f16\u7801\u5668/\u68af\u5ea6\u963b\u65ad\u7684\u4f9d\u8d56\uff0c\u5e76\u5c06\u8bad\u7ec3\u76ee\u6807\u7b80\u5316\u4e3a\u5355\u4e00\u8d85\u53c2\u6570\u3002", "result": "\u5728\u4e09\u5927\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u80f8\u90e8X\u5149\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cNOVA\u4f7f\u7528ClinicalBERT\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u3001ViT\u89c6\u89c9\u7f16\u7801\u5668\u4ece\u96f6\u8bad\u7ec3\uff0c\u5747\u4f18\u4e8e\u591a\u79cd\u6807\u51c6\u65b9\u6cd5\uff0c\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u7a33\u5b9a\u3001\u8868\u73b0\u66f4\u4e00\u81f4\u3002", "conclusion": "\u975e\u5bf9\u6bd4\u5f0f\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e0d\u4ec5\u80fd\u5e26\u6765\u66f4\u7b80\u6d01\u3001\u7a33\u5b9a\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd8\u80fd\u53d6\u5f97\u6bd4\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\uff0c\u5c55\u793a\u4e86\u8fd9\u4e00\u65b9\u6cd5\u672a\u6765\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u4f18\u52bf\u3002"}}
{"id": "2602.01395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01395", "abs": "https://arxiv.org/abs/2602.01395", "authors": ["Almog Tavor", "Itay Ebenspanger", "Neil Cnaan", "Mor Geva"], "title": "Rethinking Selective Knowledge Distillation", "comment": null, "summary": "Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86\u5728\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u9009\u62e9\u6027\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u751f\u6a21\u578b\u71b5\u8fdb\u884c\u4f4d\u7f6e\u9009\u62e9\u7684\u84b8\u998f\u65b9\u6cd5\uff08SE-KD\uff09\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u589e\u957f\uff0c\u6807\u51c6\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u5f0f\u56e0\u76d1\u7763\u4fe1\u606f\u5197\u4f59\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u548c\u5b58\u50a8\u9700\u6c42\u9ad8\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u6709\u9488\u5bf9\u6027\u7684\u84b8\u998f\u7b56\u7565\uff0c\u4f46\u9009\u62e9\u54ea\u4e9b\u76d1\u7763\u4fe1\u53f7\u3001\u7b5b\u9009\u539f\u5219\u53ca\u5176\u534f\u540c\u4f5c\u7528\u5c1a\u65e0\u5b9a\u8bba\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u84b8\u998f\u4e2d\u201c\u5728\u54ea\u91cc\u3001\u5982\u4f55\u9009\u201d\u7684\u95ee\u9898\uff0c\u65e8\u5728\u5bfb\u627e\u66f4\u4f18\u7684\u84b8\u998f\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u5c06\u9009\u62e9\u6027KD\u5728\u6a21\u578b\u7684\u4f4d\u7f6e\u4fe1\u606f\u3001\u8bcd\u6c47\u7c7b\u578b\u3001\u6837\u672c\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u533a\u5206\uff0c\u5e76\u5bf9\u4e0d\u540c\u7684\u91cd\u8981\u6027\u4fe1\u53f7\u4e0e\u9009\u62e9\u7b56\u7565\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u751f\u6a21\u578b\u71b5\u7684\u4f4d\u7f6e\u4fe1\u606f\u9009\u62e9\uff08SE-KD\uff09\uff0c\u5e76\u6269\u5c55\u81f3\u8bcd\u7c7b\u548c\u6837\u672c\u7ef4\u5ea6\uff08SE-KD 3X\uff09\uff0c\u8fbe\u5230\u8fdb\u4e00\u6b65\u4f18\u5316\u6548\u7387\u7684\u76ee\u7684\u3002", "result": "SE-KD\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u5747\u4f18\u4e8e\u4f20\u7edf\u5168\u76d1\u7763\u84b8\u998f\uff0c\u5728\u63d0\u5347\u51c6\u786e\u7387\u3001\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u7684\u5185\u5b58\u4e0e\u65f6\u95f4\u6548\u7387\u3002SE-KD 3X\u5728\u6548\u7387\u4e0a\u8fdb\u4e00\u6b65\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e8670%\u5899\u65f6\u300118%\u5cf0\u503c\u5185\u5b58\u548c80%\u5b58\u50a8\u7684\u51cf\u7701\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u751f\u71b5\u7684\u9009\u62e9\u6027\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6781\u5927\u6539\u5584\u4e86\u8d44\u6e90\u4f18\u5316\u4e0e\u8bad\u7ec3\u6548\u7387\uff0c\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u5bc6\u96c6\u84b8\u998f\u65b9\u6cd5\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01268", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01268", "abs": "https://arxiv.org/abs/2602.01268", "authors": ["Jaehyeon Cho", "Jhonghyun An"], "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth", "comment": "Accepted to ICRA 2026", "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.", "AI": {"tldr": "\u672c\u6587\u5c06\u5355\u76ee\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u6df1\u5ea6\u8f93\u51fa\uff0c\u901a\u8fc7\u4e0e\u7a00\u758f\u7684\u6d4b\u8ddd\u6570\u636e\u6821\u51c6\uff0c\u8f6c\u5316\u4e3a\u4f2a\u5ea6\u91cf\u6df1\u5ea6\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u6b64\u5148\u9a8c\u6539\u5584\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u76ee\u524d\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u5927\u578b\u57fa\u7840\u6a21\u578b\u867d\u5728\u96f6\u6837\u672c\uff08zero-shot\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u8f93\u51fa\u4e00\u822c\u4e3a\u76f8\u5bf9\u6df1\u5ea6\u800c\u975e\u7edd\u5bf9\u5ea6\u91cf\u6df1\u5ea6\uff0c\u5bfc\u81f4\u5176\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u3002\u89e3\u51b3\u5982\u4f55\u5c06\u5927\u6a21\u578b\u7684\u80fd\u529b\u8fc1\u79fb\u5230\u5b9e\u9645\u3001\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u6210\u4e3a\u4e9f\u9700\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u9996\u5148\u7528\u5c11\u91cf\u7a00\u758f\u771f\u5b9e\u8ddd\u79bb\u6d4b\u91cf\u70b9\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u7684\u76f8\u5bf9\u6df1\u5ea6\u8fdb\u884c\u6821\u51c6\uff0c\u8f6c\u4e3a\u4f2a\u5ea6\u91cf\u6df1\u5ea6\u5148\u9a8c\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e00\u4e2a\u6539\u8fdb\u7f51\u7edc\uff0c\u4f9d\u9760\u8fd9\u4e9b\u5148\u9a8c\u5728\u53ef\u9760\u533a\u57df\u5185\u8fdb\u884c\u9884\u6d4b\uff0c\u5728\u4e0d\u53ef\u9760\u533a\u57df\u5219\u5141\u8bb8\u504f\u79bb\uff0c\u4ece\u800c\u5728\u5f88\u5c11\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u3002", "result": "\u7cfb\u7edf\u80fd\u5728\u7f3a\u5c11\u9a8c\u8bc1\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u7ef4\u6301\u5c3a\u5ea6\u4e00\u81f4\u6027\u4e0e\u8fb9\u7f18\u9510\u5229\u6027\uff0c\u5728\u201c\u5c11\u6837\u672c\u201d\uff08few-shot\uff09\u6570\u636e\u60c5\u5f62\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u8f93\u51fa\u7684\u6df1\u5ea6\u66f4\u7b26\u5408\u771f\u5b9e\u9700\u6c42\u3002", "conclusion": "\u5c06\u5355\u76ee\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u80fd\u529b\u4e0e\u5c11\u91cf\u7a00\u758f\u5b9e\u6d4b\u70b9\u7ed3\u5408\uff0c\u662f\u89e3\u51b3\u5b9e\u9645\u6807\u6ce8\u7a00\u7f3a\u4e0b\uff0c\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u90e8\u7f72\u6df1\u5ea6\u8865\u5168\u7684\u6709\u6548\u8def\u5f84\uff0c\u5177\u6709\u73b0\u5b9e\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.00661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00661", "abs": "https://arxiv.org/abs/2602.00661", "authors": ["Ahsan Raza Siyal", "Markus Haltmeier", "Ruth Steiger", "Elke Ruth Gizewski", "Astrid Ellen Grams"], "title": "Schr\u00f6dinger-Inspired Time-Evolution for 4D Deformation Forecasting", "comment": null, "summary": "Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schr\u00f6dinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $\u03c8= A e^{i\u03c6}$, which is evolved forward in time using a differentiable, unrolled Schr\u00f6dinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schr\u00f6dinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u4e09\u7ef4\u968f\u65f6\u95f4\u6f14\u5316\u73b0\u8c61\uff084D\uff09\u7684\u9ad8\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u9884\u6d4b\u3002", "motivation": "\u5982\u4f55\u5b9e\u73b0\u5bf9\u533b\u5b66\u6210\u50cf\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u7b49\u9886\u57df\u4e2d\u7684\u590d\u67424D\uff083D+\u65f6\u95f4\uff09\u73b0\u8c61\u8fdb\u884c\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u6027\u7684\u65f6\u7a7a\u9884\u6d4b\uff0c\u76ee\u524d\u65b9\u6cd5\u591a\u4e0d\u5177\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u957f\u65f6\u9884\u6d4b\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u6784\u5efa\u4e86\u859b\u5b9a\u8c14\u65b9\u7a0b\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u663e\u5f0f\u7684\u65f6\u95f4\u6f14\u5316\u7b97\u5b50\u5d4c\u5165\u5728\u5377\u79ef\u7f51\u7edc\u4e2d\u3002\u6a21\u578b\u4ece\u89c2\u5bdf\u5230\u7684\u4f53\u6570\u636e\u5e8f\u5217\u4e2d\u5b66\u4e60\u5230\u6bcf\u4f53\u7d20\u7684\u5e45\u503c\u3001\u76f8\u4f4d\u548c\u52bf\u80fd\u573a\uff0c\u6784\u9020\u590d\u6ce2\u51fd\u6570\uff0c\u5e76\u5229\u7528\u53ef\u5fae\u5206\u3001\u5c55\u5f00\u7684\u859b\u5b9a\u8c14\u65f6\u95f4\u63a8\u8fdb\u5668\u8fdb\u884c\u65f6\u5e8f\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u5148\u9a8c\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210\u5f62\u53d8\u4e0e\u62d3\u6251\u7ed3\u6784\u53d8\u5316\u7684\u4eff\u771f\u6570\u636e\u4e0a\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u53ef\u7cbe\u786e\u3001\u7a33\u5b9a\u5730\u9884\u6d4b\u672a\u6765\u76844D\u72b6\u6001\uff0c\u5305\u62ec\u4f53\u79ef\u5f3a\u5ea6\u548c\u5f62\u53d8\u573a\uff0c\u5e76\u6709\u6548\u4fdd\u6301\u4e86\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5c06\u859b\u5b9a\u8c14\u578b\u6f14\u5316\u7b97\u5b50\u7aef\u5230\u7aef\u96c6\u6210\u8fdb4D\u795e\u7ecf\u9884\u6d4b\u7f51\u7edc\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u3001\u7a33\u5b9a\u6027\u4e0e\u89e3\u5256\u5b66\u4fdd\u771f\u65b9\u9762\u6811\u7acb\u4e86\u65b0\u7684\u65b9\u6cd5\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u53ef\u89e3\u91ca\u3001\u7a33\u5b9a\u7684\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2602.01401", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01401", "abs": "https://arxiv.org/abs/2602.01401", "authors": ["Niansong Zhang", "Sunwoo Kim", "Shreesha Srinath", "Zhiru Zhang"], "title": "From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis", "comment": null, "summary": "The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u63a2\u8ba8\u4e86\u5728\u5927\u6a21\u578b\u548cAI\u9a71\u52a8\u786c\u4ef6\u8bbe\u8ba1\u5174\u8d77\u7684\u80cc\u666f\u4e0b\uff0c\u9ad8\u5c42\u6b21\u7efc\u5408\uff08HLS\uff09\u662f\u5426\u4ecd\u7136\u91cd\u8981\uff0c\u5e76\u8ba4\u4e3aHLS\u4f9d\u65e7\u4e0d\u53ef\u66ff\u4ee3\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4e0eAI\u6280\u672f\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u8fd0\u7528\u589e\u591a\uff0c\u4eba\u4eec\u5f00\u59cb\u8d28\u7591\u9ad8\u5c42\u6b21\u7efc\u5408\uff08HLS\uff09\u5728\u65b0\u7684\u667a\u80fd\u65f6\u4ee3\u662f\u5426\u8fd8\u6709\u4ef7\u503c\u3002\u4f5c\u8005\u8bd5\u56fe\u56de\u7b54\u8fd9\u4e00\u7591\u95ee\uff0c\u5e76\u91cd\u65b0\u754c\u5b9aHLS\u7684\u4f5c\u7528\u3002", "method": "\u672c\u6587\u5c5e\u4e8e\u7acb\u573a\u6027\u8bba\u6587\uff0c\u4e3b\u8981\u901a\u8fc7\u8bba\u8bc1\u548c\u5256\u6790\uff1a\u9996\u5148\u89e3\u91caHLS\u4f5c\u4e3a\u62bd\u8c61\u5c42\u7684\u4ef7\u503c\uff0c\u518d\u5206\u6790\u73b0\u6709HLS\u5de5\u5177\u7684\u5c40\u9650\u2014\u2014\u6bd4\u5982\u6027\u80fd\u53cd\u9988\u4e0d\u8db3\u3001\u63a5\u53e3\u50f5\u5316\u548c\u8c03\u8bd5\u53d7\u9650\uff0c\u5e76\u8ba8\u8bbaAI\u4ee3\u7406\u5982\u4f55\u4f18\u5316\u8fd9\u4e9b\u95ee\u9898\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e2aHLS\u4e0eAI\u9010\u6b65\u878d\u5408\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u4f5c\u8005\u6307\u51fa\uff0cHLS\u56e0\u5176\u5feb\u901f\u8fed\u4ee3\u3001\u53ef\u79fb\u690d\u6027\u548c\u8bbe\u8ba1\u53ef\u53d8\u6027\uff0c\u5929\u7136\u9002\u5408\u4e0eAI\u4ee3\u7406\u7ed3\u5408\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u80fd\u4e3aAI\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u9ec4\u91d1\u53c2\u8003\u3002\u540c\u65f6\uff0c\u5f53\u524d\u7684HLS\u5de5\u5177\u6709\u591a\u65b9\u9762\u4e0d\u8db3\uff0cAI\u6280\u672f\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u9ad8\u5c42\u6b21\u7efc\u5408\u5728AI\u65f6\u4ee3\u4ecd\u4e0d\u53ef\u6216\u7f3a\uff0c\u672a\u6765\u968f\u7740AI\u80fd\u529b\u6f14\u8fdb\uff0c\u8bbe\u8ba1\u8d23\u4efb\u5c06\u4ece\u4eba\u7c7b\u9010\u6b65\u8f6c\u79fb\u5230AI\uff0cHLS\u5c06\u5728\u8fd9\u4e2a\u8f6c\u53d8\u4e2d\u626e\u6f14\u6865\u6881\u548c\u52a0\u901f\u5668\u7684\u89d2\u8272\u3002"}}
{"id": "2602.01673", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01673", "abs": "https://arxiv.org/abs/2602.01673", "authors": ["Enguang Fan"], "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss", "comment": null, "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86NetVLAD\u4e0e\u4f20\u7edfDBoW\u5728SLAM\u4e2d\u95ed\u73af\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u53d1\u73b0NetVLAD\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u53ef\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "motivation": "SLAM\u4e2d\u7684\u95ed\u73af\u68c0\u6d4b\u4f20\u7edf\u65b9\u6cd5DBoW\u6548\u7387\u9ad8\u4f46\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u9c81\u68d2\u6027\u597d\u4f46\u666e\u904d\u88ab\u8ba4\u4e3a\u4e0d\u591f\u5b9e\u65f6\uff0c\u8bba\u6587\u5e0c\u671b\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u63cf\u8ff0\u7b26\u662f\u5426\u771f\u80fd\u5e94\u7528\u4e8e\u5b9e\u65f6SLAM\u3002", "method": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\uff0c\u5229\u7528Faiss\u52a0\u901f\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5c06NetVLAD\u7528\u4f5c\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\uff0c\u4e0eDBoW\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5e76\u5f15\u5165\u4e86\u7cbe\u7ec6\u5316Top-K\u67e5\u5168\u7387-\u67e5\u51c6\u7387\u66f2\u7ebf\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u591a\u4e2a\u6216\u96f6\u5339\u914d\u7684\u60c5\u5f62\u3002", "result": "NetVLAD\u5728Faiss\u52a0\u901f\u4e0b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u67e5\u8be2\u901f\u5ea6\uff0c\u4e14\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8eDBoW\u3002", "conclusion": "NetVLAD\u7ed3\u5408\u9ad8\u6548\u641c\u7d22\u540e\uff0c\u5728SLAM\u4e2d\u53ef\u66ff\u4ee3DBoW\u4f5c\u4e3a\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u3002"}}
{"id": "2602.00669", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.00669", "abs": "https://arxiv.org/abs/2602.00669", "authors": ["Marina Crespo Aguirre", "Jonathan Williams-Ramirez", "Dina Zemlyanker", "Xiaoling Hu", "Lucas J. Deden-Binder", "Rogeny Herisse", "Mark Montine", "Theresa R. Connors", "Christopher Mount", "Christine L. MacDonald", "C. Dirk Keene", "Caitlin S. Latimer", "Derek H. Oakley", "Bradley T. Hyman", "Ana Lawry Aguila", "Juan Eugenio Iglesias"], "title": "Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation", "comment": "12 pages of main content, 5 pages of supplement", "summary": "Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u8865\u5168\u5207\u7247\uff0c\u628a\u5404\u5411\u5f02\u6027\u76842D\u8111\u5207\u7247\u7167\u7247\u91cd\u5efa\u4e3a\u5404\u5411\u540c\u6027\u76843D\u4f53\u79ef\uff0c\u63d0\u9ad8\u4e86\u89e3\u5256\u7ed3\u6784\u7684\u5206\u8fa8\u7387\u548c\u6d4b\u91cf\u7cbe\u5ea6\u3002", "motivation": "\u4ee5\u5f80\u5229\u75282D\u89e3\u5256\u7167\u7247\u91cd\u5efa3D\u8111\u4f53\u79ef\u65f6\uff0c\u4f1a\u56e0\u539a\u5207\u7247\u800c\u5bfc\u81f4\u7ed3\u6784\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\uff0c\u9650\u5236\u4e86\u89e3\u5256\u51c6\u786e\u6027\u548c\u81ea\u52a8\u5206\u5272\u6027\u80fd\u3002\u4e3a\u6539\u5584\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u57fa\u4e8e\u7167\u7247\u91cd\u5efa\u7ed3\u679c\u7684\u79d1\u5b66\u548c\u5e94\u7528\u4ef7\u503c\uff0c\u63d0\u51fa\u8be5\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5728\u4f20\u7edf\u57fa\u4e8e2D\u89e3\u5256\u7167\u7247\u7684\u8111\u4f53\u79ef\u91cd\u5efa\u6d41\u7a0b\u4e2d\uff0c\u52a0\u5165\u4e00\u4e2a\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5728\u57df\u968f\u673a\u5316\u7684\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7136\u540e\u81ea\u52a8\u9884\u6d4b\u8865\u5168\u7f3a\u5931\u5207\u7247\uff0c\u4f7f3D\u91cd\u5efa\u7ed3\u6784\u4ece\u5404\u5411\u5f02\u6027\u63d0\u5347\u4e3a\u5404\u5411\u540c\u6027\uff0c\u517c\u5bb9\u5404\u79cd\u89e3\u5256\u65b9\u6848\u4e14\u5bf9\u539a\u5207\u7247\u6709\u9c81\u68d2\u6027\u3002", "result": "\u7ecf\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u8865\u5168\u540e\u7684\u4f53\u79ef\u53ef\u5927\u5e45\u63d0\u5347\u81ea\u52a8\u5206\u5272\u7cbe\u5ea6\uff0cDice\u7cfb\u6570\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u76ae\u5c42\u548c\u767d\u8d28\u533a\u57df\u6548\u679c\u663e\u8457\u3002\u8fd8\u63d0\u9ad8\u4e86\u8868\u9762\u91cd\u5efa\u548c\u4e0e\u6807\u51c6\u8111\u56fe\u8c31\u7684\u914d\u51c6\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89e3\u5256\u7167\u7247\u91cd\u5efa\u8111\u90e8\u7ed3\u6784\u7684\u5206\u8fa8\u7387\u548c\u89e3\u5256\u4fdd\u771f\u5ea6\uff0c\u589e\u5f3a\u4e86\u795e\u7ecf\u75c5\u7406\u5b66\u4e0e\u795e\u7ecf\u5f71\u50cf\u5b66\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u76f8\u5173\u5de5\u5177\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2602.01447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01447", "abs": "https://arxiv.org/abs/2602.01447", "authors": ["Hieu Minh Duong", "Rupa Ghosh", "Cong Hoan Nguyen", "Eugene Levin", "Todd Gary", "Long Nguyen"], "title": "SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction", "comment": null, "summary": "Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.", "AI": {"tldr": "SentiFuse\u6846\u67b6\u901a\u8fc7\u878d\u5408\u591a\u79cd\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u5206\u6790\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u5c06\u5b83\u4eec\u9ad8\u6548\u6574\u5408\uff0c\u96be\u4ee5\u5145\u5206\u53d1\u6325\u6a21\u578b\u4e92\u8865\u6027\u3002", "method": "\u63d0\u51fa\u4e86SentiFuse\u6846\u67b6\uff0c\u5305\u542b\u6807\u51c6\u5316\u5c42\uff0c\u53ef\u652f\u6301\u51b3\u7b56\u7ea7\u878d\u5408\u3001\u7279\u5f81\u7ea7\u878d\u5408\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b49\u591a\u79cd\u7b56\u7565\uff0c\u5b9e\u73b0\u5f02\u6784\u60c5\u611f\u6a21\u578b\u7684\u7cfb\u7edf\u7ec4\u5408\u3002", "result": "\u5728Crowdflower\u3001GoEmotions\u548cSentiment140\u4e09\u4e2a\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cSentiFuse\u7a33\u5b9a\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u548c\u7b80\u5355\u96c6\u6210\uff0c\u7279\u5f81\u7ea7\u878d\u5408F1\u5206\u6570\u6700\u9ad8\u63d0\u53474%\uff0c\u81ea\u9002\u5e94\u878d\u5408\u5728\u5426\u5b9a\u3001\u6df7\u5408\u60c5\u611f\u7b49\u590d\u6742\u60c5\u611f\u8868\u73b0\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u7cfb\u7edf\u6027\u6574\u5408\u4e0d\u540c\u60c5\u611f\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u53ef\u4ee5\u5728\u591a\u6837\u5316\u6587\u672c\u548c\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u60c5\u611f\u5206\u6790\u6548\u679c\u3002"}}
{"id": "2602.01780", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01780", "abs": "https://arxiv.org/abs/2602.01780", "authors": ["Shicheng Yin", "Kaixuan Yin", "Weixing Chen", "Yang Liu", "Guanbin Li", "Liang Lin"], "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models", "comment": "Codes will be available at https://github.com/HCPLabSYSU/DDP-WM", "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\uff08DDP-WM\uff09\uff0c\u901a\u8fc7\u52a8\u529b\u5b66\u89e3\u8026\u6709\u6548\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u9002\u5408\u673a\u5668\u4eba\u5b9e\u65f6\u89c4\u5212\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u4f18\u4e8e\u4f20\u7edfTransformer\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7a20\u5bc6Transformer\u7684\u4e16\u754c\u6a21\u578b\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4e2d\u7531\u4e8e\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u517c\u5177\u9ad8\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u7684\u65b0\u578b\u4e16\u754c\u6a21\u578b\u3002", "method": "\u63d0\u51faDisentangled Dynamics Prediction (DDP)\u539f\u7406\uff0c\u5c06\u573a\u666f\u7684\u6f5c\u5728\u72b6\u6001\u6f14\u5316\u5206\u89e3\u4e3a\u4e3b\u8981\u7531\u7269\u7406\u4ea4\u4e92\u9a71\u52a8\u7684\u7a00\u758f\u4e3b\u52a8\u529b\u5b66\u548c\u7531\u73af\u5883\u80cc\u666f\u9a71\u52a8\u7684\u6b21\u8981\u53d8\u5316\u3002\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u7684\u5386\u53f2\u5904\u7406\u4e0e\u52a8\u6001\u5b9a\u4f4d\u518d\u52a0\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u8fd9\u4e24\u8005\u7684\u5206\u79bb\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u4f18\u5316\u8868\u73b0\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\uff08\u5bfc\u822a\u3001\u7cbe\u5bc6\u64cd\u4f5c\u3001\u590d\u6742\u591a\u4f53\u4ea4\u4e92\u7b49\uff09\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u6709\u663e\u8457\u7684\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u3002\u7279\u522b\u662f\u5728Push-T\u4efb\u52a1\u4e0a\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53479\u500d\uff0cMPC\u6210\u529f\u7387\u753190%\u63d0\u5347\u81f398%\u3002", "conclusion": "DDP-WM\u6210\u4e3a\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u4e16\u754c\u6a21\u578b\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00671", "abs": "https://arxiv.org/abs/2602.00671", "authors": ["Yangzhi Ma", "Bojun Liu", "Wenting Liao", "Dong Liu", "Zhu Li", "Li Li"], "title": "HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression", "comment": null, "summary": "While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u70b9\u4e91\u52a8\u6001\u9ad8\u65af\u5206\u5e03\u538b\u7f29\u6846\u67b6\uff08HPC\uff09\uff0c\u6781\u5927\u51cf\u5c11\u5b58\u50a8\u7a7a\u95f4\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\uff0c\u9002\u5408\u9ad8\u6548\u6d41\u5f0f\u4f20\u8f93\u3002", "motivation": "\u968f\u7740\u52a8\u6001\u9ad8\u65af\u6295\u5f71\uff08Dynamic Gaussian Splatting\uff09\u6280\u672f\u5728\u81ea\u7531\u89c6\u89d2\u89c6\u9891\u7684\u6e32\u67d3\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u5982\u4f55\u5728\u5185\u5b58\u5360\u7528\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\uff0c\u4ee5\u4fbf\u9002\u5e94\u9ad8\u6548\u6d41\u5f0f\u4f20\u8f93\uff0c\u6210\u4e3a\u5f85\u89e3\u51b3\u7684\u91cd\u8981\u95ee\u9898\u3002\u76ee\u524d\u4e3b\u6d41\u7684\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u5197\u4f59\u6216\u538b\u7f29\u7d27\u51d1\u5ea6\u4e0d\u591f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7684\u70b9\u4e91\u5f0f\u6f5c\u5728\u8868\u793a\u65b9\u5f0f\uff0c\u5bf9\u6bcf\u4e2a\u9ad8\u65af\u70b9\u5355\u72ec\u7f16\u7801\uff0c\u7ed3\u5408\u5b9a\u5236\u5316\u7684\u805a\u5408\u673a\u5236\u63d0\u5347\u5c40\u90e8\u7d27\u51d1\u6027\uff0c\u907f\u514d\u65e0\u6548\u7a7a\u95f4\u7684\u53c2\u6570\u5197\u4f59\u3002\u540c\u65f6\u9996\u521b\u6027\u5730\u5229\u7528\u5e27\u95f4\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u76f8\u5173\u6027\u8fdb\u884c\u6a21\u578b\u538b\u7f29\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u7684\u538b\u7f29\u6d41\u7a0b\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0HPC\u6846\u67b6\u5728\u538b\u7f29\u6548\u7387\u548c\u6062\u590d\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1167%\u5b58\u50a8\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u6e32\u67d3\u3002", "conclusion": "HPC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u9ad8\u65af\u6295\u5f71\u5728\u6d41\u5f0f\u4f20\u8f93\u4e2d\u7684\u5b58\u50a8\u548c\u538b\u7f29\u6311\u6218\uff0c\u517c\u5177\u9ad8\u7d27\u51d1\u6027\u4e0e\u6e32\u67d3\u8d28\u91cf\uff0c\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01451", "abs": "https://arxiv.org/abs/2602.01451", "authors": ["Umme Abira Azmary", "MD Ikramul Kayes", "Swakkhar Shatabda", "Farig Yousuf Sadeque"], "title": "Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language", "comment": null, "summary": "Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BanglaCQA\uff0c\u8fd9\u662f\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u9006\u4e8b\u5b9e\u95ee\u7b54\uff08Counterfactual QA\uff09\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u8be5\u6570\u636e\u96c6\u5206\u6790\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2dQA\u6a21\u578b\u5bf9\u7f16\u7801\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u8f93\u5165\u7684\u4f9d\u8d56\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u5b5f\u52a0\u62c9\u8bedQA\u6570\u636e\uff0c\u96be\u4ee5\u5206\u6790\u6a21\u578b\u5728\u7b54\u6848\u751f\u6210\u65f6\u5230\u5e95\u66f4\u4f9d\u8d56\u9884\u7f16\u7801\u77e5\u8bc6\u8fd8\u662f\u8bed\u5883\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u3001\u8bed\u8a00\u590d\u6742\u7684\u80cc\u666f\u4e0b\u3002", "method": "1\uff09\u6784\u5efaBanglaCQA\u9006\u4e8b\u5b9e\u6570\u636e\u96c6\uff0c\u5305\u542b\u9006\u4e8b\u5b9e\u6bb5\u843d\u548c\u53ef\u56de\u7b54\u6027\u6ce8\u91ca\uff1b2\uff09\u63d0\u51fa\u9488\u5bf9\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff08\u5355\u8bed\u548c\u591a\u8bed\uff09\u53ca\u4ec5\u89e3\u7801\u5668LLM\u6a21\u578b\u7684\u5fae\u8c03\u548c\u63d0\u793a\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5bf9\u53c2\u6570\u5316\u4e0e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u4f9d\u8d56\uff1b3\uff09\u91c7\u7528\u57fa\u4e8eLLM\u548c\u4eba\u5de5\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u4ef7\u7b54\u6848\u8d28\u91cf\uff1b4\uff09\u7cfb\u7edf\u5206\u6790\u591a\u79cdQA\u8bbe\u7f6e\u4e0b\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5173\u6ce8\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u5728\u9006\u4e8b\u5b9e\u60c5\u5883\u4e2d\u63d0\u53d6\u53c2\u6570\u5316\u77e5\u8bc6\u7684\u6548\u679c\u3002", "result": "\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u5728\u9006\u4e8b\u5b9e\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u4ec5\u89e3\u7801\u5668LLM\u6a21\u578b\u4e2d\uff0c\u80fd\u66f4\u6709\u6548\u5730\u62bd\u53d6\u6a21\u578b\u7684\u53c2\u6570\u5316\u77e5\u8bc6\uff1b\u4e0d\u540c\u7c7b\u578b\u6a21\u578b\u5728\u4e0d\u540cQA\u8bbe\u7f6e\u548c\u573a\u666f\u4e0b\u8868\u73b0\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u4e0d\u4f46\u4e3a\u5b5f\u52a0\u62c9\u8bedQA\u5206\u6790\u77e5\u8bc6\u6765\u6e90\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u8fd8\u53d1\u73b0\u4e86\u9006\u4e8b\u5b9e\u63a8\u7406\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u672a\u6765\u62d3\u5c55\u9006\u4e8b\u5b9e\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.02220", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02220", "abs": "https://arxiv.org/abs/2602.02220", "authors": ["Bo Miao", "Weijia Liu", "Jun Luo", "Lachlan Shinnick", "Jian Liu", "Thomas Hamilton-Smith", "Yuhe Yang", "Zijie Wu", "Vanja Videnovic", "Feras Dayoub", "Anton van den Hengel"], "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation", "comment": null, "summary": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86HieraNav\u4efb\u52a1\u548cLangMap\u57fa\u51c6\uff0c\u4ee5\u591a\u5c42\u6b21\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u8bed\u8a00\u76ee\u6807\u5f15\u5bfc3D\u5ba4\u5185\u5bfc\u822a\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u4e0e\u7269\u4f53\u5173\u7cfb\u5728\u667a\u80fd\u4f53\u4e2d\u7684\u771f\u5b9e\u5e94\u7528\u3002", "motivation": "\u7269\u4f53\u4e0e\u8bed\u8a00\u7684\u5173\u7cfb\u662f\u4eba\u673a\u4ea4\u6d41\u548c\u5177\u8eab\u667a\u80fd\u7684\u6838\u5fc3\u3002\u73b0\u6709\u5bfc\u822a\u4efb\u52a1\u5728\u8bed\u4e49\u5c42\u7ea7\u3001\u8bcd\u6c47\u5e7f\u5ea6\u548c\u771f\u5b9e\u573a\u666f\u4e0a\u7684\u8986\u76d6\u6709\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u57fa\u51c6\u548c\u4efb\u52a1\u6765\u6d4b\u8bd5\u548c\u63a8\u52a8\u57fa\u4e8e\u8bed\u8a00\u7684\u5bfc\u822a\u667a\u80fd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86HieraNav\u4efb\u52a1\uff0c\u5c06\u5bfc\u822a\u76ee\u6807\u7ec6\u5206\u4e3a\u573a\u666f\u3001\u623f\u95f4\u3001\u533a\u57df\u3001\u5b9e\u4f8b\u56db\u4e2a\u8bed\u4e49\u5c42\u7ea7\uff0c\u5e76\u5efa\u7acb\u4e86LangMap\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e3D\u5ba4\u5185\u626b\u63cf\uff0c\u4eba\u5de5\u6807\u6ce8\u4e86\u533a\u57df\u548c\u5b9e\u4f53\u63cf\u8ff0\uff0c\u6db5\u76d6414\u7c7b\u7269\u4f53\uff0c18K+\u5bfc\u822a\u4efb\u52a1\u3002\u6bcf\u4e2a\u5bfc\u822a\u76ee\u6807\u914d\u6709\u7cbe\u7b80\u548c\u8be6\u7ec6\u4e24\u79cd\u63cf\u8ff0\uff0c\u652f\u6301\u591a\u6307\u4ee4\u98ce\u683c\u4e0b\u7684\u8bc4\u4f30\u3002", "result": "LangMap\u6807\u6ce8\u8d28\u91cf\u663e\u8457\u4f18\u4e8eGOAT-Bench\uff0c\u5224\u522b\u51c6\u786e\u7387\u63d0\u534723.8%\uff0c\u4e14\u6240\u7528\u8bcd\u6c47\u51cf\u5c11\u56db\u500d\u3002\u5404\u79cd\u6a21\u578b\u7684\u8bc4\u6d4b\u663e\u793a\uff0c\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u8bb0\u5fc6\u63d0\u5347\u5bfc\u822a\u6210\u529f\u7387\uff0c\u4f46\u957f\u5c3e\u3001\u5c0f\u4f53\u79ef\u3001\u5f3a\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u8fdc\u8ddd\u79bb\u3001\u591a\u76ee\u6807\u4efb\u52a1\u4f9d\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "HieraNav\u548cLangMap\u4e3a\u4ee5\u8bed\u8a00\u9a71\u52a8\u7684\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u7ec6\u7c92\u5ea6\u7684\u8bc4\u6d4b\u5e73\u53f0\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.00683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00683", "abs": "https://arxiv.org/abs/2602.00683", "authors": ["Thong Thanh Nguyen"], "title": "Video Understanding: Through A Temporal Lens", "comment": "PhD Thesis, NUS, 2025", "summary": "This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using \"recurrent adapters\" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new \"temporal-oriented recipe\" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u591a\u9879\u521b\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u5e8f\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\uff0c\u5305\u62ec\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\u3001\u53c2\u6570\u9ad8\u6548\u7684\u65f6\u5e8f\u5efa\u6a21\u3001\u957f\u89c6\u9891\u5efa\u6a21\u65b0\u67b6\u6784\u3001\u7ec6\u7c92\u5ea6\u52a8\u9759\u5173\u7cfb\u5bf9\u6bd4\u5b66\u4e60\u7b49\uff0c\u5168\u9762\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u5efa\u6a21\u89c6\u9891\u5143\u7d20\u4e4b\u95f4\u7684\u65f6\u95f4\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u6316\u6398\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u52a8\u6001\u8054\u7cfb\u3002\u672c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u4ece\u800c\u63a8\u52a8\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "1. \u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u548c\u9c81\u68d2\u6027\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u53ca\u89d2\u5ea6\u635f\u5931\uff0c\u5b9e\u73b0\u81ea\u52a8\u6807\u6ce8\u30022. \u63d0\u51fa\u57fa\u4e8e\u201c\u5faa\u73af\u9002\u914d\u5668\u201d\u7684\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u9002\u5e94\u5c0f\u6570\u636e\u73af\u5883\u4e0b\u7684\u65f6\u5e8f\u52a8\u6001\u30023. \u5f15\u5165\u72b6\u6001\u7a7a\u95f4\u5c42\uff08SSL\uff09\u67b6\u6784\uff0c\u63d0\u5347\u957f\u89c6\u9891\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u65b0\u6570\u636e\u96c6\u4f5c\u4e3a\u957f\u89c6\u9891\u57fa\u51c6\u30024. \u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u52a8\u9759\u5173\u7cfb\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u30025. \u5bf9LVLMs\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u89c6\u89c9-\u8bed\u8a00\u63a5\u53e3\u4e3a\u65f6\u5e8f\u63a8\u7406\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u201c\u65f6\u95f4\u5bfc\u5411\u201d\u914d\u65b9\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u957f\u89c6\u9891\u5efa\u6a21\u53d6\u5f97\u7a81\u7834\uff0c\u5e76\u5728\u65b0\u8bbe\u5b9a\u7684\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u7ec6\u7c92\u5ea6\u52a8\u9759\u5173\u7cfb\u5efa\u6a21\u548c\u65f6\u95f4\u5bfc\u5411\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u6a21\u578b\u7684\u63a8\u7406\u4e0e\u8868\u8fbe\u80fd\u529b\u3002\u7cfb\u7edf\u5206\u6790\u9a8c\u8bc1\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bf9\u63a5\u4e2d\u65f6\u5e8f\u5efa\u6a21\u7684\u5173\u952e\u70b9\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u878d\u5408\u548c\u4f18\u5316\u65f6\u5e8f\u5173\u7cfb\u5efa\u6a21\uff0c\u672c\u8bba\u6587\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u6a21\u578b\u5bf9\u590d\u6742\u3001\u52a8\u6001\u89c6\u9891\u5185\u5bb9\u7684\u8868\u8fbe\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2602.01472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01472", "abs": "https://arxiv.org/abs/2602.01472", "authors": ["Jie Deng", "Shining Liang", "Jun Li", "Hongzhi Li", "Yutao Xie"], "title": "ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure", "comment": null, "summary": "Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5e76\u5229\u7528\u4e86\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u5728\u591a\u95ee\u73af\u5883\u4e0b\u81ea\u52a8\u538b\u7f29\u63a8\u7406\u8fc7\u7a0b\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u7cbe\u8c03\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u51cf\u77ed\u63a8\u7406\u957f\u5ea6\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u4efb\u52a1\u65f6\uff0c\u5e38\u751f\u6210\u8fc7\u957f\u7684\u7b54\u9898\u601d\u8def\u94fe\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u7f29\u77ed\u63a8\u7406\u957f\u5ea6\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5f00\u9500\uff0c\u4f46\u9700\u4fdd\u8bc1\u51c6\u786e\u6027\u3002", "method": "\u4f5c\u8005\u9996\u5148\u53d1\u73b0\u5e76\u5f52\u7eb3\u4e86\u4e00\u4e2a\u79f0\u4e3aSelf-Compression\u7684\u73b0\u8c61\uff1a\u5f53\u6a21\u578b\u540c\u65f6\u9762\u5bf9\u591a\u4e2a\u72ec\u7acb\u53ef\u7b54\u95ee\u9898\u65f6\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u81ea\u7136\u53d8\u77ed\u3002\u636e\u6b64\uff0c\u63d0\u51faConPress\u65b9\u6cd5\uff0c\u7528\u591a\u95ee\u9898\u63d0\u793a\u8bf1\u53d1\u81ea\u538b\u7f29\uff0c\u91c7\u6837\u6a21\u578b\u8f93\u51fa\uff0c\u89e3\u6790\u7b5b\u9009\u5f97\u5230\u7b80\u6d01\u4e14\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u7528\u4f5c\u7cbe\u8c03\u6570\u636e\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u6559\u5e08\u3001\u4eba\u5de5\u526a\u679d\u6216\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u538b\u7f29\u3002", "result": "ConPress\u65b9\u6cd5\u4ec5\u97008k\u7ec6\u8c03\u6837\u672c\uff0c\u5728MATH500\u6570\u636e\u96c6\u4e0a\u63a8\u7406token\u6570\u51cf\u5c11\u4e8659%\uff0cAIME25\u6570\u636e\u96c6\u51cf\u5c1133%\uff0c\u540c\u65f6\u51c6\u786e\u7387\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6a21\u578b\u80fd\u901a\u8fc7\u81ea\u8eab\u4e0a\u4e0b\u6587\u538b\u529b\u8bf1\u5bfc\u51fa\u7b80\u6d01\u63a8\u7406\uff0c\u5e76\u7528\u4f5c\u81ea\u4e3e\u7cbe\u8c03\u6570\u636e\uff0c\u6709\u6548\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u800c\u4e0d\u635f\u5931\u51c6\u786e\u6027\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00687", "abs": "https://arxiv.org/abs/2602.00687", "authors": ["Yuankun Zeng", "Shaohui Li", "Zhi Li", "Shulan Ruan", "Yu Liu", "You He"], "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication", "comment": null, "summary": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u5f0f\u6e90\u7f16\u7801\u601d\u60f3\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u901a\u4fe1\u6846\u67b6V2X-DSC\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5e26\u5bbd\u538b\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u76843D\u611f\u77e5\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u9700\u878d\u5408\u5404\u65b9\u89c2\u6d4b\u4fe1\u606f\u4ee5\u63d0\u53473D\u7406\u89e3\u6548\u679c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5171\u4eab\u4e2d\u95f4\u7279\u5f81\uff08\u5982\u5bc6\u96c6BEV\u7279\u5f81\uff09\u65f6\uff0c\u6781\u5927\u6d88\u8017\u4e86V2X\u901a\u4fe1\u5e26\u5bbd\uff0c\u5f71\u54cd\u7cfb\u7edf\u5b9e\u9645\u5e94\u7528\u3002\u4f5c\u8005\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u5bf9\u540c\u4e00\u73b0\u5b9e\u4e16\u754c\u6709\u9ad8\u76f8\u5173\u89c2\u6d4b\uff0c\u56e0\u6b64\u5b58\u5728\u5197\u4f59\u4fe1\u606f\u7684\u538b\u7f29\u7a7a\u95f4\u3002", "method": "\u63d0\u51faV2X-DSC\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u6761\u4ef6\u7f16\u89e3\u7801\u5668\uff08DCC\uff09\uff1a\u53d1\u9001\u7aef\u5c06BEV\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u7f16\u7801\uff0c\u63a5\u6536\u7aef\u5229\u7528\u672c\u5730\u7279\u5f81\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u606f\u8fdb\u884c\u6761\u4ef6\u91cd\u5efa\uff0c\u4ec5\u4fdd\u7559\u672c\u5730\u611f\u77e5\u4e2d\u7f3a\u5931\u7684\u521b\u65b0\u4fe1\u606f\u3002\u8be5\u7ed3\u6784\u901a\u8fc7\u6709\u6761\u4ef6\u7684\u4fe1\u606f\u5206\u914d\u4e0e\u91cd\u5efa\u8bad\u7ec3\uff0c\u4fc3\u8fdb\u589e\u91cf\u5f0f\u8868\u5f81\u5b66\u4e60\uff0c\u6709\u6548\u6291\u5236\u5197\u4f59\u4e0e\u566a\u58f0\u3002", "result": "\u5728DAIR-V2X\u3001OPV2V\u548cV2X-Real\u516c\u5f00\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728KB\u91cf\u7ea7\u901a\u4fe1\u5e26\u5bbd\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u66f4\u9ad8\u7684\u611f\u77e5\u51c6\u786e\u7387\u4e0e\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u5e76\u80fd\u4f5c\u4e3a\u901a\u4fe1\u5c42\u517c\u5bb9\u591a\u79cd\u534f\u4f5c\u878d\u5408\u4e3b\u5e72\u7f51\u7edc\u3002", "conclusion": "V2X-DSC\u901a\u8fc7\u5206\u5e03\u5f0f\u6e90\u7f16\u7801\u89d2\u5ea6\u521b\u65b0\u6027\u5730\u538b\u7f29\u548c\u878d\u5408\u591a\u667a\u80fd\u4f53\u611f\u77e5\u7279\u5f81\uff0c\u5728\u6781\u4f4e\u5e26\u5bbd\u4e0b\u663e\u8457\u63d0\u9ad8\u534f\u4f5c3D\u611f\u77e5\u7684\u6548\u7387\u4e0e\u6548\u679c\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u9645\u5e94\u7528\u4e0e\u63a8\u5e7f\u610f\u4e49\u3002"}}
{"id": "2602.01479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01479", "abs": "https://arxiv.org/abs/2602.01479", "authors": ["Xueqing Peng", "Ruoyu Xiang", "Fan Zhang", "Mingzi Song", "Mingyang Jiang", "Yan Wang", "Lingfei Qian", "Taiki Hara", "Yuqing Guo", "Jimin Huang", "Junichi Tsujii", "Sophia Ananiadou"], "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance", "comment": null, "summary": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ebisu\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u65e5\u8bed\u91d1\u878d\u8bed\u8a00\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u5305\u62ec\u4e24\u4e2a\u7531\u4e13\u5bb6\u6ce8\u91ca\u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709LLMs\u5728\u590d\u6742\u65e5\u8bed\u91d1\u878d\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e5f\u4e0d\u7406\u60f3\u3002", "motivation": "\u65e5\u8bed\u72ec\u7279\u7684\u8bed\u8a00\u7ed3\u6784\u3001\u4e66\u5199\u4f53\u7cfb\u548c\u9ad8\u8bed\u5883\u4ea4\u6d41\u65b9\u5f0f\u4f7f\u5f97\u91d1\u878d\u9886\u57df\u7684\u8bed\u8a00\u7406\u89e3\u5bf9LLM\u6784\u6210\u6781\u5927\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u6807\u51c6\u96be\u4ee5\u8986\u76d6\u8fd9\u4e9b\u7279\u6027\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u91d1\u878dNLP\u7684\u8fdb\u6b65\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86Ebisu\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1a1\uff09\u6295\u8d44\u8005\u95ee\u7b54\u4e2d\u5bf9\u9690\u6027\u627f\u8bfa\u548c\u62d2\u7edd\u7684\u8bc6\u522b\uff08JF-ICR\uff09\uff1b2\uff09\u4ece\u4e13\u4e1a\u62ab\u9732\u4e2d\u63d0\u53d6\u548c\u6392\u5e8f\u5d4c\u5957\u91d1\u878d\u672f\u8bed\uff08JF-TE\uff09\u3002\u4efb\u52a1\u5747\u7ecf\u4e13\u5bb6\u6807\u6ce8\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u901a\u7528\u3001\u65e5\u8bed\u9002\u5e94\u548c\u91d1\u878d\u4e13\u7528LLM\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u65e0\u8bba\u6a21\u578b\u89c4\u6a21\u591a\u5927\uff0c\u6216\u662f\u5426\u505a\u8fc7\u8bed\u8a00/\u9886\u57df\u9002\u5e94\uff0c\u5404\u7c7b\u6a21\u578b\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u90fd\u6709\u9650\uff0c\u65e0\u6cd5\u7a33\u5b9a\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\uff0c\u663e\u793a\u8be5\u9886\u57df\u6311\u6218\u6027\u6781\u5927\u3002", "conclusion": "Ebisu\u4f5c\u4e3a\u516c\u5f00\u6570\u636e\u548c\u8bc4\u6d4b\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u63a8\u52a8\u65e5\u8bed\u91d1\u878dNLP\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u8de8\u8bed\u8a00\u3001\u8de8\u6587\u5316\u573a\u666f\u4e2d\u7684\u6280\u672f\u96be\u9898\uff0c\u4e3a\u4eca\u540e\u7684\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2602.00702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00702", "abs": "https://arxiv.org/abs/2602.00702", "authors": ["Ruikui Wang", "Jinheng Feng", "Lang Tian", "Huaishao Luo", "Chaochao Li", "Liangbo Zhou", "Huan Zhang", "Youzheng Wu", "Xiaodong He"], "title": "JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning", "comment": null, "summary": "Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.", "AI": {"tldr": "JoyAvatar\u6a21\u578b\u7a81\u7834\u4e86\u73b0\u6709\u865a\u62df\u4eba\u89c6\u9891\u751f\u6210\u5728\u6587\u672c\u6307\u4ee4\u5bf9\u9f50\u548c\u590d\u6742\u52a8\u4f5c\u63a7\u5236\u4e0a\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u6d41\u7545\u7684\u5168\u8eab\u52a8\u4f5c\u548c\u52a8\u6001\u573a\u666f\u7684\u957f\u65f6\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u4eba\u7cfb\u7edf\u5728\u8bf4\u8bdd\u3001\u6f14\u8bb2\u7b49\u573a\u666f\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u96be\u4ee5\u54cd\u5e94\u5305\u542b\u5927\u5e45\u80a2\u4f53\u52a8\u4f5c\u3001\u52a8\u6001\u955c\u5934\u3001\u573a\u666f\u5207\u6362\u6216\u4eba-\u7269\u4e92\u52a8\u7b49\u590d\u6742\u6587\u672c\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86JoyAvatar\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u9879\u521b\u65b0\uff1a\uff081\uff09\u53cc\u6559\u5e08\u589e\u5f3a\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8ba9\u6a21\u578b\u65e2\u80fd\u7ee7\u627f\u57fa\u7840\u6a21\u578b\u7684\u6587\u672c\u53ef\u63a7\u6027\uff0c\u53c8\u80fd\u5b66\u4e60\u97f3\u89c6\u9891\u540c\u6b65\uff1b\uff082\uff09\u57fa\u4e8e\u53bb\u566a\u65f6\u95f4\u6b65\u5bf9\u591a\u6a21\u6001\u6761\u4ef6\uff08\u5982\u97f3\u9891\u3001\u6587\u672c\uff09\u5f3a\u5ea6\u52a8\u6001\u8c03\u8282\uff0c\u7f13\u89e3\u5f02\u6784\u4fe1\u53f7\u95f4\u7684\u51b2\u7a81\u3002", "result": "GSB\u8bc4\u6d4b\u663e\u793a\uff0cJoyAvatar\u5728\u5168\u8eab\u52a8\u4f5c\u3001\u52a8\u6001\u955c\u5934\u63a7\u5236\u3001\u53e3\u578b\u5bf9\u9f50\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u8d85\u8d8a\u4e86Omnihuman-1.5\u548cKlingAvatar 2.0\uff0c\u5e76\u652f\u6301\u591a\u89d2\u8272\u5bf9\u8bdd\u548c\u89d2\u8272\u626e\u6f14\u7b49\u590d\u6742\u5e94\u7528\u3002", "conclusion": "JoyAvatar\u6781\u5927\u6269\u5c55\u4e86\u865a\u62df\u4eba\u89c6\u9891\u751f\u6210\u7684\u8868\u73b0\u529b\u548c\u6307\u4ee4\u5bf9\u9f50\u80fd\u529b\uff0c\u4e3a\u5168\u8eab\u52a8\u6001\u3001\u591a\u4eba\u4e92\u52a8\u7b49\u9ad8\u7ea7\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.01511", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01511", "abs": "https://arxiv.org/abs/2602.01511", "authors": ["Ran Xu", "Tianci Liu", "Zihan Dong", "Tony You", "Ilgee Hong", "Carl Yang", "Linjun Zhang", "Tao Zhao", "Haoyu Wang"], "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training", "comment": "The first two authors contributed equally", "summary": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.", "AI": {"tldr": "Rubric-ARM\u662f\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u3001\u4e3b\u89c2\u9886\u57df\u4e0b\u7684AI\u8f93\u51fa\u8d28\u91cf\uff0c\u8054\u5408\u4f18\u5316\u8bc4\u5206\u6807\u51c6\u751f\u6210\u5668\u548c\u8bc4\u5206\u5458\uff0c\u901a\u8fc7\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u8f93\u51fa\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u53ea\u7ed9\u51fa\u5355\u4e00\u5206\u6570\uff0c\u96be\u4ee5\u8bc4\u4f30\u5982\u521b\u610f\u5199\u4f5c\u7b49\u4e3b\u89c2\u3001\u591a\u9762\u7684\u751f\u6210\u4efb\u52a1\uff0c\u5b58\u5728\u8bc4\u5224\u7c92\u5ea6\u7c97\u7cd9\u3001\u9002\u5e94\u6027\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faRubric-ARM\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u8bc4\u5206\u6807\u51c6\u751f\u6210\u5668\uff08rubric generator\uff09\u548c\u8bc4\u5206\u5458\uff08judge\uff09\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u4ece\u504f\u597d\u53cd\u9988\u4e2d\u4f18\u5316\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u5904\u7406\u540c\u65f6\u4f18\u5316\u5e26\u6765\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "Rubric-ARM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u53d6\u5f97\u5f53\u524d\u6700\u4f18\u8868\u73b0\uff0c\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0b\u663e\u8457\u63d0\u5347\u7b56\u7565\u7684\u4e00\u81f4\u6027\u548c\u8868\u73b0\u3002", "conclusion": "Rubric-ARM\u6709\u6548\u63d0\u5347\u4e86AI\u5728\u4e3b\u89c2\u590d\u6742\u9886\u57df\u4e0b\u7684\u5956\u52b1\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u66f4\u7075\u6d3b\u3001\u7ec6\u81f4\u7684\u8f93\u51fa\u8d28\u91cf\u8bc4\u4f30\u548c\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.00703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00703", "abs": "https://arxiv.org/abs/2602.00703", "authors": ["Zhongtian Huang", "Zhi Chen", "Zi Huang", "Xin Yu", "Daniel Smith", "Chaitanya Purushothama", "Erik Van Oosterom", "Alex Wu", "William Salter", "Yan Li", "Scott Chapman"], "title": "StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components", "comment": null, "summary": "Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $\u03bc$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u7cb1\u6c14\u5b54\u6210\u5206\u7684\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u96c6\u5e76\u6807\u6ce8\u5927\u91cf\u663e\u5fae\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u4f2a\u6807\u6ce8\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u7ed3\u6784\u6c14\u5b54\u7684\u81ea\u52a8\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u9ad8\u7cb1\u662f\u4e00\u79cd\u8010\u65f1\u3001\u5bf9\u6c14\u5019\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\u7684\u7cae\u98df\u4f5c\u7269\uff0c\u63d0\u5347\u5176\u6c34\u5206\u5229\u7528\u6548\u7387\u5bf9\u4e8e\u7cae\u98df\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u6c14\u5b54\u4f5c\u4e3a\u8c03\u8282\u6c34\u5206\u4ee3\u8c22\u548c\u5149\u5408\u4f5c\u7528\u7684\u5173\u952e\u7ed3\u6784\uff0c\u7cbe\u786e\u8868\u578b\u5206\u6790\u96be\u5ea6\u5927\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b11,060\u4e2a\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\u7684\u9ad8\u7cb1\u6c14\u5b54\u6210\u5206\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u9ad8\u5206\u8fa8\u7387\u663e\u5fae\u56fe\u7247\u5207\u5206\u4e3a\u5c0f\u5757\u4ee5\u68c0\u6d4b\u5fae\u5c0f\u7ed3\u6784\u3002\u7136\u540e\u5229\u7528\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4f2a\u6807\u6ce8\u7b56\u7565\uff0c\u5bf9\u672a\u6807\u6ce8\u56fe\u7247\u751f\u6210\u989d\u5916\u768456,428\u4e2a\u5e26\u6807\u7b7e\u6837\u672c\uff0c\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u91cf\u3002\u5bf9\u6bd4\u591a\u79cd\u5206\u5272\u7b97\u6cd5\uff0c\u8bc4\u4f30\u5904\u7406\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7ecf\u8fc7patch\u5207\u5206\u4e0e\u534a\u76d1\u7763\u4f2a\u6807\u6ce8\u540e\uff0c\u4e3b\u6d41\u6a21\u578b\u5206\u5272\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a\u8bed\u4e49\u5206\u5272mIoU\u4ece65.93%\u63d0\u5347\u81f370.35%\uff0c\u5b9e\u4f8b\u5206\u5272AP\u4ece28.30%\u63d0\u5347\u81f346.10%\u3002", "conclusion": "\u7ed3\u5408patch\u9884\u5904\u7406\u548c\u534a\u76d1\u7763\u5b66\u4e60\u53ef\u6709\u6548\u63d0\u5347\u6c14\u5b54\u5fae\u5c0f\u7ed3\u6784\u7684\u5206\u5272\u6548\u679c\uff0c\u4e3a\u9ad8\u901a\u91cf\u6027\u72b6\u5206\u6790\u548c\u4f5c\u7269AI\u8868\u578b\u68c0\u6d4b\u6280\u672f\u7684\u89c4\u6a21\u5316\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01560", "abs": "https://arxiv.org/abs/2602.01560", "authors": ["Keito Inoshita", "Michiaki Omura", "Tsukasa Yamanaka", "Go Maeda", "Kentaro Tsuji"], "title": "Argument Rarity-based Originality Assessment for AI-Assisted Writing", "comment": null, "summary": "As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u8bc4\u4f30\u5b66\u751f\u4f5c\u6587\u539f\u521b\u6027\u7684\u65b0\u65b9\u6cd5\u2014\u2014\u57fa\u4e8e\u8bba\u8bc1\u7f55\u89c1\u6027\u7684\u539f\u521b\u6027\u8bc4\u4f30\uff08AROA\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u3001\u8bba\u70b9\u3001\u8bc1\u636e\u7f55\u89c1\u6027\u53ca\u8ba4\u77e5\u6df1\u5ea6\u56db\u4e2a\u7ef4\u5ea6\u5bf9\u4f5c\u6587\u8fdb\u884c\u5206\u6790\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u9ad8\u8d28\u91cf\u6587\u672c\u5e38\u91c7\u7528\u5e38\u89c4\u8bba\u70b9\u6a21\u5f0f\uff0c\u5176\u539f\u521b\u6027\u76f8\u5bf9\u8f83\u4f4e\uff0c\u800cAI\u751f\u6210\u6587\u672c\u867d\u7ed3\u6784\u590d\u6742\u4f46\u8bba\u70b9\u539f\u521b\u6027\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u8f7b\u677e\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\uff0c\u4f20\u7edf\u4ee5\u5199\u4f5c\u8d28\u91cf\u4e3a\u6838\u5fc3\u7684\u8bc4\u4ef7\u65b9\u5f0f\u9010\u6e10\u5931\u53bb\u610f\u4e49\u3002\u5982\u679c\u6559\u80b2\u7684\u672c\u8d28\u662f\u57f9\u517b\u6279\u5224\u6027\u601d\u7ef4\u548c\u539f\u521b\u65b0\u89c2\u70b9\uff0c\u5219\u5199\u4f5c\u8bc4\u4f30\u4e5f\u9700\u4ece\u201c\u8d28\u91cf\u201d\u8f6c\u5411\u201c\u539f\u521b\u6027\u201d\u3002", "method": "AROA\u6846\u67b6\u5c06\u201c\u539f\u521b\u6027\u201d\u5b9a\u4e49\u4e3a\u5728\u53c2\u8003\u8bed\u6599\u5e93\u4e2d\u7684\u7f55\u89c1\u6027\uff0c\u4ece\u7ed3\u6784\u7f55\u89c1\u6027\u3001\u8bba\u70b9\u7f55\u89c1\u6027\u3001\u8bc1\u636e\u7f55\u89c1\u6027\u548c\u8ba4\u77e5\u6df1\u5ea6\u56db\u65b9\u9762\u91cf\u5316\u4f5c\u6587\u5404\u90e8\u5206\u7684\u201c\u7a00\u6709\u5ea6\u201d\uff0c\u5e76\u7ed3\u5408\u8d28\u91cf\u8c03\u6574\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u4f5c\u6587\u8d28\u91cf\u4e0e\u539f\u521b\u6027\u7684\u72ec\u7acb\u8bc4\u4f30\u3002", "result": "\u5728\u4eba\u7c7b\u4f5c\u6587\u4e0eAI\u4f5c\u6587\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6587\u672c\u8d28\u91cf\u4e0e\u8bba\u70b9\u7f55\u89c1\u6027\u5448\u5f3a\u8d1f\u76f8\u5173\uff0c\u5373\u9ad8\u8d28\u91cf\u6587\u672c\u5f80\u5f80\u4f7f\u7528\u5e38\u89c1\u8bba\u70b9\u6a21\u5f0f\u3002AI\u4f5c\u6587\u5728\u7ed3\u6784\u590d\u6742\u5ea6\u4e0a\u80fd\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u4f46\u8bba\u70b9\u7f55\u89c1\u6027\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4f5c\u6587\uff0c\u663e\u793aLLMs\u5728\u89c2\u70b9\u539f\u521b\u6027\u4e0a\u6709\u9650\u3002", "conclusion": "AROA\u4e3a\u5199\u4f5c\u539f\u521b\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u8d28\u91cf\u548c\u539f\u521b\u6027\u53ef\u4ee5\u72ec\u7acb\u8861\u91cf\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u6559\u80b2\u8bc4\u4ef7\u8303\u5f0f\u4ece\u201c\u8d28\u91cf\u5bfc\u5411\u201d\u5411\u201c\u539f\u521b\u5bfc\u5411\u201d\u8f6c\u53d8\u3002"}}
{"id": "2602.00729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00729", "abs": "https://arxiv.org/abs/2602.00729", "authors": ["Qihe Pan", "Yiming Wu", "Xing Zhao", "Liang Xie", "Guodao Sun", "Ronghua Liang"], "title": "Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation", "comment": "This paper has been accepted for publication in the proceedings of 2026 IEEE ICASSP Conference", "summary": "Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5316\u5986\u8fc1\u79fb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u3001\u7279\u5f81\u89e3\u8026\u548c\u53ef\u63a7\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6784\u5efa\u3001\u7279\u5f81\u89e3\u8026\u6269\u6563\u6846\u67b6\u548c\u6587\u672c\u5f15\u5bfc\u7684\u7cbe\u7ec6\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u3001\u591a\u6837\u4e14\u53ef\u63a7\u7684\u5316\u5986\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u5316\u5986\u8fc1\u79fb\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3001\u8eab\u4efd\u4e0e\u5316\u5986\u7279\u5f81\u89e3\u8026\u4e0d\u4f73\u3001\u53ef\u63a7\u6027\u5f31\u7b49\u95ee\u9898\u3002\u4e3a\u4e86\u63d0\u5347\u5316\u5986\u8fc1\u79fb\u7684\u8d28\u91cf\u3001\u771f\u5b9e\u611f\u4ee5\u53ca\u7528\u6237\u81ea\u5b9a\u4e49\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1\uff09\u91c7\u7528\u201c\u8bad\u7ec3-\u751f\u6210-\u7b5b\u9009-\u518d\u8bad\u7ec3\u201d\u6d41\u7a0b\uff0c\u7ed3\u5408\u5408\u6210\u3001\u771f\u5b9e\u548c\u7b5b\u9009\u6837\u672c\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u96c6\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7279\u5f81\u89e3\u8026\u6846\u67b6\uff0c\u5c06\u8eab\u4efd\u7279\u5f81\u4e0e\u5316\u5986\u7279\u5f81\u5206\u79bb\uff0c\u4fdd\u8bc1\u9762\u90e8\u7ed3\u6784\u548c\u80a4\u8272\u4e0d\u53d8\uff1b3\uff09\u5f15\u5165\u6587\u672c\u5f15\u5bfc\u673a\u5236\uff0c\u5b9e\u73b0\u773c\u90e8\u3001\u5507\u90e8\u3001\u8138\u90e8\u7b49\u5c40\u90e8\u5316\u5986\u7684\u81ea\u7136\u8bed\u8a00\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0e\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u611f\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5316\u5986\u8fc1\u79fb\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u89e3\u8026\u8eab\u4efd\u4e0e\u5316\u5986\u7279\u5f81\uff0c\u540c\u65f6\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u673a\u5236\u63d0\u5347\u4e86\u5316\u5986\u63a7\u5236\u7684\u7cbe\u7ec6\u5ea6\uff0c\u63a8\u52a8\u4e86\u5316\u5986\u8fc1\u79fb\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01566", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01566", "abs": "https://arxiv.org/abs/2602.01566", "authors": ["Chiwei Zhu", "Benfeng Xu", "Mingxuan Du", "Shaohan Wang", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents", "comment": "19 pages, 6 figures", "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFS-Researcher\u7cfb\u7edf\uff0c\u7a81\u7834\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u4fe1\u606f\u6536\u96c6\u4e0e\u62a5\u544a\u64b0\u5199\uff0c\u663e\u8457\u63d0\u5347\u62a5\u544a\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u6d89\u53ca\u957f\u4efb\u52a1\u5e8f\u5217\uff0c\u5e38\u8d85\u51fa\u73b0\u6709\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0a\u9650\uff0c\u5bfc\u81f4\u8bc1\u636e\u91c7\u96c6\u548c\u62a5\u544a\u5199\u4f5c\u53d7\u9650\uff0c\u5f71\u54cd\u6700\u7ec8\u6548\u679c\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u7a81\u7834\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u652f\u6301\u66f4\u590d\u6742\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u4efb\u52a1\u3002", "method": "FS-Researcher\u91c7\u7528\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u67b6\u6784\u3002\u5728\u8be5\u4f53\u7cfb\u4e0b\uff0cContext Builder\u667a\u80fd\u4f53\u5982\u540c\u56fe\u4e66\u7ba1\u7406\u5458\uff0c\u8d1f\u8d23\u6d4f\u89c8\u7f51\u7edc\u3001\u7ed3\u6784\u5316\u8bb0\u5f55\u7b14\u8bb0\u53ca\u6574\u7406\u539f\u59cb\u8d44\u6599\u5230\u53ef\u6269\u5c55\u7684\u5206\u5c42\u77e5\u8bc6\u5e93\u3002Report Writer\u667a\u80fd\u4f53\u5219\u9010\u8282\u9ad8\u6548\u64b0\u5199\u7814\u7a76\u62a5\u544a\uff0c\u5e76\u5c06\u77e5\u8bc6\u5e93\u4f5c\u4e3a\u4e8b\u5b9e\u4f9d\u636e\u3002\u6587\u4ef6\u7cfb\u7edf\u65e2\u662f\u6301\u4e45\u5316\u7684\u5916\u90e8\u8bb0\u5fc6\uff0c\u4e5f\u662f\u667a\u80fd\u4f53\u95f4\u3001\u4f1a\u8bdd\u95f4\u7684\u5171\u4eab\u534f\u8c03\u5a92\u4ecb\uff0c\u4ece\u800c\u5b9e\u73b0\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u8fed\u4ee3\u7814\u7a76\u3002", "result": "\u5728DeepResearch Bench\u548cDeepConsult\u4e24\u4e2a\u5f00\u653e\u6027\u57fa\u51c6\u4e0a\uff0cFS-Researcher\u5728\u591a\u79cd\u4e3b\u5e72\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e1a\u754c\u9886\u5148\u7684\u62a5\u544a\u8d28\u91cf\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\uff0cContext Builder\u5206\u914d\u7684\u7b97\u529b\u8d8a\u591a\uff0c\u6700\u7ec8\u62a5\u544a\u7684\u8d28\u91cf\u8d8a\u9ad8\uff0c\u9a8c\u8bc1\u4e86\u6587\u4ef6\u7cfb\u7edf\u8303\u5f0f\u4e0b\u7684\u6709\u6548\u63a8\u7406\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "FS-Researcher\u7a81\u7834\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u74f6\u9888\uff0c\u901a\u8fc7\u5916\u90e8\u6587\u4ef6\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u7814\u7a76\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4e3a\u590d\u6742\u957f\u4efb\u52a1\u5e8f\u5217\u4e0b\u7684\u81ea\u52a8\u5316\u7814\u7a76\u548c\u5199\u4f5c\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00739", "abs": "https://arxiv.org/abs/2602.00739", "authors": ["Zhengyan Qin", "Liyuan Qiu"], "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries", "comment": null, "summary": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the \"double surface artifact\" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5b58\u5728\u201c\u53cc\u8868\u9762\u4f2a\u5f71\u201d\u7684\u53cc\u5c42\u70b9\u4e91\u4e2d\u5206\u79bb\u51fa\u5185\u5916\u5c42\u8868\u9762\uff0c\u9002\u7528\u4e8eTSDF\u878d\u5408\u5f15\u8d77\u7684\u91cd\u5efa\u4f2a\u5f71\uff0c\u517c\u5bb9\u5c01\u95ed\u548c\u5f00\u653e\u8fb9\u754c\u70b9\u4e91\u3002", "motivation": "\u73b0\u6709TSDF\u878d\u5408\u65b9\u6cd5\u5728\u91cd\u5efa3D\u573a\u666f\uff08\u5982\u5ba4\u5185\u6216\u533b\u5b66\u56fe\u50cf\uff09\u65f6\uff0c\u5e38\u56e0\u622a\u65ad\u9608\u503c\u4e0d\u5bf9\u79f0\u4ea7\u751f\u53cc\u5c42\u8868\u9762\u4f2a\u5f71\uff0c\u9020\u6210\u91cd\u5efa\u8868\u9762\u91cd\u53e0\u65e0\u5e8f\uff0c\u5f71\u54cd\u540e\u7eed\u5904\u7406\uff0c\u6025\u9700\u9ad8\u6548\u5206\u79bb\u771f\u5c42\u8868\u9762\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u4ece\u5177\u6709\u5f00\u653e\u8fb9\u754c\uff08\u5b58\u5728\u5b54\u6d1e\uff09\u6216\u5c01\u95ed\u7ed3\u6784\u7684\u53cc\u5c42\u70b9\u4e91\u4e2d\uff0c\u5206\u79bb\u51fa\u771f\u5b9e\u7684\u5185\u5c42\u8868\u9762\uff0c\u5e76\u4ee5\u9ad8\u6548\uff08\u7ea610\u79d2\u5904\u74064\u4e07\u70b9\uff09\u65b9\u5f0f\u6d88\u9664\u91cd\u53e0\u548c\u8868\u9762\u6cd5\u5411\u6df7\u4e71\u7684\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5728\u7ea610\u79d2\u5185\u5bf9\u542b4\u4e07\u4e2a\u70b9\u7684\u70b9\u4e91\u8fdb\u884c\u5185\u5c42\u8868\u9762\u63d0\u53d6\uff0c\u6548\u679c\u9002\u7528\u4e8e\u5c01\u95ed\u6216\u5f00\u653e\u8fb9\u754c\u6a21\u578b\uff0c\u6709\u6548\u53bb\u9664\u4e86\u4f2a\u5f71\u4e0e\u9519\u5c42\u3002", "conclusion": "\u6240\u63d0\u6269\u6563\u7b97\u6cd5\u6a21\u5757\u8f7b\u91cf\uff0c\u9002\u5408\u4f5c\u4e3aTSDF\u878d\u5408\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5ba4\u5185\u5efa\u6a21\u548c\u533b\u5b66\u6210\u50cf\u7b49\u5bf9\u70b9\u4e91\u8868\u9762\u7cbe\u5ea6\u6709\u9ad8\u8981\u6c42\u5e94\u7528\u7684\u7ed3\u679c\u8d28\u91cf\uff0c\u65e0\u9700\u66ff\u4ee3\u590d\u6742\u7684\u91cd\u5efa\u7ba1\u7ebf\u3002"}}
{"id": "2602.01572", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01572", "abs": "https://arxiv.org/abs/2602.01572", "authors": ["Yeqin Zhang", "Yunfei Wang", "Jiaxuan Chen", "Ke Qin", "Yizheng Zhao", "Cam-Tu Nguyen"], "title": "LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States", "comment": null, "summary": "Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u901a\u8fc7\u805a\u5408\u6ce8\u610f\u529b\u503c\u5411\u91cf\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u9690\u85cf\u72b6\u6001\uff09\u6765\u83b7\u5f97\u66f4\u4f18\u7684\u53e5\u5b50\u8868\u793a\uff0c\u5e76\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u6700\u7ec8\u5c42\u9690\u85cf\u72b6\u6001\u6765\u63d0\u53d6\u53e5\u5b50\u8868\u793a\uff0c\u4f46\u8fd9\u4e9b\u9690\u85cf\u72b6\u6001\u4e3b\u8981\u4e3a\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u800c\u4f18\u5316\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u53e5\u5b50\u7ea7\u7684\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u3002\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u66f4\u9002\u5408\u65e0\u76d1\u7763\u8bed\u4e49\u8868\u793a\u63d0\u53d6\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u201cValue Aggregation\u201d(VA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u4e0d\u540c\u5c42\u548ctoken\u805a\u5408\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684value\u5411\u91cf\u6765\u751f\u6210\u53e5\u5b50\u8868\u793a\u3002\u66f4\u8fdb\u4e00\u6b65\uff0c\u4f5c\u8005\u57fa\u4e8e\u7279\u5b9a\u7684prompt\u4e0e\u8ba1\u7b97\u52a0\u6743\u805a\u5408\uff0c\u63d0\u51fa\u4e86\u201cAligned Weighted VA\u201d(AlignedWVA)\u7b97\u6cd5\uff0c\u4f7f\u805a\u5408\u540e\u7684\u8868\u793a\u4e0eLLM\u7684\u6b8b\u5dee\u7a7a\u95f4\u66f4\u597d\u5730\u5bf9\u9f50\u3002\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u5bf9LLM\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u9700\u8bad\u7ec3\u7684VA\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u540c\u7c7bLLM\u53e5\u5b50\u5d4c\u5165\u65b9\u6848\uff0c\u7ecf\u8fc7\u5bf9\u9f50\u52a0\u6743\u540e\u7684AlignedWVA\u5728\u591a\u9879\u53e5\u5b50\u5d4c\u5165\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u6602\u8d35\u7684MetaEOL\u96c6\u6210\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u65e0\u8bad\u7ec3SOTA\u8868\u73b0\u3002", "conclusion": "\u805a\u5408\u548c\u5bf9\u9f50\u6ce8\u610f\u529bvalue\u5411\u91cf\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u53e5\u5b50\u8868\u5f81\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u51b7\u542f\u52a8\u6216\u65e0\u76d1\u7763\u8bed\u4e49\u62bd\u53d6\u5e26\u6765\u66f4\u5177\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00749", "abs": "https://arxiv.org/abs/2602.00749", "authors": ["Xiangming Wang", "Benteng Sun", "Yungeng Liu", "Haijin Zeng", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression", "comment": null, "summary": "Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \\textbf{\\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \\times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HSI-VAR\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u590d\u539f\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u66ff\u4ee3\u8ba1\u7b97\u91cf\u8f83\u5927\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u4fee\u590d\u3002HSI-VAR\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u6784\u7ec6\u8282\u4fdd\u7559\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u9876\u5c16\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u9ad8\u5149\u8c31\u56fe\u50cf\u590d\u539f\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u91cf\u5927\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\uff0c\u8981\u4e48\u5bfc\u81f4\u56fe\u50cf\u8fc7\u4e8e\u5e73\u6ed1\u3001\u7ec6\u8282\u635f\u5931\uff08\u56de\u5f52\u6a21\u578b\uff09\u3002\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u9ad8\u6548\u4e14\u80fd\u4fdd\u7559\u7ec6\u8282\u7684\u590d\u539f\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u521b\u65b0\u6027\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u4e0e\u6548\u679c\u74f6\u9888\u3002", "method": "HSI-VAR\u5c06\u9ad8\u5149\u8c31\u56fe\u50cf\u590d\u539f\u89c6\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u9010\u6b65\u5efa\u6a21\u5149\u8c31\u548c\u7a7a\u95f4\u4f9d\u8d56\u3002\u6838\u5fc3\u6539\u8fdb\u5305\u62ec\uff1a\uff081\uff09\u6f5c\u53d8\u91cf\u6761\u4ef6\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7cbe\u786e\u91cd\u5efa\uff1b\uff082\uff09\u9000\u5316\u611f\u77e5\u5f15\u5bfc\uff0c\u628a\u4e0d\u540c\u9000\u5316\u7c7b\u578b\u7f16\u7801\u8fdbEmbedding\u7a7a\u95f4\uff0c\u5b9e\u73b0\u81ea\u52a8\u590d\u539f\u63a7\u5236\u5e76\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff1b\uff083\uff09\u7a7a\u95f4-\u5149\u8c31\u81ea\u9002\u5e94\u6a21\u5757\uff0c\u7ec6\u5316\u7a7a\u95f4\u548c\u5149\u8c31\u7684\u6062\u590d\u7ec6\u8282\u3002", "result": "\u5728\u4e5d\u4e2a\u9ad8\u5149\u8c31\u56fe\u50cf\u590d\u539f\u57fa\u51c6\u4e0a\uff0cHSI-VAR\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5982\u5728ICVL\u6570\u636e\u96c6\u63d0\u53473.77 dB PSNR\uff0c\u5e76\u8fdc\u8d85\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u6062\u590d\u80fd\u529b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe95.5\u500d\u3002", "conclusion": "HSI-VAR\u7ed3\u5408\u9ad8\u6548\u7b97\u529b\u4e0e\u7ec6\u8282\u6062\u590d\u4f18\u52bf\uff0c\u7a81\u7834\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u590d\u539f\u9886\u57df\u7684\u73b0\u6709\u74f6\u9888\uff0c\u4e3a\u5b9e\u9645\u590d\u6742\u9000\u5316\u7684\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01587", "abs": "https://arxiv.org/abs/2602.01587", "authors": ["Zehua Cheng", "Jianwei Yang", "Wei Dai", "Jiahao Sun"], "title": "Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment", "comment": "10 pages", "summary": "Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8ba4\u8bc1\u7684\u8bed\u4e49\u5e73\u6ed1\u65b9\u6cd5\uff08CSS\uff09\uff0c\u5927\u5e45\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u81ea\u9002\u5e94\u8d8a\u72f1\u653b\u51fb\u65f6\uff0c\u73b0\u6709\u4f9d\u8d56\u7ecf\u9a8c\u9632\u5fa1\u7684\u65b9\u6cd5\uff08\u5982GCG\uff09\u4f9d\u65e7\u5b58\u5728\u8f83\u5927\u6f0f\u6d1e\uff0c\u6025\u9700\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u53ef\u8bc1\u660e\u5b89\u5168\u6027\u7684\u9632\u62a4\u673a\u5236\u3002", "method": "1. \u63d0\u51faCertified Semantic Smoothing\uff08CSS\uff09\u673a\u5236\uff0c\u901a\u8fc7Stratified Randomized Ablation\u5bf9\u8f93\u5165\u8fdb\u884c\u4e0d\u53d8\u7ed3\u6784\u63d0\u793a\u548c\u53ef\u53d8\u8f7d\u8377\u7684\u5206\u533a\uff0c\u7ed3\u5408\u8d85\u51e0\u4f55\u5206\u5e03\u8fdb\u884c\u4e25\u683c\u7684l0\u8303\u6570\u975e\u5bf9\u79f0\u6027\u4fdd\u8bc1\u3002\n2. \u9488\u5bf9\u7a00\u758f\u4e0a\u4e0b\u6587\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5f15\u5165\u566a\u58f0\u589e\u5f3a\u5bf9\u9f50\u5fae\u8c03\uff08NAAT\uff09\uff0c\u5c06\u6a21\u578b\u4f18\u5316\u4e3a\u8bed\u4e49\u53bb\u566a\u5668\u3002", "result": "\u5728Llama-3\u6a21\u578b\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u6210\u529f\u7387\u4ece84.2%\u964d\u81f31.2%\uff0c\u540c\u65f6\u4fdd\u630194.1%\u7684\u6b63\u5e38\u7528\u4f8b\u6548\u80fd\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5b57\u7b26\u7ea7\u7684\u9632\u62a4\uff08\u6548\u80fd\u964d\u81f374.3%\uff09\u3002", "conclusion": "\u672c\u6846\u67b6\u4e3a\u5927\u6a21\u578b\u9632\u8d8a\u72f1\u653b\u51fb\u63d0\u4f9b\u4e86\u786e\u5b9a\u6027\u5b89\u5168\u8ba4\u8bc1\uff0c\u4fdd\u8bc1\u6a21\u578b\u5728\u53ef\u8bc1\u8303\u56f4\u5185\u5bf9\u6240\u6709\u5bf9\u6297\u53d8\u79cd\u90fd\u5177\u5907\u9c81\u68d2\u6027\uff0c\u5bf9\u5b89\u5168\u5bf9\u8bdd\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00763", "abs": "https://arxiv.org/abs/2602.00763", "authors": ["Dylan Yves", "Khush Agarwal", "Jonathan Hoyin Chan", "Patcharapit Promoppatum", "Aroonkamon Pattanasiricharoen"], "title": "Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints", "comment": "9 pages, 6 figures", "summary": "Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684U-Net\u6a21\u578b\u5728\u8d85\u58f0\u56fe\u50cf\u4e2d\u795e\u7ecf\u5206\u5272\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u7ec4\u6210\u548c\u6807\u6ce8\u7b56\u7565\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u8d85\u58f0\u5f15\u5bfc\u4e0b\u8fdb\u884c\u533a\u57df\u9ebb\u9189\u65f6\uff0c\u7cbe\u786e\u5b9a\u4f4d\u795e\u7ecf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u5bf9\u6bd4\u5dee\u3001\u6591\u70b9\u566a\u58f0\u548c\u4e2a\u4f53\u89e3\u5256\u5dee\u5f02\uff0c\u4eba\u5de5\u8bc6\u522b\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u3001\u9c81\u68d2\u7684\u795e\u7ecf\u5206\u5272\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eU-Net\u795e\u7ecf\u7f51\u7edc\u5bf9\u81c2\u4e1b\u795e\u7ecf\u7684\u8d85\u58f0\u56fe\u50cf\u8fdb\u884c\u5206\u5272\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u8d85\u58f0\u8bbe\u5907\uff08SIEMENS\u548cPhilips\uff09\u7684\u6df7\u5408\u8bad\u7ec3\u6570\u636e\u5bf9\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u4e86\u5c06\u4efb\u52a1\u4ece\u4e8c\u5206\u7c7b\uff08\u53ea\u5206\u5272\u795e\u7ecf\uff09\u6269\u5c55\u5230\u591a\u5206\u7c7b\uff08\u5305\u62ec\u52a8\u8109\u3001\u9759\u8109\u3001\u795e\u7ecf\u3001\u808c\u8089\uff09\u65f6\u7684\u8868\u73b0\u53d8\u5316\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u795e\u7ecf\u5c3a\u5bf8\u4e0e\u5206\u5272\u51c6\u786e\u7387\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u6df7\u5408\u4e0d\u540c\u673a\u5668\u6240\u5f97\u6570\u636e\u80fd\u63d0\u5347\u67d0\u4e9b\u914d\u7f6e\u4e0b\u4f4e\u6027\u80fd\u6e90\u7684\u5206\u5272\u8868\u73b0\uff0c\u4f46\u5728\u76ee\u6807\u57df\u4e0e\u8bad\u7ec3\u57df\u5b8c\u5168\u5339\u914d\u65f6\uff0c\u5355\u4e00\u6765\u6e90\u8bad\u7ec3\u6548\u679c\u66f4\u4f73\u3002\u591a\u5206\u7c7b\u5206\u5272\u4f1a\u5bfc\u81f4\u795e\u7ecf\u5206\u5272\u6027\u80fd\u6bd4\u4e8c\u5206\u7c7b\u4e0b\u964d9%-61%\uff0c\u4e3b\u8981\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8fb9\u754c\u4e0d\u6e05\u5f71\u54cd\u3002\u795e\u7ecf\u533a\u57df\u8d8a\u5927\uff0c\u5206\u5272\u51c6\u786e\u7387\u8d8a\u9ad8\uff08r=0.587\uff0cp<0.001\uff09\uff0c\u5c0f\u795e\u7ecf\u5206\u5272\u4f9d\u7136\u662f\u96be\u70b9\u3002", "conclusion": "\u9488\u5bf9\u4e34\u5e8a\u5b9e\u9645\u8d85\u58f0\u795e\u7ecf\u5206\u5272\u4efb\u52a1\uff0c\u5efa\u8bae\u5728\u6784\u5efa\u8bad\u7ec3\u96c6\u4e0e\u9009\u62e9\u6807\u6ce8\u7b56\u7565\u65f6\u614e\u91cd\u8003\u91cf\u6570\u636e\u6e90\u548c\u5206\u7c7b\u6570\uff0c\u7279\u522b\u5173\u6ce8\u5c0f\u795e\u7ecf\u533a\u57df\u7684\u8bad\u7ec3\u6539\u8fdb\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.01590", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01590", "abs": "https://arxiv.org/abs/2602.01590", "authors": ["Shaohan Wang", "Benfeng Xu", "Licheng Zhang", "Mingxuan Du", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles", "comment": "Preprint. Work in progress", "summary": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Wiki Live Challenge\uff08WLC\uff09\u57fa\u51c6\u548c\u5bf9\u5e94\u7684Wiki Eval\u8bc4\u4f30\u4f53\u7cfb\uff0c\u7528\u6700\u65b0\u7684\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6761\u76ee\uff08GA\uff09\u4e3a\u9ad8\u6807\u51c6\u3001\u7ec6\u7c92\u5ea6\u5730\u8bc4\u6d4bDRAs\uff08\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e0e\u62a5\u544a\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709DRAs\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709DRAs\u901a\u5e38\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u53c2\u8003\u7b54\u6848\u6216\u8bc4\u4ef7\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u867d\u7136\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u6743\u5a01\u6027\u3001\u5ba2\u89c2\u6027\u548c\u7ec6\u81f4\u5ea6\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u9ad8\u53ef\u9760\u6027\u4e14\u80fd\u7ec6\u81f4\u8bc4\u4f30\u667a\u80fd\u4f53\u8868\u73b0\u7684\u65b0\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u5229\u7528\u6700\u8fd1100\u7bc7\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6761\u76ee\uff0c\u8bbe\u8ba1\u4e86Wiki Live Challenge\u57fa\u51c6\u548cWiki Eval\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u5305\u62ec39\u4e2a\u5199\u4f5c\u8d28\u91cf\u8bc4\u4ef7\u7ef4\u5ea6\u548c\u4e25\u683c\u7684\u4e8b\u5b9e\u53ef\u9a8c\u8bc1\u6027\u6307\u6807\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u6bd4\u8f83\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\u3002", "result": "\u5bf9\u591a\u79cdDRA\u7cfb\u7edf\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u5199\u4f5c\u7684\u4e13\u5bb6\u7ea7\u7ef4\u57fa\u6761\u76ee\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cWiki Live Challenge\u80fd\u6709\u6548\u4f53\u73b0\u4e0d\u540cDRA\u7684\u4f18\u52a3\u3002", "conclusion": "WLC\u57fa\u51c6\u548cWiki Eval\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6743\u5a01\u3001\u66f4\u7ec6\u81f4\u548c\u5ba2\u89c2\u7684\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00795", "abs": "https://arxiv.org/abs/2602.00795", "authors": ["Wenhao Li", "Xianjing Meng", "Qiangchang Wang", "Zhongyi Han", "Zhibin Wu", "Yilong Yin"], "title": "DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning", "comment": "Accepted by ICLR 2026", "summary": "Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DVLA-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u8bed\u4e49\u6784\u5efa\u548c\u5f3a\u5316\u5b66\u4e60\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u4ece\u4f4e\u5c42\u5230\u9ad8\u5c42\u8bed\u4e49\u7684\u9010\u6b65\u548c\u81ea\u9002\u5e94\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u8868\u73b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u6700\u4f18\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e30\u5bcc\u89c6\u89c9\u8868\u5f81\uff0c\u4f46\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u5728\u4e0d\u540c\u8bed\u4e49\u5c42\u6b21\uff08\u4ece\u4f4e\u7ea7\u5230\u9ad8\u7ea7\uff09\u7684\u9010\u6b65\u548c\u52a8\u6001\u5bf9\u9f50\uff0c\u5bfc\u81f4\u8bed\u4e49\u589e\u76ca\u6709\u9650\u3002", "method": "\u63d0\u51faDVLA-RL\u65b9\u6cd5\uff0c\u5305\u62ec\u53cc\u5c42\u8bed\u4e49\u6784\u5efa\uff08DSC\uff09\uff0c\u5229\u7528\u7c7b\u522b\u540d\u548c\u652f\u6301\u6837\u672c\u5f15\u5bfcLLMs\u751f\u6210\u533a\u5206\u6027\u5c5e\u6027\u5e76\u5408\u6210\u5168\u9762\u7684\u7c7b\u522b\u63cf\u8ff0\uff1b\u4ee5\u53caRL\u95e8\u63a7\u6ce8\u610f\u529b\uff08RLA\uff09\uff0c\u5c06\u8de8\u6a21\u6001\u878d\u5408\u89c6\u4e3a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u501f\u52a9\u8f7b\u91cf\u7ea7\u7b56\u7565\u7f51\u7edc\u81ea\u9002\u5e94\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u548c\u8de8\u6ce8\u610f\u529b\u6bd4\u91cd\uff0c\u5b9e\u73b0\u5728\u4e0d\u540c\u7f51\u7edc\u5c42\u7075\u6d3b\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002", "result": "DVLA-RL\u65b9\u6cd5\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3001\u4e09\u4e2a\u4e0d\u540c\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9010\u5c42\u52a8\u6001\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u4f4e\u7ea7\u5c5e\u6027\u4e0e\u9ad8\u7ea7\u8bed\u4e49\uff0cDVLA-RL\u6781\u5927\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u7c7b\u522b\u533a\u5206\u4e0e\u6cdb\u5316\u8868\u73b0\u3002"}}
{"id": "2602.01598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01598", "abs": "https://arxiv.org/abs/2602.01598", "authors": ["Mingwen Zhang", "Minqiang Yang", "Changsheng Ma", "Yang Yu", "Hui Bai", "Chen Xu", "Xiangzhen Kong", "Bin Hu"], "title": "The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation", "comment": null, "summary": "Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \\textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \\textbf{when to ask} (via Strategy Anchoring) from \\textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \\textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSocratic Inquiry Framework (SIF)\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7406\u5b66\u5927\u6a21\u578b\u7684\u4e3b\u52a8\u63d0\u95ee\u80fd\u529b\uff0c\u4f7f\u5176\u4ece\u88ab\u52a8\u5e94\u7b54\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u5f15\u5bfc\u3002", "motivation": "\u5f53\u524d\u7684\u5fc3\u7406\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u591a\u6570\u53ea\u80fd\u88ab\u52a8\u56de\u5e94\u7528\u6237\uff0c\u5f80\u5f80\u505c\u7559\u5728\u8868\u9762\u5b89\u6170\uff0c\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u4fe1\u5ff5\u7684\u6df1\u5ea6\u63a2\u7d22\u548c\u884c\u4e3a\u5f15\u5bfc\uff0c\u65e0\u6cd5\u6709\u6548\u6a21\u62df\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\u4e2d\u7684\u4e3b\u52a8\u63d0\u95ee\u6838\u5fc3\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86Socratic Inquiry Framework\uff08SIF\uff09\u2014\u2014\u4e00\u4e2a\u8f7b\u91cf\u3001\u5373\u63d2\u5373\u7528\u7684\u610f\u56fe\u89c4\u5212\u5668\uff0c\u5c06\u201c\u4f55\u65f6\u53d1\u95ee\u201d\u548c\u201c\u53d1\u95ee\u5185\u5bb9\u201d\u89e3\u8026\uff0c\u5206\u522b\u901a\u8fc7\u7b56\u7565\u951a\u5b9a\u548c\u6a21\u677f\u68c0\u7d22\u5b9e\u73b0\uff0c\u65e0\u9700\u7aef\u5230\u7aef\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002\u540c\u65f6\u6784\u5efa\u4e86Socratic-QA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u76d1\u7763\u4e3b\u52a8\u63d0\u95ee\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165SIF\u540e\uff0c\u6a21\u578b\u7684\u4e3b\u52a8\u63d0\u95ee\u9891\u7387\u3001\u5bf9\u8bdd\u6df1\u5ea6\u53ca\u6cbb\u7597\u4e00\u81f4\u6027\u90fd\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u5730\u4ece\u88ab\u52a8\u5b89\u629a\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u63a2\u7d22\u3002", "conclusion": "SIF\u4e3a\u5fc3\u7406\u5b66\u5927\u6a21\u578b\u6811\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u4ece\u53cd\u5e94\u5f0f\u56de\u5e94\u5411\u4e3b\u52a8\u8ba4\u77e5\u5f15\u5bfc\u7684\u8f6c\u53d8\uff0c\u4e3a\u540e\u7eed\u5fc3\u7406\u5065\u5eb7AI\u5de5\u5177\u7684\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01618", "abs": "https://arxiv.org/abs/2602.01618", "authors": ["Panuthep Tasawong", "Jian Gang Ngui", "Alham Fikri Aji", "Trevor Cohn", "Peerat Limkonchotiwat"], "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia", "comment": "Under reivew", "summary": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u667a\u80fd\u4f53\u9a71\u52a8\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6784\u5efa\u4e1c\u5357\u4e9a\uff08SEA\uff09\u672c\u5730\u548c\u591a\u8bed\u79cd\u7684AI\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5e76\u636e\u6b64\u5f00\u53d1\u4e86SEA-Guard\u7cfb\u5217\u6a21\u578b\uff0c\u5728\u672c\u5730\u6587\u5316\u73af\u5883\u4e0b\u63d0\u5347\u4e86AI\u5b89\u5168\u9632\u62a4\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u6a21\u578b\u591a\u57fa\u4e8e\u82f1\u6587\u6570\u636e\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u83b7\u5f97\u5176\u4ed6\u8bed\u8a00\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u5730\u533a\u6587\u5316\u4e0e\u89c4\u8303\u7684\u51c6\u786e\u628a\u63e1\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u7075\u654f\u8bc6\u522b\u672c\u5730\u6709\u5bb3\u5185\u5bb9\u3002\u56e0\u6b64\u4e9f\u9700\u65b9\u6cd5\u4ee5\u4f4e\u6210\u672c\u9ad8\u6548\u7387\u5730\u751f\u4ea7\u8d34\u5408\u672c\u5730\u6587\u5316\u7684\u5b89\u5168\u6570\u636e\u548c\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u751f\u6210\u6db5\u76d6\u4e1c\u5357\u4e9a\u5404\u56fd\u3001\u5404\u6587\u5316\u654f\u611f\u70b9\u7684\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u591a\u8bed\u79cd\u7684SEA-Guard\u5b89\u5168\u6a21\u578b\u3002\u8bc4\u6d4b\u91c7\u7528\u4e86\u591a\u57fa\u51c6\u548c\u591a\u79cd\u6587\u5316\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSEA-Guard\u7cfb\u5217\u6a21\u578b\u5728\u68c0\u6d4b\u4e1c\u5357\u4e9a\u672c\u5730\u6709\u5bb3/\u654f\u611f\u5185\u5bb9\u65b9\u9762\u8f83\u73b0\u6709\u65b9\u6cd5\u660e\u663e\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u901a\u7528\u5b89\u5168\u573a\u666f\u4e0b\u8868\u73b0\u4f9d\u7136\u7a33\u5b9a\u3002", "conclusion": "SEA-Guard\u4e3a\u4e1c\u5357\u4e9a\u53ca\u591a\u6587\u5316\u73af\u5883\u4e0bAI\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u5177\u672c\u5730\u9002\u5e94\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\uff0c\u8fd8\u8bc1\u660e\u4e86\u667a\u80fd\u4f53\u751f\u6210\u6570\u636e\u7684\u53ef\u884c\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002"}}
{"id": "2602.01640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01640", "abs": "https://arxiv.org/abs/2602.01640", "authors": ["Shuai Zhang", "Jiayu Hu", "Zijie Chen", "Zeyuan Ding", "Yi Zhang", "Yingji Zhang", "Ziyi Zhou", "Junwei Liao", "Shengjie Zhou", "Yong Dai", "Zhenzhong Lan", "Xiaozhu Ju"], "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain", "comment": null, "summary": "Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u5177\u8eab\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc4\u6d4b\u6846\u67b6A2Eval\uff0c\u901a\u8fc7\u4e24\u79cd\u667a\u80fd\u4ee3\u7406\u534f\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u52a8\u3001\u4f4e\u6210\u672c\u7684\u8bc4\u6d4b\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u6d4b\u6548\u7387\u5e76\u4fee\u6b63\u4e86\u6a21\u578b\u6392\u540d\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u7684\u5177\u8eabVLM\u8bc4\u6d4b\u4f9d\u8d56\u4eba\u5de5\u6784\u5efa\u7684\u9759\u6001\u57fa\u51c6\u96c6\uff0c\u4e0d\u4ec5\u91cd\u590d\u6027\u9ad8\u3001\u8986\u76d6\u9762\u5931\u8861\uff0c\u8fd8\u6781\u5ea6\u4f9d\u8d56\u8ba1\u7b97\u4e0e\u4eba\u5de5\u6807\u6ce8\u8d44\u6e90\uff0c\u5bfc\u81f4\u6210\u672c\u4e0a\u5347\u548c\u6a21\u578b\u6392\u540d\u5931\u771f\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u6a21\u578b\u7684\u6301\u7eed\u8fed\u4ee3\u548c\u53d1\u5c55\u3002", "method": "A2Eval\u6846\u67b6\u5305\u542b\u6570\u636e\u4ee3\u7406\u4e0e\u8bc4\u6d4b\u4ee3\u7406\uff1a\u6570\u636e\u4ee3\u7406\u81ea\u52a8\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u7ef4\u5ea6\u5e76\u751f\u6210\u5747\u8861\u7684\u7cbe\u7b80\u8bc4\u6d4b\u96c6\uff1b\u8bc4\u6d4b\u4ee3\u7406\u81ea\u52a8\u6784\u5efa\u548c\u6821\u9a8c\u8bc4\u6d4b\u6d41\u7a0b\uff0c\u5b9e\u73b0\u5168\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u9ad8\u4fdd\u771f\u8bc4\u6d4b\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u548c13\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cA2Eval\u80fd\u5c06\u8bc4\u6d4b\u96c6\u538b\u7f2985%\u3001\u603b\u4f53\u8ba1\u7b97\u6210\u672c\u964d\u4f4e77%\u3001\u8bc4\u6d4b\u901f\u5ea6\u63d0\u53474.6\u500d\uff0c\u4e14\u4fdd\u969c\u8bc4\u6d4b\u8d28\u91cf\u4e0d\u53d8\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u6709\u6548\u4fee\u6b63\u6392\u540d\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u4e0e\u4eba\u5de5\u8bc4\u6d4b\u7684\u4e00\u81f4\u6027\uff08Spearman\u2019s rho=0.85\u3001Kendall\u2019s tau=0.81\uff09\u3002", "conclusion": "A2Eval\u4e3a\u5177\u8eabVLM\u8bc4\u6d4b\u6811\u7acb\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u516c\u6b63\u3001\u66f4\u4f4e\u6210\u672c\u7684\u65b0\u6807\u51c6\uff0c\u6709\u671b\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002\u4ee3\u7801\u548c\u6570\u636e\u4e5f\u5c06\u5f00\u6e90\uff0c\u4fc3\u8fdb\u793e\u533a\u5e94\u7528\u3002"}}
{"id": "2602.00813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00813", "abs": "https://arxiv.org/abs/2602.00813", "authors": ["Tong Wang", "Yunhan Zhao", "Shu Kong"], "title": "Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u590d\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u751f\u6210\u5e76\u5229\u7528\u201c\u5fc3\u7406\u56fe\u50cf\u201d\u8fdb\u884c\u68c0\u7d22\uff0c\u6781\u5927\u63d0\u5347\u4e86\u96f6\u6837\u672cCIR\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dCIR\u4efb\u52a1\u7684\u6700\u5927\u6311\u6218\u5728\u4e8e\u201c\u5fc3\u7406\u56fe\u50cf\u201d\u53ea\u5728\u8bed\u4e49\u5c42\u9762\u5b58\u5728\uff0c\u7f3a\u4e4f\u5b9e\u4f53\u53c2\u8003\uff0c\u5bfc\u81f4\u68c0\u7d22\u51c6\u786e\u6027\u53d7\u9650\u3002\u4f20\u7edf\u65b9\u6cd5\u591a\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u800c\u975e\u76f4\u63a5\u751f\u6210\u8be5\u5fc3\u7406\u56fe\u50cf\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u672c\u6587\u63d0\u51faParacosm\u65b9\u6cd5\uff1a\u9996\u5148\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u548c\u4fee\u6539\u6587\u672c\u76f4\u63a5\u751f\u6210\u201c\u5fc3\u7406\u56fe\u50cf\u201d\uff1b\u5176\u6b21\uff0c\u4e3a\u6570\u636e\u5e93\u4e2d\u6bcf\u5f20\u771f\u5b9e\u56fe\u50cf\u751f\u6210\u5bf9\u5e94\u5408\u6210\u56fe\u50cf\uff0c\u5c06\u591a\u6a21\u6001\u68c0\u7d22\u6620\u5c04\u5230\u5408\u6210\u56fe\u50cf\u7a7a\u95f4\uff1b\u6700\u540e\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u7684\u76f8\u4f3c\u6027\u5b9e\u73b0\u76ee\u6807\u56fe\u50cf\u7684\u5339\u914d\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff08\u96f6\u6837\u672c\uff09\u3002", "result": "Paracosm\u5728\u56db\u4e2a\u4e3b\u6d41\u590d\u5408\u56fe\u50cf\u68c0\u7d22\u57fa\u51c6\u4e0a\u5747\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u76f4\u63a5\u751f\u6210\u5e76\u5229\u7528\u201c\u5fc3\u7406\u56fe\u50cf\u201d\u80fd\u5927\u5e45\u63d0\u5347CIR\u7684\u96f6\u6837\u672c\u68c0\u7d22\u51c6\u786e\u7387\u3002\u6240\u63d0\u65b9\u6cd5\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u548c\u663e\u8457\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01654", "abs": "https://arxiv.org/abs/2602.01654", "authors": ["Jiaqian Li", "Yanshu Li", "Kuan-Hao Huang"], "title": "Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models", "comment": null, "summary": "Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSteering Vector Fields (SVF)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0d\u540c\u6fc0\u6d3b\u72b6\u6001\u4e0b\u7684\u5f15\u5bfc\u65b9\u5411\uff0c\u5b9e\u73b0\u66f4\u5f3a\u5927\u4e14\u53ef\u9760\u7684\u6a21\u578b\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709Steering vectors\uff08SV\uff09\u5728\u590d\u6742\u4efb\u52a1\u548c\u957f\u6587\u672c\u751f\u6210\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684Steering vectors\uff08SV\uff09\u65b9\u6cd5\uff0c\u867d\u7136\u63a8\u7406\u65f6\u9ad8\u6548\u3001\u65e0\u9700\u5fae\u8c03\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u9759\u6001\u5411\u91cf\uff0c\u5bb9\u6613\u5728\u4e0d\u540c\u8bed\u5883\u4e0b\u5931\u6548\uff0c\u5bfc\u81f4\u63a7\u5236\u6548\u679c\u4e0d\u4f73\u6216\u53cd\u5411\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u957f\u6587\u672c\u751f\u6210\u548c\u591a\u5c5e\u6027\u63a7\u5236\u573a\u666f\u4e0b\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u66f4\u7cbe\u7ec6\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5f15\u5bfc\u65b9\u5f0f\uff0c\u4ece\u800c\u63d0\u5347LLM\u7684\u53ef\u63a7\u6027\u4e0e\u5b9e\u8df5\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5c06\u5f15\u5bfc\u95ee\u9898\u4ece\u51e0\u4f55\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\uff0c\u63d0\u51faSteering Vector Fields\uff08SVF\uff09\u6846\u67b6\u3002SVF\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u6982\u5ff5\u8bc4\u5206\u51fd\u6570\uff0c\u5e76\u5229\u7528\u5f53\u524d\u6fc0\u6d3b\u70b9\u5904\u7684\u68af\u5ea6\u4f5c\u4e3a\u5f15\u5bfc\u65b9\u5411\uff0c\u5b9e\u73b0\u5c42\u95f4\u534f\u540c\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u5e72\u9884\u3002\u8be5\u65b9\u6cd5\u7edf\u4e00\u652f\u6301\u591a\u5c42\u3001\u591a\u5c5e\u6027\u548c\u957f\u6587\u672c\u7684\u5f15\u5bfc\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u591a\u9879\u5f15\u5bfc\u4efb\u52a1\u4e0a\uff0cSVF\u76f8\u6bd4\u4f20\u7edfSV\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u5f3a\u3001\u66f4\u53ef\u9760\u7684\u63a7\u5236\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u957f\u6587\u672c\u751f\u6210\u548c\u591a\u5c5e\u6027\u63a7\u5236\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SVF\u6781\u5927\u63d0\u5347\u4e86\u63a8\u7406\u65f6\u63a7\u5236LLM\u7684\u53ef\u5b9e\u7528\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6a21\u578b\u5f15\u5bfc\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00821", "abs": "https://arxiv.org/abs/2602.00821", "authors": ["Konstantinos Moutselos", "Ilias Maglogiannis"], "title": "Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis", "comment": "8 pages, 5 figures", "summary": "The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a \"Segment-by-Synthesis\" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u76ae\u80a4\u75c5\u5b66\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u4e0d\u4f9d\u8d56\u4f20\u7edf\u7684\u53bb\u6807\u8bc6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u751f\u6210\u5f0f\u6a21\u578b\u5728\u672c\u5730\u5b9e\u73b0\u9ad8\u4fdd\u771f\u75c5\u7406\u7279\u5f81\u4fdd\u7559\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u76ae\u80a4\u75c5\u5b66\u4e34\u5e8a\u5e94\u7528\u53d7\u9650\u4e8e\u60a3\u8005\u9690\u79c1\u4fdd\u62a4\u548c\u75c5\u7406\u7279\u5f81\u4fdd\u771f\u7684\u5bf9\u7acb\u9700\u6c42\u3002\u4f20\u7edf\u53bb\u6807\u8bc6\u65b9\u6cd5\u635f\u5bb3\u8bca\u65ad\u4fe1\u606f\uff0c\u800c\u4e3b\u6d41\u751f\u6210\u65b9\u6cd5\u53c8\u9700\u8981\u8fc7\u9ad8\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eRectified Flow Transformers\uff08FlowEdit\uff09\u7684\u65e0\u53cd\u6f14\u9ad8\u6548\u8eab\u4efd\u8f6c\u6362\u673a\u5236\uff0c\u80fd\u5728\u4e34\u5e8a\u8282\u70b9\u672c\u5730\u5b9e\u73b020\u79d2\u5185\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u5904\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u201cSegment-by-Synthesis\u201d\u673a\u5236\u672c\u5730\u751f\u6210\u5bf9\u7167\u5065\u5eb7\u4e0e\u60a3\u75c5\u56fe\u50cf\u5bf9\uff0c\u4ece\u800c\u63d0\u53d6\u4e0d\u542b\u751f\u7269\u8bc6\u522b\u7279\u5f81\u7684\u7ea2\u6591\u5dee\u5f02\u63a9\u7801\u3002", "result": "\u5728\u9ad8\u5206\u8fa8\u7387\u4e34\u5e8a\u56fe\u50cf\u4e0a\u521d\u6b65\u9a8c\u8bc1\u4e2d\uff0c\u8de8\u5408\u6210\u8eab\u4efdIoU\u7a33\u5b9a\u6027\u8d85\u8fc70.67\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u75c5\u7406\u7279\u5f81\u548c\u53bb\u6807\u8bc6\u95f4\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u5728\u8fb9\u7f18\u751f\u6210\u5408\u89c4\u7684\u9690\u79c1\u4ee3\u7406\u6570\u636e\uff0c\u964d\u4f4e\u6e90\u7aef\u68af\u5ea6\u6cc4\u9732\u98ce\u9669\uff0c\u4e3a\u76ae\u80a4\u56fe\u50cf\u9ad8\u7cbe\u5ea6\u8054\u90a6\u5206\u6790\u5728\u9690\u79c1\u654f\u611f\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01660", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01660", "abs": "https://arxiv.org/abs/2602.01660", "authors": ["Zhongyuan Peng", "Caijun Xu", "Changyi Xiao", "Shibo Hong", "Eli Zhang", "Stephen Huang", "Yixin Cao"], "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation", "comment": "11 pages, 5 tables, 5 figures", "summary": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6CoDiQ\uff0c\u80fd\u7cbe\u51c6\u63a7\u5236\u81ea\u52a8\u751f\u6210\u95ee\u9898\u7684\u96be\u5ea6\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u8bad\u7ec3\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u96be\u9898\u6570\u636e\u96c6\u3002", "motivation": "\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u9ad8\u96be\u5ea6\u3001\u7ade\u8d5b\u7ea7\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002\u73b0\u6709\u81ea\u52a8\u51fa\u9898\u65b9\u6cd5\u96be\u4ee5\u7cbe\u51c6\u63a7\u5236\u96be\u5ea6\u3001\u5f00\u9500\u5927\u3001\u96be\u4ee5\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u96be\u9898\uff0c\u9650\u5236\u4e86LRM\u80fd\u529b\u63d0\u5347\u3002", "method": "\u63d0\u51faCoDiQ\u6846\u67b6\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u63a8\u7406token\u914d\u989d\u63a7\u5236\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u4fdd\u8bc1\u9898\u76ee\u53ef\u89e3\u3002\u5bf9Qwen3-8B\u6a21\u578b\u8fdb\u884c\u6539\u9020\u751f\u6210\u66f4\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u57fa\u4e8e\u6b64\u751f\u6210\u4e864.4\u4e07\u6761\u7ade\u8d5b\u7ea7\u9ad8\u8d28\u91cf\u95ee\u9898\u7684\u6570\u636e\u96c6\uff08CoDiQ-Corpus\uff09\u3002\u8fd9\u4e9b\u9898\u76ee\u7ecf\u8fc7\u4eba\u5de5\u8bc4\u4f30\u96be\u5ea6\u9ad8\u4e14\u53ef\u89e3\u3002", "result": "\u4eba\u5de5\u8bc4\u4f30\u663e\u793aCoDiQ-Corpus\u4e2d\u7684\u95ee\u9898\u96be\u5ea6\u9ad8\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u540c\u65f6\u4ecd\u670982%\u4ee5\u4e0a\u7684\u53ef\u89e3\u6027\u3002\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3LRM\u663e\u8457\u63d0\u5347\u4e86\u5176\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u7cbe\u7ec6\u63a7\u5236\u96be\u5ea6\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u4ef7\u503c\u3002", "conclusion": "CoDiQ\u80fd\u591f\u9ad8\u6548\u3001\u7cbe\u7ec6\u5730\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u96be\u5ea6\u7684\u8bad\u7ec3\u9898\uff0c\u6709\u6548\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4e0e\u5de5\u5177\uff0c\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00839", "abs": "https://arxiv.org/abs/2602.00839", "authors": ["Mingwei Li", "Hehe Fan", "Yi Yang"], "title": "TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation", "comment": "Project Page: https://longxiang-ai.github.io/TransNormal", "summary": "Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25\u00b0 accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransNormal\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u900f\u660e\u7269\u4f53\u5355\u76ee\u6cd5\u7ebf\u4f30\u8ba1\u7684\u7cbe\u786e\u5ea6\uff0c\u5e76\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u9700\u8981\u5bf9\u900f\u660e\u7269\u4f53\u7684\u7cbe\u786e\u51e0\u4f55\u7406\u89e3\uff0c\u4f46\u900f\u660e\u7269\u4f53\u56e0\u6298\u5c04\u4e0e\u53cd\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u4f20\u7edf\u6df1\u5ea6\u4e0e\u6cd5\u7ebf\u4f20\u611f\u5668\u5728\u5b9e\u9645\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e25\u91cd\u59a8\u788d\u4e86AI\u7cfb\u7edf\u7684\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u900f\u660e\u7269\u4f53\u6cd5\u7ebf\u4f30\u8ba1\u5de5\u5177\u5341\u5206\u5173\u952e\u3002", "method": "\u63d0\u51faTransNormal\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff08diffusion prior\uff09\u7528\u4e8e\u5355\u6b65\u6cd5\u7ebf\u56de\u5f52\uff0c\u901a\u8fc7cross-attention\u673a\u5236\u6574\u5408DINOv3\u7279\u5f81\u83b7\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd8\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u76ee\u6807\u53ca\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6b63\u5219\uff0c\u63d0\u5347\u7ec6\u8282\u7ed3\u6784\u7684\u4fdd\u7559\u3002\u540c\u65f6\u5efa\u7acb\u4e86\u9762\u5411\u900f\u660e\u5b9e\u9a8c\u5668\u76bf\u7684\u7269\u7406\u6a21\u62df\u6570\u636e\u96c6TransNormal-Synthetic\u3002", "result": "TransNormal\u5728ClearGrasp\u57fa\u51c6\u4e0a\u63d0\u5347\u663e\u8457\uff0c\u5e73\u5747\u8bef\u5dee\u964d\u4f4e24.4%\uff0c11.25\u00b0\u51c6\u786e\u7387\u63d0\u534722.8%\uff1b\u5728ClearPose\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8bef\u5dee\u964d\u4f4e15.2%\u3002", "conclusion": "TransNormal\u6709\u6548\u89e3\u51b3\u4e86\u900f\u660e\u7269\u4f53\u5355\u76ee\u6cd5\u7ebf\u4f30\u8ba1\u7684\u96be\u9898\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u5927\u5e45\u63d0\u5347\u4e86\u73b0\u6709\u6280\u672f\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u7b49\u5e94\u7528\u573a\u666f\u4e2d\u7684AI\u843d\u5730\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2602.01672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01672", "abs": "https://arxiv.org/abs/2602.01672", "authors": ["Siheng Xiong", "Oguzhan Gungordu", "Blair Johnson", "James C. Kerce", "Faramarz Fekri"], "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control", "comment": "Work in progress", "summary": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepControl\u7684\u4fe1\u606f\u83b7\u53d6\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6a21\u578b\u867d\u80fd\u5916\u90e8\u68c0\u7d22\u4fe1\u606f\uff0c\u4f46\u68c0\u7d22\u8fc7\u7a0b\u5e38\u5e26\u6765\u5197\u4f59\u3001\u4e0a\u4e0b\u6587\u9971\u548c\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u4ec5\u4f9d\u8d56\u6700\u7ec8\u7ed3\u679c\uff0c\u96be\u4ee5\u7cbe\u7ec6\u8c03\u63a7\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u201c\u4fe1\u606f\u6548\u7528\u201d\u7406\u8bba\u7684DeepControl\u6846\u67b6\uff0c\u901a\u8fc7\u8861\u91cf\u68c0\u7d22\u8bc1\u636e\u7684\u8fb9\u9645\u4ef7\u503c\uff0c\u8bbe\u8ba1\u4e86\u68c0\u7d22\u5ef6\u7eed\u548c\u7c92\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u81ea\u9002\u5e94\u8c03\u8282\u68c0\u7d22\u65f6\u673a\u548c\u4fe1\u606f\u6269\u5c55\u91cf\u3002\u6b64\u5916\u91c7\u7528\u9000\u706b\u7b56\u7565\uff0c\u5e2e\u52a9\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u4e2d\u5b66\u4f1a\u9ad8\u6548\u7684\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cDeepControl\u5728Qwen2.5-7B\u548cQwen2.5-3B\u6a21\u578b\u4e0a\u5206\u522b\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u53479.4%\u548c8.6%\uff0c\u5e76\u5728\u591a\u79cd\u68c0\u7d22/\u975e\u68c0\u7d22\u63a8\u7406\u65b9\u6cd5\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u5bf9\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6a21\u578b\u7684\u590d\u6742\u73af\u5883\u9002\u5e94\u6027\u81f3\u5173\u91cd\u8981\uff0cDeepControl\u663e\u8457\u4f18\u5316\u4e86\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\u548c\u6a21\u578b\u6700\u7ec8\u8868\u73b0\u3002"}}
{"id": "2602.00841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00841", "abs": "https://arxiv.org/abs/2602.00841", "authors": ["Jintao Cheng", "Weibin Li", "Zhijian He", "Jin Wu", "Chi Man Vong", "Wei Zhang"], "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition", "comment": "14pages, 5 figures", "summary": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684VPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u9636\u51e0\u4f55\u7edf\u8ba1\u5efa\u6a21\u573a\u666f\u7ed3\u6784\uff0c\u5229\u7528SPD\u6d41\u5f62\u4e0a\u7684\u534f\u65b9\u5dee\u63cf\u8ff0\u548c\u9ece\u66fc\u6620\u5c04\u5b9e\u73b0\u9c81\u68d2\u8868\u5f81\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\u5bf9\u73af\u5883\u548c\u89c6\u89d2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u4e14\u5927\u591a\u4f9d\u8d56\u5927\u89c4\u6a21\u6709\u76d1\u7763\u6570\u636e\u6216\u7b80\u5355\u7684\u4f4e\u9636\u7edf\u8ba1\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u590d\u6742\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7528\u5bf9\u79f0\u6b63\u5b9a(SPD)\u6d41\u5f62\u4e0a\u7684\u534f\u65b9\u5dee\u63cf\u8ff0\u7b26\u5efa\u6a21\u573a\u666f\u7ed3\u6784\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u9ece\u66fc\u6620\u5c04\u5c06\u5176\u8f6c\u5230\u6b27\u5f0f\u7a7a\u95f4\u4ee5\u6d88\u9664\u566a\u58f0\u5f71\u54cd\uff0c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u57fa\u4e8e\u56fa\u5b9a\u9884\u8bad\u7ec3\u7279\u5f81\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4e0eSOTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u517c\u5bb9\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u589e\u5f3aVPR\u5bf9\u73af\u5883\u548c\u89c6\u89d2\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u5728\u65e0\u9700\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5373\u53ef\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u9002\u5408\u4f4e\u8d44\u6e90\u6216\u6cdb\u5316\u573a\u666f\u5e94\u7528\u3002"}}
{"id": "2602.01687", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01687", "abs": "https://arxiv.org/abs/2602.01687", "authors": ["Jung H. Lee", "Sujith Vijayan"], "title": "Counting Hypothesis: Potential Mechanism of In-Context Learning", "comment": "19 pages, 7 main Figures, 1 Table and 6 Supp. Figures", "summary": "In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u5176\u7f16\u7801\u7b56\u7565\u53ef\u80fd\u662f\u5b9e\u73b0ICL\u7684\u6838\u5fc3\u673a\u5236\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u5141\u8bb8\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u9700\u4fee\u6539\u5185\u7ed3\u6784\u3001\u4ec5\u51ed\u5c11\u91cf\u793a\u4f8b\u5b8c\u6210\u591a\u79cd\u4efb\u52a1\uff1b\u4f46\u5176\u673a\u7406\u5c1a\u4e0d\u6e05\u695a\uff0c\u96be\u4ee5\u7ea0\u9519\u548c\u8bca\u65ad\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u7406\u89e3\u5176\u672c\u8d28\u548c\u5c40\u9650\u3002", "method": "\u53d7ICL\u6027\u8d28\u548cLLMs\u529f\u80fd\u6a21\u5757\u542f\u53d1\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u201c\u8ba1\u6570\u5047\u8bbe\u201d\uff0c\u5373LLMs\u53ef\u80fd\u901a\u8fc7\u7279\u5b9a\u7684\u7f16\u7801\u7b56\u7565\u5b9e\u73b0ICL\uff0c\u5e76\u4e3a\u6b64\u63d0\u4f9b\u4e86\u8f85\u52a9\u6027\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u5bf9ICL\u673a\u5236\u8fdb\u884c\u4e86\u63a2\u7d22\uff0c\u627e\u5230\u4e86\u652f\u6301\u201c\u8ba1\u6570\u5047\u8bbe\u201d\u7684\u4e8b\u5b9e\u4e0e\u8bc1\u636e\u3002", "conclusion": "LLMs\u5bf9ICL\u7684\u652f\u6301\u6838\u5fc3\u5728\u4e8e\u5176\u7f16\u7801\u7b56\u7565\uff0c\u672c\u6587\u7684\u7814\u7a76\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3ICL\u7684\u5c40\u9650\u6027\u548c\u539f\u7406\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u4e0e\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.00865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00865", "abs": "https://arxiv.org/abs/2602.00865", "authors": ["Brandon Leblanc", "Charalambos Poullis"], "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware", "comment": "Submitted to the Canadian Conference on Robotics and Vision (CRV). 10 pages, 5 figures", "summary": "While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Distill3R\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u6280\u672f\u5c06\u5927\u89c4\u6a21\u4e09\u7ef4\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u53ef\u5728\u5355\u53f0\u5de5\u4f5c\u7ad9\u8bad\u7ec3\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u4e09\u7ef4\u91cd\u5efa\u9886\u57df\u7684\u8ba1\u7b97\u95e8\u69db\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5f53\u524d\u591a\u89c6\u89d2\u4e09\u7ef4\u91cd\u5efa\u9886\u57df\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u65f6\u5bf9\u5927\u89c4\u6a21\u8ba1\u7b97\u96c6\u7fa4\u7684\u9ad8\u5ea6\u4f9d\u8d56\uff0c\u964d\u4f4e\u7814\u7a76\u8005\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4ece\u800c\u8ba9\u66f4\u591a\u5b9e\u9a8c\u5ba4\u80fd\u591f\u53c2\u4e0e\u4e09\u7ef4\u89c6\u89c9\u7814\u7a76\u3002", "method": "\u4e3b\u8981\u521b\u65b0\u5305\u62ec\uff1a\uff081\uff09\u79bb\u7ebf\u7f13\u5b58\u7ba1\u9053\uff0c\u5c06\u6559\u5e08\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5b66\u751f\u6a21\u578b\u7684\u8bad\u7ec3\u89e3\u8026\uff0c\u5e76\u7528\u538b\u7f29\u540e\u7684\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u9ad8\u6548\u84b8\u998f\uff1b\uff082\uff09\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u84b8\u998f\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u5b66\u751f\u6a21\u578b\u5728\u5e38\u89c4\u786c\u4ef6\u4e0a\u8bad\u7ec3\u3002\u63d0\u51fa\u7684\u5b66\u751f\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3a\u6559\u5e08\u76841/9\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u53475\u500d\u3002", "result": "\u5b66\u751f\u6a21\u578b\uff0872M\u53c2\u6570\uff09\u4fdd\u6301\u4e86\u5fc5\u8981\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u4e09\u7ef4\u51e0\u4f55\u7406\u89e3\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u53ef\u5728\u5355\u53f0\u5de5\u4f5c\u7ad93\u5929\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u8fdc\u4f4e\u4e8e\u6559\u5e08\u6a21\u578b\u6240\u9700\u7684\u5de8\u91cfGPU\u548c\u4e00\u5468\u65f6\u95f4\u3002", "conclusion": "Distill3R\u4e3a\u4e09\u7ef4\u89c6\u89c9\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u4f4e\u6210\u672c\u7684\u8bad\u7ec3\u914d\u65b9\uff0c\u6709\u52a9\u4e8e\u66f4\u591a\u7814\u7a76\u56e2\u961f\u5f00\u5c55\u81ea\u4e3b\u4e09\u7ef4\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\uff0c\u4e3a\u9ad8\u6548\u8fb9\u7f18\u90e8\u7f72\u548c\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d77\u70b9\u3002"}}
{"id": "2602.01698", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01698", "abs": "https://arxiv.org/abs/2602.01698", "authors": ["Wenhui Tan", "Fiorenzo Parascandolo", "Enver Sangineto", "Jianzhong Ju", "Zhenbo Luo", "Qian Cao", "Rita Cucchiara", "Ruihua Song", "Jian Luan"], "title": "Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u591a\u6837\u6027\u91c7\u6837\u65f6\u5b58\u5728\u201c\u63a2\u7d22\u584c\u7f29\u201d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u89e3\u7801\u7b56\u7565Latent Exploration Decoding\uff08LED\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u63a8\u7406\u51c6\u786e\u7387\u3002", "motivation": "\u8fd1\u5e74\u6765\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u4e0e\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u7136\u800c\uff0c\u671f\u95f4\u91c7\u53d6\u7684\u540e\u8bad\u7ec3\u65b9\u5f0f\u5bfc\u81f4\u6e29\u5ea6\u91c7\u6837\u5728\u589e\u52a0\u901a\u8fc7\u7387\uff08pass@$n$\uff09\u65b9\u9762\u5931\u6548\uff0c\u5373\u4f7f\u589e\u52a0\u91c7\u6837\u591a\u6837\u6027\u4e5f\u6ca1\u6709\u63d0\u5347\u6548\u679c\u3002\u4f5c\u8005\u5206\u6790\u5230\u540e\u9a8c\u6982\u7387\u5206\u5e03\u7684\u71b5\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0d\u540c\u5c42\u95f4\u7684\u71b5\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faLatent Exploration Decoding\uff08LED\uff09\u65b9\u6cd5\uff0c\u5373\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u9ad8\u71b5\u6f5c\u5728\u8868\u793a\uff0c\u5728\u89e3\u7801\u65f6\u901a\u8fc7\u7d2f\u8ba1\u6c42\u548c\u805a\u5408\u4e0d\u540c\u5c42\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u4e14\u81ea\u52a8\u9009\u62e9\u71b5\u503c\u6700\u5927\u7684\u5c42\u4f5c\u4e3a\u91c7\u6837\u5019\u9009\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u589e\u52a0\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cLED\u65b9\u6cd5\u65e0\u9700\u518d\u8bad\u7ec3\u6216\u5f15\u5165\u65b0\u53c2\u6570\u5373\u53ef\u5c06pass@1\u4e0epass@16\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e860.61\u548c1.03\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LED\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u7f51\u7edc\u4e2d\u95f4\u5c42\u7684\u9ad8\u71b5\u8868\u8fbe\uff0c\u6709\u6548\u7f13\u89e3\u4e86RL\u540e\u8bad\u7ec3\u5bfc\u81f4\u7684\u63a2\u7d22\u584c\u7f29\u95ee\u9898\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u591a\u6837\u6027\u4e0e\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00883", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00883", "abs": "https://arxiv.org/abs/2602.00883", "authors": ["Alicja Polowczyk", "Agnieszka Polowczyk", "Piotr Borycki", "Joanna Waczy\u0144ska", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models", "comment": null, "summary": "Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u96f6\u8bad\u7ec3\u3001\u65e0\u9700\u4fee\u6539\u6743\u91cd\u5373\u53ef\u5728\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7ea0\u6b63\u548c\u51cf\u5c11\u56fe\u50cf\u751f\u6210\u4f2a\u5f71\u7684\u65b9\u6cd5DIAMOND\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u8fd1\u5e74\u6765\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\uff0c\u4f46\u5728\u5b9e\u9645\u548c\u4e13\u4e1a\u5e94\u7528\u4e0a\uff0c\u751f\u6210\u56fe\u50cf\u4e2d\u89c6\u89c9\u53ca\u89e3\u5256\u4f2a\u5f71\u4ecd\u672a\u6709\u6548\u89e3\u51b3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e3a\u751f\u6210\u540e\u7684\u4fee\u590d\uff0c\u4e0d\u4ec5\u6548\u7387\u4f4e\uff0c\u8fd8\u9700\u5bf9\u6a21\u578b\u6743\u91cd\u505a\u4fb5\u5165\u6027\u6539\u52a8\u6216\u6d88\u8017\u5927\u91cf\u7b97\u529b\u8fdb\u884c\u533a\u57df\u7ec6\u5316\u5904\u7406\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faDIAMOND\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u65e0\u9700\u66f4\u6539\u6a21\u578b\u6743\u91cd\uff0c\u800c\u662f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u4f30\u7b97\u6bcf\u4e00\u6b65\u7684\u201c\u5e72\u51c0\u201d\u6837\u672c\uff0c\u5bf9\u751f\u6210\u8f68\u8ff9\u8fdb\u884c\u7ea0\u6b63\uff0c\u5b9e\u65f6\u907f\u514d\u4f2a\u5f71\u4ea7\u751f\u3002\u6b64\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4e0d\u589e\u52a0\u989d\u5916\u8bad\u7ec3\u6216\u8c03\u6574\uff0c\u5c5e\u4e8e\u96f6\u6837\u672c\uff08zero-shot\uff09\u63a8\u7406\u4fee\u6b63\u3002", "result": "DIAMOND\u65b9\u6cd5\u5728\u591a\u4e2a\u6269\u6563\u6a21\u578b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u60c5\u51b5\u4e0b\u7684\u9ad8\u4fdd\u771f\u3001\u65e0\u4f2a\u5f71\u56fe\u50cf\u751f\u6210\u6548\u679c\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "DIAMOND\u4e3a\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5e26\u6765\u4e86\u8bad\u7ec3\u65e0\u5173\u3001\u63a8\u7406\u65f6\u8f68\u8ff9\u7ea0\u6b63\u7684\u9ad8\u6548\u65b9\u6848\uff0c\u65e0\u9700\u4fee\u6539\u6743\u91cd\u5373\u53ef\u5927\u5e45\u51cf\u5c11\u4f2a\u5f71\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u7684\u5b9e\u9645\u5e94\u7528\u548c\u4e13\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u6709\u524d\u666f\u7684\u89e3\u51b3\u8def\u5f84\u3002"}}
{"id": "2602.01708", "categories": ["cs.CL", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.01708", "abs": "https://arxiv.org/abs/2602.01708", "authors": ["Langyuan Cui", "Chun Kai Ling", "Hwee Tou Ng"], "title": "Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory", "comment": "23 pages, 10 figures, under review at ICML 2026", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \\textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.", "AI": {"tldr": "\u672c\u6587\u5c06LLM\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u8f6c\u5316\u4e3a\"\u4e8c\u5341\u95ee\"\u6e38\u620f\u53ca\u5176\u5bf9\u6297\u53d8\u4f53\uff0c\u63d0\u51fa\u4e86Game of Thought (GoT)\u6846\u67b6\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u4f18\u5316\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u8f83\u73b0\u6709\u624b\u6bb5\u6709\u66f4\u597d\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u5728\u7f3a\u4e4f\u8db3\u591f\u4fe1\u606f\u5b8c\u6210\u4efb\u52a1\u65f6\uff0c\u4e3b\u52a8\u5bfb\u627e\u7f3a\u5931\u4fe1\u606f\u7684\u80fd\u529b\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3a\u7b80\u5316\u5904\u7406\u5e38\u505a\u51fa\u4e0d\u73b0\u5b9e\u5047\u8bbe\uff0c\u5bfc\u81f4\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u5bf9\u9ad8\u98ce\u9669\u5e94\u7528\u9886\u57df\u5f71\u54cd\u4e25\u91cd\u3002", "method": "\u4f5c\u8005\u7528\"\u4e8c\u5341\u95ee\"\u6e38\u620f\u8bc4\u4f30LLM\u7684\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u5bf9\u6297\u573a\u666f\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e24\u4eba\u96f6\u548c\u6269\u5c55\u5f0f\u535a\u5f08\uff08SLS\u95ee\u9898\uff09\uff0c\u8fdb\u800c\u63d0\u51faGame of Thought\uff08GoT\uff09\u6846\u67b6\uff0c\u91c7\u7528\u535a\u5f08\u8bba\u6280\u5de7\u903c\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u4ece\u800c\u63d0\u5347\u6700\u574f\u60c5\u51b5\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGoT\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u8bbe\u7f6e\u4e0b\uff0c\u6700\u574f\u60c5\u51b5\u8868\u73b0\u5747\u4f18\u4e8e\uff081\uff09\u76f4\u63a5\u63d0\u793a\u6cd5\u548c\uff082\uff09\u542f\u53d1\u5f0f\u641c\u7d22\u6cd5\u3002", "conclusion": "\u5229\u7528\u535a\u5f08\u8bba\u5efa\u6a21\u548c\u7b56\u7565\u4f18\u5316\uff0c\u4f5c\u8005\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7f3a\u4e4f\u4fe1\u606f\u65f6\u7684\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\uff0c\u5c24\u5176\u4fdd\u969c\u4e86\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u6700\u574f\u8868\u73b0\u3002"}}
{"id": "2602.00904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00904", "abs": "https://arxiv.org/abs/2602.00904", "authors": ["Kunal Mahatha", "Ali Bahri", "Pierre Marza", "Sahar Dastani", "Maria Vakalopoulou", "Stergios Christodoulidis", "Jose Dolz", "Christian Desrosiers"], "title": "OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection", "comment": null, "summary": "State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.", "AI": {"tldr": "\u63d0\u51faOCTOPUS\u67b6\u6784\uff0c\u5f25\u8865\u4f20\u7edfSSM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7a7a\u95f4\u5efa\u6a21\u4e0d\u8db3\uff0c\u517c\u987e\u5168\u5c40\u4e0e\u5c40\u90e8\u7ed3\u6784\uff0c\u5e76\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u53d6\u5f97\u66f4\u4f18\u7684\u56fe\u50cf\u5206\u5272\u4e0e\u5206\u7c7b\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfSSM\u56e0\u5176\u7ebf\u6027\u590d\u6742\u5ea6\u88ab\u89c6\u4e3aTransformer\u7684\u6709\u529b\u66ff\u4ee3\u8005\uff0c\u4f46\u5176\u56e0\u56e0\u679c\u5efa\u6a21\u9002\u7528\u4e8e\u5e8f\u5217\u6587\u672c\uff0c\u5e94\u7528\u5230\u89c6\u89c9\u9886\u57df\u65f6\u5374\u7834\u574f\u4e86\u50cf\u7d20/patch\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\u3002", "method": "\u63d0\u51faOCTOPUS\u67b6\u6784\uff0c\u5728\u6c34\u5e73\u65b9\u5411\u3001\u5782\u76f4\u65b9\u5411\u548c\u5bf9\u89d2\u7ebf\u65b9\u5411\u8fdb\u884c\u516b\u5411\u79bb\u6563\u9012\u5f52\uff0c\u5b9e\u73b0\u591a\u65b9\u5411\u4fe1\u606f\u4f20\u9012\uff0c\u4fdd\u8bc1\u4e86\u72ec\u7acbpatch\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u517c\u987e\u5168\u5c40\u548c\u5c40\u90e8\u7a7a\u95f4\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301SSM\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "OCTOPUS\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u5206\u5272\u8fb9\u754c\u4fdd\u6301\u548c\u533a\u57df\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709V-SSM\u6a21\u578b\uff0c\u540c\u65f6\u5206\u7c7b\u7cbe\u5ea6\u4e5f\u66f4\u9ad8\u3002", "conclusion": "OCTOPUS\u4e3a\u591a\u65b9\u5411\u9012\u5f52\u5efa\u6a21\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7a7a\u95f4\u611f\u77e5\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u67b6\u6784\uff0c\u9002\u5408\u4e8e\u53ef\u6269\u5c55\u7684\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2602.01709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01709", "abs": "https://arxiv.org/abs/2602.01709", "authors": ["Xingshan Zeng", "Lingzhi Wang", "Weiwen Liu", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "title": "ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation", "comment": null, "summary": "Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \\emph{\\name}, \\emph{\\underline{A}gentic \\underline{R}isk-Aware \\underline{T}est-Time Scaling via \\underline{I}terative \\underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \\emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u8ba1\u7b97\u65b9\u6cd5\u2014\u2014Agentic Risk-Aware Test-Time Scaling via Iterative Simulation\uff08ARTIS\uff09\uff0c\u901a\u8fc7\u4eff\u771f\u4f18\u5148\u4e8e\u771f\u5b9e\u6267\u884c\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\u5728\u4e0d\u53ef\u9006\u3001\u98ce\u9669\u884c\u4e3a\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u91cf\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5bf9\u6d89\u53ca\u5916\u90e8\u73af\u5883\u4ea4\u4e92\u3001\u98ce\u9669\u4e0d\u53ef\u9006\u7684\u667a\u80fd\u4f53\u573a\u666f\u4ecd\u4e0d\u8db3\u3002\u56e0\u4e3a\uff0c\u771f\u5b9e\u884c\u52a8\u7684\u5931\u8bef\u4ee3\u4ef7\u9ad8\u6602\uff0c\u9700\u8981\u5728\u6d4b\u8bd5\u65f6\u5b89\u5168\u5730\u63a2\u7d22\u6700\u4f18\u64cd\u4f5c\u8def\u5f84\u3002", "method": "ARTIS \u6846\u67b6\u5c06\u63a2\u7d22\u4e0e\u627f\u8bfa\u89e3\u8026\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u771f\u5b9e\u6267\u884c\u524d\uff0c\u901a\u8fc7\u53cd\u590d\u4eff\u771f\u4e0e\u73af\u5883\u4ea4\u4e92\u4ee5\u6d88\u9664\u4e0d\u786e\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u53d1\u73b0\u666e\u901aLLM\u4eff\u771f\u5668\u96be\u4ee5\u6a21\u62df\u7f55\u89c1\u98ce\u9669\u5931\u8d25\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u5de5\u5177\u578b\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u5730\u751f\u6210\u6570\u636e\u3001\u518d\u5e73\u8861\u8bad\u7ec3\uff0c\u5f3a\u5316\u6a21\u578b\u5bf9\u9ad8\u98ce\u9669\u5931\u8d25\u7684\u8bc6\u522b\u548c\u5e94\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u8f6e\u3001\u591a\u6b65\u667a\u80fd\u4f53\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u8fed\u4ee3\u4eff\u771f\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002\u5f15\u5165\u98ce\u9669\u611f\u77e5\u4eff\u771f\u5668\u540e\uff0c\u8fd9\u4e00\u63d0\u5347\u5728\u4e0d\u540c\u6a21\u578b\u4e0e\u4efb\u52a1\u95f4\u66f4\u52a0\u7a33\u5b9a\u548c\u4e00\u81f4\u3002", "conclusion": "ARTIS \u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e0b\u4f5c\u4e3a\u667a\u80fd\u4f53\u51b3\u7b56\u8005\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u98ce\u9669\u611f\u77e5\u4eff\u771f\u662f\u8fbe\u6210\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u3002"}}
{"id": "2602.00946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00946", "abs": "https://arxiv.org/abs/2602.00946", "authors": ["Dhruv Parikh", "Haoyang Fan", "Rajgopal Kannan", "Viktor Prasanna"], "title": "ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models", "comment": "Technical Report", "summary": "Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \\textit{either} vision-encoder saliency (broad but query-agnostic) \\textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \\emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \\textbf{ConsensusDrop}, a training-free framework that derives a \\emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faConsensusDrop\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\u548cLLM\u8de8\u6ce8\u610f\u529b\u4fe1\u53f7\uff0c\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u7684\u89c6\u89c9token\u9009\u62e9\u548c\u538b\u7f29\u3002\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0cLLM\u9700\u8981\u5904\u7406\u5927\u91cf\u5197\u4f59\u89c6\u89c9token\uff0c\u5bfc\u81f4\u8fd0\u7b97\u975e\u5e38\u6602\u8d35\u3002\u5df2\u6709token\u88c1\u526a\u65b9\u6cd5\u53ea\u5229\u7528\u89c6\u89c9\u663e\u8457\u6027\u6216\u8de8\u6ce8\u610f\u529b\u4e2d\u7684\u4e00\u79cd\u4fe1\u53f7\uff0c\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u8054\u5408\u4e8c\u8005\u4f18\u52bf\uff0c\u63d0\u9ad8token\u9009\u62e9\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684ConsensusDrop\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\uff08\u5e7f\u6cdb\u3001\u4f46\u4e0e\u67e5\u8be2\u65e0\u5173\uff09\u548cLLM\u8de8\u6ce8\u610f\u529b\uff08\u4e0e\u67e5\u8be2\u76f8\u5173\u4f46\u7a00\u758f\u4e14\u6602\u8d35\uff09\u4fe1\u53f7\uff0c\u751f\u6210\u5171\u8bc6\u6392\u5e8f\uff0c\u5bf9token\u8fdb\u884c\u4fdd\u7559\u6216\u7531\u7f16\u7801\u5668\u5f15\u5bfc\u7684\u5408\u5e76\u3002\u8fd9\u6837\u5728\u8fdb\u5165LLM\u524d\u5373\u53ef\u9ad8\u6548\u88c1\u526atoken\u3002", "result": "\u5728\u591a\u79cd\u5f00\u6e90VLM\uff08\u5982LLaVA-1.5/NeXT\u3001Video-LLaVA\u7b49\uff09\u4e0a\uff0cConsensusDrop\u5728\u76f8\u540ctoken\u9884\u7b97\u4e0b\u663e\u8457\u4f18\u4e8e\u5df2\u6709\u88c1\u526a\u65b9\u6cd5\uff0c\u5728\u7cbe\u5ea6-\u6548\u7387\u5e73\u8861\u4e0a\u8868\u73b0\u66f4\u4f73\u3002\u5373\u4fbf\u5927\u5e45\u88c1\u526atoken\uff0c\u4e5f\u80fd\u4fdd\u6301\u8fd1\u4f3c\u57fa\u7ebf\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11TTFT\u548cKV\u7f13\u5b58\u5f00\u9500\u3002", "conclusion": "ConsensusDrop\u65e0\u9700\u8bad\u7ec3\u3001\u6613\u4e8e\u96c6\u6210\uff0c\u80fd\u9ad8\u6548\u5730\u51cf\u5c11VLM\u89c6\u89c9token\uff0c\u517c\u987e\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f18\u96c5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01714", "abs": "https://arxiv.org/abs/2602.01714", "authors": ["Mouath Abu-Daoud", "Leen Kharouf", "Omar El Hajj", "Dana El Samad", "Mariam Al-Omari", "Jihad Mallat", "Khaled Saleh", "Nizar Habash", "Farah E. Shamout"], "title": "MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark", "comment": null, "summary": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MedAraBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d6\u591a\u79cd\u533b\u5b66\u4e13\u4e1a\u7684\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u95ee\u7b54\u5927\u6570\u636e\u96c6\uff0c\u65e8\u5728\u586b\u8865\u963f\u8bed\u533b\u5b66NLP\u9886\u57df\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u7528\u5b83\u8bc4\u6d4b\u4e86\u591a\u6b3e\u5927\u6a21\u578b\uff0c\u63ed\u793a\u63d0\u5347\u6a21\u578b\u4e13\u4e1a\u6027\u548c\u591a\u8bed\u6027\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u533b\u5b66NLP\u6570\u636e\u548c\u57fa\u51c6\u532e\u4e4f\uff0c\u5f71\u54cd\u5927\u6a21\u578b\u591a\u8bed\u79cd\u80fd\u529b\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\uff0c\u9700\u8981\u5f00\u53d1\u516c\u5f00\u3001\u4f18\u8d28\u7684\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u4eba\u5de5\u6570\u5b57\u5316\u963f\u62c9\u4f2f\u5730\u533a\u533b\u5b66\u4e13\u4e1a\u4eba\u5458\u7684\u5b66\u672f\u6750\u6599\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u591a\u9009\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\uff1b\u7ecf\u8fc7\u9884\u5904\u7406\u540e\u5212\u5206\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\uff1b\u91c7\u7528\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u548cLLM-\u81ea\u52a8\u5224\u5206\u4e24\u79cd\u6846\u67b6\u8bc4\u4ef7\u6570\u636e\u8d28\u91cf\uff1b\u6700\u540e\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u516b\u4e2a\u524d\u6cbf\u5f00\u6e90\u53ca\u95ed\u6e90\u5927\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d619\u4e2a\u533b\u5b66\u4e13\u4e1a\u30015\u4e2a\u96be\u5ea6\u5c42\u6b21\u7684\u6570\u636e\u96c6\uff0c\u7ecf\u8fc7\u53cc\u91cd\u8d28\u91cf\u8bc4\u4f30\uff0c\u6570\u636e\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u3002\u591a\u6b3e\u4e3b\u6d41\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u793a\u9700\u8fdb\u4e00\u6b65\u52a0\u5f3a\u4e13\u4e1a\u5316\u548c\u963f\u62c9\u4f2f\u8bed\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u516c\u5f00\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u811a\u672c\uff0c\u4e30\u5bcc\u533b\u5b66NLP\u9886\u57df\u7684\u591a\u8bed\u79cd\u57fa\u51c6\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u5728\u4e34\u5e8a\u573a\u666f\u7684\u591a\u8bed\u8a00\u9002\u7528\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.00949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00949", "abs": "https://arxiv.org/abs/2602.00949", "authors": ["Xiang Zhang", "Boxuan Zhang", "Alireza Naghizadeh", "Mohab Mohamed", "Dongfang Liu", "Ruixiang Tang", "Dimitris Metaxas", "Dongfang Liu"], "title": "Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images", "comment": null, "summary": "Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347CAR-T/NK\u7ec6\u80de\u514d\u75ab\u7a81\u89e6\uff08IS\uff09\u7ed3\u6784\u7684\u68c0\u6d4b\u4e0e\u5206\u5272\u7cbe\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u5f71\u50cf\u5b66\u6807\u5fd7\u7269\u5728\u9884\u6d4b\u514d\u75ab\u6cbb\u7597\u7597\u6548\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u867d\u7136CAR-T/NK\u7ec6\u80de\u514d\u75ab\u7597\u6cd5\u5df2\u6539\u53d8\u764c\u75c7\u6cbb\u7597\u65b9\u5f0f\uff0c\u4e14IS\u7ed3\u6784\u8d28\u91cf\u53ef\u80fd\u9884\u6d4b\u7597\u6548\uff0c\u4f46\u53d7\u9650\u4e8e\u6ce8\u91ca\u663e\u5fae\u56fe\u50cf\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u96be\u4ee5\u6cdb\u5316\uff0c\u6210\u4e3aIS\u81ea\u52a8\u5316\u5b9a\u91cf\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\uff081\uff09\u5b9e\u4f8b\u611f\u77e5\u81ea\u52a8\u589e\u5f3a\uff08IAAA\uff09\uff1a\u5bf9\u539f\u59cbIS\u56fe\u50cf\u5e94\u7528\u4f18\u5316\u589e\u5f3a\u7b56\u7565\uff0c\u751f\u6210\u5408\u6210\u56fe\u50cf\u53ca\u5206\u5272\u63a9\u819c\uff0c\u5e76\u652f\u6301\u591a\u79cd\u6210\u50cf\u6a21\u6001\uff1b\uff082\uff09\u8bed\u4e49\u611f\u77e5AI\u589e\u5f3a\uff08SAAA\uff09\uff1a\u878d\u5408\u57fa\u4e8e\u6269\u6563\u7684\u63a9\u819c\u751f\u6210\u5668\u4e0ePix2Pix\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u5668\uff0c\u521b\u5efa\u591a\u6837\u4e14\u903c\u771f\u7684IS\u5206\u5272\u63a9\u819c\u53ca\u9ad8\u4fdd\u771f\u56fe\u50cf\u3002\u4e24\u65b9\u6cd5\u8054\u5408\u663e\u8457\u6269\u5145\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u4e24\u79cd\u589e\u5f3a\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u5728\u89c6\u89c9\u53ca\u7ed3\u6784\u5c5e\u6027\u4e0a\u9ad8\u5ea6\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\uff0c\u6781\u5927\u6539\u5584\u4e86CAR-T/NK IS\u7684\u68c0\u6d4b\u4e0e\u5206\u5272\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u5347IS\u91cf\u5316\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u57fa\u4e8e\u5f71\u50cf\u7684\u7597\u6548\u9884\u6d4b\u751f\u7269\u6807\u5fd7\u7269\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.01716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01716", "abs": "https://arxiv.org/abs/2602.01716", "authors": ["Mehdi Jafari", "Hao Xue", "Flora Salim"], "title": "Mechanistic Indicators of Steering Effectiveness in Large Language Models", "comment": null, "summary": "Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6fc0\u6d3b\u64cd\u63a7\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u9760\u6027\u8bca\u65ad\u8fdb\u884c\u4e86\u63a2\u8ba8\uff0c\u63d0\u51fa\u901a\u8fc7\u4fe1\u606f\u8bba\u91cf\uff08\u5982\u5f52\u4e00\u5316\u5206\u53c9\u56e0\u5b50\u548cKL\u6563\u5ea6\uff09\u6765\u9884\u6d4b\u64cd\u63a7\u6210\u529f\u4e0e\u5426\uff0c\u5e76\u4e3a\u4e3b\u6d41\u65b9\u6cd5\u63d0\u51fa\u4e86\u66f4\u4e25\u683c\u7684\u8bc4\u6d4b\u57fa\u7ebf\u3002", "motivation": "\u6fc0\u6d3b\u64cd\u63a7\u80fd\u8ba9LLM\u5c55\u73b0\u7279\u5b9a\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u5bf9\u4f55\u65f6\u6709\u6548\u3001\u4f55\u65f6\u5931\u6548\u673a\u5236\u4e86\u89e3\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u6216\u4eba\u5de5\u5224\u522b\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u7684\u8bca\u65ad\u624b\u6bb5\u3002\u8be5\u6587\u65e8\u5728\u586b\u8865\u5bf9\u6fc0\u6d3b\u64cd\u63a7\u673a\u5236\u53ef\u9760\u6027\u7684\u7406\u89e3\u7a7a\u767d\u3002", "method": "\u4ee5\u5f52\u4e00\u5316\u5206\u53c9\u56e0\u5b50\uff08NBF\uff09\u548c\u4e0e\u76ee\u6807\u8bed\u4e49KL\u6563\u5ea6\u4e24\u9879\u5185\u90e8\u4fe1\u53f7\u8bc4\u4f30\u6fc0\u6d3b\u64cd\u63a7\uff1b\u4ee5\u4e24\u79cd\u7ed3\u6784\u5b8c\u5168\u4e0d\u540c\u7684LLM\u4f5c\u4e3a\u4eba\u5de5\u6807\u6ce8\u4e00\u81f4\u6027\u57fa\u7840\uff0c\u7528LLM\u751f\u6210\u6807\u6ce8\u7ed3\u679c\u4e3a\u771f\u503c\uff0c\u7cfb\u7edf\u5206\u6790\u4e0a\u8ff0\u4fe1\u53f7\u80fd\u5426\u9884\u6d4b\u64cd\u63a7\u6210\u529f\uff0c\u5e76\u63d0\u5347Contrastive Activation Addition\u548c\u7a00\u758f\u81ea\u7f16\u7801\u64cd\u63a7\u7684\u8bc4\u4ef7\u57fa\u7ebf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5185\u90e8\u7684NBF\u548cKL\u7b49\u673a\u5236\u4fe1\u53f7\u5bf9\u4e8e\u5224\u65ad\u64cd\u63a7\u662f\u5426\u6210\u529f\u6709\u8f83\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\uff1a\u80fd\u591f\u6709\u6548\u5224\u522b\u4f55\u65f6\u64cd\u63a7\u8fbe\u5230\u9884\u671f\uff0c\u540c\u65f6\u4f30\u8ba1\u5931\u8d25\u6982\u7387\u3002\u5f15\u5165\u4e86\u66f4\u5f3a\u3001\u5bf9\u6bd4\u66f4\u6e05\u6670\u7684\u8bc4\u6d4b\u57fa\u7ebf\u3002", "conclusion": "\u673a\u68b0\u6027\u5185\u90e8\u4fe1\u53f7\u4e0d\u4ec5\u80fd\u591f\u8f83\u597d\u5730\u8bca\u65ad\u6fc0\u6d3b\u64cd\u63a7\u7684\u6210\u8d25\uff0c\u8fd8\u4e3a\u672a\u6765\u53d1\u5c55\u66f4\u7a33\u5065\u53ef\u63a7\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.00956", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00956", "abs": "https://arxiv.org/abs/2602.00956", "authors": ["Faisal Ahmed"], "title": "Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification", "comment": "20 pages, 6 Figures", "summary": "Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.\n  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u4e0eDenseNet121\u7f51\u7edc\u7ed3\u5408\u7684\u65b0\u578b\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5728OASIS\u7ed3\u6784\u6027MRI\u6570\u636e\u96c6\u4e0a\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u56db\u5206\u7c7b\uff0c\u53d6\u5f97\u4e8699.93%\u7684\u51c6\u786e\u7387\u548c100%\u7684AUC\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u7684\u65e9\u671f\u4e0e\u51c6\u786e\u8bca\u65ad\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u975e\u5e38\u5173\u952e\uff0c\u4f46\u73b0\u6709\u4f9d\u8d56\u795e\u7ecf\u5f71\u50cf\u7684\u8f85\u52a9\u7cfb\u7edf\u8bc6\u522b\u7387\u4ecd\u6709\u5f85\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5bf9\u8111\u90e8\u7ed3\u6784\u4e2d\u96be\u4ee5\u6355\u83b7\u7684\u62d3\u6251\u7279\u5f81\u3002", "method": "\u4f5c\u8005\u5c06\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edcDenseNet121\u7ed3\u5408\uff0cTDA\u7528\u4e8e\u63d0\u53d6\u5927\u8111\u7ed3\u6784\u7684\u62d3\u6251\u7279\u5f81\uff0cDenseNet121\u63d0\u53d6MRI\u5207\u7247\u7684\u7a7a\u95f4\u5c42\u6b21\u7279\u5f81\uff0c\u4e8c\u8005\u7279\u5f81\u878d\u5408\u540e\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u56db\u9636\u6bb5\u7684\u5206\u7c7b\u3002", "result": "\u5728OASIS-1 Kaggle MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe99.93%\uff0cAUC\u4e3a100%\uff0c\u660e\u663e\u8d85\u8fc7\u6700\u65b0\u7684CNN\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u96c6\u6210\u548c\u591a\u5c3a\u5ea6\u67b6\u6784\u3002", "conclusion": "\u5c06\u62d3\u6251\u7279\u5f81\u5f15\u5165\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u6781\u5927\u589e\u5f3a\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u7684TDA+DenseNet121\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u9ad8\u51c6\u786e\u7387\u7684\u5de5\u5177\u3002"}}
{"id": "2602.01717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01717", "abs": "https://arxiv.org/abs/2602.01717", "authors": ["Hyunsik Kim", "Haeri Kim", "Munhak Lee", "Kyungmin Lee"], "title": "BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition", "comment": "accepted to ICASSP 2026", "summary": "Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eUTF-16\u7684BBPE\uff08BBPE16\uff09\u5206\u8bcd\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u63d0\u5347\u591a\u8bed\u79cd\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5728\u5904\u7406\u975e\u62c9\u4e01\u6587\u5b57\uff08\u5982\u4e2d\u65e5\u97e9\uff09\u65f6\u7684\u6548\u7387\u548c\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u591a\u8bed\u79cdASR\u5e38\u7528\u7684UTF-8\u5b57\u8282\u7ea7BPE\uff08BBPE\uff09\u867d\u7136\u5177\u5907\u8bed\u79cd\u65e0\u5173\u6027\u548c\u5168Unicode\u8986\u76d6\uff0c\u4f46\u5728\u4e2d\u65e5\u97e9\u7b49\u975e\u62c9\u4e01\u6587\u5b57\u4e0a\u4f1a\u4ea7\u751f\u8fc7\u957f\u7684token\u5e8f\u5217\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5b58\u50a8\u8d1f\u62c5\u52a0\u91cd\u3002\u9700\u8981\u6709\u66f4\u9ad8\u6548\u3001\u7edf\u4e00\u4e14\u517c\u5bb9\u591a\u8bed\u79cd\u7684\u5206\u8bcd\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86BBPE16\u5206\u8bcd\u5668\uff0c\u91c7\u7528UTF-16\u7f16\u7801\uff0c\u4f7f\u5f97\u5927\u90e8\u5206\u73b0\u4ee3\u5b57\u7b26\u96c6\u4ee52\u5b57\u8282\u5bf9\u9f50\u3002\u6b64\u8bbe\u8ba1\u5728\u4fdd\u6301BBPE\u8bed\u79cd\u65e0\u5173\u548cUnicode\u8986\u76d6\u4f18\u52bf\u7684\u524d\u63d0\u4e0b\uff0c\u4f18\u5316\u4e86\u8de8\u8bed\u79cdtoken\u5171\u4eab\uff0c\u51cf\u5c11\u4e86token\u6570\u91cf\u3002\u901a\u8fc7\u5728\u5355\u8bed\u3001\u53cc\u8bed\u3001\u4e09\u8bed\u4ee5\u53ca\u6301\u7eed\u591a\u8bed\u5b66\u4e60\u7684ASR\u5b9e\u9a8c\u4e2d\u4e0e\u4f20\u7edfBBPE\u5bf9\u6bd4\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "BBPE16\u5728\u8bc6\u522b\u51c6\u786e\u5ea6\u4e0a\u8fbe\u5230\u751a\u81f3\u4f18\u4e8e\u539f\u6709BBPE\u3002\u5728\u4e2d\u6587\u573a\u666f\u4e0b\uff0ctoken\u6570\u91cf\u51cf\u5c11\u7ea610.4%\uff0c\u89e3\u7801\u6b65\u9aa4\u51cf\u5c11\u7ea610.3%\uff0c\u6574\u4f53\u52a0\u5feb\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "BBPE16\u517c\u5177\u8bed\u79cd\u65e0\u5173\u3001\u6548\u7387\u63d0\u5347\u548c\u8f83\u4f4e\u5b58\u50a8\u5f00\u9500\uff0c\u662f\u591a\u8bed\u79cdASR\u5206\u8bcd\u7684\u5b9e\u7528\u9009\u62e9\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e2d\u65e5\u97e9\u7b49\u975e\u62c9\u4e01\u6587\u5b57\u5bc6\u96c6\u7684\u573a\u666f\u3002"}}
{"id": "2602.00971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00971", "abs": "https://arxiv.org/abs/2602.00971", "authors": ["Meng Luo", "Bobo Li", "Shanqing Xu", "Shize Zhang", "Qiuchan Chen", "Menglu Han", "Wenhao Chen", "Yanxiang Huang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning", "comment": "Accepted by ICLR 2026", "summary": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HitEmotion\u57fa\u51c6\u548cTMPO\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u548c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6df1\u5c42\u60c5\u611f\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u771f\u5b9e\u6709\u6548\u7684\u60c5\u611f\u667a\u80fd\u5fc5\u987b\u4f9d\u8d56\u5bf9\u5fc3\u667a\u7406\u8bba\u7684\u660e\u786e\u5efa\u6a21\u3002", "method": "1\uff09\u63d0\u51faHitEmotion\u57fa\u51c6\uff0c\u91c7\u7528\u57fa\u4e8eToM\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u7528\u4e8e\u68c0\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u8ba4\u77e5\u6df1\u5ea6\u4e0b\u7684\u80fd\u529b\u65ad\u70b9\uff1b2\uff09\u63d0\u51faToM\u5f15\u5bfc\u7684\u63a8\u7406\u94fe\u534f\u540c\u8ddf\u8e2a\u5fc3\u7406\u72b6\u6001\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u8bc1\u636e\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u60c5\u611f\u63a8\u7406\uff1b3\uff09\u5f15\u5165TMPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4e2d\u95f4\u5fc3\u7406\u72b6\u6001\u4f5c\u4e3a\u8fc7\u7a0b\u7ea7\u76d1\u7763\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHitEmotion\u80fd\u66b4\u9732\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u8ba4\u77e5\u4efb\u52a1\u4e0a\u7684\u60c5\u611f\u63a8\u7406\u7f3a\u9677\u3002ToM\u5f15\u5bfc\u7684\u63a8\u7406\u94fe\u548cTMPO\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\uff0c\u5e76\u751f\u6210\u66f4\u6709\u903b\u8f91\u3001\u66f4\u7b26\u5408\u5b9e\u9645\u7684\u63a8\u7406\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b66\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u8bc4\u6d4b\u4e0e\u589e\u5f3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u8ba4\u77e5\u57fa\u7840\u60c5\u611f\u7406\u89e3\u80fd\u529b\u7684\u5de5\u5177\u5305\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8MLLMs\u60c5\u611f\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01719", "abs": "https://arxiv.org/abs/2602.01719", "authors": ["Jiwei Tang", "Shilei Liu", "Zhicheng Zhang", "Yujin Yuan", "Libin Zheng", "Wenbo Su", "Bo Zheng"], "title": "COMI: Coarse-to-fine Context Compression via Marginal Information Gain", "comment": "Accepted at ICLR 2026", "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6COMI\uff0c\u6709\u6548\u51cf\u5c0f\u8f93\u5165\u957f\u5ea6\u5e76\u53bb\u9664\u5197\u4f59\uff0c\u5728\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u4e14\u4fe1\u606f\u5197\u4f59\u4e25\u91cd\uff0c\u73b0\u6709\u7684\u538b\u7f29\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u7684\u538b\u7f29\u7b56\u7565\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u957f\u6587\u672c\u5904\u7406\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "COMI\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\uff081\uff09\u7c97\u7c92\u5ea6\u5206\u7ec4\u91cd\u5206\u914d\uff0c\u6839\u636e\u5206\u7ec4\u95f4\u7684\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\uff08MIG\uff09\u52a8\u6001\u5206\u914d\u538b\u7f29\u7387\uff0c\u4f7f\u538b\u7f29\u9884\u7b97\u4e0e\u4fe1\u606f\u5206\u5e03\u76f8\u5339\u914d\uff1b\uff082\uff09\u7ec6\u7c92\u5ea6\u7684\u7ec4\u5185token\u878d\u5408\uff0c\u901a\u8fc7\u7ec4\u5185MIG\u52a0\u6743\u5408\u5e76token\uff0c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u5e76\u51cf\u5c11\u5197\u4f59\u3002\u6838\u5fc3\u6307\u6807MIG\u7ed3\u5408\u4e86\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u8bed\u4e49\u5197\u4f59\uff0c\u6307\u5bfc\u538b\u7f29\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u548c\u6458\u8981\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u591a\u79cd\u4e3b\u6d41\u5927\u6a21\u578b\uff08\u5982LLaMA-2-7B\u3001Qwen2-7B\uff09\u4e0a\uff0cCOMI\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee5Qwen2-7B\u5728NaturalQuestions\u4e3a\u4f8b\uff0c\u572832\u500d\u538b\u7f29\u60c5\u51b5\u4e0bEM\u63d0\u5347\u7ea625\u5206\u3002", "conclusion": "COMI\u80fd\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u6709\u6548\u53bb\u9664\u5197\u4f59\u5e76\u4fdd\u7559\u91cd\u8981\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01725", "abs": "https://arxiv.org/abs/2602.01725", "authors": ["Yurun Chen", "Zeyi Liao", "Ping Yin", "Taotao Xie", "Keting Yin", "Shengyu Zhang"], "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models", "comment": null, "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u673a\u667a\u80fd\u4f53(CUAs)\u7684\u9884\u6d4b\u578b\u62a4\u680f(SafePred)\uff0c\u901a\u8fc7\u9884\u6d4b\u77ed\u671f\u548c\u957f\u671f\u98ce\u9669\u6765\u63d0\u524d\u89c4\u907f\u5371\u5bb3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u62a4\u680f\u4e3b\u8981\u662f\u88ab\u52a8\u53cd\u5e94\u578b\uff0c\u805a\u7126\u4e8e\u5f53\u524d\u89c2\u6d4b\u7a7a\u95f4\u5185\u7684\u9650\u5236\uff0c\u8fd9\u53ea\u80fd\u9632\u6b62\u5373\u65f6\u98ce\u9669\uff0c\u4f46\u65e0\u6cd5\u8bc6\u522b\u548c\u89c4\u907f\u5ef6\u65f6\u51fa\u73b0\u7684\u957f\u671f\u9ad8\u98ce\u9669\u884c\u4e3a\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u548c\u907f\u514d\u957f\u8fdc\u98ce\u9669\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86SafePred\u9884\u6d4b\u62a4\u680f\u6846\u67b6\u3002\u5176\u6838\u5fc3\u662f\u5efa\u7acb\u98ce\u9669-\u51b3\u7b56\u95ed\u73af\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u672a\u6765\u53ef\u80fd\u7684\u98ce\u9669\uff08\u542b\u77ed\u671f\u548c\u957f\u671f\uff09\uff0c\u5b9e\u73b0\u98ce\u9669\u8868\u5f81\u5e76\u5728\u51b3\u7b56\u524d\u526a\u679d\u9ad8\u98ce\u9669\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u5c06\u98ce\u9669\u9884\u6d4b\u7ed3\u679c\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u51b3\u7b56\u6307\u5bfc\uff0c\u5305\u62ec\u9010\u6b65\u5e72\u9884\u548c\u4efb\u52a1\u91cd\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSafePred\u80fd\u5927\u5e45\u5ea6\u51cf\u5c11\u9ad8\u98ce\u9669\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e8697.6%\u4ee5\u4e0a\u7684\u5b89\u5168\u8868\u73b0\uff0c\u76f8\u6bd4\u73b0\u6709\u53cd\u5e94\u5f0f\u57fa\u7ebf\uff0c\u4efb\u52a1\u6548\u7528\u4e5f\u63d0\u5347\u4e86\u6700\u9ad821.4%\u3002", "conclusion": "SafePred\u80fd\u591f\u6709\u6548\u9884\u6d4b\u5e76\u89c4\u907f\u8ba1\u7b97\u673a\u667a\u80fd\u4f53\u9762\u4e34\u7684\u77ed\u671f\u53ca\u957f\u671f\u9ad8\u98ce\u9669\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.00995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00995", "abs": "https://arxiv.org/abs/2602.00995", "authors": ["Nick DiSanto", "Ehsan Khodapanah Aghdam", "Han Liu", "Jacob Watson", "Yuankai K. Tao", "Hao Li", "Ipek Oguz"], "title": "VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes", "comment": "Accepted to SPIE Medical Imaging 2026", "summary": "Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u8f74\u8840\u7ba1\u611f\u77e5\u76d1\u7763\u7684\u65b0\u65b9\u6cd5\uff08VAMOS-OCTA\uff09\uff0c\u7528\u4e8e\u4fee\u590d\u624b\u6301OCTA\u5728\u626b\u63cf\u4e2d\u56e0\u8fd0\u52a8\u5e26\u6765\u7684\u4f53\u79ef\u56fe\u50cf\u9000\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e862D\u30013D\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u624b\u6301OCTA\u5c3d\u7ba1\u9002\u7528\u4e8e\u4e0d\u914d\u5408\u6216\u513f\u7ae5\u53d7\u8bd5\u8005\uff0c\u4f46\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u5bfc\u81f4\u6210\u50cf\u51fa\u73b0\u7a7a\u767d\u548c\u4e25\u91cd\u4fe1\u606f\u4e22\u5931\u3002\u4ee5\u5f80\u65b9\u6cd5\u5927\u591a\u53ea\u4e13\u6ce8\u4e8e2D\u6295\u5f71\u6062\u590d\uff0c\u96be\u4ee5\u517c\u987eB-scan\u7684\u9510\u5ea6\u548c\u6574\u4f53\u8840\u7ba1\u8fde\u7eed\u6027\u7684\u91cd\u5efa\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5f3a\u5927\u4e14\u5168\u9762\u7684\u65b9\u6cd5\u63d0\u5347\u56fe\u50cf\u53ef\u7528\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86VAMOS-OCTA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u75282.5D U-Net\u7f51\u7edc\uff0c\u8f93\u5165\u4e00\u7ec4\u76f8\u90bbB-scan\uff0c\u91cd\u5efa\u4e2d\u95f4\u88ab\u8fd0\u52a8\u635f\u574f\u7684B-scan\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u8840\u7ba1\u611f\u77e5\u591a\u8f74\u6b63\u4ea4\u76d1\u7763\u635f\u5931\uff08VAMOS loss\uff09\uff0c\u7ed3\u5408\u8840\u7ba1\u52a0\u6743\u91cd\u5efa\u3001\u8f74\u5411\u4e0e\u6a2a\u5411\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u539f\u751fB-scan\u4e0e\u591a\u6295\u5f71\u8840\u7ba1\u7ed3\u6784\u7684\u8054\u5408\u6062\u590d\u3002\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u5e26\u4f2a\u5f71OCTA\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5e76\u8bc4\u6d4b\u3002", "result": "VAMOS-OCTA\u5728\u89c6\u89c9\u548c\u50cf\u7d20\u7ea7\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u6e05\u6670\u7684\u6bdb\u7ec6\u8840\u7ba1\u3001\u5b8c\u6574\u7684\u8840\u7ba1\u8fde\u7eed\u6027\u548c\u5e72\u51c0\u76842D\u6295\u5f71\uff0c\u5c24\u5176\u5728\u4e25\u91cd\u8fd0\u52a8\u635f\u4f24\u573a\u666f\u4e0b\u4e5f\u6709\u8f83\u597d\u8868\u73b0\u3002", "conclusion": "\u591a\u8f74\u8840\u7ba1\u611f\u77e5\u76d1\u7763\u4e3aOCTA\u4e09\u7ef4\u8fd0\u52a8\u8865\u507f\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u7ea6\u675f\uff0c\u80fd\u5168\u9762\u63d0\u5347\u4f53\u79ef\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u7ed3\u6784\u8fd8\u539f\u80fd\u529b\uff0c\u4e3a\u624b\u6301OCTA\u5e94\u7528\u62d3\u5bbd\u4e86\u4e34\u5e8a\u4e0e\u7814\u7a76\u524d\u666f\u3002"}}
{"id": "2602.01747", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01747", "abs": "https://arxiv.org/abs/2602.01747", "authors": ["Hongseok Choi", "Serynn Kim", "Wencke Liermann", "Jin Seong", "Jin-Xia Huang"], "title": "Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training", "comment": "22 pages, 4 figures", "summary": "Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u4e09\u9879\u5173\u952e\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86AES\u5728\u6570\u636e\u6709\u9650\u548c\u5b8c\u6574\u6570\u636e\u4e0b\u7684\u8bc4\u5206\u80fd\u529b\uff0c\u5e76\u5728ASAP++\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2dAES\u7cfb\u7edf\u5f80\u5f80\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u6781\u5ea6\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u5efa\u6a21\u548c\u5b9e\u9645\u63a8\u5e7f\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5728\u4f4e\u6807\u6ce8\u6570\u636e\u91cf\u65f6\u4f9d\u7136\u80fd\u5927\u5e45\u63d0\u5347AES\u6027\u80fd\u3002", "method": "1\uff09\u63d0\u51fa\u201c\u4e24\u9636\u6bb5\u5fae\u8c03\u201d\u5e76\u501f\u52a9\u4f4e\u79e9\u9002\u5e94\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u76ee\u6807\u5199\u4f5c\u9898\u76ee\u7684\u6cdb\u5316\u529b\uff1b2\uff09\u63d0\u51fa\u5f97\u5206\u5bf9\u9f50\u6280\u672f\uff0c\u589e\u5f3a\u9884\u6d4b\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff1b3\uff09\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u7b56\u7565\u4f7f\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5229\u7528\u4f2a\u6807\u7b7e\u6269\u5c55\u8bad\u7ec3\u96c6\u540c\u65f6\u6291\u5236\u566a\u58f0\u6269\u6563\u3002\u4e09\u8005\u96c6\u6210\u5728DualBERT\u67b6\u6784\u4e0a\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u5b9e\u9a8c\u3002", "result": "\u5728ASAP++\u6570\u636e\u96c6\u4e0a\uff0c\u4e09\u4e2a\u65b9\u6cd5\u572832\u6761\u8bad\u7ec3\u6570\u636e\u6781\u5c11\u7684\u73af\u5883\u4e0b\u5747\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e09\u8005\u5408\u7528\u540e\u8fbe\u5230\u4ec5\u7528\u7ea61000\u6761\u6807\u6ce8\u6570\u636e\u5373\u53ef\u83b7\u5f9791.2%\u7684\u5b8c\u6574\u6570\u636e\u8bad\u7ec3\u6c34\u5e73\uff1bScore Alignment\u65b9\u6cd5\u5728\u5b8c\u6574\u548c\u7a00\u7f3a\u6570\u636e\u4e0b\u5747\u663e\u8457\u63d0\u5347\u5206\u6570\u51c6\u786e\u7387\uff0c\u4e14\u96c6\u6210\u4e8eDualBERT\u540e\u8fbe\u4e1a\u754c\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u5173\u952e\u6280\u672f\u80fd\u6709\u6548\u89e3\u51b3AES\u5728\u6570\u636e\u7a00\u7f3a\u4e0e\u5145\u8db3\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u6027\u80fd\u6311\u6218\uff0c\u4e0d\u4ec5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5206\u6570\u5206\u5e03\u62df\u5408\uff0c\u4e3aAES\u5b9e\u9645\u5e94\u7528\u548c\u63a8\u5e7f\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u652f\u6491\u3002"}}
{"id": "2602.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01000", "abs": "https://arxiv.org/abs/2602.01000", "authors": ["Vagish Kumar", "Souvik Chakraborty"], "title": "CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound", "comment": null, "summary": "Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53d7\u8111\u76ae\u5c42\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edcCortiNet\uff0c\u7528\u4e8e\u80c6\u56ca\u75be\u75c5\u8d85\u58f0\u8bca\u65ad\uff0c\u517c\u987e\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u8d85\u58f0\u56fe\u50cf\u5206\u8fa8\u7387\u4f4e\u4e14\u591a\u566a\u58f0\uff0c\u4f20\u7edf\u5927\u6a21\u578b\u867d\u80fd\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u7387\u4f46\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u5177\u8f7b\u91cf\u5316\u548c\u9ad8\u6027\u80fd\u7684\u65b0\u578b\u6a21\u578b\u3002", "method": "CortiNet\u91c7\u7528\u4eff\u4eba\u8111\u76ae\u5c42\u53cc\u6d41\u7ed3\u6784\uff0c\u5206\u522b\u5904\u7406\u4f4e\u9891\u7ed3\u6784\u4fe1\u606f\u4e0e\u9ad8\u9891\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u89e3\u91ca\u7684\u591a\u5c3a\u5ea6\u4fe1\u53f7\u5206\u89e3\u548c\u4e13\u95e8\u7f16\u7801\u6d41\uff0c\u6700\u7ec8\u5229\u7528\u878d\u5408\u673a\u5236\u6574\u5408\u4f18\u52bf\u7279\u5f81\uff1b\u540c\u65f6\u4ec5\u5728\u7ed3\u6784\u5206\u652f\u4e0a\u8fdb\u884c\u89e3\u91ca\u6027\u5206\u6790\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5305\u542b9\u7c7b\u300110692\u5f20\u4e13\u5bb6\u6807\u6ce8\u80c6\u56ca\u75be\u75c5\u8d85\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCortiNet\u4ee5\u8fdc\u5c11\u4e8e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u91cf\u8fbe\u523098.74%\u7684\u9ad8\u8bca\u65ad\u51c6\u786e\u7387\u3002", "conclusion": "CortiNet\u80fd\u5728\u4fdd\u8bc1\u8bca\u65ad\u51c6\u786e\u7387\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u533b\u5b66\u89e3\u91ca\u6027\uff0c\u5bf9\u8d85\u58f0\u8f85\u52a9\u8bca\u7597\u6709\u8f83\u597d\u4fc3\u8fdb\u4f5c\u7528\u3002"}}
{"id": "2602.01752", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01752", "abs": "https://arxiv.org/abs/2602.01752", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Li Guo"], "title": "WorldCup Sampling for Multi-bit LLM Watermarking", "comment": null, "summary": "As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.", "AI": {"tldr": "WorldCup\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6bd4\u7279\u6c34\u5370\u6846\u67b6\uff0c\u76f4\u63a5\u5c06\u4fe1\u606f\u5d4c\u5165LLM\u751f\u6210\u7684\u6587\u672c\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u5bb9\u91cf\u3001\u9c81\u68d2\u6027\u548c\u89e3\u7801\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6587\u672c\u7684\u201c\u4eba\u7c7b\u5316\u201d\u7a0b\u5ea6\u63d0\u9ad8\uff0c\u5982\u4f55\u6709\u6548\u8ffd\u6eaf\u4e0e\u8ba4\u8bc1\u5185\u5bb9\u6765\u6e90\u6210\u4e3a\u96be\u9898\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u591a\u501f\u52a9\u96f6\u6bd4\u7279\u65b9\u6848\uff0c\u65e0\u6cd5\u9ad8\u6548\u627f\u8f7d\u4e30\u5bcc\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u4f5c\u8005\u8bd5\u56fe\u8bbe\u8ba1\u4e00\u79cd\u5bb9\u91cf\u66f4\u5927\u3001\u53ef\u9760\u6027\u66f4\u5f3a\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWorldCup\u591a\u6bd4\u7279\u6c34\u5370\u67b6\u6784\uff0c\u5c06\u91c7\u6837\u89c6\u4e3a\u901a\u4fe1\u901a\u9053\uff0c\u901a\u8fc7\u5206\u5c42\u7ade\u4e89\u673a\u5236\u548c\u8f85\u52a9\u4fe1\u53f7\uff0c\u76f4\u63a5\u5728token\u9009\u62e9\u65f6\u5d4c\u5165\u6d88\u606f\u6bd4\u7279\uff0c\u540c\u65f6\u5229\u7528\u71b5\u611f\u77e5\u8c03\u5236\u4ee5\u7ef4\u6301\u6587\u672c\u8d28\u91cf\uff0c\u5e76\u91c7\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u89e3\u7801\u4fdd\u969c\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6062\u590d\u3002", "result": "\u5728\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0cWorldCup\u5728\u5bb9\u91cf\u3001\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u89e3\u7801\u6548\u7387\u7b49\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u591a\u6bd4\u7279\u6c34\u5370\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WorldCup\u4e3aLLMs\u6587\u672c\u6c34\u5370\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\uff0c\u517c\u987e\u9ad8\u6548\u5d4c\u5165\u3001\u9c81\u68d2\u6062\u590d\u4e0e\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u548c\u5b9e\u9645\u6eaf\u6e90\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.01004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01004", "abs": "https://arxiv.org/abs/2602.01004", "authors": ["Zihao Zhao", "Shengting Cao", "Muchao Ye"], "title": "SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning", "comment": null, "summary": "Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u65b9\u6cd5SRVAU-R1\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u65b9\u6cd5\u5728\u89c6\u9891\u5f02\u5e38\u7406\u89e3\uff08VAU\uff09\u4efb\u52a1\u4e2d\u4ecd\u505c\u7559\u5728\u5bf9\u5f02\u5e38\u8868\u9762\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u5bf9\u5f02\u5e38\u884c\u4e3a\u7684\u6df1\u5c42\u63a8\u7406\uff0c\u5c24\u5176\u662f\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u7ea0\u9519\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86SRVAU-R1\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u53cd\u601d\u611f\u77e5\u7684\u5b66\u4e60\u65b9\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9VAU\u7684\u53cd\u601d\u578b\u94fe\u5f0f\u601d\u7ef4\uff08Chain-of-Thought\uff09\u6570\u636e\u96c6\uff0c\u6570\u636e\u7ed3\u6784\u5305\u62ec\u521d\u6b65\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u53ca\u4fee\u6b63\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b2\uff09\u7ed3\u5408\u6709\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\uff0c\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSRVAU-R1\u5728\u65f6\u95f4\u5f02\u5e38\u5b9a\u4f4d\u51c6\u786e\u6027\u4ee5\u53ca\u63a8\u7406\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0cSRVAU-R1\u6709\u6548\u63d0\u5347\u4e86MLLM\u5728\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u5e26\u6765\u65b0\u7684\u8fdb\u5c55\u3002"}}
{"id": "2602.01757", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01757", "abs": "https://arxiv.org/abs/2602.01757", "authors": ["Doohyun Kim", "Donghwa Kang", "Kyungjae Lee", "Hyeongboo Baek", "Brent Byunghoon Kang"], "title": "Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings", "comment": "10 pages", "summary": "The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZero2Text\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u5728\u65e0\u8bad\u7ec3\u6570\u636e\u3001\u9ed1\u76d2\u548c\u8de8\u57df\u73af\u5883\u4e0b\u6210\u529f\u6062\u590d\u88ab\u5411\u91cf\u6570\u636e\u5e93\u4fdd\u62a4\u7684\u6587\u672c\uff0c\u4ece\u800c\u7834\u89e3\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5728\u7ebf\u9012\u5f52\u8c03\u6574\u751f\u6210\u7b56\u7565\uff0c\u53ef\u7a81\u7834\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\uff0c\u5982\u5dee\u5206\u9690\u79c1\uff0c\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5411\u91cf\u6570\u636e\u5e93\u5728RAG\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5d4c\u5165\u6570\u636e\u6613\u88ab\u9006\u5411\u63a8\u65ad\uff08embedding inversion\uff09\uff0c\u5e26\u6765\u91cd\u5927\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u9006\u5411\u653b\u51fb\u65b9\u6cd5\u6216\u8ba1\u7b97\u91cf\u5927\u3001\u6216\u9700\u4e0d\u53ef\u5f97\u7684\u57df\u5185\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5a01\u80c1\u8bc4\u4f30\u548c\u9632\u5fa1\u624b\u6bb5\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3001\u80fd\u5728\u9ed1\u76d2\u73af\u5883\u4e0b\u5b9e\u65bd\u4e14\u6548\u679c\u663e\u8457\u7684\u653b\u51fb\u65b0\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "Zero2Text\u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u6570\u636e\u9006\u5411\u6846\u67b6\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u8fed\u4ee3\u5f0f\u5730\u5728\u7ebf\u8c03\u6574\u751f\u6210\u6587\u672c\uff0c\u4f7f\u5176\u5d4c\u5165\u5c3d\u53ef\u80fd\u63a5\u8fd1\u76ee\u6807\u5411\u91cf\uff0c\u65e0\u9700\u4f7f\u7528\u9759\u6001\u8bad\u7ec3\u96c6\u3002\u5177\u4f53\u5b9e\u73b0\u4e3a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5019\u9009\u6587\u672c\uff0c\u901a\u8fc7\u5cad\u56de\u5f52\u8fdb\u884c\u52a8\u6001\u6295\u5f71\uff0c\u6bcf\u8f6e\u751f\u6210\u66f4\u903c\u8fd1\u76ee\u6807\u5d4c\u5165\u7684\u5185\u5bb9\uff0c\u76f4\u5230\u76f8\u4f3c\u5ea6\u6536\u655b\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u6570\u636e\u5bf9\u3001\u65e0\u9700\u5fae\u8c03\u548c\u5168\u9ed1\u76d2\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cZero2Text\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u8868\u73b0\u5353\u8d8a\u3002\u4f8b\u5982\uff0c\u5728MS MARCO\u6570\u636e\u96c6\u4e0eOpenAI\u9ed1\u76d2\u6a21\u578b\u5bf9\u6297\u65f6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5ROUGE-L\u6307\u6807\u63d0\u53471.8\u500d\uff0cBLEU-2\u6307\u6807\u63d0\u53476.4\u500d\u3002\u540c\u65f6\uff0c\u5728\u672a\u77e5\u9886\u57df\u6570\u636e\u4e0a\u4e5f\u80fd\u6709\u6548\u6062\u590d\u539f\u59cb\u53e5\u5b50\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u6570\u636e\u6cc4\u6f0f\u5bf9\uff08\u5373\u96f6\u66b4\u9732\uff09\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8fd8\u9a8c\u8bc1\u4e86\u4e3b\u6d41\u9632\u5fa1\u673a\u5236\uff08\u6bd4\u5982\u5dee\u5206\u9690\u79c1\uff09\u5bf9\u8fd9\u79cd\u81ea\u9002\u5e94\u653b\u51fb\u4e5f\u96be\u4ee5\u594f\u6548\u3002", "conclusion": "Zero2Text\u6846\u67b6\u7a81\u7834\u4e86\u73b0\u6709\u9006\u5411\u653b\u51fb\u65b9\u6cd5\u7684\u4f20\u7edf\u9650\u5236\uff0c\u663e\u793a\u5373\u4f7f\u5728\u9ed1\u76d2\u3001\u96f6\u6837\u672c\u73af\u5883\u4e0b\uff0c\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u7684\u6570\u636e\u4ecd\u6613\u53d7\u5a01\u80c1\uff0c\u5bf9\u8bbe\u8ba1\u5b89\u5168\u7684RAG\u7cfb\u7edf\u5177\u6709\u91cd\u5927\u8b66\u793a\u610f\u4e49\u3002\u73b0\u6709\u9632\u5fa1\u624b\u6bb5\u96be\u4ee5\u963b\u6321\u6b64\u7c7b\u653b\u51fb\uff0c\u9700\u91cd\u65b0\u601d\u8003\u5411\u91cf\u6570\u636e\u5e93\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\u3002"}}
{"id": "2602.01012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01012", "abs": "https://arxiv.org/abs/2602.01012", "authors": ["Yiyang Su", "Minchul Kim", "Jie Zhu", "Christopher Perry", "Feng Liu", "Anil Jain", "Xiaoming Liu"], "title": "LocalScore: Local Density-Aware Similarity Scoring for Biometrics", "comment": null, "summary": "Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLocalScore\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u753b\u5eca\u7279\u5f81\u5206\u5e03\u7684\u5c40\u90e8\u5bc6\u5ea6\u63d0\u5347\u5f00\u653e\u96c6\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u3001\u65e0\u5173\u7f51\u7edc\u7ed3\u6784\u548c\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u5f00\u653e\u96c6\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u5728\u9047\u5230\u672a\u767b\u8bb0\u7684\u4e2a\u4f53\u65f6\u8bc6\u522b\u80fd\u529b\u8f83\u5f31\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u56e0\u538b\u7f29\u540c\u4e00\u8eab\u4efd\u7684\u591a\u6837\u6027\u7279\u5f81\u800c\u5bfc\u81f4\u51b3\u7b56\u8fb9\u754c\u4e0d\u4f73\uff0c\u6025\u9700\u63d0\u5347\u7cfb\u7edf\u5bf9\u672a\u6ce8\u518c\u5bf9\u8c61\u7684\u5224\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51faLocalScore\u7b97\u6cd5\uff0c\u5f15\u5165k\u8fd1\u90bb\u601d\u60f3\uff0c\u5229\u7528\u753b\u5eca\u4e2d\u7279\u5f81\u7684\u5c40\u90e8\u5bc6\u5ea6\u4fe1\u606f\u8fdb\u884c\u5224\u5206\u3002\u65b9\u6cd5\u65e0\u9700\u6539\u53d8\u73b0\u6709\u7f51\u7edc\u7ed3\u6784\u6216\u635f\u5931\u51fd\u6570\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u5404\u7c7b\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "result": "LocalScore\u5728\u591a\u79cd\u6a21\u6001\u4e0b\u5b9e\u9a8c\uff0c\u5f00\u653e\u96c6\u68c0\u7d22FNIR@FPIR\u753153%\u964d\u81f340%\uff0c\u9a8c\u8bc1\u4efb\u52a1TAR@FAR\u753151%\u5347\u81f374%\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u5e38\u89c4\u65b9\u6cd5\u3002\u6587\u4e2d\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\uff0c\u8bf4\u660e\u5728\u4f55\u79cd\u6570\u636e\u96c6\u7279\u6027\u4e0b\u83b7\u5f97\u6700\u5927\u63d0\u5347\u3002", "conclusion": "LocalScore\u7b80\u5355\u6709\u6548\uff0c\u6781\u6613\u96c6\u6210\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5177\u6709\u5c0f\u6837\u672c\u53d8\u5f02\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2602.01771", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.01771", "abs": "https://arxiv.org/abs/2602.01771", "authors": ["Jingyao Wu", "Bin Lu", "Zijun Di", "Xiaoying Gan", "Meng Jin", "Luoyi Fu", "Xinbing Wang", "Chenghu Zhou"], "title": "<SOG_k>: One LLM Token for Explicit Graph Structural Understanding", "comment": null, "summary": "Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7279\u6b8atoken\u5c06\u56fe\u7ed3\u6784\u9ad8\u6548\u5730\u6574\u5408\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7406\u89e3\u548c\u63a8\u7406\u56fe\u6570\u636e\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u56fe\u7ed3\u6784\uff08\u5982\u793e\u4ea4\u7f51\u7edc\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\uff09\u65f6\u5b58\u5728\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u7684\u5c06\u56fe\u8f6c\u6210\u81ea\u7136\u8bed\u8a00\u6216\u8fde\u7eed\u5411\u91cf\u7684\u65b9\u5f0f\u90fd\u5b58\u5728\u6548\u7387\u548c\u8868\u8fbe\u4e0d\u5b8c\u5907\u7684\u7f3a\u9677\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u5206\u8bcd\u5668\uff0c\u5c06\u6bcf\u4e2a\u56fe\u7684\u7ed3\u6784\u6620\u5c04\u4e3a\u5355\u4e00\u7684\u7279\u6b8atoken\uff08<SOG_k>\uff09\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u6df7\u5408\u7ed3\u6784\u7684\u95ee\u7b54\u8bed\u6599\uff0c\u5b9e\u73b0\u65b0\u7ed3\u6784token\u548c\u6587\u672ctoken\u7684\u5bf9\u9f50\uff0c\u4fbf\u4e8e\u5927\u6a21\u578b\u51c6\u786e\u7406\u89e3\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u4e94\u4e2a\u56fe\u7ea7\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u65b9\u6cd5\u5e26\u67659.9%-41.4%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u5904\u7406\u56fe\u6570\u636e\u65f6\u7684\u7ed3\u6784\u8868\u8fbe\u96be\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u7ed3\u6784\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u7075\u6d3b\u6269\u5c55\u5230\u8282\u70b9\u7ea7\u4efb\u52a1\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01020", "abs": "https://arxiv.org/abs/2602.01020", "authors": ["Jichen Yang", "Jikai Zhang", "Benjamin Wildman-Tobriner", "Maciej A. Mazurowski"], "title": "Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning", "comment": "9 pages, 3 figures", "summary": "The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u6574\u7406\u7684\u7532\u72b6\u817a\u7ed3\u8282\u8d85\u58f0\u5f71\u50cf\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u5b66\u4e60\u764c\u75c7\u8bca\u65ad\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u52a8\u6574\u7406\u7684\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684AUC\u503c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7532\u72b6\u817a\u7ed3\u8282\u826f\u6076\u6027\u5206\u7c7b\u4e2d\u63a5\u8fd1\u6216\u8fbe\u5230\u533b\u751f\u6c34\u5e73\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u96be\u4ee5\u83b7\u5f97\uff0c\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u65b9\u6cd5\u4e9f\u9700\u8bc4\u4f30\u5176\u5b9e\u9645\u4ef7\u503c\u3002", "method": "\u6bd4\u8f83\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u3001\u5168\u91cf\u81ea\u52a8\u6574\u7406\u6570\u636e\u96c6\u53ca\u9ad8\u7cbe\u5ea6\u5b50\u96c6\u5206\u522b\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684AUC\u8868\u73b0\uff0c\u901a\u8fc7\u5927\u6837\u672c\u5b9e\u9a8c\u548c\u7edf\u8ba1\u5206\u6790\u9a8c\u8bc1\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u4eba\u5de5\u6807\u6ce8\u6587\u96c6\u8bad\u7ec3\u6a21\u578bAUC\u4e3a0.643\uff0c\u81ea\u52a8\u6574\u7406\u6570\u636e\u96c6AUC\u4e3a0.694\uff0c\u663e\u8457\u9ad8\u4e8e\u4eba\u5de5\u7ec4\u3002\u4f7f\u7528\u9ad8\u7cbe\u5ea6\u5b50\u96c6\uff08AUC 0.689\uff09\u4e0e\u5168\u91cf\u81ea\u52a8\u96c6\uff08AUC 0.694\uff09\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u81ea\u52a8\u6574\u7406\u7684\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u6240\u6709\u81ea\u52a8\u6574\u7406\u6570\u636e\u4f18\u4e8e\u53ea\u6311\u9009\u9ad8\u7cbe\u5ea6\u5b50\u96c6\uff0c\u5efa\u8bae\u7ed3\u5408\u5168\u90e8\u81ea\u52a8\u6570\u636e\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2602.01778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01778", "abs": "https://arxiv.org/abs/2602.01778", "authors": ["Kangtao Lv", "Jiwei Tang", "Langming Liu", "Haibin Chen", "Weidong Zhang", "Shilei Liu", "Yongwei Wang", "Yujin Yuan", "Wenbo Su", "Bo Zheng"], "title": "Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model", "comment": "15 pages,6 figures", "summary": "The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u6570\u636e\u5206\u5e03\u89d2\u5ea6\u7cfb\u7edf\u7814\u7a76\u4e86\u5176\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u4e0b\u6587\u538b\u7f29\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u53ef\u4f18\u5316\u538b\u7f29\u6548\u679c\u7684\u5efa\u8bae\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u4fe1\u606f\u5197\u4f59\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u7814\u7a76\u591a\u805a\u7126\u4e8e\u6a21\u578b\u6539\u8fdb\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u5206\u5e03\u5bf9\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u5f71\u54cd\u3002\u8be5\u6587\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u4ece\u8f93\u5165\u6570\u636e\u548c\u6a21\u578b\u5185\u5728\u6570\u636e\u4e24\u65b9\u9762\uff0c\u5229\u7528autoencoder\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5206\u6790\u6570\u636e\u5206\u5e03\u5bf9\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u538b\u7f29\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a\uff081\uff09\u7f16\u7801\u5668\u6d4b\u5f97\u7684\u8f93\u5165\u71b5\u4e0e\u538b\u7f29\u8d28\u91cf\u5448\u8d1f\u76f8\u5173\uff0c\u800c\u51bb\u7ed3\u89e3\u7801\u5668\u65f6\uff0c\u89e3\u7801\u5668\u7684\u71b5\u4e0e\u538b\u7f29\u8d28\u91cf\u65e0\u663e\u8457\u6027\u5173\u7cfb\uff1b\uff082\uff09\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u5728\u6570\u636e\u5dee\u8ddd\u4f1a\u663e\u8457\u964d\u4f4e\u538b\u7f29\u6536\u76ca\uff0c\u4e14\u96be\u4ee5\u7f13\u89e3\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u5206\u5e03\u5bf9\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u5e76\u7ed9\u51fa\u4e86\u4f18\u5316\u538b\u7f29\u6548\u679c\u7684\u5b9e\u9645\u5efa\u8bae\u3002"}}
{"id": "2602.01033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01033", "abs": "https://arxiv.org/abs/2602.01033", "authors": ["Chentian Sun"], "title": "GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration", "comment": "A 5-page paper with 1 figure, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.", "AI": {"tldr": "GMAC\u662f\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u5efa\u6a21\u548c\u4eba\u5de5\u6807\u5b9a\u7684\u591a\u76f8\u673a\u5916\u53c2\u81ea\u52a8\u4f30\u8ba1\u7b97\u6cd5\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u91cd\u5efa\u7f51\u7edc\u9690\u5f0f\u51e0\u4f55\u7279\u5f81\uff0c\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\u4e5f\u80fd\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u51c6\u786e\u5730\u5b9e\u73b0\u591a\u76f8\u673a\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709\u591a\u76f8\u673a\u5916\u53c2\u6570\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u6807\u5b9a\u7269\u3001\u51e0\u4f55\u5efa\u6a21\u6216\u4e13\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u548c\u5728\u7ebf\u573a\u666f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51faGMAC\u6846\u67b6\uff0c\u5c06\u5916\u53c2\u4f5c\u4e3a\u5168\u5c40\u53d8\u91cf\u5d4c\u5165\u5230\u591a\u89c6\u89d2\u91cd\u5efa\u7f51\u7edc\u7684\u9690\u5f0f\u51e0\u4f55\u8868\u793a\u4e2d\uff0c\u4e0d\u9700\u5168\u65b0\u7f51\u7edc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7cbe\u7b80\u7ed3\u6784\u548c\u8f7b\u91cf\u7ea7\u56de\u5f52\u5934\u5b9e\u73b0\u76f4\u63a5\u5916\u53c2\u9884\u6d4b\u3002\u540c\u65f6\u8054\u5408\u4f18\u5316\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u548c\u591a\u89c6\u89d2\u5faa\u73af\u4e00\u81f4\u6027\uff0c\u4ee5\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u591a\u76f8\u673a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cGMAC\u80fd\u591f\u65e0\u987b\u660e\u786e3D\u91cd\u5efa\u548c\u624b\u52a8\u6807\u5b9a\uff0c\u51c6\u786e\u3001\u7a33\u5b9a\u5730\u5b8c\u6210\u591a\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\u3002", "conclusion": "GMAC\u4e3a\u591a\u76f8\u673a\u7cfb\u7edf\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u5728\u7ebf\u6807\u5b9a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2602.01785", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01785", "abs": "https://arxiv.org/abs/2602.01785", "authors": ["Yuling Shi", "Chaoxiang Xie", "Zhensu Sun", "Yeheng Chen", "Chenxu Zhang", "Longfei Yun", "Chengcheng Wan", "Hongyu Zhang", "David Lo", "Xiaodong Gu"], "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "comment": "Code and data are available at https://github.com/YerbaPage/CodeOCR", "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u5c06\u6e90\u7801\u4ee5\u56fe\u50cf\u5f62\u5f0f\u8f93\u5165\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u4ee5\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u6548\u7387\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u663e\u8457\u7684token\u538b\u7f29\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u589e\u5927\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u7684\u5927\u6a21\u578b\u9047\u5230\u4e0a\u4e0b\u6587\u7ebf\u6027\u6269\u5c55\u5bfc\u81f4\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u6e90\u7801\u8868\u793a\u65b9\u6cd5\uff0c\u5229\u7528MLLMs\u5bf9\u56fe\u50cf\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u6e90\u7801\u6e32\u67d3\u4e3a\u56fe\u7247\u5f62\u5f0f\u4ee5\u538b\u7f29token\u6570\u3002", "method": "\u4f5c\u8005\u5c06\u6e90\u7801\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u8c03\u6574\u5206\u8fa8\u7387\u4ee5\u5b9e\u73b0token\u538b\u7f29\uff0c\u5e76\u5c06\u8fd9\u4e9b\u56fe\u50cf\u53ca\u5176\u8bed\u6cd5\u9ad8\u4eae\u9001\u5165\u591a\u6a21\u6001\u5927\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u538b\u7f29\u6bd4\u3001\u7406\u89e3\u80fd\u529b\u548c\u8bed\u6cd5\u9ad8\u4eae\u7b49\u591a\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0MLLM\u5bf9\u6e90\u7801\u6587\u4ef6\u53ef\u5b9e\u73b0\u6700\u9ad88\u500d\u7684token\u538b\u7f29\uff0c\u5e76\u80fd\u501f\u52a9\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u8bed\u6cd5\u9ad8\u4eae\uff09\u63d0\u53474\u500d\u538b\u7f29\u4e0b\u7684\u8865\u5168\u8868\u73b0\u3002\u5bf9\u4e8e\u514b\u9686\u68c0\u6d4b\u7b49\u4efb\u52a1\uff0c\u5728\u591a\u79cd\u538b\u7f29\u6bd4\u4f8b\u4e0b\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u6587\u672c\u8f93\u5165\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u901a\u8fc7\u56fe\u50cf\u5316\u4ee3\u7801\u8868\u793a\u5b9e\u73b0\u9ad8\u6548\u4ee3\u7801\u7406\u89e3\u6210\u4e3a\u53ef\u80fd\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5de8\u5927\u7684\u6f5c\u529b\u548c\u5f53\u524d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u91c7\u7528\u56fe\u50cf\u6a21\u6001\u8fdb\u884c\u5927\u89c4\u6a21\u4ee3\u7801\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.01035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01035", "abs": "https://arxiv.org/abs/2602.01035", "authors": ["Chentian Sun"], "title": "FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence", "comment": "A 5-page paper, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FUSE-Flow\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u7684\u591a\u89c6\u89d2\u70b9\u4e91\u91cd\u5efa\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\uff0c\u652f\u6301\u591a\u6444\u50cf\u5934\u6269\u5c55\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u591a\u76f8\u673a\u53ca\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u4f20\u611f\u5668\u786c\u4ef6\u666e\u53ca\uff0c\u4f46\u5982\u4f55\u5728\u4e25\u683c\u7684\u5b9e\u65f6\u8981\u6c42\u4e0b\u5c06\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u6df1\u5ea6\u6570\u636e\u878d\u5408\u4e3a\u9ad8\u8d28\u91cf\u70b9\u4e91\uff0c\u4ecd\u9762\u4e34\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u5185\u5b58\u6d88\u8017\u5927\u3001\u53ef\u6269\u5c55\u6027\u5dee\u7b49\u6311\u6218\uff0c\u96be\u4ee5\u540c\u65f6\u517c\u987e\u5b9e\u65f6\u6027\u80fd\u3001\u91cd\u5efa\u8d28\u91cf\u4ee5\u53ca\u591a\u6444\u50cf\u5934\u6269\u5c55\u3002", "method": "\u63d0\u51faFUSE-Flow\uff0c\u91c7\u7528\u9010\u5e27\u3001\u65e0\u72b6\u6001\u4e14\u7ebf\u6027\u53ef\u6269\u5c55\u7684\u70b9\u4e91\u6d41\u5f0f\u91cd\u5efa\u65b9\u5f0f\u3002\u6bcf\u5e27\u72ec\u7acb\u751f\u6210\u70b9\u4e91\u7247\u6bb5\uff0c\u901a\u8fc7\u6d4b\u91cf\u7f6e\u4fe1\u5ea6\u4e0e3D\u8ddd\u79bb\u4e00\u81f4\u6027\u53cc\u6743\u91cd\u8fdb\u884c\u566a\u58f0\u6291\u5236\u4e0e\u7ec6\u8282\u4fdd\u6301\u3002\u9488\u5bf9\u5927\u89c4\u6a21\u591a\u6444\u50cf\u5934\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7a7a\u95f4\u54c8\u5e0c\u52a0\u6743\u805a\u5408\uff0c\u6309\u70b9\u4e91\u5bc6\u5ea6\u5212\u5206\u7a7a\u95f4\u5e76\u6311\u9009\u4ee3\u8868\u70b9\uff0c\u8fdb\u884c\u52a0\u6743\u878d\u5408\u3002\u5168\u6d41\u7a0bGPU\u5e76\u884c\u5316\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u3001\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFUSE-Flow\u5728\u91cd\u53e0\u3001\u6df1\u5ea6\u4e0d\u8fde\u7eed\u53ca\u52a8\u6001\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7a33\u5b9a\u6027\u4e0e\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u5e76\u80fd\u5728\u73b0\u4ee3GPU\u4e0a\u6301\u7eed\u4fdd\u6301\u5b9e\u65f6\u5e27\u7387\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "FUSE-Flow\u9ad8\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u70b9\u4e91\u5b9e\u65f6\u91cd\u5efa\u7684\u96be\u9898\uff0c\u517c\u987e\u4e86\u5b9e\u65f6\u6027\u80fd\u3001\u91cd\u5efa\u8d28\u91cf\u548c\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u6269\u5c55\u6027\uff0c\u5bf9\u6c89\u6d78\u5f0f\u8ba1\u7b97\u3001\u673a\u5668\u4eba\u5bfc\u822a\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01807", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01807", "abs": "https://arxiv.org/abs/2602.01807", "authors": ["DongNyeong Heo", "Heelyoul Choi"], "title": "Sentence Curve Language Models", "comment": null, "summary": "Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53e5\u5b50\u8868\u793a\u65b9\u6cd5\u201c\u53e5\u5b50\u66f2\u7ebf\u201d\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86SCLM\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5bf9\u76ee\u6807\u8bcd\u7684\u5d4c\u5165\u662f\u9759\u6001\u7684\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u6355\u6349\u76ee\u6807\u53e5\u5b50\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u5168\u5c40\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u7528\u6837\u6761\u66f2\u7ebf\uff08sentence curve\uff09\u5bf9\u6574\u4e2a\u53e5\u5b50\u8fdb\u884c\u8fde\u7eed\u8868\u793a\uff0c\u8fd9\u79cd\u8868\u793a\u7684\u65b9\u6cd5\u901a\u8fc7\u591a\u4e2a\u63a7\u5236\u70b9\u5f71\u54cd\u53e5\u4e2d\u591a\u4e2a\u8bcd\uff0c\u63d0\u5347\u5bf9\u53e5\u5b50\u6574\u4f53\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b64\u8868\u793a\u7684SCLM\uff08Sentence Curve Language Model\uff09\uff0c\u8ba9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u53e5\u5b50\u66f2\u7ebf\u800c\u4e0d\u662f\u72ec\u7acb\u7684\u8bcd\u5d4c\u5165\u3002\u540c\u65f6\u7406\u8bba\u5206\u6790\u4e86\u53e5\u5b50\u66f2\u7ebf\u9884\u6d4b\u5e26\u6765\u7684\u6b63\u5219\u5316\u6548\u679c\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u4e0d\u540c\u66f2\u7ebf\u7c7b\u578b\u7684\u5f71\u54cd\u3002", "result": "SCLM\u5728IWSLT14\u548cWMT14\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u76ee\u524d\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6700\u4f18\u8868\u73b0\uff08SOTA\uff09\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u7a33\u5b9a\uff0c\u65e0\u9700\u590d\u6742\u7684\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\uff0c\u5728LM1B\u6570\u636e\u96c6\u4e0a\u4e0e\u79bb\u6563\u578b\u6269\u6563\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u4e5f\u5c55\u73b0\u51fa\u6709\u6f5c\u529b\u7684\u8868\u73b0\u3002", "conclusion": "\u65b0\u7684\u53e5\u5b50\u66f2\u7ebf\u8868\u793a\u53caSCLM\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u5176\u5bf9\u53e5\u5b50\u5168\u5c40\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u601d\u8def\u3002"}}
{"id": "2602.01037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01037", "abs": "https://arxiv.org/abs/2602.01037", "authors": ["Guangshuo Qin", "Zhiteng Li", "Zheng Chen", "Weihang Zhang", "Linghe Kong", "Yulun Zhang"], "title": "VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models", "comment": null, "summary": "Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\\% on Kimi-VL and 3.09\\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u4e13\u5bb6\u91cf\u5316\u65b9\u6cd5\uff08VEQ\uff09\uff0c\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00MoE\u6a21\u578b\u7684\u91cf\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u540e\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6df7\u5408\u4e13\u5bb6\uff08MoE VLMs\uff09\u867d\u7136\u6027\u80fd\u7a81\u51fa\uff0c\u4f46\u56e0\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u96be\u4ee5\u63a8\u5e7f\u5e94\u7528\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5bf9\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u95f4\u5dee\u5f02\u53ca\u4e0d\u540c\u4e13\u5bb6\u8d21\u732e\u5ea6\u7684\u5f02\u8d28\u6027\u8ba4\u8bc6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u56e0\u6b64\u4e9f\u9700\u8bbe\u8ba1\u66f4\u9002\u5e94\u8fd9\u79cd\u5f02\u8d28\u6027\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u89c6\u89c9\u4e13\u5bb6\u91cf\u5316\uff08VEQ\uff09\u201d\uff0c\u5305\u62ec\u4e24\u90e8\u5206\uff1a1\uff09\u6a21\u6001-\u4e13\u5bb6\u611f\u77e5\u91cf\u5316\uff0c\u5229\u7528\u4e13\u5bb6\u6fc0\u6d3b\u9891\u7387\u5bf9\u4e0d\u540c\u4e13\u5bb6\u8d4b\u4e88\u5dee\u5f02\u5316\u8bef\u5dee\u4f18\u5148\u7ea7\uff1b2\uff09\u6a21\u6001\u4eb2\u548c\u611f\u77e5\u91cf\u5316\uff0c\u7ed3\u5408token\u4e0e\u4e13\u5bb6\u7684\u4eb2\u548c\u5ea6\u53ca\u6a21\u6001\u4fe1\u606f\uff0c\u57fa\u4e8e\u589e\u5f3aHessian\u77e9\u9635\u5f15\u5bfc\u6821\u51c6\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u91cf\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVEQ\u5728\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u3001\u76f4\u63a5\u91cf\u5316\u540e\uff0c\u7a33\u5b9a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684SOTA\u65b9\u6cd5\u3002\u5728W3A16\u91cf\u5316\u914d\u7f6e\u4e0b\uff0cKimi-VL\u548cQwen3-VL\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53472.04%\u548c3.09%\u3002", "conclusion": "VEQ\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86MoE\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u538b\u529b\uff0c\u5e76\u4e14\u91cf\u5316\u540e\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u5c55\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u5065\u58ee\u6027\uff0c\u5bf9\u5b9e\u9645\u5927\u6a21\u578b\u90e8\u7f72\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.01838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01838", "abs": "https://arxiv.org/abs/2602.01838", "authors": ["Abdelrahman Mansour", "Khaled W. Alshaer", "Moataz Elsaban"], "title": "AXE: Low-Cost Cross-Domain Web Structured Information Extraction", "comment": null, "summary": "Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized \"pruning\" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.", "AI": {"tldr": "AXE\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u88c1\u526aHTML DOM\u6811\u6765\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u8ba9\u5c0f\u578bLLM\u5b9e\u73b0\u9ad8\u6548\u7cbe\u51c6\u7684\u6570\u636e\u62bd\u53d6\uff0c\u4e14\u4fdd\u8bc1\u6eaf\u6e90\u53ef\u8ffd\u8e2a\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7f51\u9875\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u5728\u624b\u5de5\u89c4\u5219\u4e0e\u5927\u6a21\u578b\u9ad8\u6210\u672c\u4e4b\u95f4\u6743\u8861\uff0c\u7f3a\u4e4f\u540c\u65f6\u517c\u5177\u7b80\u6613\u6027\u3001\u4f4e\u6210\u672c\u548c\u9ad8\u53ef\u9760\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "AXE\u5c06HTML DOM\u89c6\u4e3a\u9700\u8981\u4fee\u526a\u7684\u6811\uff0c\u901a\u8fc7\u53bb\u9664\u65e0\u5173\u8282\u70b9\uff0c\u4fdd\u7559\u9ad8\u5bc6\u5ea6\u5173\u952e\u4fe1\u606f\uff0c\u518d\u75280.6B\u53c2\u6570\u7684\u5c0f\u578bLLM\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u5e76\u901a\u8fc7Grounded XPath Resolution\uff08GXR\uff09\u673a\u5236\u786e\u4fdd\u6bcf\u4e2a\u62bd\u53d6\u7ed3\u679c\u90fd\u80fd\u8ffd\u6eaf\u81f3DOM\u539f\u8282\u70b9\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "AXE\u5728SWDE\u6570\u636e\u96c6\u4e0a\u4ee5F1 88.1%\u8fbe\u5230\u4e86SOTA\u96f6\u6837\u672c\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u4e14\u7ecf\u8fc7\u5b8c\u5168\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "AXE\u4ee5\u6781\u4f4e\u7684\u8d44\u6e90\u6d88\u8017\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u5ea6\u3001\u53ef\u8ffd\u6eaf\u7684\u7f51\u9875\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u517c\u5177\u7684\u73b0\u5b9e\u8def\u5f84\u3002"}}
{"id": "2602.01038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01038", "abs": "https://arxiv.org/abs/2602.01038", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Andrea Colaco"], "title": "From Videos to Conversations: Egocentric Instructions for Task Assistance", "comment": null, "summary": "Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u5316\u4e3a\u53cc\u4eba\u591a\u6a21\u6001\u4efb\u52a1\u6307\u5bfc\u5bf9\u8bdd\u7684\u6846\u67b6\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u4e86HowToDIV\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001AR\u8f85\u52a9AI\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4efb\u52a1\u5bf9\u8bdd\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7b49\u5e94\u7528\u4e9f\u9700AI\u4efb\u52a1\u6307\u5bfc\u52a9\u624b\uff0c\u4f46\u73b0\u5b9e\u7684\u591a\u6a21\u6001\u4efb\u52a1\u5bf9\u8bdd\u6570\u636e\u6781\u5176\u7a00\u7f3a\uff0c\u4eba\u5de5\u91c7\u96c6\u6210\u672c\u9ad8\u3001\u96be\u5ea6\u5927\uff0c\u9650\u5236\u4e86\u9886\u57df\u8fdb\u5c55\u3002\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u751f\u6210\u65b9\u5f0f\u4ee5\u4e30\u5bcc\u76f8\u5173\u6570\u636e\u8d44\u6e90\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u6d41\u7a0b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5355\u4eba\u7684\u6559\u5b66\u89c6\u9891\u81ea\u52a8\u8f6c\u6362\u6210\u5305\u542b\u4e13\u5bb6-\u65b0\u624b\u591a\u8f6e\u4ea4\u6d41\u7684\u53cc\u4eba\u5bf9\u8bdd\uff0c\u540c\u65f6\u540c\u6b65\u4e0e\u89c6\u9891\u5185\u5bb9\u5bf9\u5e94\u3002\u57fa\u4e8e\u6b64\u6d41\u7a0b\uff0c\u4f5c\u8005\u751f\u6210\u4e86HowToDIV\u6570\u636e\u96c6\uff1a\u5305\u62ec507\u573a\u5bf9\u8bdd\u30016,636\u4e2a\u95ee\u7b54\u3001\u603b\u65f6\u957f24\u5c0f\u65f6\u7684\u89c6\u9891\uff0c\u8986\u76d6\u591a\u4e2a\u5b9e\u9645\u4efb\u52a1\u9886\u57df\u3002", "result": "\u6784\u5efa\u4e86HowToDIV\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u7528Gemma 3\u548cQwen 2.5\u6a21\u578b\u8fdb\u884c\u4e86\u4efb\u52a1\u57fa\u7ebf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u591a\u6a21\u6001\u7a0b\u5e8f\u5316\u4efb\u52a1\u52a9\u624b\u7814\u7a76\u7684\u4ef7\u503c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\u4e3a\u6536\u96c6\u771f\u5b9e\u3001\u590d\u6742\u4efb\u52a1\u5bf9\u8bdd\u6570\u636e\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u6269\u5c55\u6027\u7684\u65b0\u8def\u5f84\uff0c\u4e3a\u540e\u7eed\u591a\u6a21\u6001AI\u52a9\u624b\u548cAR\u8f85\u52a9\u4efb\u52a1\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.01840", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01840", "abs": "https://arxiv.org/abs/2602.01840", "authors": ["Jiwei Tang", "Shilei Liu", "Zhicheng Zhang", "Qingsong Lv", "Runsong Zhao", "Tingwei Lu", "Langming Liu", "Haibin Chen", "Yujin Yuan", "Hai-Tao Zheng", "Wenbo Su", "Bo Zheng"], "title": "Read As Human: Compressing Context via Parallelizable Close Reading and Skimming", "comment": "13 pages,5 figures", "summary": "Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAM\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u9605\u8bfb\u7b56\u7565\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u6548\u7387\u4e0e\u6548\u679c\uff0c\u5728\u591a\u4e2a\u95ee\u7b54\u548c\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u9ad8\u6027\u80fd\u4e14\u5927\u5e45\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\uff0c\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u5197\u4f59\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u9ad8\u6027\u80fd\u3001\u901f\u5ea6\u548c\u89e3\u91ca\u6027\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u7814\u53d1\u51fa\u66f4\u9ad8\u6548\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u9605\u8bfb\u4e60\u60ef\u7684\u5904\u7406\u6846\u67b6\u3002", "method": "RAM\u501f\u9274\u4eba\u7c7b\u9605\u8bfb\u4e60\u60ef\uff0c\u5c06\u6587\u672c\u5206\u6bb5\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u6df7\u5408\u9605\u8bfb\u7b56\u7565\uff1a\u5bf9\u4e0e\u67e5\u8be2\u9ad8\u5ea6\u76f8\u5173\u7684\u6bb5\u843d\u201c\u7cbe\u8bfb\u201d\uff0c\u800c\u5bf9\u76f8\u5173\u6027\u4f4e\u7684\u6bb5\u843d\u201c\u7565\u8bfb\u201d\u5e76\u538b\u7f29\u6210\u6458\u8981\u5411\u91cf\uff0c\u518d\u5c06\u4e8c\u8005\u62fc\u63a5\u540e\u8f93\u5165\u89e3\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u6b63\u8d1f\u6837\u672c\u5bf9\u6bd4\u5b66\u4e60\u7684\u76ee\u6807\uff0c\u63d0\u5347\u4e86\u5bf9\u7cbe\u8bfb\u548c\u7565\u8bfb\u754c\u9650\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u548c\u6458\u8981\u4efb\u52a1\u7684\u6d4b\u8bd5\u4e2d\uff0cRAM\u5728\u4e24\u79cd\u4e3b\u6d41\u5e95\u5ea7\u6a21\u578b\u4e0b\u90fd\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u6700\u957f\u8fbe32K\u5b57\u7684\u8f93\u5165\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe12\u500d\u3002", "conclusion": "RAM\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u5904\u7406\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4f7f\u5f97\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u517c\u987e\u6027\u80fd\u548c\u901f\u5ea6\uff0c\u5e76\u5177\u5907\u8f83\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.01046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01046", "abs": "https://arxiv.org/abs/2602.01046", "authors": ["Jiawei Lin", "Shizhao Sun", "Danqing Huang", "Ting Liu", "Ji Li", "Jiang Bian"], "title": "ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction", "comment": null, "summary": "Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4e09\u5143\u7ec4\u6570\u636e\u7684\u81ea\u52a8\u8bbe\u8ba1\u5e03\u5c40\u7f16\u8f91\u6846\u67b6ReLayout\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f16\u8f91\u8d28\u91cf\u3001\u51c6\u786e\u6027\u548c\u5e03\u5c40\u7ed3\u6784\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u5316\u91cd\u65b0\u8bbe\u8ba1\uff0c\u65e0\u9700\u4eba\u5de5\u8c03\u6574\uff0c\u662f\u63d0\u5347\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u7684\u5173\u952e\u3002\u73b0\u6709\u8bbe\u8ba1\u5e03\u5c40\u7f16\u8f91\u9762\u4e34\u7528\u6237\u610f\u56fe\u8868\u8fbe\u6a21\u7cca\u548c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7b49\u96be\u9898\u3002\u8bba\u6587\u65e8\u5728\u8ba9\u7f16\u8f91\u64cd\u4f5c\u66f4\u81ea\u52a8\u3001\u667a\u80fd\uff0c\u5b9e\u73b0\u7b26\u5408\u7528\u6237\u610f\u56fe\u53c8\u80fd\u4fdd\u7559\u539f\u6709\u7ed3\u6784\u7684\u5e03\u5c40\u7f16\u8f91\u3002", "method": "1\uff09\u6807\u51c6\u5316\u56db\u79cd\u57fa\u7840\u4e14\u91cd\u8981\u7684\u7f16\u8f91\u64cd\u4f5c\uff0c\u89c4\u8303\u7f16\u8f91\u6d41\u7a0b\u30022\uff09\u5f15\u5165\u5173\u7cfb\u56fe\u6765\u7ea6\u675f\u672a\u7f16\u8f91\u5143\u7d20\u7684\u5e03\u5c40\u7ed3\u6784\uff0c\u9632\u6b62\u8bbe\u8ba1\u6df7\u4e71\u30023\uff09\u63d0\u51fa\u5173\u7cfb\u611f\u77e5\u8bbe\u8ba1\u91cd\u5efa\uff08RADR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\uff0c\u4ec5\u5229\u7528\u539f\u59cb\u8bbe\u8ba1\u5143\u7d20\u3001\u5173\u7cfb\u56fe\u548c\u5408\u6210\u7684\u64cd\u4f5c\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u4e09\u5143\u7ec4\u6570\u636e\u30024\uff09\u91c7\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u7edf\u4e00\u5b9e\u73b0\u591a\u79cd\u7f16\u8f91\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u7f16\u8f91\u8d28\u91cf\u3001\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u7559\u6027\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff1b\u7528\u6237\u8c03\u7814\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ReLayout\u53ef\u4ee5\u5b9e\u73b0\u591a\u79cd\u7f16\u8f91\u64cd\u4f5c\u4e14\u5bf9\u7ed3\u6784\u6709\u826f\u597d\u4fdd\u7559\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f9d\u65e7\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u667a\u80fd\u5e03\u5c40\u7f16\u8f91\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01875", "abs": "https://arxiv.org/abs/2602.01875", "authors": ["Langming Liu", "Kangtao Lv", "Haibin Chen", "Weidong Zhang", "Yejing Wang", "Shilei Liu", "Xin Tong", "Yujin Yuan", "Yongwei Wang", "Wenbo Su", "Bo Zheng"], "title": "PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning", "comment": null, "summary": "Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of \"low-probability truth\" and \"high-probability falsehood\". Recent approaches, such as teaching models to say \"I don't know\" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \\textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is \"\\textbf{debiasing then learning}.\" It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making \"room\" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u9884\u8bad\u7ec3\u6846\u67b6PretrainRL\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u53d1\u751f\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u771f\u5047\u5206\u5e03\u4e0d\u5747\uff0c\u6a21\u578b\u66f4\u6613\u5b66\u5230\u9519\u8bef\u4f46\u9ad8\u6982\u7387\u7684\u4fe1\u606f\uff0c\u5bf9\u4f4e\u6982\u7387\u4e8b\u5b9e\u7684\u5b66\u4e60\u5374\u8f83\u5f31\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u8ba9\u6a21\u578b\u56de\u7b54\u201c\u6211\u4e0d\u77e5\u9053\u201d\u6216\u4e8b\u540e\u77e5\u8bc6\u7f16\u8f91\u4e0d\u80fd\u4ece\u6839\u672c\u89e3\u51b3\u95ee\u9898\uff0c\u6709\u7684\u8fd8\u4f1a\u5bfc\u81f4\u9057\u5fd8\u539f\u6709\u77e5\u8bc6\u3002", "method": "\u4f5c\u8005\u63d0\u51faPretrainRL\u6846\u67b6\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u9884\u8bad\u7ec3\u9636\u6bb5\u3002\u6838\u5fc3\u7406\u5ff5\u662f\u201c\u53bb\u504f\u518d\u5b66\u4e60\u201d\uff1b\u901a\u8fc7\u6709\u7b56\u7565\u5730\u964d\u4f4e\u9ad8\u6982\u7387\u9519\u8bef\u7b54\u6848\u6743\u91cd\uff0c\u63d0\u5347\u6a21\u578b\u5b66\u4e60\u5230\u4f4e\u6982\u7387\u771f\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u65b9\u6cd5\u5305\u62ec\u9ad8\u6548\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u6316\u6398\u9ad8\u6982\u7387\u5047\u4fe1\u606f\u3001\u5f15\u5165\u65b0\u6982\u7387\u5ea6\u91cf\u6307\u6807\u7b49\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPretrainRL\u663e\u8457\u51cf\u5c11\u4e86\u5927\u6a21\u578b\u7684\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u5e76\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u5730\u6539\u53d8\u9884\u8bad\u7ec3\u8fc7\u7a0b\u6982\u7387\u5206\u5e03\uff0cPretrainRL\u80fd\u4ece\u6839\u672c\u6027\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u4e3a\u6d88\u9664\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01047", "abs": "https://arxiv.org/abs/2602.01047", "authors": ["Xinrong Chen", "Xu Chu", "Yingmin Qiu", "Hengyuan Zhang", "Jing Xiong", "Shiyu Tang", "Shuai Liu", "Shaokang Yang", "Cheng Yang", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance", "comment": null, "summary": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Residual Decoding (ResDec)\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u51cf\u5c11\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u5347\u89c6\u89c9\u4fe1\u606f\u7684\u771f\u5b9e\u6027\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "LVLM\u867d\u7136\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u8bed\u8a00\u5148\u9a8c\u5f71\u54cd\uff0c\u5bb9\u6613\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u65e0\u5173\u7684\u5e7b\u89c9\u5185\u5bb9\u3002\u8fd9\u79cd\u5e7b\u89c9\u524a\u5f31\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u6709\u6548\u7684\u6291\u5236\u65b9\u6cd5\u3002", "method": "ResDec\u662f\u4e00\u79cd\u521b\u65b0\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002\u5b83\u5728\u6a21\u578b\u89e3\u7801\u9636\u6bb5\u5229\u7528\u5386\u53f2\u4fe1\u606f\uff0c\u7ed3\u5408LVLM\u5185\u90e8\u9690\u5f0f\u63a8\u7406\u53catoken logits\u968f\u65f6\u95f4\u6f14\u5316\u7684\u673a\u5236\uff0c\u52a8\u6001\u4fee\u6b63\u8bed\u8a00\u5148\u9a8c\u5e26\u6765\u7684\u504f\u5dee\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u53d1\u751f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cResDec\u663e\u8457\u6291\u5236\u4e86\u56e0\u8bed\u8a00\u5148\u9a8c\u5bfc\u81f4\u7684\u5e7b\u89c9\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u951a\u5b9a\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u5bf9\u8c61\u5e7b\u89c9\u3002\u540c\u65f6\uff0c\u5728\u591a\u9879LVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "ResDec\u4e3a\u6291\u5236LVLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u65b0\u601d\u8def\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5177\u5907\u5b9e\u9645\u63a8\u5e7f\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01885", "abs": "https://arxiv.org/abs/2602.01885", "authors": ["Tiantian Chen", "Jiaqi Lu", "Ying Shen", "Lin Zhang"], "title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support", "comment": "12 pages, 7 figures. Accepted to The Web Conference (WWW) 2026", "summary": "Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faES-MemEval\u57fa\u51c6\u53caEvoEmo\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u957f\u671f\u60c5\u611f\u652f\u6301\u4e2d\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5b9e\u9a8c\u5c55\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u4e0e\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u5bf9\u8bdd\u57fa\u51c6\u4e3b\u8981\u805a\u7126\u4e8e\u9759\u6001\u4e8b\u5b9e\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u573a\u666f\u4e0b\u7528\u6237\u4fe1\u606f\u5206\u6563\u3001\u9690\u542b\u4e14\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5982\u5728\u7ebf\u60c5\u611f\u652f\u6301\u7b49\u590d\u6742\u5e94\u7528\u4e2d\u66f4\u4e3a\u7a81\u51fa\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b0\u7684\u8bc4\u6d4b\u6807\u51c6\u4e0e\u6570\u636e\u96c6\u6765\u771f\u5b9e\u8bc4\u4ef7\u5927\u6a21\u578b\u5728\u957f\u671f\u8bb0\u5fc6\u4e0e\u7528\u6237\u5efa\u6a21\u4e0a\u7684\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86ES-MemEval\u8fd9\u4e00\u65b0\u57fa\u51c6\uff0c\u6db5\u76d6\u4fe1\u606f\u63d0\u53d6\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u51b2\u7a81\u68c0\u6d4b\u3001\u56de\u907f\u4e0e\u7528\u6237\u5efa\u6a21\u7b49\u4e94\u9879\u957f\u671f\u8bb0\u5fc6\u6838\u5fc3\u80fd\u529b\uff0c\u4efb\u52a1\u5305\u542b\u95ee\u7b54\u3001\u6458\u8981\u4e0e\u5bf9\u8bdd\u751f\u6210\u3002\u540c\u65f6\u6784\u5efa\u4e86EvoEmo\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u6a21\u62df\u957f\u671f\u4e2a\u6027\u5316\u60c5\u611f\u652f\u6301\u573a\u666f\u3002", "result": "\u5bf9\u6bd4\u591a\u79cd\u957f\u4e0a\u4e0b\u6587LLM\u3001\u5546\u7528\u6a21\u578b\u548cRAG\u6a21\u578b\u540e\u53d1\u73b0\uff1a\u663e\u5f0f\u957f\u671f\u8bb0\u5fc6\u5bf9\u4e8e\u63d0\u5347\u4e2a\u6027\u5316\u548c\u51cf\u5c11\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff1bRAG\u65b9\u6cd5\u63d0\u5347\u4e86\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u4f46\u9762\u5bf9\u65f6\u5e8f\u548c\u52a8\u6001\u7528\u6237\u4fe1\u606f\u65f6\u8868\u73b0\u6b20\u4f73\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u957f\u8bb0\u5fc6\u65b9\u6cd5\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff0c\u5355\u9760\u68c0\u7d22\u6216\u6269\u5c55\u4e0a\u4e0b\u6587\u96be\u4ee5\u6ee1\u8db3\u52a8\u6001\u3001\u9690\u5f0f\u548c\u957f\u671f\u7528\u6237\u5efa\u6a21\u9700\u6c42\u3002\u672a\u6765\u9700\u66f4\u6df1\u5ea6\u96c6\u6210\u8bb0\u5fc6\u4e0e\u68c0\u7d22\u673a\u5236\u4ee5\u652f\u6301\u4e2a\u6027\u5316\u3001\u957f\u65f6\u5bf9\u8bdd\u7cfb\u7edf\u3002"}}
{"id": "2602.01055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01055", "abs": "https://arxiv.org/abs/2602.01055", "authors": ["Bo Deng", "Yitong Tang", "Jiake Li", "Yuxin Huang", "Li Wang", "Yu Zhang", "Yufei Zhan", "Hua Lu", "Xiaoshen Zhang", "Jieyun Bai"], "title": "Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis", "comment": null, "summary": "Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \\href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u8d85\u58f0\u5f71\u50cf\u5206\u6790\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5728FM_UIA~2026\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0c\u5960\u5b9a\u4e86\u8d85\u58f0\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u7814\u7a76\u7684\u91cd\u8981\u57fa\u7840\u3002", "motivation": "\u8d85\u58f0\u5f71\u50cf\u5206\u6790\u9762\u4e34\u89e3\u5256\u7ed3\u6784\u548c\u91c7\u96c6\u534f\u8bae\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u901a\u7528\u5206\u6790\u6a21\u578b\u3002\u76ee\u524d\u591a\u6570\u65b9\u6cd5\u4efb\u52a1\u5355\u4e00\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u53ef\u540c\u65f6\u9002\u5e94\u591a\u4efb\u52a1\u3001\u591a\u573a\u666f\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86FM_UIA~2026\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u57fa\u51c6\uff0c\u6db5\u76d6\u5206\u5272\u3001\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u56de\u5f52\u7b4927\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u591a\u5934\u591a\u4efb\u52a1\u5b66\u4e60\uff08MH-MTL\uff09\u7f51\u7edc\u3002\u6a21\u578b\u91c7\u7528EfficientNet-B4\u4e3a\u4e3b\u5e72\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u91d1\u5b57\u5854\u7ed3\u6784\uff08FPN\uff09\uff0c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u7b56\u7565\u5206\u914d\u4e0d\u540c\u5c42\u6b21\u7684\u8bed\u4e49\u4e0e\u7a7a\u95f4\u4fe1\u606f\u3002\u8bad\u7ec3\u8fc7\u7a0b\u91c7\u7528\u590d\u5408\u635f\u5931\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u53ca\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7edf\u4e00\u591a\u4efb\u52a1\u57fa\u7ebf\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u5747\u5c55\u793a\u4e86\u8f83\u5f3a\u7684\u6709\u6548\u6027\u548c\u7a33\u5065\u6027\uff0c\u9002\u5408\u4f5c\u4e3a\u540e\u7eed\u7814\u7a76\u548c\u4e1a\u754c\u5e94\u7528\u7684\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8d85\u58f0\u5f71\u50cf\u5206\u6790\u63d0\u51fa\u4e86\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u63a8\u52a8\u8d85\u58f0\u5f71\u50cf\u9886\u57df\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u793e\u533a\u590d\u73b0\u548c\u5b8c\u5584\u3002"}}
{"id": "2602.01917", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01917", "abs": "https://arxiv.org/abs/2602.01917", "authors": ["Chengguang Gan", "Yoshihiro Tsujii", "Yunhao Liang", "Tatsunori Mori", "Shiwen Ni", "Hiroki Itoh"], "title": "GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs", "comment": null, "summary": "Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \\textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \\textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \\textbf{GuideWeb Agent} achieves \\textbf{30.79\\%} accuracy in guide target element prediction, while obtaining BLEU scores of \\textbf{44.94} for intent generation and \\textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGuideWeb\uff0c\u4e00\u4e2a\u7528\u4e8e\u5728\u771f\u5b9e\u7f51\u9875UI\u4e0a\u81ea\u52a8\u751f\u6210\u64cd\u4f5c\u6307\u5357\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bc4\u6d4b\u65b9\u6848\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u81ea\u52a8\u6307\u5357\u751f\u6210\u4efb\u52a1\u4f9d\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709DAP\uff08\u6570\u5b57\u91c7\u7eb3\u5e73\u53f0\uff09\u8ba9\u975e\u4e13\u4e1a\u7528\u6237\u80fd\u591f\u5236\u4f5c\u64cd\u4f5c\u6307\u5357\uff0c\u4f46\u7531\u4e8e\u7f51\u7ad9\u7ecf\u5e38\u53d8\u66f4\uff0c\u7ef4\u62a4\u6307\u5357\u9700\u9891\u7e41\u624b\u5de5\u66f4\u65b0\uff0c\u5341\u5206\u8017\u529b\u3002\u4e3a\u6b64\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86GuideWeb\u57fa\u51c6\uff0c\u5c06\u6307\u5357\u751f\u6210\u5b9a\u4e49\u4e3a\u9009\u53d6\u5408\u9002\u7684\u7f51\u9875\u5143\u7d20\u5e76\u751f\u6210\u7b80\u660e\u6307\u5357\u6587\u672c\u7684\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u8bc4\u4f30\u4f53\u7cfb\uff0c\u7efc\u5408\u8003\u5bdf\u76ee\u6807\u5143\u7d20\u9009\u62e9\u548c\u751f\u6210\u6587\u672c\u7684\u51c6\u786e\u6027\u4e0e\u8d28\u91cf\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86GuideWeb Agent\u81ea\u52a8\u751f\u6210\u7cfb\u7edf\u3002", "result": "GuideWeb Agent\u5728\u76ee\u6807\u5143\u7d20\u9009\u53d6\u4e0a\u83b7\u5f9730.79%\u7684\u51c6\u786e\u7387\uff0c\u610f\u56fe\u751f\u6210BLEU\u5f97\u5206\u4e3a44.94\uff0c\u6307\u5357\u6587\u672c\u751f\u6210BLEU\u5f97\u5206\u4e3a21.34\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u7f51\u9875\u64cd\u4f5c\u6307\u5357\u4f9d\u7136\u662f\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u6210\u679c\u8fd8\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u573a\u666f\u9700\u6c42\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u8fdb\u6b65\u3002"}}
{"id": "2602.01057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01057", "abs": "https://arxiv.org/abs/2602.01057", "authors": ["Ling Chen", "Bao Yang"], "title": "Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\u7684\u65ad\u5c42\u91cd\u5efa\u65b0\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u4ee5\u5f80\u7684\u57fa\u4e8e\u6295\u5f71\u7684\u9ad8\u65af\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u7269\u7406\u4e00\u81f4\u6027\u4e0e\u6295\u5f71\u7cbe\u5ea6\uff0c\u5e76\u53ef\u7075\u6d3b\u5904\u7406\u975e\u7ebf\u6027\u51e0\u4f55\u6821\u6b63\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u7684\u6210\u50cf\u65b9\u6cd5\uff08\u5982R2-Gaussian\uff09\u901a\u8fc7\u4eff\u5c04\u8fd1\u4f3c\u63d0\u5347\u4e86\u53ef\u5fae\u5206\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u91cd\u5efa\u7cbe\u5ea6\u5e76\u96be\u4ee5\u7eb3\u5165\u590d\u6742\u7684\u51e0\u4f55\u6821\u6b63\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4f7f\u75283D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\uff0c\u7cbe\u786e\u89e3\u6790\u5730\u8ba1\u7b97\u5c04\u7ebf\u4e0e\u9ad8\u65af\u539f\u8bed\u7684\u7ebf\u79ef\u5206\uff0c\u4e14\u5728\u6b63\u6295\u5f71\u5efa\u6a21\u65f6\u4e3a\u5c04\u7ebf\u7684\u8d77\u70b9\u4e0e\u65b9\u5411\u63d0\u4f9b\u663e\u5f0f\u63a7\u5236\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4eff\u5c04\u8fd1\u4f3c\u7684\u7f3a\u9677\u3002", "result": "\u65b0\u65b9\u6cd5\u66f4\u51c6\u786e\u5730\u6a21\u62df\u4e86\u7269\u7406\u6210\u50cf\u8fc7\u7a0b\uff0c\u5728\u5b9e\u9645CT\u7b49\u65ad\u5c42\u6210\u50cf\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6295\u5f71\u548c\u91cd\u5efa\u7684\u91cf\u5316\u7cbe\u5ea6\uff0c\u4f7f\u5305\u62ecPET\u7b49\u5728\u5185\u7684\u66f4\u591a\u65ad\u5c42\u6210\u50cf\u7cfb\u7edf\u9002\u7528\u3002", "conclusion": "\u57fa\u4e8e3D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\u7684\u6846\u67b6\u4e0d\u4ec5\u6539\u5584\u4e86\u65ad\u5c42\u91cd\u5efa\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u7cbe\u5ea6\uff0c\u4e5f\u4fbf\u4e8e\u96c6\u6210\u590d\u6742\u7684\u975e\u7ebf\u6027\u6821\u6b63\uff0c\u6709\u671b\u62d3\u5c55\u9ad8\u65af\u6a21\u578b\u5728\u65ad\u5c42\u6210\u50cf\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.01919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01919", "abs": "https://arxiv.org/abs/2602.01919", "authors": ["Hend Al-Khalifa"], "title": "From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted \"Vibe Coding\"", "comment": "Accepted in The Seventh Workshop on Teaching Natural Language Processing (Teaching NLP @ EACL2026)", "summary": "The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u5728NLP\u9ad8\u5e74\u7ea7\u8bfe\u7a0b\u4e2d\u5b9e\u8df5\u4e86LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8f85\u52a9\u7f16\u7801\u6559\u5b66\u6cd5\u3002\u7ed3\u679c\u663e\u793a\u5b66\u751f\u6ee1\u610f\u5ea6\u9ad8\uff0c\u7406\u8bba\u7406\u89e3\u6709\u63d0\u5347\uff0c\u4f46\u5b58\u5728LLM\u9a8c\u8bc1\u7b49\u6311\u6218\u3002\u652f\u6301\u5728AI\u65f6\u4ee3\u8f6c\u5411\u4ee5\u6982\u5ff5\u638c\u63e1\u4e3a\u6838\u5fc3\u7684\u6559\u5b66\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cNLP\u6559\u5b66\u9762\u4e34\u673a\u9047\u548c\u6311\u6218\u3002\u73b0\u6709\u7f16\u7a0b\u6559\u5b66\u5bb9\u6613\u8ba9\u5b66\u751f\u9677\u5165\u8bed\u6cd5\u7ec6\u8282\uff0c\u96be\u4ee5\u4e13\u6ce8\u4e8e\u6982\u5ff5\u7406\u89e3\u3002\u672c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528LLMs\u8f85\u52a9\u6559\u5b66\uff0c\u63d0\u5347\u5b66\u751f\u5728NLP\u8bfe\u7a0b\u4e2d\u7684\u7406\u8bba\u638c\u63e1\u548c\u6279\u5224\u601d\u7ef4\u3002", "method": "\u5728\u4e00\u95e8\u9ad8\u5e74\u7ea7\u672c\u79d1NLP\u8bfe\u7a0b\u4e2d\uff0c\u5b66\u751f\u57287\u4e2a\u5b9e\u9a8c\u4e2d\u7531LLMs\u534f\u52a9\u7f16\u7801\uff0c\u8bfe\u7a0b\u8003\u6838\u4ee5\u6982\u5ff5\u7406\u89e3\u548c\u6279\u5224\u6027\u53cd\u601d\u4e3a\u4e3b\uff0c\u652f\u6301\u673a\u5236\u5305\u62ec\u5f3a\u5236\u7684prompt\u8bb0\u5f55\u4e0e\u53cd\u601d\u6027\u8003\u6838\u3002\u8bfe\u7a0b\u7ed3\u675f\u540e\uff0c\u901a\u8fc7\u5b66\u751f\u53cd\u9988\u8c03\u67e5\u8fdb\u884c\u6548\u679c\u8bc4\u4f30\u3002", "result": "19\u540d\u5b66\u751f\u7684\u53cd\u9988\u663e\u793a\u5404\u9879\u6ee1\u610f\u5ea6\uff08\u5982\u53c2\u4e0e\u5ea6\u3001\u6982\u5ff5\u5b66\u4e60\u4e0e\u8003\u6838\u516c\u5e73\u6027\uff09\u5747\u5f88\u9ad8\uff084.4-4.6/5.0\uff09\u3002\u591a\u6570\u5b66\u751f\u8ba4\u4e3aLLM\u8f85\u52a9\u7f16\u7801\u964d\u4f4e\u4e86\u8c03\u8bd5\u8d1f\u62c5\uff0c\u5e2e\u52a9\u805a\u7126\u7406\u8bba\u3002\u4f46\u4e5f\u51fa\u73b0\u201c\u65f6\u95f4\u4e0d\u8db3\u201d\u201c\u8f93\u51fa\u9700\u9a8c\u8bc1\u201d\u201c\u4efb\u52a1\u63cf\u8ff0\u9700\u66f4\u6e05\u6670\u201d\u7b49\u6311\u6218\u3002", "conclusion": "\u7ed3\u6784\u5408\u7406\u7684LLM\u8f85\u52a9\u6559\u5b66\u80fd\u663e\u8457\u63d0\u5347\u5b66\u751f\u5bf9NLP\u7406\u8bba\u4e0e\u6279\u5224\u6027\u601d\u7ef4\u7684\u628a\u63e1\uff0c\u6709\u52a9\u4e8e\u5b66\u751f\u9002\u5e94AI\u65f6\u4ee3\u3002\u8bfe\u7a0b\u8bbe\u8ba1\u5e94\u5408\u7406\u5f15\u5bfcLLM\u4f7f\u7528\uff0c\u5e76\u901a\u8fc7\u65e5\u5fd7\u8bb0\u5f55\u548c\u53cd\u601d\u6027\u8003\u6838\u5e94\u5bf9\u6f5c\u5728\u6311\u6218\u3002"}}
{"id": "2602.01059", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.01059", "abs": "https://arxiv.org/abs/2602.01059", "authors": ["Ying Shu", "Pujian Zhan", "Huiqi Yang", "Hehe Fan", "Youfang Lin", "Kai Lv"], "title": "DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification", "comment": null, "summary": "Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \\textbf{D}ual-\\textbf{R}egularized Bidirectional \\textbf{Transformer} (\\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f18\u70b9\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u884c\u4eba\u518d\u8bc6\u522b\u4e2d\u7684\u906e\u6321\u548c\u59ff\u52bf\u53d8\u5316\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u518d\u8bc6\u522b\u65b9\u6cd5\u591a\u53ea\u4f9d\u8d56\u5355\u4e00\u7c7b\u578b\u7684\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u7ed3\u5408\u80fd\u6316\u6398\u5c40\u90e8\u7ec6\u8282\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u80fd\u83b7\u53d6\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6b63\u5219\u5316\u53cc\u5411Transformer\uff08DRFormer\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u6b63\u5219\u5316\u673a\u5236\uff0c\u6709\u6548\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINO\uff09\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5b9e\u73b0\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u534f\u540c\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u8868\u73b0\u3002", "conclusion": "DRFormer\u6846\u67b6\u80fd\u591f\u6709\u6548\u6574\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u5728\u884c\u4eba\u518d\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5b9e\u9a8c\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u4e24\u7c7b\u6a21\u578b\u534f\u540c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.01965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01965", "abs": "https://arxiv.org/abs/2602.01965", "authors": ["Kwun Hang Lau", "Fangyuan Zhang", "Boyu Ruan", "Yingli Zhou", "Qintian Guo", "Ruiyuan Zhang", "Xiaofang Zhou"], "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation", "comment": null, "summary": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u611f\u77e5RAG\u65b9\u6cd5CatRAG\uff0c\u80fd\u52a8\u6001\u6839\u636e\u67e5\u8be2\u8c03\u6574\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u591a\u8df3\u8bc1\u636e\u68c0\u7d22\u7684\u5b8c\u6574\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548cPageRank\u7684\u7ed3\u6784\u5316RAG\u65b9\u6cd5\uff0c\u56e0\u9759\u6001\u8fb9\u6743\u3001\u5ffd\u89c6\u67e5\u8be2\u8bed\u4e49\uff0c\u5bfc\u81f4\u6a21\u578b\u5e38\u9677\u5165\u9ad8\u8fde\u63a5\u201c\u67a2\u7ebd\u201d\u8282\u70b9\uff0c\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\uff0c\u96be\u4ee5\u5b8c\u6574\u83b7\u53d6\u591a\u8df3\u67e5\u8be2\u6240\u9700\u7684\u5168\u90e8\u8bc1\u636e\u94fe\u3002\u4e9f\u9700\u80fd\u52a8\u6001\u8c03\u6574KG\u904d\u5386\u7684RAG\u65b9\u6cd5\uff0c\u63d0\u5347\u63a8\u7406\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51faCatRAG\u6846\u67b6\uff1a\u4ee5HippoRAG 2\u4e3a\u57fa\u7840\uff0c\uff081\uff09\u7b26\u53f7\u951a\u5b9a\uff1a\u57fa\u4e8e\u5b9e\u4f53\u6ce8\u5165\u5f31\u7ea6\u675f\uff0c\u89c4\u8303\u968f\u673a\u6e38\u8d70\uff1b\uff082\uff09\u67e5\u8be2\u611f\u77e5\u52a8\u6001\u8fb9\u6743\uff1a\u6839\u636e\u67e5\u8be2\u52a8\u6001\u8c03\u6574\u56fe\u7ed3\u6784\uff0c\u589e\u5f3a\u76f8\u5173\u8def\u5f84\u3001\u526a\u679d\u65e0\u5173\u8def\u5f84\uff1b\uff083\uff09\u5173\u952e\u4e8b\u5b9e\u6bb5\u843d\u6743\u91cd\u589e\u5f3a\uff1a\u901a\u8fc7\u7ed3\u6784\u6027\u504f\u7f6e\u5c06\u6e38\u8d70\u951a\u5b9a\u4e8e\u53ef\u80fd\u7684\u8bc1\u636e\u3002\u7efc\u5408\u591a\u7b56\u7565\u63d0\u5347\u67e5\u5168\u7387\u4e0e\u63a8\u7406\u8fde\u901a\u6027\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cCatRAG\u5168\u9762\u4f18\u4e8e\u4e3b\u6d41\u7ed3\u6784\u5316\u68c0\u7d22\u57fa\u7ebf\u3002\u9664\u5e38\u89c4\u67e5\u5168\u7387\u63d0\u5347\u5916\uff0cCatRAG\u5728\u6062\u590d\u5b8c\u6574\u8bc1\u636e\u94fe\u3001\u63a8\u7406\u201c\u65e0\u7f3a\u53e3\u201d\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u663e\u8457\u4f18\u8d8a\u6027\u3002", "conclusion": "CatRAG\u6709\u6548\u89e3\u51b3\u4e86\u9759\u6001\u77e5\u8bc6\u56fe\u68c0\u7d22\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86RAG\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u5b8c\u6574\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u8df3\u8bc1\u636e\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01069", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01069", "abs": "https://arxiv.org/abs/2602.01069", "authors": ["Seema K. Poudel", "Sunny K. Khadka"], "title": "PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors", "comment": null, "summary": "Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7ea6\u675f\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u663e\u5fae\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u663e\u5fae\u56fe\u50cf\u5206\u5272\u5e38\u56e0\u6d4b\u91cf\u566a\u58f0\u3001\u5f31\u8fb9\u754c\u548c\u6807\u6ce8\u6837\u672c\u5c11\u7b49\u539f\u56e0\u6210\u4e3a\u4e00\u4e2a\u4e0d\u826f\u5b9a\u52bf\u9006\u95ee\u9898\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u6613\u4ea7\u751f\u6210\u679c\u4e0d\u7a33\u5b9a\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u7269\u7406\u542f\u53d1\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u6cdb\u5316\u66f4\u597d\u7684\u5206\u5272\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u5c06\u56fe\u50cf\u5206\u5272\u8868\u8ff0\u4e3aPDE\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u53d8\u5206\u6b63\u5219\u5316\u5c06\u53cd\u5e94\u6269\u6563\u65b9\u7a0b\u548c\u76f8\u573a\u754c\u9762\u80fd\u7b49\u7269\u7406\u5148\u9a8c\u5f15\u5165\u6df1\u5ea6\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e3a\u53ef\u5fae\u6b8b\u5dee\u9879\u3002\u91c7\u7528LIVECell\u663e\u5fae\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u5e76\u4ee5UNet\u4e3a\u65e0\u7ea6\u675f\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5728\u4e0d\u540c\u7ec6\u80de\u7c7b\u578b\u95f4\u7684\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u5b9e\u9a8c\u4e2d\uff0cPDE\u6b63\u5219\u5316\u6a21\u578b\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8fb9\u754c\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002\u5c24\u5176\u5728\u6837\u672c\u91cf\u8f83\u5c11\u65f6\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c06PDE\u7ea6\u675f\u548c\u7269\u7406\u5148\u9a8c\u5f15\u5165\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u663e\u5fae\u56fe\u50cf\u5206\u5272\u7684\u6548\u679c\uff0c\u540c\u65f6\u4e3a\u53d8\u5206\u65b9\u6cd5\u3001\u7edf\u8ba1\u5b66\u4e60\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95f4\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01967", "abs": "https://arxiv.org/abs/2602.01967", "authors": ["Wonjun Lee", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition", "comment": null, "summary": "Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4e13\u5bb6\u6a21\u578bMoe-Ctc\uff0c\u901a\u8fc7\u4e2d\u95f4CTC\u76d1\u7763\u4e0e\u53e3\u97f3\u611f\u77e5\u8def\u7531\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347ASR\u5728\u4e0d\u540c\u53e3\u97f3\u4e0a\u7684\u6cdb\u5316\u53ca\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8bc6\u522b\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u4e3b\u8981\u4f9d\u636e\u9ad8\u8d44\u6e90\u53e3\u97f3\u6570\u636e\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5bf9\u4e8e\u5176\u4ed6\u53e3\u97f3\u8bc6\u522b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u4e14\u73b0\u6709\u6cdb\u53e3\u97f3\u6216\u4e13\u53e3\u97f3\u65b9\u6cd5\u5747\u5b58\u5728\u9650\u5236\u3002\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u6a21\u578b\uff0c\u517c\u5177\u5bf9\u5df2\u77e5\u53ca\u672a\u77e5\u53e3\u97f3\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faMixture-of-Experts\u67b6\u6784\uff08Moe-Ctc\uff09\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u914d\u6709\u72ec\u7acb\u7684CTC\u5934\u90e8\u3002\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u53e3\u97f3\u611f\u77e5\u8def\u7531\u4fc3\u8fdb\u4e13\u5bb6\u5b66\u5230\u5bf9\u5e94\u53e3\u97f3\u7279\u5f81\uff0c\u63a8\u65ad\u65f6\u8f6c\u4e3a\u65e0\u6807\u7b7e\u8def\u7531\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u5f15\u5165\u5e26\u8def\u7531\u589e\u5f3a\u7684\u635f\u5931\u51fd\u6570\u4ee5\u4fdd\u8bc1\u6a21\u578b\u4f18\u5316\u7a33\u5b9a\u3002", "result": "\u5728Mcv-Accent\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMoe-Ctc\u5728\u4f4e\u3001\u9ad8\u8d44\u6e90\u6761\u4ef6\u4e0b\u5bf9\u5df2\u89c1\u4e0e\u672a\u89c1\u53e3\u97f3\u5747\u6709\u63d0\u5347\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebfFastConformer\uff0c\u8bcd\u9519\u8bef\u7387\u6700\u9ad8\u4e0b\u964d29.3%\u3002", "conclusion": "Moe-Ctc\u67b6\u6784\u6709\u6548\u5728\u591a\u53e3\u97f3\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u6570\u636e\u6761\u4ef6\u548c\u591a\u7c7b\u578b\u53e3\u97f3\u4e0b\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2602.01077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01077", "abs": "https://arxiv.org/abs/2602.01077", "authors": ["Haopeng Li", "Shitong Shao", "Wenliang Zhong", "Zikai Zhou", "Lichen Bai", "Hui Xiong", "Zeke Xie"], "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers", "comment": "17 pages", "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.", "AI": {"tldr": "PISA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5747\u5300\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u901a\u8fc7\u5bf9\u975e\u5173\u952e\u5757\u8fdb\u884c\u9ad8\u6548\u8fd1\u4f3c\uff0c\u63d0\u5347\u4e86\u6269\u6563\u53d8\u6362\u5668\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e3a\u63d0\u5347\u901f\u5ea6\uff0c\u7ecf\u5e38\u76f4\u63a5\u4e22\u5f03\u975e\u5173\u952e\u4fe1\u606f\uff0c\u4f46\u8fd9\u6837\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u635f\u5931\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002\u8be5\u6587\u53d1\u73b0\u975e\u5173\u952e\u4f4d\u7f6e\u7684\u6ce8\u610f\u529b\u5206\u6570\u5177\u6709\u5206\u5e03\u7a33\u5b9a\u6027\uff0c\u53ef\u4ee5\u9ad8\u6548\u8fd1\u4f3c\u800c\u4e0d\u662f\u4e22\u5f03\uff0c\u8fd9\u4e3a\u6539\u8fdb\u7a00\u758f\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236PISA\uff0c\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u3002PISA\u5bf9\u5173\u952e\u5757\u7cbe\u786e\u8ba1\u7b97\uff0c\u5bf9\u975e\u5173\u952e\u5757\u7528\u5757\u7ea7\u6cf0\u52d2\u5c55\u5f00\u8fdb\u884c\u9ad8\u6548\u8fd1\u4f3c\uff0c\u4ece\u800c\u5168\u7a0b\u8986\u76d6\u5173\u6ce8\u8303\u56f4\uff0c\u590d\u6742\u5ea6\u964d\u4f4e\u81f3\u4e9a\u4e8c\u6b21\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002", "result": "\u5728\u5927\u578b\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u6a21\u578bWan2.1-14B\u548cHunyuan-Video\u4e0a\uff0cPISA\u5b9e\u73b0\u4e861.91\u500d\u548c2.57\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e2d\u4fdd\u6301\u4e86\u6700\u4f73\u751f\u6210\u8d28\u91cf\u3002\u5728FLUX\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u52a0\u901f1.2\u500d\u4e14\u65e0\u89c6\u89c9\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "PISA\u901a\u8fc7\u521b\u65b0\u6027\u5730\u8fd1\u4f3c\u5904\u7406\u975e\u5173\u952e\u5757\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u901f\u5ea6\u4e0e\u8d28\u91cf\u517c\u5f97\uff0c\u4e3a\u9ad8\u6548\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2602.01969", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01969", "abs": "https://arxiv.org/abs/2602.01969", "authors": ["Bin Cao", "Huixian Lu", "Chenwen Ma", "Ting Wang", "Ruizhe Li", "Jing Fan"], "title": "Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models", "comment": "Work in process", "summary": "Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u6b63\u4ea4\u5c42\u6b21\u5206\u89e3\uff08OHD\uff09\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u590d\u6742\u8868\u683c\uff08\u5982\u591a\u5c42\u8868\u5934\u3001\u5408\u5e76\u5355\u5143\u683c\u4e0e\u5f02\u6784\u5e03\u5c40\uff09\u7684\u7406\u89e3\u548c\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u8868\u5f81\u65b9\u6cd5\uff08\u5982\u7ebf\u6027\u5316\u6216\u89c4\u6574\u5316\u7f51\u683c\uff09\u96be\u4ee5\u6355\u6349\u590d\u6742\u8868\u683c\u7684\u5c42\u6b21\u7ed3\u6784\u4e0e\u8de8\u7ef4\u5ea6\u4f9d\u8d56\uff0c\u5bfc\u81f4\u7ed3\u6784\u8bed\u4e49\u548c\u6587\u672c\u8868\u8ff0\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd LLM \u5bf9\u590d\u6742\u8868\u683c\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa OHD \u6846\u67b6\uff0c\u5229\u7528\u7a7a\u95f4-\u8bed\u4e49\u534f\u540c\u7ea6\u675f\u7684\u6b63\u4ea4\u6811\u5f52\u7eb3\uff08OTI\uff09\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u8868\u683c\u5206\u522b\u5206\u89e3\u4e3a\u5217\u6811\u548c\u884c\u6811\uff0c\u4ece\u800c\u6355\u6349\u7eb5\u5411\u548c\u6a2a\u5411\u7684\u5c42\u6b21\u4f9d\u8d56\u3002\u540c\u65f6\u8bbe\u8ba1\u53cc\u901a\u9053\u7684\u5173\u8054\u534f\u8bae\u5bf9\u6bcf\u4e2a\u5355\u5143\u683c\u7684\u8bed\u4e49\u8c31\u7cfb\u8fdb\u884c\u5bf9\u79f0\u91cd\u5efa\uff0c\u5e76\u5f15\u5165 LLM \u5bf9\u591a\u5c42\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728 AITQA \u548c HiTab \u4e24\u4e2a\u590d\u6742\u8868\u683c\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0cOHD \u6846\u67b6\u5728\u591a\u4e2a\u8bc4\u6d4b\u6307\u6807\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u8868\u5f81\u8303\u5f0f\u3002", "conclusion": "OHD \u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u683c\u5206\u89e3\u53ca\u591a\u5c42\u8bed\u4e49\u4fe1\u606f\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86 LLM \u5bf9\u590d\u6742\u8868\u683c\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5b9e\u9645\u590d\u6742\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u6548\u679c\u7a81\u51fa\u3002"}}
{"id": "2602.01081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01081", "abs": "https://arxiv.org/abs/2602.01081", "authors": ["Haitao Zhang", "Yingying Wang", "Jiaxiang Wang", "Haote Xu", "Hongyang Zhang", "Yirong Chen", "Yue Huang", "Xinghao Ding"], "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization", "comment": "9 pages, 4 figures", "summary": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MedAD-38K\u5927\u89c4\u6a21\u591a\u6a21\u6001\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u8ba4\u77e5\u6ce8\u5165\u548c\u4e00\u81f4\u6027\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u63d0\u5347\u5927\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u4e0e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6a21\u578bMedAD-R1\u5728\u8be5\u57fa\u51c6\u4e0a\u5237\u65b0\u4e86SOTA\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u4e8e\u7b80\u5355\u4e14\u788e\u7247\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u548c\u591a\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u573a\u666f\u51c6\u786e\u3001\u900f\u660e\u63a8\u7406\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86MedAD-38K\u5927\u89c4\u6a21\u591a\u6a21\u6001\u3001\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8bca\u65ad\u601d\u8def\u94fe\uff08Chain-of-Thought, CoT\uff09\u6807\u6ce8\u548c\u7ed3\u6784\u5316VQA\u5bf9\u3002\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba4\u77e5\u6ce8\u5165\u91c7\u7528SFT\u5b66\u4e60\u533b\u5b66\u57fa\u7840\u77e5\u8bc6\u5e76\u5efa\u7acb\u601d\u8003-\u4f5c\u7b54\u8303\u5f0f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u65b0\u7b97\u6cd5Con-GRPO\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u4fdd\u8bc1\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u903b\u8f91\u76f8\u5173\u4e14\u8fde\u8d2f\u3002", "result": "MedAD-R1\u6a21\u578b\u5728MedAD-38K\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5df2\u6709\u5f3a\u57fa\u7ebf\u6a21\u578b\u8d85\u8fc710%\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u5728\u900f\u660e\u548c\u4e00\u81f4\u6027\u63a8\u7406\u8def\u5f84\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u89e3\u91ca\u6027\u4e0e\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u8f85\u52a9\u533b\u751f\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u7406\u89e3\u7684\u81ea\u52a8\u5316\u652f\u6301\u3002"}}
{"id": "2602.01977", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01977", "abs": "https://arxiv.org/abs/2602.01977", "authors": ["Shuainan Liu", "Xuanang Chen", "Ben He", "Le Sun"], "title": "Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing", "comment": null, "summary": "Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u7a7a\u95f4\u7684\u865a\u62df\u77e5\u8bc6\u8bc4\u4f30\u65b9\u6cd5\uff08EVK\uff09\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u8bc4\u6d4b\u57fa\u51c6\u4e0e\u77e5\u8bc6\u4fdd\u6301\u63d2\u4ef6\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u7684\u6548\u679c\u3002", "motivation": "\u76ee\u524d\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5e38\u7528\u7684\u6570\u636e\u96c6\u4ec5\u8986\u76d6\u6709\u9650\u6837\u672c\uff0c\u96be\u4ee5\u8bc4\u4f30\u7f16\u8f91\u5bf9\u6574\u4f53\u73b0\u6709\u77e5\u8bc6\u4f53\u7cfb\u7684\u5e7f\u6cdb\u5f71\u54cd\uff0c\u5bfc\u81f4\u77e5\u8bc6\u6f02\u79fb\u7b49\u6f5c\u5728\u95ee\u9898\u96be\u4ee5\u88ab\u53d1\u73b0\u548c\u91cf\u5316\u3002", "method": "\u63d0\u51faEmbedding-Virtualized Knowledge\uff08EVK\uff09\uff0c\u901a\u8fc7\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6709\u63a7\u5236\u7684\u6270\u52a8\uff0c\u523b\u753b\u548c\u6269\u5c55\u6a21\u578b\u7684\u77e5\u8bc6\u8868\u8fbe\u533a\u57df\u3002\u57fa\u4e8eEVK\uff0c\u6784\u5efa\u4e86EVK-Bench\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u77e5\u8bc6\u6f02\u79fb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u5373\u63d2\u5373\u7528\u7684EVK-Align\u6a21\u5757\u6765\u7ea6\u675f\u7f16\u8f91\u65f6\u7684\u77e5\u8bc6\u6f02\u79fb\uff0c\u53ef\u4e0e\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aEVK\u8bc4\u6d4b\u53ef\u4ee5\u63ed\u793a\u4f20\u7edf\u91c7\u6837\u8bc4\u6d4b\u65e0\u6cd5\u53d1\u73b0\u7684\u77e5\u8bc6\u6f02\u79fb\uff0cEVK-Align\u6a21\u5757\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u7f16\u8f91\u51c6\u786e\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u77e5\u8bc6\u7f16\u8f91\u6548\u679c\u7684\u5168\u9762\u8bc4\u4f30\u4e0e\u77e5\u8bc6\u4fdd\u6301\u5e26\u6765\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7814\u53d1\u66f4\u9c81\u68d2\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u3002"}}
{"id": "2602.01089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01089", "abs": "https://arxiv.org/abs/2602.01089", "authors": ["Zhiqi Zhang", "Xinhao Zhong", "Yi Sun", "Shuoyang Sun", "Bin Chen", "Shu-Tao Xia", "Xuan Wang"], "title": "Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models", "comment": null, "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5DVE\uff0c\u53ef\u5728flow matching\u6269\u6563\u6a21\u578b\u4e2d\u7cbe\u51c6\u53bb\u9664\u4e0d\u826f\u6216\u654f\u611f\u6982\u5ff5\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u5f3a\u5927\uff0c\u4f46\u6613\u751f\u6210NSFW\u3001\u4e0d\u5f53\u98ce\u683c\u6216\u7279\u5b9a\u5bf9\u8c61\u5185\u5bb9\uff0c\u5f71\u54cd\u5b89\u5168\u53ef\u63a7\u90e8\u7f72\u3002\u4ee5\u5f80\u64e6\u9664\u65b9\u6848\u591a\u57fa\u4e8eDDPM\u4e14\u9700\u6602\u8d35\u5fae\u8c03\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u65b0\u5174\u7684flow matching\u6a21\u578b\uff0c\u56e0\u6b64\u4e9f\u9700\u521b\u65b0\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faDVE\uff08Differential Vector Erasure\uff09\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u89c2\u5bdf\uff1a\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u8bed\u4e49\u6982\u5ff5\u8574\u542b\u4e8e\u901f\u5ea6\u573a\u7684\u65b9\u5411\u7ed3\u6784\u3002DVE\u901a\u8fc7\u6784\u5efa\u201c\u76ee\u6807\u6982\u5ff5-\u951a\u5b9a\u6982\u5ff5\u201d\u7684\u5fae\u5206\u5411\u91cf\u573a\uff0c\u63a8\u7406\u65f6\u5c06\u901f\u5ea6\u573a\u6295\u5f71\u5230\u7279\u6b8a\u65b9\u5411\uff0c\u4ece\u800c\u4ec5\u64e6\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u5b9e\u73b0\u7cbe\u51c6\u5e72\u9884\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728FLUX\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cDVE\u5728NSFW\u5185\u5bb9\u6291\u5236\u3001\u827a\u672f\u98ce\u683c\u79fb\u9664\u4e0e\u7269\u4f53\u6d88\u9664\u7b49\u591a\u9879\u4efb\u52a1\u4e2d\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u56fe\u50cf\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u3002", "conclusion": "DVE\u4e3aflow matching\u6a21\u578b\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u8bad\u7ec3\u65e0\u5173\u7684\u6982\u5ff5\u64e6\u9664\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5176\u5b89\u5168\u4e0e\u53ef\u63a7\u90e8\u7f72\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u654f\u611f\u6216\u53d7\u9650\u5185\u5bb9\u7684\u7cbe\u7ec6\u6d88\u9664\u3002"}}
{"id": "2602.01982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01982", "abs": "https://arxiv.org/abs/2602.01982", "authors": ["Yanrui Du", "Sendong Zhao", "Yibo Gao", "Danyang Zhao", "Qika Lin", "Ming Ma", "Jiayun Li", "Yi Jiang", "Kai He", "Qianyi Xu", "Bing Qin", "Mengling Feng"], "title": "S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs", "comment": null, "summary": "Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u81ea\u91c7\u6837(CoT)\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7c7b System 1\uff08\u5feb\u901f\u601d\u8003\uff09\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u65e0\u9700\u6559\u5e08\u6807\u6ce8\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u548c\u533b\u5b66\u7b49\u591a\u9886\u57df\u83b7\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u94fe\u5f0f\u601d\u8003(CoT)\u867d\u7136\u63d0\u5347\u4e86\u5927\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u5b58\u5728\u5197\u4f59\u63a8\u7406\u4e14\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u8ba9LLMs\u50cf\u4eba\u7c7b\u90a3\u6837\u5177\u5907\u9ad8\u6548\u3001\u5feb\u901f\u7684\u76f4\u89c9\u63a8\u7406(System 1)\uff0c\u540c\u65f6\u964d\u4f4e\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u81ea\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u591a\u6837\u5e76\u5bf9\u9f50\u7279\u5b9a\u98ce\u683c\u548c\u957f\u5ea6\u7684\u63a8\u7406\u75d5\u8ff9\uff0c\u5e76\u7ed3\u5408\u7b54\u6848\u8fc7\u6ee4\u3001\u53cc\u8ba4\u77e5\u7cfb\u7edf\u548c\u9010\u6b65\u538b\u7f29\u8bfe\u7a0b\uff0c\u6709\u6548\u5730\u8fdb\u884c\u9ad8\u6548CoT\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u4ec5\u57fa\u4e8e\u81ea\u4e00\u81f4\u9884\u6d4b\u9a71\u52a8\u81ea\u8fdb\u5316\u8bad\u7ec3\uff0c\u65e0\u9700\u6807\u6ce8\u91d1\u7b54\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u57fa\u51c6\u548c\u8de8\u533b\u5b66\u9886\u57df\u7684\u63a8\u5e7f\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e38\u89c4\u548cR1\u98ce\u683cLLMs\u7684\u6027\u80fd\u3002", "conclusion": "\u81ea\u91c7\u6837+\u6fc0\u6d3b\u5f15\u5bfc\u7684CoT\u5b66\u4e60\u65b9\u5f0f\u514b\u670d\u4e86\u4f20\u7edfSFT\u65b9\u6cd5\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\uff0c\u4e3aLLMs\u7c7b\u4eba\u8ba4\u77e5\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u5e76\u5177\u5907\u8f83\u597d\u901a\u7528\u6027\u548c\u63a8\u5e7f\u6027\u3002"}}
{"id": "2602.01095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01095", "abs": "https://arxiv.org/abs/2602.01095", "authors": ["Jinghong Zheng", "Changlong Jiang", "Yang Xiao", "Jiaqi Li", "Haohong Kuang", "Hang Xu", "Ran Wang", "Zhiguo Cao", "Min Du", "Joey Tianyi Zhou"], "title": "PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space", "comment": "Accepted at NeurIPS 2025", "summary": "3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u4eba\u4f53\u59ff\u6001\u91cd\u5efa\u65b9\u6cd5PandaPose\uff0c\u901a\u8fc7\u5f15\u51653D\u951a\u70b9\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u51cf\u5c112D-3D\u5efa\u6a21\u4e2d\u7684\u8bef\u5dee\u6269\u6563\u4e0e\u81ea\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ece\u5355\u5f20RGB\u56fe\u50cf\u6062\u590d3D\u4eba\u4f53\u59ff\u6001\u7684\u65b9\u6cd5\uff0c\u5f80\u5f80\u76f4\u63a5\u5c062D\u5173\u8282\u6620\u5c04\u4e3a3D\u5173\u8282\uff0c\u4f46\u6613\u51fa\u73b0\u4e24\u4e2a\u95ee\u9898\uff1a\u4e00\u662f\u8f93\u5165\u76842D\u9884\u6d4b\u8bef\u5dee\u4f1a\u76f4\u63a5\u5f71\u54cd3D\u9884\u6d4b\u7ed3\u679c\uff1b\u4e8c\u662f\u96be\u4ee5\u5904\u7406\u81ea\u906e\u6321\u7b49\u590d\u6742\u60c5\u51b5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u4e2d\u95f4\u8868\u793a\u6765\u51cf\u5c11\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc72D\u59ff\u6001\u5148\u9a8c\u4f20\u64ad\u52303D\u951a\u70b9\u7a7a\u95f4\u7684\u7edf\u4e00\u4e2d\u95f4\u8868\u8fbe\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u5305\u62ec\uff1a(1) \u5728\u6807\u51c6\u5750\u6807\u7cfb\u4e2d\u4e3a\u6bcf\u4e2a\u5173\u8282\u8bbe\u7f6e3D\u951a\u70b9\uff0c\u4f5c\u4e3a\u51c6\u786e\u7684\u5148\u9a8c\uff1b(2) \u6df1\u5ea6\u611f\u77e5\u7684\u5173\u8282\u7279\u5f81\u63d0\u5347\uff0c\u5206\u5c42\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\uff0c\u89e3\u51b3\u81ea\u906e\u6321\u6b67\u4e49\uff1b(3) \u951a\u70b9-\u7279\u5f81\u4ea4\u4e92\u89e3\u7801\u5668\uff0c\u5c063D\u951a\u70b9\u4e0e\u7279\u5f81\u76f8\u7ed3\u5408\u751f\u6210\u7edf\u4e00\u951a\u70b9\u67e5\u8be2\uff0c\u8054\u5408\u5173\u8282\u96c6\u3001\u89c6\u89c9\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u6700\u7ec8\u7ecf\u8fc7\u878d\u5408\u9884\u6d4b3D\u5173\u8282\u4f4d\u7f6e\u3002", "result": "\u5728Human3.6M, MPI-INF-3DHP\u548c3DPW\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728Human3.6M\u7684\u590d\u6742\u6761\u4ef6\u4e0b\uff0c\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u8bef\u5dee\u964d\u4f4e\u4e8614.7%\uff0c\u5e76\u6709\u4f18\u79c0\u7684\u5b9a\u6027\u8868\u73b0\u3002", "conclusion": "PandaPose\u901a\u8fc7\u5f15\u51653D\u951a\u70b9\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u76843D\u4eba\u4f53\u59ff\u6001\u91cd\u5efa\uff0c\u5728\u51c6\u786e\u6027\u548c\u5904\u7406\u590d\u6742\u573a\u666f\uff08\u5982\u81ea\u906e\u6321\uff09\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.01999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01999", "abs": "https://arxiv.org/abs/2602.01999", "authors": ["Yanrui Du", "Yibo Gao", "Sendong Zhao", "Jiayun Li", "Haochun Wang", "Qika Lin", "Kai He", "Bing Qin", "Mengling Feng"], "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs", "comment": null, "summary": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c42\u7ea7\u6fc0\u6d3b\u8f68\u8ff9\u5206\u6790\uff0c\u63ed\u793a\u4e86R1\u578b\u5927\u6a21\u578b\u53cd\u601d\u884c\u4e3a\u80cc\u540e\u7684\u7ed3\u6784\u5316\u5206\u9636\u6bb5\u673a\u5236\uff0c\u53d1\u73b0\u5176\u53cd\u601d\u8fc7\u7a0b\u5177\u6709\u7c7b\u4eba\u5143\u8ba4\u77e5\u7279\u5f81\u3002", "motivation": "\u5c3d\u7ba1R1\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u81ea\u6211\u53cd\u601d\u80fd\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u5185\u90e8\u5b9e\u73b0\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u610f\u5728\u63a2\u7d22\u548c\u63ed\u793a\u6a21\u578b\u4e3a\u4f55\u4ee5\u53ca\u5982\u4f55\u4ea7\u751f\u53cd\u601d\u884c\u4e3a\u3002", "method": "\u4f5c\u8005\u4ee5\u53cd\u601d\u884c\u4e3a\u7684\u4ea7\u751f\u4e3a\u5207\u5165\u70b9\uff0c\u5229\u7528logit lens\u65b9\u6cd5\u5c42\u5c42\u8ffd\u8e2a\u6fc0\u6d3b\u8f68\u8ff9\uff0c\u7ec6\u81f4\u5206\u6790\u6bcf\u4e00\u5c42\u7684\u8bed\u4e49\u548c\u884c\u4e3a\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u9636\u6bb5\u4e4b\u95f4\u7684\u56e0\u679c\u94fe\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u53cd\u601d\u884c\u4e3a\u5448\u73b0\u4e09\u9636\u6bb5\uff1a\u524d\u671f\u201c\u6f5c\u5728\u63a7\u5236\u5c42\u201d\u4ee5\u8fd1\u4f3c\u7ebf\u6027\u65b9\u5411\u7f16\u7801\u601d\u8003\u9884\u7b97\uff1b\u4e2d\u671f\u201c\u8bed\u4e49\u652f\u70b9\u5c42\u201d\u5219\u8bdd\u8bed\u63d0\u793a\uff08\u8f6c\u6298\u3001\u603b\u7ed3\uff09\u5360\u4e3b\u5bfc\uff1b\u540e\u671f\u201c\u884c\u4e3a\u5916\u663e\u5c42\u201d\u4e2d\u53cd\u601d\u76f8\u5173\u8bcd\u6982\u7387\u663e\u8457\u63d0\u5347\u3002\u5404\u9636\u6bb5\u4e4b\u95f4\u5b58\u5728\u660e\u786e\u7684\u56e0\u679c\u94fe\uff1a\u8f93\u5165\u8bed\u4e49\u8c03\u63a7\u6fc0\u6d3b\u6295\u5f71\uff0c\u8fdb\u800c\u5728\u201c\u8bed\u4e49\u652f\u70b9\u5c42\u201d\u5f62\u6210\u63d0\u793a\u7ade\u4e89\uff0c\u5e76\u5f71\u54cd\u6700\u7ec8\u884c\u4e3a\u8f93\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86R1\u578bLLM\u53cd\u601d\u884c\u4e3a\u7684\u5206\u5c42\u56e0\u679c\u673a\u5236\uff0c\u6697\u793a\u5176\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5143\u8ba4\u77e5\u5904\u7406\u6d41\u7a0b\u3002"}}
{"id": "2602.01101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01101", "abs": "https://arxiv.org/abs/2602.01101", "authors": ["Felix Breiteneder", "Mohammad Belal", "Muhammad Saad Saeed", "Shahed Masoudian", "Usman Naseem", "Kulshrestha Juhi", "Markus Schedl", "Shah Nawaz"], "title": "Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning", "comment": "Accepted at WWW2026", "summary": "Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6a21\u6001\u4e0d\u5b8c\u6574\uff08\u5982\u6587\u672c\u7f3a\u5931\uff09\u573a\u666f\u4e0b\u6709\u66f4\u597d\u8868\u73b0\u7684\u6709\u5bb3\u8ff7\u56e0\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u6295\u5f71\u5404\u6a21\u6001\u5b66\u4e60\u5171\u4eab\u8868\u5f81\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6709\u5bb3\u8ff7\u56e0\u68c0\u6d4b\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u5b8c\u5907\u7684\u591a\u6a21\u6001\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\u4fe1\u606f\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5982\u7531\u4e8eOCR\u8d28\u91cf\u5dee\uff0c\u6587\u672c\u5f80\u5f80\u4f1a\u7f3a\u5931\uff0c\u5bfc\u81f4\u5df2\u6709\u65b9\u6cd5\u8868\u73b0\u5927\u5e45\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u63a2\u7a76\u5e76\u63d0\u5347\u6a21\u6001\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u591a\u79cd\u6a21\u6001\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\uff09\u8fdb\u884c\u72ec\u7acb\u6295\u5f71\uff0c\u5b66\u4e60\u51fa\u53ef\u4ee5\u5728\u7f3a\u5931\u6a21\u6001\u65f6\u4ecd\u53ef\u7528\u7684\u5171\u4eab\u8868\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff1a\u5f53\u6587\u672c\u4fe1\u606f\u7f3a\u5931\u65f6\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u66f4\u597d\u5730\u6574\u5408\u4e86\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u5728\u7f3a\u4e4f\u6587\u672c\u7684\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u8865\u4e86\u4ee5\u5f80\u7814\u7a76\u53ea\u9488\u5bf9\u5b8c\u5907\u6a21\u6001\u6570\u636e\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u6709\u5bb3\u8ff7\u56e0\u68c0\u6d4b\u65b9\u6cd5\u5728\u73b0\u5b9e\u73af\u5883\u3001\u5c24\u5176\u662f\u4fe1\u606f\u4e0d\u5b8c\u5907\u65f6\u7684\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2602.02007", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02007", "abs": "https://arxiv.org/abs/2602.02007", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Hanqi Yan", "Yulan He", "Lin Gui"], "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation", "comment": null, "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9762\u5411\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u673a\u5236\u6539\u8fdb\u65b9\u6cd5\u2014\u2014xMemory\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u76f8\u4f3c\u5ea6\u68c0\u7d22\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u548c\u5c42\u6b21\u7ed3\u6784\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\uff0c\u6709\u6548\u63d0\u5347\u95ee\u7b54\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u4ee5\u5f80RAG\u5047\u8bbe\uff08\u5927\u89c4\u6a21\u5f02\u8d28\u8bed\u6599\u591a\u6837\u6027\uff09\u4e0e\u667a\u80fd\u4f53\u8bb0\u5fc6\u5b9e\u9645\u60c5\u51b5\uff08\u8fde\u8d2f\u5bf9\u8bdd\u6d41\uff0c\u91cd\u590d\u548c\u9ad8\u5ea6\u76f8\u5173\u5185\u5bb9\uff09\u4e0d\u7b26\uff0c\u5bfc\u81f4\u56fa\u5b9atop-k\u68c0\u7d22\u7ed3\u679c\u5197\u4f59\u3001\u53bb\u91cd\u65b9\u6cd5\u53c8\u6613\u8bef\u5220\u5173\u952e\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u9700\u6539\u53d8\u68c0\u7d22\u903b\u8f91\u3002", "method": "xMemory\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u8bb0\u5fc6\u62c6\u89e3\u4e3a\u8bed\u4e49\u7ec4\u4ef6\uff0c\u6784\u5efa\u5c42\u6b21\u5316\u7ed3\u6784\uff0c\u901a\u8fc7\u7a00\u758f-\u8bed\u4e49\u76ee\u6807\u5f15\u5bfc\u8bb0\u5fc6\u62c6\u5206\u548c\u805a\u5408\u3002\u63a8\u7406\u65f6\uff0c\u91c7\u7528\u81ea\u9876\u5411\u4e0b\u68c0\u7d22\uff0c\u4f18\u5148\u9009\u62e9\u9ad8\u5c42\u6b21\u3001\u7ed3\u6784\u5316\u4e3b\u9898\u8282\u70b9\uff0c\u4ec5\u5728\u80fd\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027\u65f6\u518d\u6269\u5c55\u5e95\u5c42\u7ec6\u8282\uff0c\u5b9e\u73b0\u66f4\u5c11\u5197\u4f59\u3001\u4e3b\u9898\u96c6\u4e2d\u7684\u77e5\u8bc6\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u548cPerLTQA\u4e24\u5957\u6d4b\u8bd5\u96c6\uff0c\u4ee5\u53ca\u4e09\u79cd\u4e3b\u6d41LLM\u4e0a\u9a8c\u8bc1\uff0cxMemory\u5728\u7b54\u6848\u8d28\u91cf\u548cToken\u4f7f\u7528\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "xMemory\u80fd\u591f\u6709\u6548\u89e3\u51b3\u667a\u80fd\u4f53\u8bb0\u5fc6\u573a\u666f\u4e0b\u8bed\u4e49\u5197\u4f59\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u8bb0\u5fc6\u7ec4\u7ec7\u548c\u521b\u65b0\u68c0\u7d22\u65b9\u5f0f\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u548c\u591a\u4e8b\u5b9e\u95ee\u7b54\u4efb\u52a1\u5e26\u6765\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.01118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01118", "abs": "https://arxiv.org/abs/2602.01118", "authors": ["Jingjing Wang", "Qirui Hu", "Chong Bao", "Yuke Zhu", "Hujun Bao", "Zhaopeng Cui", "Guofeng Zhang"], "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions", "comment": null, "summary": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LightCity\uff0c\u4e00\u4e2a\u5177\u6709\u591a\u6837\u5316\u7167\u660e\u6761\u4ef6\u548c\u771f\u5b9e\u95f4\u63a5\u5149\u3001\u9634\u5f71\u6548\u679c\u7684\u9ad8\u8d28\u91cf\u57ce\u5e02\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5176\u5bf9\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u57fa\u7840\u4efb\u52a1\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u6790\u3002", "motivation": "\u57ce\u5e02\u573a\u666f\u4e0b\u7684\u9006\u5411\u6e32\u67d3\u5bf9\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u7167\u660e\u73af\u5883\uff08\u5982\u591a\u5149\u6e90\u3001\u95f4\u63a5\u5149\u548c\u9634\u5f71\uff09\uff0c\u5bfc\u81f4\u56fa\u6709\u5206\u89e3\u548c\u4e09\u7ef4\u91cd\u5efa\u7b49\u4efb\u52a1\u9762\u4e34\u96be\u9898\uff0c\u4e14\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u6765\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86LightCity\u6570\u636e\u96c6\uff0c\u5305\u542b300\u591a\u79cd\u5929\u7a7a\u8d34\u56fe\u3001\u81ea\u4e3b\u63a7\u5236\u7167\u660e\u3001\u8986\u76d6\u8857\u666f\u548c\u7a7a\u4e2d\u89c6\u89d2\u76845\u4e07\u591a\u5f20\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u542b\u6df1\u5ea6\u3001\u6cd5\u7ebf\u3001\u6750\u8d28\u3001\u5149\u7167\u7b49\u4e30\u5bcc\u5c5e\u6027\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u5229\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u4e09\u7c7b\u57fa\u7840\u4efb\u52a1\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "LightCity\u6570\u636e\u96c6\u80fd\u591f\u5c55\u73b0\u590d\u6742\u7167\u660e\u53ca\u95f4\u63a5\u5149\u6548\u4e0b\u7684\u57ce\u5e02\u73af\u5883\uff0c\u4e3a\u9006\u5411\u6e32\u67d3\u7b49\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u4f18\u8d28\u7d20\u6750\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5206\u6790\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5149\u7167\u4e0b\u7684\u8868\u73b0\u53ca\u5176\u5c40\u9650\u3002", "conclusion": "LightCity\u4e3a\u7814\u7a76\u57ce\u5e02\u573a\u666f\u4e0b\u590d\u6742\u7167\u660e\u5904\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9006\u5411\u6e32\u67d3\u3001\u56fa\u6709\u5206\u89e3\u3001\u4e09\u7ef4\u91cd\u5efa\u7b49\u9886\u57df\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.02010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02010", "abs": "https://arxiv.org/abs/2602.02010", "authors": ["Kang Liu", "Yongkang Liu", "Xiaocui Yang", "Peidong Wang", "Wen Zhang", "Shi Feng", "Yifei Zhang", "Daling Wang"], "title": "NEAT: Neuron-Based Early Exit for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) often suffer from \\emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \\textbf{NEAT}, a \\textbf{N}euron-based \\textbf{E}arly re\\textbf{A}soning exi\\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\\% to 28\\% when averaged over the four benchmarks, while maintaining accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5NEAT\uff0c\u80fd\u591f\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u5197\u4f59\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u8fc7\u5ea6\u63a8\u7406\uff0c\u5373\u5728\u5f97\u5230\u6b63\u786e\u89e3\u540e\u4f9d\u7136\u7ee7\u7eed\u63a8\u7406\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\u901a\u5e38\u9700\u8981\u5916\u90e8\u8bad\u7ec3\u6216\u6570\u636e\uff0c\u5b58\u5728\u989d\u5916\u5f00\u9500\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u52a8\u6001\u7684\u65e9\u671f\u63a8\u7406\u9000\u51fa\u6846\u67b6NEAT\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u68c0\u6d4b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u72b6\u6001\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u989d\u5916\u6d4b\u8bd5\u8ba1\u7b97\uff0c\u5c31\u80fd\u52a8\u6001\u5224\u65ad\u4f55\u65f6\u63d0\u524d\u9000\u51fa\u6216\u8005\u6291\u5236\u5197\u4f59\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u4efb\u52a1\u3001\u516d\u79cd\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cNEAT\u5728\u4fdd\u8bc1\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u578b\u5e73\u5747token\u6570\u51cf\u5c11\u4e8622%-28%\u3002", "conclusion": "NEAT\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8bad\u7ec3\u65e0\u5173\u7684\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.01127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01127", "abs": "https://arxiv.org/abs/2602.01127", "authors": ["Matej Suchanek", "Klara Janouskova", "Ondrej Vasatko", "Jiri Matas"], "title": "Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis", "comment": null, "summary": "Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.\n  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Koo-Fu CLIP\uff0c\u5c06\u76d1\u7763\u7ebf\u6027\u5224\u522b\u5206\u6790\u65b9\u6cd5\u5e94\u7528\u4e8eCLIP\u5d4c\u5165\uff0c\u4ee5\u63d0\u5347\u7c7b\u522b\u533a\u5206\u80fd\u529b\u548c\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\uff0c\u5728ImageNet\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u5e76\u652f\u6301\u663e\u8457\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u751f\u6210\u7684\u5d4c\u5165\u672a\u9488\u5bf9\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u4f18\u5316\uff0c\u5bfc\u81f4\u7c7b\u522b\u533a\u5206\u4e0d\u5f3a\u4e14\u7279\u5f81\u7ef4\u5ea6\u8fc7\u9ad8\uff0c\u5f71\u54cd\u9ad8\u6548\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u63d0\u51faKoo-Fu CLIP\uff0c\u901a\u8fc7Fukunaga-Koontz\u7ebf\u6027\u5224\u522b\u5206\u6790\u4f5c\u7528\u4e8eCLIP\u5d4c\u5165\u7684\u767d\u5316\u7a7a\u95f4\uff0c\u6291\u5236\u7c7b\u5185\u5dee\u5f02\u3001\u589e\u5f3a\u7c7b\u95f4\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u5b9e\u73b0\u7279\u5f81\u538b\u7f29\u548c\u7c7b\u522b\u5206\u79bb\u4f18\u5316\u3002", "result": "\u5728ImageNet-1K\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKoo-Fu CLIP\u5c06top-1\u51c6\u786e\u7387\u753175.1%\u63d0\u5347\u81f379.1%\uff1b\u8fdb\u4e00\u6b65\u6269\u5c55\u523014K\u548c21K\u7c7b\u65f6\u5747\u6709\u7a33\u5b9a\u589e\u76ca\uff0c\u652f\u630110-12\u500d\u7684\u7279\u5f81\u538b\u7f29\u4e14\u51e0\u4e4e\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "Koo-Fu CLIP\u4e3aCLIP\u8868\u5f81\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u6709\u76d1\u7763\u9002\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6548\u7387\u548c\u51c6\u786e\u7387\uff1b\u5177\u5907\u5927\u89c4\u6a21\u68c0\u7d22\u4e0e\u5206\u7c7b\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02053", "abs": "https://arxiv.org/abs/2602.02053", "authors": ["Pengyu Wang", "Benfeng Xu", "Licheng Zhang", "Shaohan Wang", "Mingxuan Du", "Chiwei Zhu", "Zhendong Mao"], "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora", "comment": "https://github.com/BstWPY/WildGraphBench", "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86WildGraphBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u56fe\u8c31\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u8be5\u57fa\u51c6\u5229\u7528Wikipedia\u7684\u5916\u90e8\u5f15\u7528\u4f5c\u4e3a\u68c0\u7d22\u8bed\u6599\u548c\u771f\u5b9e\u8bed\u53e5\u6807\u7b7e\uff0c\u5305\u542b1100\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u4e0d\u540c\u590d\u6742\u5ea6\u7684QA\u4e0e\u6458\u8981\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u73b0\u6709GraphRAG\u5728\u591a\u6e90\u8bc1\u636e\u805a\u5408\u65f6\u6709\u6548\uff0c\u4f46\u5bf9\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u5904\u7406\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6458\u8981\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GraphRAG\u8bc4\u6d4b\u591a\u4ee5\u77ed\u5c0f\u7cbe\u608d\u7684\u77e5\u8bc6\u7247\u6bb5\u4e3a\u5916\u90e8\u77e5\u8bc6\uff0c\u96be\u4ee5\u53cd\u6620\u9762\u5bf9\u957f\u6587\u672c\u548c\u5927\u89c4\u6a21\u5f02\u6784\u6587\u6863\u65f6\u7684\u771f\u5b9e\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u80fd\u66f4\u8d34\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u5229\u7528Wikipedia\u7684\u7ed3\u6784\uff0c\u4ece12\u4e2a\u4e3b\u9898\u62bd\u6837\u6587\u7ae0\uff0c\u5c06\u5916\u90e8\u53c2\u8003\u6587\u732e\u505a\u4e3a\u68c0\u7d22\u8bed\u6599\uff0c\u4ee5\u5e26\u6709\u5f15\u7528\u7684\u8bed\u53e5\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u8bbe\u8ba1\u4e09\u7c7b\u4efb\u52a1\uff08\u5355\u4e8b\u5b9e\u95ee\u7b54\u3001\u591a\u4e8b\u5b9e\u95ee\u7b54\u3001\u7ae0\u8282\u7ea7\u6458\u8981\uff09\uff0c\u5171\u6536\u96c61100\u4e2a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u505a\u5b9e\u9a8c\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524dGraphRAG\u6846\u67b6\u5728\u591a\u8bc1\u636e\u805a\u5408\u65f6\u8868\u73b0\u4f18\u79c0\uff0c\u63d0\u5347\u4e86\u591a\u4e8b\u5b9e\u95ee\u7b54\u80fd\u529b\uff0c\u4f46\u5728\u6d89\u53ca\u5927\u91cf\u6765\u6e90\u548c\u9700\u8981\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u6458\u8981\u4efb\u52a1\u4e0a\uff0c\u5bb9\u6613\u7247\u9762\u805a\u7126\u9ad8\u5c42\u6b21\u8868\u8ff0\uff0c\u5bfc\u81f4\u5173\u952e\u7ec6\u8282\u4e22\u5931\u3002", "conclusion": "WildGraphBench\u80fd\u66f4\u771f\u5b9e\u5730\u8861\u91cfGraphRAG\u7cfb\u7edf\u5728\u590d\u6742\u68c0\u7d22\u573a\u666f\u4e0b\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e86\u8be5\u7c7b\u7cfb\u7edf\u805a\u5408\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4eca\u540e\u7b97\u6cd5\u6539\u8fdb\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u5206\u6790\u3002"}}
{"id": "2602.02084", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02084", "abs": "https://arxiv.org/abs/2602.02084", "authors": ["Jane Luo", "Chengyu Yin", "Xin Zhang", "Qingtao Li", "Steven Liu", "Yiming Huang", "Jie Wu", "Hao Liu", "Yangyu Huang", "Yu Kang", "Fangkai Yang", "Ying Xin", "Scarlett Li"], "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder", "comment": null, "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7801\u4ed3\u5e93\u7406\u89e3\u4e0e\u751f\u6210\u6846\u67b6RPG-Encoder\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u7684\u56fe\u7ed3\u6784\u6574\u5408\u4ee3\u7801\u8bed\u4e49\u548c\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4ee3\u7801\u5e93\u7684\u7406\u89e3\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ed3\u5e93\u5206\u6790\u4f9d\u8d56\u96f6\u6563\u7684API\u6587\u6863\u6216\u4f9d\u8d56\u56fe\uff0c\u8bed\u4e49\u5173\u8054\u8584\u5f31\uff0c\u5bfc\u81f4\u81ea\u52a8\u5316\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u65f6\u5b58\u5728\u63a8\u7406\u65ad\u5c42\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u4ee3\u7801\u5e93\u7684\u7cbe\u7ec6\u5316\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51faRPG-Encoder\uff0c\u5176\u6838\u5fc3\u662f\u5c06\u4ee3\u7801\u8bed\u4e49\u7279\u5f81\u4e0e\u4f9d\u8d56\u5173\u7cfb\u878d\u5408\u5230\u7edf\u4e00\u7684Repository Planning Graph (RPG)\u7ed3\u6784\u4e2d\u3002\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u7f16\u7801\u539f\u59cb\u4ee3\u7801\u4e3a\u7ed3\u5408\u8bed\u4e49\u548c\u4f9d\u8d56\u7684RPG\uff1b(2) \u901a\u8fc7\u589e\u91cf\u6f14\u5316RPG\u7684\u62d3\u6251\uff0c\u964d\u4f4e\u968f\u4ed3\u5e93\u89c4\u6a21\u589e\u957f\u7684\u7ef4\u62a4\u6210\u672c\uff0c\u5f00\u9500\u51cf\u5c1195.7%\uff1b(3) \u63d0\u4f9b\u7ed3\u6784\u611f\u77e5\u7684\u7edf\u4e00\u63a5\u53e3\uff0c\u4fbf\u4e8e\u9ad8\u6548\u5bfc\u822a\u548c\u64cd\u4f5c\u3002", "result": "\u5728SWE-bench Verified\u6570\u636e\u96c6\u4e0a\uff0cRPG-Encoder\u53d6\u5f9793.7%\u7684Acc@5\uff0c\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd510%\u4ee5\u4e0a\uff1b\u5728RepoCraft\u6570\u636e\u96c6\u4e0a\uff0cRPG\u7684\u4ee3\u7801\u91cd\u5efa\u8986\u76d6\u7387\u8fbe98.5%\uff0c\u663e\u793a\u51fa\u6781\u9ad8\u7684\u4ed3\u5e93\u7ed3\u6784\u8fd8\u539f\u80fd\u529b\u3002", "conclusion": "RPG-Encoder\u901a\u8fc7\u95ed\u73af\u7684\u751f\u6210-\u7406\u89e3\u673a\u5236\u548c\u9ad8\u4fdd\u771fRPG\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u4ee3\u7801\u5e93\u7684\u9ad8\u7cbe\u5ea6\u7406\u89e3\u548c\u5b9a\u4f4d\uff0c\u4e3a\u81ea\u52a8\u5316\u7684\u4ee3\u7801\u5206\u6790\u4e0e\u751f\u6210\u4efb\u52a1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.01163", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01163", "abs": "https://arxiv.org/abs/2602.01163", "authors": ["Chunliang Hua", "Zeyuan Yang", "Lei Zhang", "Jiayang Sun", "Fengwen Chen", "Chunlan Zeng", "Xiao Hu"], "title": "Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models", "comment": null, "summary": "Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9065\u611f\u5f71\u50cf\u548c\u591a\u6a21\u6001\u5927\u6a21\u578b\u7528\u4e8e\u65e0\u4eba\u673a\u7d27\u6025\u964d\u843d\u70b9\u5168\u5c40\u98ce\u9669\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u98ce\u9669\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u964d\u843d\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u964d\u843d\u70b9\u9009\u62e9\u4e3b\u8981\u4f9d\u8d56\u51e0\u4f55\u4fe1\u606f\uff0c\u96be\u4ee5\u611f\u77e5\u5982\u4eba\u7fa4\u3001\u4e34\u65f6\u5efa\u7b51\u7b49\u590d\u6742\u8bed\u4e49\u98ce\u9669\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f15\u5165\u66f4\u80fd\u7406\u89e3\u573a\u666f\u8bed\u4e49\u7684\u667a\u80fd\u65b9\u6cd5\u6765\u4fdd\u969c\u65e0\u4eba\u673a\u5b89\u5168\u964d\u843d\u3002", "method": "\u65b9\u6cd5\u63d0\u51fa\u4e86\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u65b9\u6848\uff1a\u9996\u5148\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u5bf9\u9065\u611f\u56fe\u50cf\u521d\u6b65\u7b5b\u9009\u5019\u9009\u533a\u57df\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4ee3\u7406\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u5174\u8da3\u70b9\uff08POI\uff09\u6570\u636e\u878d\u5408\uff0c\u8bc6\u522b\u7ec6\u5fae\u7684\u8bed\u4e49\u98ce\u9669\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8fd8\u6784\u5efa\u5e76\u516c\u5f00\u4e86ELSS\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u98ce\u9669\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u8fdc\u8d85\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u6a21\u578b\u751f\u6210\u4e86\u53ef\u89e3\u91ca\u7684\u3001\u7c7b\u4eba\u7684\u964d\u843d\u5efa\u8bae\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u51b3\u7b56\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u7ed3\u5408\u9065\u611f\u5f71\u50cf\u4e0e\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u65b9\u6848\u80fd\u66f4\u5168\u9762\u5730\u8bc6\u522b\u65e0\u4eba\u673a\u964d\u843d\u98ce\u9669\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u81ea\u52a8\u51b3\u7b56\uff0c\u4f18\u4e8e\u53ea\u57fa\u4e8e\u51e0\u4f55\u7684\u4f20\u7edf\u65b9\u6848\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2602.02090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02090", "abs": "https://arxiv.org/abs/2602.02090", "authors": ["Yikai Zeng", "Yingchao Piao", "Jianhui Li"], "title": "LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs", "comment": null, "summary": "Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.", "AI": {"tldr": "LEC-KG \u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff0c\u5b9e\u73b0\u5bf9\u9886\u57df\u6587\u672c\u7684\u9ad8\u8d28\u91cf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u5bf9\u957f\u5c3e\u5173\u7cfb\u548c\u672a\u89c1\u5b9e\u4f53\u8868\u73b0\u4f18\u79c0\u3002", "motivation": "\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u96be\u5ea6\u8f83\u5927\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b9e\u4f53\u63cf\u8ff0\u5f02\u6784\u3001\u5173\u7cfb\u5206\u5e03\u957f\u5c3e\u3001\u7f3a\u4e4f\u7edf\u4e00\u7ed3\u6784\u3002\u9700\u8981\u7ed3\u5408\u8bed\u4e49\u548c\u7ed3\u6784\u4f18\u52bf\uff0c\u63d0\u9ad8\u62bd\u53d6\u51c6\u786e\u6027\u548c\u8986\u76d6\u9762\u3002", "method": "\u63d0\u51fa\u4e86LEC-KG\u53cc\u5411\u534f\u4f5c\u6846\u67b6\uff1a\u5305\u62ec\u5c42\u6b21\u5316\u7c97\u5230\u7ec6\u5173\u7cfb\u62bd\u53d6\u4ee5\u89e3\u51b3\u957f\u5c3e\u95ee\u9898\uff0c\u57fa\u4e8e\u8bc1\u636e\u7684\u94fe\u5f0f\u601d\u8003\u53cd\u9988\u5c06\u7ed3\u6784\u5efa\u8bae\u56de\u6eaf\u5230\u539f\u6587\uff0c\u4ee5\u53ca\u8bed\u4e49\u521d\u59cb\u5316\u652f\u6301\u672a\u89c1\u5b9e\u4f53\u7684\u7ed3\u6784\u6821\u9a8c\u3002KGE\u4e0eLLM\u6a21\u5757\u53cd\u590d\u4ea4\u4e92\uff0c\u4e92\u76f8\u63d0\u5347\u3002", "result": "\u5728\u4e2d\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08SDG\uff09\u62a5\u544a\u4e0a\uff0cLEC-KG\u53d6\u5f97\u76f8\u5bf9LLM\u57fa\u7ebf\u660e\u663e\u63d0\u5347\uff0c\u5c24\u5176\u5bf9\u4e8e\u4f4e\u9891\u5173\u7cfb\u62bd\u53d6\u66f4\u6709\u6548\u3002", "conclusion": "LEC-KG \u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53ef\u5c06\u653f\u7b56\u7c7b\u975e\u7ed3\u6784\u5316\u6587\u672c\u9ad8\u6548\u8f6c\u5316\u4e3a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\uff0c\u89e3\u51b3\u4e86\u591a\u4e2a\u73b0\u5b9e\u6311\u6218\u3002"}}
{"id": "2602.01173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01173", "abs": "https://arxiv.org/abs/2602.01173", "authors": ["Lancheng Gao", "Ziheng Jia", "Zixuan Xing", "Wei Sun", "Huiyu Duan", "Guangtao Zhai", "Xiongkuo Min"], "title": "EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment", "comment": null, "summary": "Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EEmoDB\u6570\u636e\u96c6\u548cEEmo-Logic\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u7247\u5f15\u53d1\u60c5\u611f\u7684\u591a\u7ef4\u5ea6\u3001\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u56fe\u7247\u60c5\u611f\u7406\u89e3\u4e0a\u5b58\u5728\u7c92\u5ea6\u7c97\u3001\u63a8\u7406\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u673a\u5668\u540c\u7406\u5fc3\u548c\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u7684\u9700\u6c42\u3002\u4f5c\u8005\u65e8\u5728\u63a8\u52a8\u8ba1\u7b97\u673a\u5bf9\u56fe\u7247\u60c5\u611f\u7684\u7ec6\u81f4\u8bc6\u522b\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u4e86EEmoDB\u6570\u636e\u96c6\uff0c\u5305\u62ec125k\u56fe\u7247\u76841.2M\u81ea\u52a8\u751f\u6210QA\u5bf9\uff08EEmoDB-QA\uff09\uff0c\u4ee5\u53ca25k\u56fe\u7247\u76843.6\u4e07\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff08EEmoDB-Assess\uff09\uff0c\u6db5\u76d65\u4e2a\u5206\u6790\u7ef4\u5ea6\u548c5\u7c7b\u4efb\u52a1\u3002\n2. \u63d0\u51faEEmo-Logic\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u7fa4\u4f53\u76f8\u5bf9\u504f\u597d\u4f18\u5316\uff08GRPO\uff09\u7ed3\u5408\u65b0\u578b\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u60c5\u611f\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "EEmo-Logic\u6a21\u578b\u5728EEmoDB\u53ca\u8de8\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u60c5\u611f\u95ee\u7b54\u4e0e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4efb\u52a1\u5747\u53d6\u5f97\u9886\u5148\u8868\u73b0\uff0c\u8868\u73b0\u51fa\u5f3a\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EEmoDB\u6570\u636e\u96c6\u548cEEmo-Logic\u6a21\u578b\u5927\u5e45\u63d0\u5347\u4e86\u56fe\u7247\u60c5\u611f\u7406\u89e3\u7684\u7c92\u5ea6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u540c\u7406\u5fc3\u548c\u9ad8\u7ea7\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.02099", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02099", "abs": "https://arxiv.org/abs/2602.02099", "authors": ["Keqin Peng", "Yuanxin Ouyang", "Xuebo Liu", "Zhiliang Tian", "Ruijian Han", "Yancheng Yuan", "Liang Ding"], "title": "Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u60e9\u673a\u5236\uff08DDCA\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u65b9\u6cd5\u5728\u6fc0\u53d1\u591a\u6b65\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u5bb9\u6613\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u8f68\u8ff9\u3002\u7b80\u5355\u7684\u957f\u5ea6\u60e9\u7f5a\u4f1a\u4e25\u91cd\u635f\u5bb3\u51c6\u786e\u7387\uff0c\u5bfc\u81f4\u6548\u7387\u4e0e\u51c6\u786e\u6027\u96be\u4ee5\u517c\u5f97\u3002\u5206\u6790\u53d1\u73b0\uff0c\u95ee\u9898\u4e3b\u8981\u5728\u4e8e\uff081\uff09\u957f\u5ea6\u57fa\u7ebf\u7a00\u91ca\uff1a\u9519\u8bef\u7b54\u6848\uff08\u5956\u52b1\u4e3a0\uff09\u538b\u4f4e\u4e86\u57fa\u7ebf\uff0c\u6b63\u786e\u89e3\u53d7\u5230\u8fc7\u5ea6\u60e9\u7f5a\uff1b\uff082\uff09\u96be\u5ea6-\u60e9\u7f5a\u5931\u914d\uff1a\u9759\u6001\u60e9\u7f5a\u65e0\u6cd5\u9488\u5bf9\u9898\u76ee\u96be\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u3002", "method": "\u63d0\u51faDynamic Decoupled Conditional Advantage\uff08DDCA\uff09\u65b9\u6cd5\uff0c\u5177\u4f53\u505a\u6cd5\u4e3a\uff1a\u53ea\u5728\u6b63\u786e\u89e3\u7c07\u5185\u90e8\u8ba1\u7b97\u957f\u5ea6\u4f18\u52bf\uff0c\u675c\u7edd\u57fa\u7ebf\u7a00\u91ca\uff1b\u7528\u5c0f\u7ec4\u901a\u8fc7\u7387\u6307\u6807\u52a8\u6001\u8c03\u6574\u60e9\u7f5a\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u96be\u5ea6\u81ea\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u5957\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728GSM8K\u3001MATH500\u3001AMC23\u3001AIME25\u7b49\u6570\u636e\u96c6\u4e0a\uff0cDDCA\u76f8\u8f83\u4e8e\u539f\u6709\u81ea\u9002\u5e94\u57fa\u7ebf\uff0c\u80fd\u5927\u5e45\u51cf\u5c11\u8f93\u51fatoken\u6570\u91cf\uff08\u5982\u7b80\u5355\u4efb\u52a1\u4e0a\u51cf\u5c11\u7ea660%\uff0c\u96be\u4efb\u52a1\u4e0a\u51cf\u5c1120%\u4ee5\u4e0a\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "DDCA\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002\u5b83\u89e3\u51b3\u4e86\u4f20\u7edf\u957f\u5ea6\u60e9\u7f5a\u673a\u5236\u7684\u7ed3\u6784\u6027\u5f0a\u7aef\uff0c\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01183", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01183", "abs": "https://arxiv.org/abs/2602.01183", "authors": ["Chunming He", "Rihan Zhang", "Fengyang Xiao", "Dingming Zhang", "Zhiwen Cao", "Sina Farsiu"], "title": "Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion", "comment": "8 figures, 11 tables", "summary": "Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCurriSeg\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u53cd\u8bfe\u7a0b\u539f\u5219\uff0c\u63d0\u5347\u4e86\u5bf9\u4e0a\u4e0b\u6587\u7f20\u7ed5\u5185\u5bb9\u5206\u5272\u4efb\u52a1\uff08\u5982\u4f2a\u88c5\u7269\u4f53\u68c0\u6d4b\uff09\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u751f\u7269\u5b66\u4e60\u9075\u5faa\u4ece\u6613\u5230\u96be\u7684\u89c4\u5f8b\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8ba4\u77e5\u548c\u9c81\u68d2\u6027\u3002\u73b0\u6709\u5206\u5272\u65b9\u6cd5\u5728\u5e94\u5bf9\u76ee\u6807\u4e0e\u80cc\u666f\u89c6\u89c9\u7279\u5f81\u76f8\u4f3c\uff08\u4e0a\u4e0b\u6587\u7f20\u7ed5\uff09\u7684\u4efb\u52a1\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u7f51\u7edc\u7ed3\u6784\u6539\u8fdb\uff0c\u800c\u5ffd\u7565\u4e86\u5b66\u4e60\u7b56\u7565\u5bf9\u9c81\u68d2\u6027\u7684\u8d21\u732e\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u5904\u7406\u8fd9\u7c7b\u6311\u6218\u6027\u4efb\u52a1\uff0c\u4f5c\u8005\u53d7\u8bfe\u7a0b\u5b66\u4e60\u542f\u53d1\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "method": "CurriSeg\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u8bfe\u7a0b\u7b5b\u9009\uff0c\u901a\u8fc7\u5206\u6790\u6837\u672c\u635f\u5931\u7684\u65f6\u95f4\u7edf\u8ba1\uff0c\u52a8\u6001\u6311\u9009\u96be\u800c\u6709\u4fe1\u606f\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u6392\u9664\u566a\u58f0\u6837\u672c\uff0c\u5b9e\u73b0\u7a33\u5065\u80fd\u529b\u63d0\u5347\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u53cd\u8bfe\u7a0b\u4fc3\u8fdb\uff0c\u901a\u8fc7\u9891\u8c31\u76f2\u5fae\u8c03\uff0c\u6291\u5236\u9ad8\u9891\u4fe1\u606f\uff0c\u5f3a\u5316\u5bf9\u4f4e\u9891\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u4e0d\u589e\u52a0\u6a21\u578b\u53c2\u6570\uff0c\u4e5f\u4e0d\u589e\u52a0\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u4e0a\u4e0b\u6587\u7f20\u7ed5\u5185\u5bb9\u5206\u5272\u57fa\u51c6\u4e0a\uff0cCurriSeg\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "conclusion": "CurriSeg\u6709\u6548\u5229\u7528\u8bfe\u7a0b\u4e0e\u53cd\u8bfe\u7a0b\u7684\u8fdb\u9636\u4e0e\u6311\u6218\u5173\u7cfb\uff0c\u5728\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02104", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02104", "abs": "https://arxiv.org/abs/2602.02104", "authors": ["Shaltiel Shmidman", "Avi Shmidman", "Amir DN Cohen", "Moshe Koppel"], "title": "Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs", "comment": null, "summary": "Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86Dicta-LM 3.0\uff0c\u4e00\u5957\u4e13\u4e3a\u5e0c\u4f2f\u6765\u8bed\u548c\u82f1\u8bed\u8bad\u7ec3\u7684\u5927\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00 LLM \u8bad\u7ec3\u96be\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u529e\u6cd5\u3002", "motivation": "\u5728\u5927\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u4e30\u5bcc\u7684\u80cc\u666f\u4e0b\uff0c\u975e\u82f1\u8bed\uff08\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u5e0c\u4f2f\u6765\u8bed\uff09\u7684\u5927\u6a21\u578b\u4f9d\u7136\u7a00\u7f3a\uff0c\u5bfc\u81f4\u5bf9\u4e3b\u6743\u578b LLM \u7684\u5f3a\u70c8\u9700\u6c42\u3002", "method": "\u57fa\u4e8eMistral-Small-3.1\u3001NVIDIA Nemotron Nano V2\u548cQwen3-1.7B\u4e09\u5927\u4e3b\u6a21\u578b\uff0c\u5206\u522b\u8bad\u7ec3\u3001\u8c03\u6574\u51fa24B\u300112B\u30011.7B\u4e09\u4e2a\u4e0d\u540c\u53c2\u6570\u91cf\u7684\u6a21\u578b\uff0c\u8986\u76d6base\u548cchat\u7248\u672c\uff08\u542b\u5de5\u5177\u8c03\u7528\u652f\u6301\uff09\uff0c\u6bcf\u4e2a\u6a21\u578b\u652f\u630165k\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u5e76\u65b0\u6784\u5efa\u4e86\u6db5\u76d6\u7ffb\u8bd1\u3001\u6458\u8981\u3001Winograd\u3001\u4ee5\u8272\u5217\u77e5\u8bc6\u95ee\u7b54\u548c\u97f3\u6807\u8fd8\u539f\u7684\u5e0c\u4f2f\u6765\u8bed\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u4e25\u8c28\u8bc4\u6d4b\u3002", "result": "\u6210\u529f\u53d1\u5e03\u4e86\u4e09\u79cd\u53c2\u6570\u91cf\u548c\u591a\u79cd\u53d8\u4f53\u7684\u5e0c\u4f2f\u6765-\u82f1\u8bed LLM\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u4e1a\u8bc4\u6d4b\u57fa\u51c6\uff0c\u9a8c\u8bc1\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00 LLM \u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u75db\u70b9\uff0c\u63d0\u51fa\u7684\u6784\u5efa\u548c\u9002\u914d\u6d41\u7a0b\u4e3a\u591a\u8bed\u79cdNLP\u9886\u57df\u63d0\u4f9b\u4e86\u501f\u9274\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u975e\u82f1\u8bed\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01194", "abs": "https://arxiv.org/abs/2602.01194", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Fenghua Ling", "Lei Bai"], "title": "EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting", "comment": null, "summary": "Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u53d8\u6362\u5668\uff08EMFormer\uff09\u53ca\u5b8c\u6574\u9884\u6d4b\u6d41\u7a0b\uff0c\u4ee5\u63d0\u5347\u957f\u671f\u5929\u6c14\u9884\u62a5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u671f\u5929\u6c14\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5fae\u8c03\u6280\u672f\u5ef6\u957f\u9884\u6d4b\u65f6\u957f\uff0c\u4f46\u4f1a\u9047\u5230\u707e\u96be\u6027\u9057\u5fd8\u3001\u8bef\u5dee\u79ef\u7d2f\u548c\u9ad8\u8bad\u7ec3\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u9ad8\u957f\u65f6\u6bb5\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faEMFormer\u67b6\u6784\uff0c\u901a\u8fc7\u4e00\u6b21\u5377\u79ef\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002\u5f15\u5165\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5fae\u8c03\u673a\u5236\u6539\u5584\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u907f\u514d\u635f\u5931\u77ed\u65f6\u9884\u6d4b\u80fd\u529b\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6b63\u5f26\u6743\u91cd\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u8282\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5404\u9879\u635f\u5931\u3002", "result": "\u672c\u65b9\u6cd5\u5728\u5929\u6c14\u548c\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u3002\u5728\u56fe\u50cf\u8bc6\u522b\u7b49\u89c6\u89c9\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u591a\u5c3a\u5ea6\u6a21\u5757\u5feb5.69\u500d\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "EMFormer\u67b6\u6784\u53ca\u5176\u7aef\u5230\u7aef\u6d41\u7a0b\u53ef\u517c\u987e\u957f\u671f\u9884\u6d4b\u51c6\u786e\u5ea6\u548c\u9ad8\u6548\u7387\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u5f0a\u7aef\uff0c\u5bf9\u5929\u6c14\u76f8\u5173\u9886\u57df\u548c\u66f4\u5e7f\u6cdb\u7684\u89c6\u89c9\u4efb\u52a1\u90fd\u5177\u5907\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02108", "abs": "https://arxiv.org/abs/2602.02108", "authors": ["Wenhao Li", "Daohai Yu", "Gen Luo", "Yuxin Zhang", "Fei Chao", "Rongrong Ji", "Yifan Wu", "Jiaxin Liu", "Ziyang Gong", "Zimu Liao"], "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts", "comment": null, "summary": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOOMB\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u6027\u6280\u672f\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u663e\u5b58\u6d88\u8017\uff0c\u4f7f4M-token\u957f\u5ea6\u7684\u8bad\u7ec3\u5728\u5355\u5361\u5373\u53ef\u5b9e\u73b0\u3002", "motivation": "\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8eGPU\u663e\u5b58\u5f00\u9500\uff0c\u5c24\u5176\u662f\u6fc0\u6d3b\u503c\u5360\u7528\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u6210\u4e3a\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u91c7\u7528chunk-recurrent\u6846\u67b6\u7ed3\u5408\u6fc0\u6d3b\u91cd\u8ba1\u7b97\uff0c\u5b9e\u73b0\u6fc0\u6d3b\u503cO(1)\u663e\u5b58\u5360\u7528\uff0c\u5e76\u9488\u5bf9KV cache\u91c7\u7528\u5206\u9875\u7ba1\u7406\u3001\u5f02\u6b65CPU\u8fc1\u79fb\u548c\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u6280\u672f\u964d\u4f4e\u5185\u5b58\u4e0e\u901a\u4fe1\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6bcf\u589e\u52a01\u4e07tokens\uff0c\u8bad\u7ec3\u663e\u5b58\u4ec5\u589e\u52a010MB\uff1bQwen2.5-7B\u5728\u5355\u5f20H200 GPU\u4e0a\u5373\u53ef\u4ee54M-token context\u8bad\u7ec3\uff0c\u539f\u672c\u9700\u8981\u5206\u5e03\u5f0f\u5927\u96c6\u7fa4\u3002", "conclusion": "OOMB\u5927\u5e45\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8d44\u6e90\u6548\u7387\uff0c\u5f00\u521b\u4e86\u5355\u5361\u9ad8\u6548\u8bad\u7ec3\u65b0\u6a21\u5f0f\u3002\u6e90\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01200", "abs": "https://arxiv.org/abs/2602.01200", "authors": ["Haoran Lai", "Zihang Jiang", "Kun Zhang", "Qingsong Yao", "Rongsheng Wang", "Zhiyang He", "Xiaodong Tao", "Wei Wei", "Shaohua Kevin Zhou"], "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis", "comment": null, "summary": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Med3D-R1\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bad\u7ec3\u673a\u5236\u548c\u5956\u52b1\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u533b\u5b66\u56fe\u50cf-\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u5728\u4e3b\u6d41\u57fa\u51c6\u4e0a\u53d6\u5f97\u9886\u5148\u3002", "motivation": "\u76ee\u524d3D\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u573a\u666f\u4e0b\u9762\u4e34\u4f53\u79ef\u6570\u636e\u590d\u6742\u3001\u8fc7\u62df\u5408\u6587\u672c\u8868\u9762\u6a21\u5f0f\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u5956\u52b1\u7b49\u96be\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u63a8\u7406\u548c\u8bca\u65ad\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u4f5c\u8005\u63d0\u51faMed3D-R1\uff0c\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u4e00\u662f\u76d1\u7763\u5fae\u8c03\uff08\u52a0\u5165\u6b8b\u5dee\u5bf9\u9f50\u89e3\u51b33D\u7279\u5f81\u548c\u6587\u672c\u5d4c\u5165\u7684\u5dee\u5f02\uff0c\u5f02\u5e38\u52a0\u6743\u51cf\u5c11\u7ed3\u6784\u6027\u504f\u5dee\uff09\uff0c\u4e8c\u662f\u5f3a\u5316\u5b66\u4e60\uff08\u91cd\u65b0\u8bbe\u8ba1\u5956\u52b1\u4ee5\u4fc3\u8fdb\u5206\u6b65\u3001\u8fde\u8d2f\u7684\u8bca\u65ad\u63a8\u7406\u8fc7\u7a0b\uff09\u3002", "result": "\u5728CT-RATE\u4e0eRAD-ChestCT\u4e24\u4e2a3D\u8bca\u65ad\u57fa\u51c6\u6570\u636e\u96c6\u7684\u591a\u9879\u9009\u62e9\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u5206\u522b\u53d6\u5f9741.92%\u548c44.99%\u7684\u51c6\u786e\u7387\uff0c\u5747\u4e3a\u5f53\u524d\u6700\u4f18\u3002", "conclusion": "Med3D-R1\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5f02\u5e38\u8bc6\u522b\u4e0e\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u6709\u671b\u4e3a\u5b9e\u9645\u533b\u5b66\u8bca\u65ad\u6d41\u7a0b\u5e26\u6765\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u76843D\u89c6\u89c9-\u8bed\u8a00\u7cfb\u7edf\u3002"}}
{"id": "2602.02132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02132", "abs": "https://arxiv.org/abs/2602.02132", "authors": ["Faaiz Joad", "Majd Hawasly", "Sabri Boughorbel", "Nadir Durrani", "Husrev Taha Sencar"], "title": "There Is More to Refusal in Large Language Models than a Single Direction", "comment": null, "summary": "Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\u5e76\u975e\u7531\u5355\u4e00\u6fc0\u6d3b\u7a7a\u95f4\u65b9\u5411\u63a7\u5236\uff0c\u800c\u662f\u5b58\u5728\u591a\u79cd\u4e0d\u540c\u7684\u6fc0\u6d3b\u65b9\u5411\uff0c\u6bcf\u79cd\u884c\u4e3a\u5bf9\u5e94\u4e0d\u540c\u7684\u7a7a\u95f4\uff0c\u4f46\u662f\u901a\u8fc7\u7b80\u5355\u7ebf\u6027\u64cd\u4f5c\u4f9d\u7136\u53ef\u5b9e\u73b0\u76f8\u4f3c\u7684\u62d2\u7edd\u8c03\u8282\u6548\u679c\u3002", "motivation": "\u6b64\u524d\u7814\u7a76\u8ba4\u4e3a\uff0c\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5355\u4e00\u8def\u5f84\u8fdb\u884c\u64cd\u63a7\uff0c\u63a7\u5236\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\uff1b\u672c\u6587\u8d28\u7591\u5e76\u6269\u5c55\u4e86\u8fd9\u79cd\u770b\u6cd5\uff0c\u65e8\u5728\u66f4\u6df1\u5165\u7406\u89e3\u62d2\u7edd\u884c\u4e3a\u7684\u672c\u8d28\u53ca\u5176\u53ef\u63a7\u6027\u3002", "method": "\u4f5c\u8005\u9488\u5bf9\u5341\u4e00\u7c7b\u62d2\u7edd\u548c\u4e0d\u670d\u4ece\u884c\u4e3a\uff08\u5305\u62ec\u5b89\u5168\u3001\u8bf7\u6c42\u4e0d\u5168/\u65e0\u652f\u6301\u3001\u4eba\u6027\u5316\u3001\u8fc7\u5ea6\u62d2\u7edd\u7b49\uff09\uff0c\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5206\u6790\u8fd9\u4e9b\u884c\u4e3a\u7684\u51e0\u4f55\u65b9\u5411\uff0c\u5e76\u6d4b\u8bd5\u7ebf\u6027\u8c03\u8282\u7b56\u7565\u5bf9\u5404\u7c7b\u62d2\u7edd\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u62d2\u7edd\u548c\u4e0d\u670d\u4ece\u884c\u4e3a\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5177\u6709\u4e0d\u540c\u51e0\u4f55\u65b9\u5411\uff0c\u4f46\u5728\u7ebf\u6027\u8c03\u8282\u65f6\uff0c\u5404\u65b9\u5411\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u62d2\u7edd\u4e0e\u8fc7\u5ea6\u62d2\u7edd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5f62\u6210\u4e00\u79cd\u901a\u7528\u7684\u4e00\u7ef4\u63a7\u5236\u673a\u5236\u3002", "conclusion": "\u5c3d\u7ba1\u62d2\u7edd\u884c\u4e3a\u6709\u591a\u79cd\u51e0\u4f55\u8868\u73b0\u5f62\u5f0f\uff0c\u4f46\u7ebf\u6027\u8c03\u8282\u4efb\u4e00\u76f8\u5173\u65b9\u5411\u90fd\u80fd\u4ee5\u7c7b\u4f3c\u65b9\u5f0f\u8c03\u8282\u62d2\u7edd\u7279\u6027\uff1b\u4e0d\u540c\u65b9\u5411\u7684\u4e3b\u8981\u4f5c\u7528\u5728\u4e8e\u5f71\u54cd\u6a21\u578b\u201c\u5982\u4f55\u62d2\u7edd\u201d\uff0c\u800c\u975e\u201c\u662f\u5426\u62d2\u7edd\u201d\u3002"}}
{"id": "2602.01257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01257", "abs": "https://arxiv.org/abs/2602.01257", "authors": ["Yunchuan Ma", "Laiyun Qing", "Guorong Li", "Yuqing Liu", "Yuankai Qi", "Qingming Huang"], "title": "Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment", "comment": null, "summary": "Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u6587\u672c\u4e0e\u89c6\u89c9\u4fe1\u606f\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u70b9\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u70b9\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u5173\u6ce8\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u606f\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u6587\u672c\u8bed\u4e49\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u6587\u672c\u7ec6\u5316\u4e0e\u5bf9\u9f50\uff08TRA\uff09\u6846\u67b6\uff0c\u5305\u62ec\u70b9\u7ea7\u6587\u672c\u7ec6\u5316\u6a21\u5757\uff08PTR\uff09\u548c\u70b9\u7ea7\u591a\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff08PMA\uff09\u3002\u9996\u5148\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u89c6\u9891\u5e27\u63cf\u8ff0\uff0cPTR\u7ed3\u5408\u70b9\u6807\u6ce8\u4e0e\u591a\u6a21\u578b\u7ec6\u5316\u63cf\u8ff0\uff0cPMA\u5c06\u7279\u5f81\u6295\u5f71\u81f3\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u70b9\u7ea7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u3002\u6700\u7ec8\uff0c\u589e\u5f3a\u7279\u5f81\u7528\u4e8e\u52a8\u4f5c\u68c0\u6d4b\u5668\u5b9a\u4f4d\u3002", "result": "\u5728\u4e94\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\u3002\u6846\u67b6\u53ef\u5728\u5355\u5f2024GB 3090\u663e\u5361\u4e0a\u8fd0\u884c\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u70b9\u76d1\u7763\u52a8\u4f5c\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u52a8\u4f5c\u7406\u89e3\u4efb\u52a1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.02140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02140", "abs": "https://arxiv.org/abs/2602.02140", "authors": ["Chenlong Wang", "Yuhang Chen", "Zhihan Hu", "Dongping Chen", "Wenhu Chen", "Sarah Wiegreffe", "Tianyi Zhou"], "title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models", "comment": null, "summary": "Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGapEval\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff08UMM\uff09\u5728\u7406\u89e3\u548c\u751f\u6210\u4e24\u9879\u80fd\u529b\u4e0a\u7684\u5dee\u8ddd\uff0c\u53d1\u73b0\u5b83\u4eec\u4e4b\u95f4\u4f9d\u7136\u5b58\u5728\u660e\u663e\u7684\u4e0d\u4e00\u81f4\u3002", "motivation": "\u867d\u7136UMM\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e24\u79cd\u80fd\u529b\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u4e86\u7edf\u4e00\u548c\u878d\u5408\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u786e\u5b9a\u6a21\u578b\u5185\u90e8\u7684\u8ba4\u77e5\u4e00\u81f4\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86GapEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u53cc\u5411\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4f7f\u6bcf\u4e2a\u95ee\u9898\u90fd\u53ef\u4ee5\u7528\u56fe\u50cf\u6216\u6587\u672c\u4e24\u79cd\u5f62\u5f0f\u56de\u7b54\uff0c\u4ece\u800c\u5bf9\u6a21\u578b\u7684\u53cc\u5411\u63a8\u7406\u80fd\u529b\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u8fdb\u884c\u5bf9\u79f0\u8bc4\u6d4b\u3002\u540c\u65f6\uff0c\u5f00\u5c55\u4e86\u77e5\u8bc6\u64cd\u63a7\u76f8\u5173\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5927\u91cf\u4e0d\u540c\u67b6\u6784\u7684UMM\u5728\u7406\u89e3\u4e0e\u751f\u6210\u4e24\u4e2a\u65b9\u5411\u4e0a\u5b58\u5728\u6301\u7eed\u6027\u5dee\u8ddd\uff0c\u8868\u660e\u76ee\u524d\u7684\u6a21\u578b\u53ea\u5b9e\u73b0\u4e86\u8868\u9762\u7684\u7edf\u4e00\uff0c\u5e76\u672a\u8fbe\u5230\u6df1\u5c42\u8ba4\u77e5\u878d\u5408\u3002\u6b64\u5916\uff0c\u77e5\u8bc6\u5728\u591a\u6a21\u6001\u95f4\u8868\u73b0\u4e3a\u5272\u88c2\u72b6\u6001\u3002", "conclusion": "\u5f53\u524dUMM\u5728\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e0a\u8fd8\u672a\u5b9e\u73b0\u77e5\u8bc6\u4e0e\u80fd\u529b\u7684\u6df1\u5ea6\u6574\u5408\uff0c\u8de8\u6a21\u6001\u7684\u4e00\u81f4\u6027\u548c\u80fd\u529b\u540c\u6b65\u6027\u5747\u6709\u5f85\u63d0\u5347\u3002\u8fd9\u4e3a\u540e\u7eed\u6a21\u578b\u7814\u7a76\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.02159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02159", "abs": "https://arxiv.org/abs/2602.02159", "authors": ["Lingkun Long", "Yushi Huang", "Shihao Bai", "Ruihao Gong", "Jun Zhang", "Ao Zhou", "Jianlei Yang"], "title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM", "AI": {"tldr": "Focus-dLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u6846\u67b6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65f6\u7684\u901f\u5ea6\uff0832K\u4e0a\u4e0b\u6587\u4e0b\u63d0\u901f\u8d8529\u500d\uff09\uff0c\u4e14\u4e0d\u635f\u5931\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u975e\u81ea\u56de\u5f52\u89e3\u7801\u548c\u5f3a\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u800c\u53d7\u5173\u6ce8\uff0c\u4f46\u56e0\u53cc\u5411\u5168\u6ce8\u610f\u529b\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5bf9\u672a\u77e5token\u4f4d\u7f6e\u5904\u7406\u4e0d\u4f73\uff0c\u9700\u65b0\u7684\u9ad8\u6548\u7a00\u758f\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u76f8\u5173\u6027\u7684\u5386\u53f2\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u6307\u793a\u5668\uff0c\u7528\u4ee5\u9884\u6d4b\u672a\u63a9\u7801\u533a\u57df\uff0c\u5e76\u63d0\u51fasink-aware\u526a\u679d\u7b56\u7565\uff0c\u7cbe\u51c6\u4f30\u7b97\u5e76\u53bb\u9664\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u4fdd\u7559\u5173\u952e\u7684\u6ce8\u610f\u529bsink\u70b9\u3002\u6b64\u5916\uff0c\u5c06sink\u70b9\u5728\u4e0d\u540c\u5c42\u95f4\u590d\u7528\uff0c\u51cf\u5c0f\u8fdb\u4e00\u6b65\u5f00\u9500\u3002", "result": "\u572832K\u957f\u4e0a\u4e0b\u6587\u4e0b\uff0c\u65e0\u635f\u6027\u80fd\u52a0\u901f\u8d85\u8fc729\u500d\u3002\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u80fd\u517c\u987e\u51c6\u786e\u7387\u4e0e\u663e\u8457\u63a8\u7406\u6548\u7387\u63d0\u5347\u3002", "conclusion": "Focus-dLLM\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u80fd\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u652f\u6301\u957f\u4e0a\u4e0b\u6587dLLM\u63a8\u7406\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2602.01273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01273", "abs": "https://arxiv.org/abs/2602.01273", "authors": ["Xun Zhang", "Kaicheng Yang", "Hongliang Lu", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution", "comment": "Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR", "summary": "Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\\times$ and computational operations by over 60$\\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Q-DiT4SR\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u4e3aDiffusion Transformer(DiT)\u67b6\u6784\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08Real-ISR\uff09\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u7eb9\u7406\u3002", "motivation": "\u76ee\u524dDiT\u5728Real-ISR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u63a8\u7406\u8017\u65f6\u4e25\u91cd\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u867d\u7136PTQ\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u6a21\u578b\uff0c\u4f46\u73b0\u6709PTQ\u6280\u672f\u591a\u9488\u5bf9U-Net\u6216\u6587\u672c\u5230\u56fe\u50cf\u7684DiT\u6a21\u578b\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eDiT\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u5426\u5219\u4f1a\u5bfc\u81f4\u7eb9\u7406\u635f\u5931\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u4e2a\u4e13\u9488\u5bf9DiT-Real-ISR\u6a21\u578b\u7684\u9ad8\u6548\u91cf\u5316\u65b9\u6848\u3002", "method": "\uff081\uff09\u63d0\u51faH-SVD\uff0c\u5c06\u5168\u5c40\u4f4e\u79e9\u5206\u652f\u4e0e\u5c40\u90e8\u5206\u5757\u79e9\u4e3a1\u7684\u5206\u652f\u7ed3\u5408\uff0c\u5728\u53c2\u6570\u9884\u7b97\u53d7\u63a7\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5206\u89e3\uff1b\uff082\uff09\u63d0\u51faVaSMP\uff08\u65b9\u5dee\u611f\u77e5\u7684\u7a7a\u95f4-\u65f6\u95f4\u6df7\u5408\u7cbe\u5ea6\uff09\uff0c\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u81ea\u52a8\u5206\u914d\u6743\u91cd\u91cf\u5316\u6bd4\u7279\u5bbd\u5ea6\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5b9e\u73b0\u6fc0\u6d3b\u7cbe\u5ea6\u968f\u6269\u6563\u6b65\u8c03\u52a8\u6001\u8c03\u6574\uff0c\u65e0\u9700\u4f9d\u8d56\u6570\u636e\u6216\u8fc7\u591a\u6821\u51c6\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cQ-DiT4SR\u5728W4A6\u548cW4A4\u4e24\u79cd\u91cf\u5316\u7cbe\u5ea6\u4e0b\u5747\u5b9e\u73b0\u4e86\u76ee\u524d\u6700\u4f18\u6027\u80fd\u3002\u5c24\u5176\u662f\u5728W4A4\u8bbe\u7f6e\u4e0b\uff0c\u6a21\u578b\u4f53\u79ef\u51cf\u5c115.8\u500d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1160\u500d\u3002", "conclusion": "Q-DiT4SR\u4e3aDiT\u5728Real-ISR\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9ad8\u6027\u80fdSR\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u653e\u3002"}}
{"id": "2602.02160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02160", "abs": "https://arxiv.org/abs/2602.02160", "authors": ["Bowen Xu", "Shaoyu Wu", "Hao Jiang", "Kai Liu", "Xin Chen", "Lulu Hu", "Bin Yang"], "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use", "comment": null, "summary": "Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5$\\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e0b\u7f3a\u4e4f\u5b50\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6D-CORE\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5de5\u5177\u4f7f\u7528\u548c\u63a8\u7406\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5237\u65b0\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709LRMs\u5728\u9762\u5bf9\u590d\u6742\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u65f6\uff0c\u96be\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u5b50\u4efb\u52a1\u5206\u89e3\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u5b58\u5728\u201c\u61d2\u60f0\u63a8\u7406\u201d\uff08Lazy Reasoning\uff09\u95ee\u9898\u3002\u8fd9\u79cd\u4e0d\u8db3\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faD-CORE\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u81ea\u84b8\u998f\u589e\u5f3a\u6a21\u578b\u5b50\u4efb\u52a1\u5206\u89e3\u63a8\u7406\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6062\u590d\u6a21\u578b\u7684\u53cd\u601d\u63a8\u7406\u80fd\u529b\u3002", "result": "D-CORE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u52b2\u6027\u80fd\u63d0\u5347\u3002\u5728BFCLv3\u6570\u636e\u96c6\u4e0a\uff0cD-CORE-8B\u6a21\u578b\u51c6\u786e\u7387\u8fbe77.7%\uff0c\u6bd4\u73b0\u6709\u6700\u4f188B\u6a21\u578b\u9ad85.7%\uff1bD-CORE-14B\u6a21\u578b\u8fbe\u523079.3%\uff0c\u8d85\u8d8a\u4e86\u53c2\u6570\u91cf\u59275\u500d\u768470B\u6a21\u578b\u3002", "conclusion": "D-CORE\u6781\u5927\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u4efb\u52a1\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2602.01277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01277", "abs": "https://arxiv.org/abs/2602.01277", "authors": ["Yihan Xie", "Han Xia", "Zhen Yang"], "title": "TF-Lane: Traffic Flow Module for Robust Lane Perception", "comment": "9 pages, 7 figures, 7 tables", "summary": "Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u4ea4\u901a\u6d41\u4fe1\u606f\u7684\u8f66\u9053\u611f\u77e5\u6a21\u5757\uff08TFM\uff09\uff0c\u5728\u591a\u79cd\u5f00\u6e90\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u80fd\u6709\u6548\u63d0\u5347\u8f66\u9053\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u7684\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\u5728\u89c6\u89c9\u7ebf\u7d22\u4e0d\u8db3\uff08\u5982\u906e\u6321\u3001\u8f66\u9053\u7ebf\u7f3a\u5931\uff09\u65f6\uff0c\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff1b\u800c\u5229\u7528\u9ad8\u7cbe\u5730\u56fe\u867d\u80fd\u63d0\u5347\u8868\u73b0\uff0c\u4f46\u5b58\u5728\u6210\u672c\u9ad8\u3001\u5b9e\u65f6\u6027\u5dee\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u65e0\u9700\u989d\u5916\u9ad8\u6210\u672c\u3001\u4e14\u5b9e\u65f6\u6027\u5f3a\u7684\u65b0\u578b\u8f85\u52a9\u4fe1\u606f\u6e90\u3002", "method": "\u63d0\u51faTFM\u6a21\u5757\uff0c\u80fd\u591f\u5b9e\u65f6\u63d0\u53d6\u4ea4\u901a\u6d41\u7279\u5f81\uff0c\u5e76\u4e0e\u73b0\u6709\u8f66\u9053\u611f\u77e5\u7b97\u6cd5\u65e0\u7f1d\u878d\u5408\u3002\u4f5c\u8005\u901a\u8fc7\u5728\u73b0\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5f97\u51fa\u89e3\u51b3\u601d\u8def\uff0c\u91c7\u7528\u4e3b\u6d41\u5f00\u6e90\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728Nuscenes\u3001OpenLaneV2\u7b49\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u56db\u79cd\u4e3b\u6d41\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cTFM\u6a21\u5757\u53ef\u5e26\u6765\u6301\u7eed\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728Nuscenes\u6570\u636e\u96c6\u4e0amAP\u6700\u9ad8\u63d0\u53474.1%\u3002", "conclusion": "\u878d\u5408\u4ea4\u901a\u6d41\u4fe1\u606f\u7684TFM\u6a21\u5757\u65e0\u9700\u989d\u5916\u9ad8\u6210\u672c\u6295\u5165\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u73b0\u6709\u8f66\u9053\u611f\u77e5\u7cfb\u7edf\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u8f66\u9053\u68c0\u6d4b\u80fd\u529b\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02178", "abs": "https://arxiv.org/abs/2602.02178", "authors": ["Liang Lin", "Feng Xiong", "Zengbin Wang", "Kun Wang", "Junhao Dong", "Xuecai Hu", "Yong Wang", "Xiangxiang Chu"], "title": "AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6AR-MAP\uff0c\u65e0\u9700\u76f4\u63a5\u5bf9Diffusion\u5927\u8bed\u8a00\u6a21\u578b\uff08DLLM\uff09\u8fdb\u884c\u9ad8\u65b9\u5dee\u504f\u597d\u5bf9\u9f50\uff0c\u800c\u662f\u901a\u8fc7\u5df2\u6709\u7684\u81ea\u56de\u5f52\u5927\u6a21\u578b\u95f4\u63a5\u5b9e\u73b0\u6a21\u578b\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86DLLM\u5728\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "DLLM\u867d\u7136\u652f\u6301\u9ad8\u6548\u7684\u5e76\u884c\u751f\u6210\uff0c\u4f46\u5728\u504f\u597d\u5bf9\u9f50\u4e0a\u53d7\u9650\u4e8eELBO\u4f30\u8ba1\u6cd5\u5bfc\u81f4\u7684\u9ad8\u65b9\u5dee\uff0c\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u5e0c\u671b\u501f\u52a9\u5df2\u6709\u504f\u597d\u5bf9\u9f50\u3001\u81ea\u56de\u5f52\u5927\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u5347DLLM\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "method": "AR-MAP\u6846\u67b6\u628a\u5df2\u7ecf\u504f\u597d\u5bf9\u9f50\u597d\u7684\u81ea\u56de\u5f52\u5927\u6a21\u578b\uff08AR-LLM\uff09\u5f53\u6210\u9690\u5f0f\u6559\u5e08\uff0c\u901a\u8fc7\u6743\u91cd\u7f29\u653e\u6280\u672f\uff0c\u628a\u5b83\u4eec\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230DLLM\u4e0a\uff0c\u800c\u65e0\u9700\u5bf9DLLM\u91cd\u590d\u9ad8\u8ba1\u7b97\u91cf\u3001\u65b9\u5dee\u5927\u7684\u5bf9\u9f50\u8c03\u4f18\u3002", "result": "AR-MAP\u5728\u5404\u79cd\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u7684\u5e73\u5747\u5f97\u5206\u8fbe\u523069.08%\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524dDLLM\u4e13\u7528\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "AR-MAP\u6846\u67b6\u4e3aDLLM\u5bf9\u9f50\u5e26\u6765\u4e86\u66f4\u9ad8\u6027\u80fd\u548c\u66f4\u4f4e\u5f00\u9500\uff0c\u5c55\u73b0\u4e86\u8fc1\u79fb\u81ea\u81ea\u56de\u5f52\u5927\u6a21\u578b\u7684\u5bf9\u9f50\u77e5\u8bc6\u4e3a\u5e76\u884c\u751f\u6210\u6a21\u578b\u5e26\u6765\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.01278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01278", "abs": "https://arxiv.org/abs/2602.01278", "authors": ["Zhengbo Zhang", "Yihe Tian", "Wanke Xia", "Lin Chen", "Yue Sun", "Kun Ding", "Ying Wang", "Bing Xu", "Shiming Xiang"], "title": "DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction", "comment": null, "summary": "Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u5f71\u50cf\u4e2d\u519c\u6751\u9053\u8def\u63d0\u53d6\u6311\u6218\u7684\u65b0\u65b9\u6cd5DSFC-Net\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u4fe1\u606f\u63d0\u5347\u63d0\u53d6\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u57ce\u5e02\u9053\u8def\u7684\u9065\u611f\u63d0\u53d6\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u519c\u6751\u9053\u8def\u7684\u9ad8\u7c7b\u5185\u53d8\u5f02\u3001\u4f4e\u7c7b\u95f4\u533a\u5206\u3001\u690d\u88ab\u906e\u6321\u548c\u9053\u8def\u72ed\u7a84\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u519c\u6751\u5730\u533a\u9053\u8def\u63d0\u53d6\u51c6\u786e\u7387\u4f4e\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u9ad8\u519c\u6751\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u7684\u9065\u611f\u652f\u6301\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5408\u519c\u6751\u590d\u6742\u73af\u5883\u7684\u9053\u8def\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86DSFC-Net\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5206\u652f\u7f16\u7801\u5668\uff1aCNN\u5206\u652f\u8d1f\u8d23\u6355\u6349\u5c40\u90e8\u7ec6\u7c92\u5ea6\u9053\u8def\u8fb9\u754c\u548c\u77ed\u8ddd\u79bb\u8fde\u7eed\u6027\uff1b\u7a7a\u95f4-\u9891\u7387\u6df7\u5408Transformer (SFT)\u5206\u652f\u901a\u8fc7\u521b\u65b0\u7684\u4ea4\u4e92\u5f0f\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\uff08CFIA\uff09\u7ed3\u5408Laplacian\u91d1\u5b57\u5854\u7b56\u7565\u89e3\u8026\u9ad8\u4f4e\u9891\u4fe1\u606f\uff0c\u63d0\u5347\u5168\u5c40\u8bed\u4e49\u5efa\u6a21\u548c\u5bf9\u690d\u88ab\u906e\u6321\u7684\u9c81\u68d2\u6027\uff1b\u6700\u540e\u901a\u8fc7\u901a\u9053\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08CFFM\uff09\u81ea\u9002\u5e94\u878d\u5408\u53cc\u5206\u652f\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272\u3002", "result": "\u5728WHU-RuR+\u3001DeepGlobe\u4ee5\u53caMassachusetts\u7b49\u6743\u5a01\u9065\u611f\u9053\u8def\u6570\u636e\u96c6\u4e0a\uff0cDSFC-Net\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5404\u9879\u6027\u80fd\u6307\u6807\u5747\u9886\u5148\u3002", "conclusion": "DSFC-Net\u6709\u6548\u89e3\u51b3\u4e86\u519c\u6751\u9053\u8def\u9065\u611f\u63d0\u53d6\u7684\u5173\u952e\u96be\u9898\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u9891\u57df\u7684\u534f\u540c\u5efa\u6a21\u4ee5\u53ca\u5206\u652f\u4fe1\u606f\u7684\u878d\u5408\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u519c\u6751\u9053\u8def\u7684\u63d0\u53d6\u7cbe\u5ea6\uff0c\u5bf9\u57fa\u7840\u8bbe\u65bd\u5efa\u8bbe\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.02182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02182", "abs": "https://arxiv.org/abs/2602.02182", "authors": ["Tja\u0161a Ar\u010don", "Matej Klemen", "Marko Robnik-\u0160ikonja", "Kaja Dobrovoljc"], "title": "Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages", "comment": null, "summary": "Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u5143\u8bed\u8a00\u77e5\u8bc6\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u6709\u9650\uff0c\u53d7\u8bed\u6599\u8d44\u6e90\u4e30\u5bcc\u5ea6\u5f71\u54cd\u660e\u663e\u3002", "motivation": "\u5c3d\u7ba1LLM\u5e38\u7528\u4e8e\u8bed\u8a00\u4efb\u52a1\uff0c\u4f46\u5176\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\u5c1a\u4e0d\u660e\u6670\uff0c\u73b0\u6709\u57fa\u51c6\u591a\u805a\u7126\u9ad8\u8d44\u6e90\u8bed\u8a00\u53ca\u5177\u4f53\u73b0\u8c61\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5143\u8bed\u8a00\u77e5\u8bc6\u7684\u7cfb\u7edf\u8bc4\u6d4b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b\u591a\u8bed\u79cd\u8bed\u8a00\u7ed3\u6784\u95ee\u9898\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u548c\u5b8fF1\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u3001\u8bed\u8a00\u9886\u57df\u548c\u8d44\u6e90\u76f8\u5173\u56e0\u7d20\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4e3b\u6d41\u548c\u968f\u673a\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "GPT-4o\u8868\u73b0\u6700\u4f73\uff080.367\uff09\uff0c\u4f46\u6574\u4f53\u51c6\u786e\u7387\u4ec5\u4e2d\u7b49\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002\u6240\u6709\u6a21\u578b\u867d\u4f18\u4e8e\u968f\u673a\uff0c\u4f46\u672a\u8d85\u8d8a\u4e3b\u6d41\u57fa\u7ebf\uff0c\u8868\u660e\u5176\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u8bed\u6cd5\u533a\u5206\u80fd\u529b\u3002\u8bcd\u6c47\u7279\u5f81\u5f97\u5206\u6700\u9ad8\uff0c\u97f3\u7cfb\u7279\u5f81\u6700\u4f4e\uff0c\u9ad8\u6570\u5b57\u8d44\u6e90\u7684\u8bed\u8a00\u51c6\u786e\u7387\u66f4\u9ad8\u3002\u8bed\u6599\u8d44\u6e90\uff08\u5982\u7ef4\u57fa\u767e\u79d1\u89c4\u6a21\u3001\u8bed\u6599\u53ef\u7528\u6027\uff09\u662f\u8868\u73b0\u7684\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u5f53\u524dLLM\u7684\u5143\u8bed\u8a00\u77e5\u8bc6\u788e\u7247\u5316\uff0c\u4e3b\u8981\u53d7\u6570\u636e\u53ef\u7528\u6027\u5de6\u53f3\uff0c\u800c\u975e\u5177\u5907\u666e\u904d\u8bed\u6cd5\u80fd\u529b\u3002\u6587\u7ae0\u53d1\u5e03\u4e86\u76f8\u5173\u57fa\u51c6\u6570\u636e\uff0c\u4ec5\u4ee5\u4fc3\u8fdb\u66f4\u591a\u5168\u7403\u8bed\u8a00\u591a\u6837\u6027\u548c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002"}}
{"id": "2602.01283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01283", "abs": "https://arxiv.org/abs/2602.01283", "authors": ["Xianhui Zhang", "Chengyu Xie", "Linxia Zhu", "Yonghui Yang", "Weixiang Zhao", "Zifeng Cheng", "Cong Wang", "Fei Shen", "Tat-Seng Chua"], "title": "Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons", "comment": null, "summary": "Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.\n  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.\n  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.\n  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.\n  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.\n  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.\n  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5b58\u5728\u4e00\u7ec4\u8de8\u8bed\u8a00\u5171\u4eab\u7684\u5b89\u5168\u795e\u7ecf\u5143\uff08SS-Neurons\uff09\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u79cdLLM\u5b89\u5168\u6027\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u5747\u8861\uff0c\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u9632\u62a4\u8f83\u5f31\u3002\u800c\u795e\u7ecf\u5c42\u9762\u8de8\u8bed\u79cd\u5b89\u5168\u4e00\u81f4\u6027\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u5143\u5c42\u9762\u7684\u5206\u6790\u548c\u5e72\u9884\uff0c\u63d0\u9ad8\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u6027\u3002", "method": "\u9996\u5148\uff0c\u4f5c\u8005\u8bc6\u522b\u5e76\u9a8c\u8bc1\u4e86\u5355\u8bed\u5b89\u5168\u795e\u7ecf\u5143\uff08MS-Neurons\uff09\u5728\u62d2\u7edd\u4e0d\u5b89\u5168\u8bf7\u6c42\u4e2d\u7684\u56e0\u679c\u4f5c\u7528\u3002\u8fdb\u4e00\u6b65\uff0c\u8bc6\u522b\u51fa\u5728\u9ad8\u8d44\u6e90\u4e0e\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u5171\u4eab\u7684\u5b89\u5168\u795e\u7ecf\u5143\uff08SS-Neurons\uff09\uff0c\u5e76\u901a\u8fc7\u6291\u5236\u6216\u589e\u5f3a\u8fd9\u4e9b\u795e\u7ecf\u5143\uff0c\u89c2\u5bdf\u5176\u5bf9\u591a\u8bed\u79cd\u5b89\u5168\u9632\u62a4\u7684\u4e00\u81f4\u6027\u5f71\u54cd\u3002\u6700\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u6709\u9488\u5bf9\u6027\u5730\u8bad\u7ec3\u8fd9\u4e00\u6781\u5c0f\u5b50\u96c6\u795e\u7ecf\u5143\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e13\u6ce8\u4e8eSS-Neurons\u7684\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u80fd\u529b\uff0c\u5e76\u4e14\u6574\u4f53\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5b89\u5168\u6027\u63d0\u5347\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u8de8\u8bed\u8a00\u5171\u4eab\u5b89\u5168\u795e\u7ecf\u5143\u5bf9\u63d0\u5347LLM\u5728\u591a\u8bed\u79cd\u5b89\u5168\u6027\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u805a\u7126\u4e8e\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u8bad\u7ec3\u7b56\u7565\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2602.02207", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02207", "abs": "https://arxiv.org/abs/2602.02207", "authors": ["Nisansa de Silva", "Surangika Ranathunga"], "title": "Sinhala Physical Common Sense Reasoning Dataset for Global PIQA", "comment": null, "summary": "This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u50e7\u4f3d\u7f57\u8bed\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b110\u4e2a\u4eba\u5de5\u521b\u5efa\u5e76\u9a8c\u8bc1\u7684\u6837\u672c\uff0c\u7528\u4e8e\u6d4b\u8bd5AI\u5728\u65af\u91cc\u5170\u5361\u8bed\u5883\u4e0b\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5168\u7403\u9488\u5bf9\u7269\u7406\u5e38\u8bc6\u63a8\u7406\uff08PIQA\uff09\u7684\u6570\u636e\u96c6\u5927\u591a\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u548c\u533a\u57df\u7684\u8d44\u6e90\u3002\u50e7\u4f3d\u7f57\u8bed\u4f5c\u4e3a\u65af\u91cc\u5170\u5361\u7684\u5b98\u65b9\u8bed\u8a00\u4e4b\u4e00\uff0c\u76ee\u524d\u6ca1\u6709\u6b64\u7c7b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u4e3a\u63a8\u52a8\u591a\u8bed\u8a00\u5e38\u8bc6\u63a8\u7406\u7814\u7a76\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u8be5\u6570\u636e\u96c6\u3002", "method": "\u4eba\u5de5\u7f16\u5199\u5e76\u9a8c\u8bc1\u4e86110\u4e2a\u50e7\u4f3d\u7f57\u8bed\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u4e00\u4e2a\u60c5\u666f\u63cf\u8ff0\uff08prompt\uff09\u3001\u4e00\u4e2a\u6b63\u786e\u7b54\u6848\u548c\u4e00\u4e2a\u9519\u8bef\u7b54\u6848\uff0c\u4e3b\u8981\u805a\u7126\u4e8e\u4e0e\u65af\u91cc\u5170\u5361\u76f8\u5173\u7684\u751f\u6d3b\u5e38\u8bc6\u95ee\u9898\u3002", "result": "\u4f5c\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u50e7\u4f3d\u7f57\u8bed\u73af\u5883\u4e0bAI\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u76f8\u5173\u7814\u7a76\u9886\u57df\u7684\u7a7a\u767d\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u4fc3\u8fdb\u591a\u8bed\u8a00\u5e38\u8bc6\u63a8\u7406AI\u7814\u7a76\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u5982\u50e7\u4f3d\u7f57\u8bed\uff0c\u4e3a\u540e\u7eed\u5de5\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01296", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01296", "abs": "https://arxiv.org/abs/2602.01296", "authors": ["Zeran Ke", "Bin Tan", "Gui-Song Xia", "Yujun Shen", "Nan Xue"], "title": "Interacted Planes Reveal 3D Line Mapping", "comment": "submitted to TPAMI", "summary": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.", "AI": {"tldr": "LiP-Map\u662f\u4e00\u79cd\u7ed3\u5408\u7ebf\u548c\u9762\u4f18\u5316\u76843D\u7ebf\u6761\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u63d0\u53473D\u7ebf\u6761\u6620\u5c04\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u7ebf\u6761\u6620\u5c04\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u7269\u7406\u7ed3\u6784\u4e2d\u7684\u9762\u548c\u7ebf\u7684\u5173\u7cfb\uff0c\u5f71\u54cd\u4e86\u91cd\u5efa\u7684\u7ed3\u6784\u6027\u548c\u51c6\u786e\u6027\u3002\u8be5\u6587\u5e0c\u671b\u901a\u8fc7\u66f4\u5408\u7406\u5730\u5efa\u6a21\u7ebf\u4e0e\u9762\u7684\u8026\u5408\uff0c\u63d0\u53473D\u91cd\u5efa\u8868\u73b0\u3002", "method": "\u63d0\u51faLiP-Map\u6846\u67b6\uff0c\u5c06\u53ef\u5b66\u4e60\u7684\u7ebf\u548c\u9762\u4f5c\u4e3a\u57fa\u672c\u5143\u7d20\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9762-\u7ebf\u4e4b\u95f4\u7684\u7269\u7406\u548c\u62d3\u6251\u5173\u7cfb\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u7b80\u5355\u7684\u5171\u9762\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u5730\u4ece\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4e2d\u8fdb\u884c\u91cd\u5efa\u3002", "result": "\u5728ScanNetV2\u3001ScanNet++\u3001Hypersim\u30017Scenes\u4e0eTanks&Temple\u7b49100\u591a\u4e2a\u573a\u666f\u6d4b\u8bd5\u4e2d\uff0cLiP-Map\u5728\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u5747\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u57fa\u4e8e\u7ebf\u7684\u89c6\u89c9\u5b9a\u4f4d\u8868\u73b0\u3002", "conclusion": "LiP-Map\u9996\u6b21\u5c06\u9762\u62d3\u6251\u663e\u5f0f\u6574\u5408\u8fdb3D\u7ebf\u6761\u6620\u5c04\uff0c\u4e3a\u7ed3\u6784\u53163D\u91cd\u5efa\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u9002\u5408\u4eba\u5de5\u73af\u5883\u573a\u666f\uff0c\u5177\u6709\u8f83\u9ad8\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02208", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02208", "abs": "https://arxiv.org/abs/2602.02208", "authors": ["Md. Toufique Hasan", "Ayman Asad Khan", "Mika Saari", "Vaishnavi Bankhele", "Pekka Abrahamsson"], "title": "Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study", "comment": "6 pages, 2 figures, submitted to MIPRO 2026", "summary": "Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u4ee5\u82ac\u5170\u8bed\u4e3a\u4f8b\uff09\u519c\u4e1a\u9886\u57df\u7684 RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7cfb\u7edf AgriHubi\uff0c\u9488\u5bf9\u519c\u4e1a\u51b3\u7b56\u652f\u6301\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u4e24\u9879\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u82f1\u6587\u4e3a\u4e3b\u7684\u8bad\u7ec3\u6570\u636e\u3001\u7f3a\u4e4f\u5b9e\u9645\u8bc4\u6d4b\uff0c\u5c24\u5176\u5728\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u519c\u4e1a\u9886\u57df\uff0c\u5c3d\u7ba1\u6709\u9ad8\u8d28\u91cf\u6587\u6863\uff0c\u5374\u96be\u4ee5\u5229\u7528\u666e\u901a\u6a21\u578b\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u9886\u57df\u9002\u914d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86 AgriHubi \u7cfb\u7edf\uff0c\u5c06\u82ac\u5170\u8bed\u519c\u4e1a\u6587\u6863\u4e0e\u5f00\u6e90 PORO \u7cfb\u5217\u6a21\u578b\u7ed3\u5408\uff0c\u91c7\u7528\u663e\u5f0f\u6587\u672c\u6eaf\u6e90\u4e0e\u7528\u6237\u53cd\u9988\u673a\u5236\uff0c\u5e76\u5728\u516b\u8f6e\u8fed\u4ee3\u540e\uff0c\u901a\u8fc7\u4e24\u6b21\u7528\u6237\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5728\u7b54\u6848\u5b8c\u6574\u6027\u3001\u8bed\u8a00\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u7b49\u65b9\u9762\u53d6\u5f97\u660e\u663e\u63d0\u5347\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u5728\u54cd\u5e94\u8d28\u91cf\u4e0e\u5ef6\u65f6\u4e4b\u95f4\u7684\u5b9e\u9645\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u7684\u9886\u57df\u4e13\u7528 RAG \u7cfb\u7edf\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2602.01298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01298", "abs": "https://arxiv.org/abs/2602.01298", "authors": ["Ching-Kai Huang", "Wen-Chieh Lin", "Yan-Cen Lee"], "title": "Interaction-Consistent Object Removal via MLLM-Based Reasoning", "comment": null, "summary": "Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u95ee\u9898\u2014\u2014\u4e92\u52a8\u4e00\u81f4\u6027\u76ee\u6807\u79fb\u9664\uff08ICOR\uff09\uff0c\u5e76\u63d0\u51fa\u4e86REORM\u6846\u67b6\uff0c\u80fd\u66f4\u667a\u80fd\u5730\u79fb\u9664\u4e0e\u76ee\u6807\u6709\u5173\u7684\u6240\u6709\u4e92\u52a8\u5143\u7d20\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u76ee\u6807\u79fb\u9664\u65b9\u6cd5\u53ea\u53bb\u9664\u4e86\u663e\u6027\u76ee\u6807\uff0c\u5374\u9057\u7559\u4e86\u76ee\u6807\u4e0e\u5468\u56f4\u73af\u5883\u4e92\u52a8\u7684\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7ed3\u679c\u8bed\u4e49\u4e0a\u4e0d\u4e00\u81f4\u3002\u8fd9\u4e00\u95ee\u9898\u5728\u89c6\u89c9\u7f16\u8f91\u3001\u5185\u5bb9\u4fdd\u62a4\u7b49\u5e94\u7528\u4e2d\u5f71\u54cd\u8f83\u5927\u3002\u672c\u6587\u65e8\u5728\u66f4\u597d\u5730\u89e3\u51b3\u76ee\u6807\u79fb\u9664\u540e\u6b8b\u7559\u4e92\u52a8\u75d5\u8ff9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86REORM\uff08\u57fa\u4e8e\u63a8\u7406\u589e\u5f3a\u7684\u76ee\u6807\u79fb\u9664\u6846\u67b6\uff09\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u63a8\u7406\u51fa\u9700\u8981\u5171\u540c\u79fb\u9664\u7684\u4e92\u52a8\u5143\u7d20\u3002REORM\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5305\u62ecMLLM\u5206\u6790\u3001\u63a9\u7801\u5f15\u5bfc\u79fb\u9664\u3001\u4ee5\u53ca\u81ea\u7ea0\u6b63\u673a\u5236\uff0c\u5e76\u6709\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u672c\u5730\u90e8\u7f72\u7248\u672c\u3002", "result": "\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4e30\u5bcc\u4e92\u52a8\u4f9d\u8d56\u7684ICOREval\u57fa\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cREORM\u5728ICOREval\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\uff0c\u80fd\u5b9e\u73b0\u4e92\u52a8\u4e00\u81f4\u6027\u7684\u79fb\u9664\u7ed3\u679c\u3002", "conclusion": "REORM\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u4e92\u52a8\u4e00\u81f4\u6027\u76ee\u6807\u79fb\u9664\uff0c\u5728\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5bf9\u9ad8\u7ea7\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.02219", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02219", "abs": "https://arxiv.org/abs/2602.02219", "authors": ["Yuzheng Xu", "Tosho Hirasawa", "Tadashi Kozuno", "Yoshitaka Ushiku"], "title": "Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5f53\u524d\u5e38\u7528\u7684\u57fa\u4e8e\u6807\u51c6\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4f4d\u7f6e\u504f\u7f6e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u5e8f\u5747\u8861\u7684\u65b0\u7b56\u7565\u6765\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5728\u70b9\u5f0f\u6216\u6210\u5bf9\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u8f83\u5c11\u5173\u6ce8LLM\u6309\u8bc4\u5206\u6807\u51c6\uff08rubric\uff09\u8fdb\u884c\u591a\u9879\u9009\u62e9\u65f6\u53ef\u80fd\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5c24\u5176\u662f\u9009\u9879\u4f4d\u7f6e\u5bfc\u81f4\u7684\u504f\u5dee\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63d0\u5347LLM\u4f5c\u4e3a\u6587\u672c\u8bc4\u4f30\u5224\u5b98\u65f6\u7684\u516c\u6b63\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u8bbe\u7f6e\u5bf9\u7167\u5b9e\u9a8c\uff0c\u6d4b\u8bd5LLM\u5728\u4e0d\u540c\u8bc4\u5206\u9879\u4f4d\u7f6e\u4e0b\u7684\u9009\u62e9\u503e\u5411\uff0c\u5e76\u8bbe\u8ba1\u4e00\u79cd\u201c\u5747\u8861\u7f6e\u6362\u201d\u65b9\u6cd5\uff0c\u5373\u8ba9\u6bcf\u4e2a\u8bc4\u5206\u9009\u9879\u5728\u4e0d\u540c\u4f4d\u7f6e\u5747\u5300\u51fa\u73b0\uff0c\u7136\u540e\u5bf9\u591a\u6b21\u8bc4\u5206\u7ed3\u679c\u8fdb\u884c\u805a\u5408\uff0c\u4ee5\u6b64\u8bc4\u4f30\u548c\u6821\u6b63\u4f4d\u7f6e\u504f\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65e0\u8bba\u6a21\u578b\u6216\u6570\u636e\u96c6\uff0cLLM\u5bf9\u5217\u8868\u4e2d\u67d0\u4e9b\u56fa\u5b9a\u4f4d\u7f6e\u7684\u8bc4\u5206\u9879\u6709\u660e\u663e\u504f\u597d\u3002\u6240\u63d0\u51fa\u7684\u5747\u8861\u7f6e\u6362\u7b56\u7565\u6709\u6548\u8bc6\u522b\u5e76\u524a\u5f31\u4e86\u6b64\u79cd\u4f4d\u7f6e\u504f\u5dee\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u673a\u5668\u8bc4\u5224\u4e0e\u4eba\u5de5\u8bc4\u5224\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "Rubric\u5236\u7684LLM\u8bc4\u5224\u7cfb\u7edf\u4e0d\u662f\u5929\u7136\u70b9\u5f0f\u8bc4\u4f30\uff0c\u5b58\u5728\u591a\u9879\u9009\u62e9\u76f8\u5173\u504f\u5dee\u3002\u901a\u8fc7\u7b80\u5355\u7684\u6362\u4f4d\u4e0e\u805a\u5408\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u8bc4\u5206\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u516c\u6b63\u6027\u3002"}}
{"id": "2602.01303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01303", "abs": "https://arxiv.org/abs/2602.01303", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Chu Chen", "Wei Tang", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation", "comment": null, "summary": "Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5ReDiStory\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u5e27\u89c6\u89c9\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u89d2\u8272\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u5e27\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u5c06\u8eab\u4efd\u548c\u573a\u666f\u4fe1\u606f\u5408\u5e76\uff0c\u5bb9\u6613\u5bfc\u81f4\u573a\u666f\u95f4\u5e72\u6270\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6545\u4e8b\u4e2d\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u3002", "method": "ReDiStory\u5728\u63a8\u7406\u65f6\u5c06\u6587\u672c\u5d4c\u5165\u5206\u89e3\u4e3a\u8eab\u4efd\u76f8\u5173\u548c\u5e27\u7279\u5b9a\u4e24\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u53bb\u76f8\u5173\u5316\u6291\u5236\u5e27\u95f4\u5171\u4eab\u65b9\u5411\uff0c\u4ece\u800c\u51cf\u5c11\u8de8\u5e27\u5e72\u6270\uff0c\u65e0\u9700\u8c03\u6574\u6269\u6563\u6a21\u578b\u53c2\u6570\u6216\u989d\u5916\u76d1\u7763\u3002", "result": "\u5728\u540c\u6837\u7684\u6269\u6563\u6a21\u578b\u548c\u63a8\u7406\u6761\u4ef6\u4e0b\uff0cReDiStory\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89d2\u8272\u4e00\u81f4\u6027\u548c\u63d0\u793a\u5fe0\u5b9e\u5ea6\u3002\u591a\u9879\u89d2\u8272\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u660e\u663e\u4f18\u4e8e1Prompt1Story\u65b9\u6cd5\u3002", "conclusion": "ReDiStory\u80fd\u63d0\u5347\u591a\u5e27\u89c6\u89c9\u6545\u4e8b\u751f\u6210\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u4e14\u7b80\u5355\u6613\u7528\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02221", "abs": "https://arxiv.org/abs/2602.02221", "authors": ["Frederic Blum", "Johann-Mattis List"], "title": "Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation", "comment": "Accepted for the L'Change workshop @ EACL 2026", "summary": "Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u6027\u8861\u91cf\u6307\u6807\u53ca\u5176\u7b97\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5386\u53f2\u8bed\u8a00\u6bd4\u8f83\u4e2d\u8bed\u97f3\u5bf9\u5e94\u89c4\u5f8b\u7684\u6b63\u5219\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u7f3a\u4e4f\u6b63\u5219\u6027\u7684\u540c\u6e90\u8bcd\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b0\u65b9\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u4e2d\uff0c\u8bed\u97f3\u5bf9\u5e94\u7684\u6b63\u5219\u6027\u662f\u6bd4\u8f83\u8bed\u8a00\u7684\u91cd\u8981\u4f9d\u636e\uff0c\u4f46\u8fc7\u53bb\u4e3b\u8981\u4f9d\u9760\u76f4\u89c9\u5224\u5b9a\uff0c\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u4f30\u3002\u968f\u7740\u6807\u51c6\u5316\u8bcd\u6c47\u6570\u636e\u548c\u8ba1\u7b97\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u6709\u5fc5\u8981\u7528\u66f4\u5ba2\u89c2\u548c\u53ef\u91cf\u5316\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6b63\u5219\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u300e\u5e73\u8861\u5e73\u5747\u590d\u73b0\u7387\u300f\u7684\u65b0\u6b63\u5219\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u8be5\u65b9\u6cd5\u5f00\u53d1\u4e86\u80fd\u81ea\u52a8\u8bc6\u522b\u4e0d\u89c4\u5219\u540c\u6e90\u8bcd\u96c6\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002\u91c7\u7528\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u5e76\u91c7\u7528\u7559\u4e00\u6cd5\u9a8c\u8bc1\u7b97\u6cd5\u6548\u679c\uff0c\u901a\u8fc7\u66ff\u6362\u4e0d\u89c4\u5219\u8bcd\u9879\uff0c\u68c0\u9a8c\u7b97\u6cd5\u662f\u5426\u80fd\u51c6\u786e\u8fa8\u8ba4\u4ea7\u751f\u4e0d\u89c4\u5219\u7684\u8bcd\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee585%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u5bfc\u81f4\u4e0d\u89c4\u5219\u7684\u8bcd\u9879\u3002\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u5b50\u6837\u672c\u53ca\u4e0d\u89c4\u5219\u6027\u589e\u52a0\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u6b63\u5219\u6027\u5ea6\u91cf\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u7b97\u6cd5\u80fd\u591f\u63d0\u5347\u8ba1\u7b97\u8bed\u8a00\u6bd4\u8f83\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u5bf9\u672a\u6765\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u6570\u636e\u96c6\u5b8c\u5584\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.01305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01305", "abs": "https://arxiv.org/abs/2602.01305", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Wei Tang", "Chu Chen", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "StoryState: Agent-Based State Control for Consistent and Editable Storybooks", "comment": null, "summary": "Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState", "AI": {"tldr": "\u63d0\u51faStoryState\u7cfb\u7edf\uff0c\u901a\u8fc7\u663e\u5f0f\u7ba1\u7406\u6545\u4e8b\u72b6\u6001\uff0c\u5b9e\u73b0\u5bf9\u591a\u9875\u6545\u4e8b\u4e66\u7684\u7cbe\u7ec6\u5316\u7f16\u8f91\uff0c\u63d0\u5347\u89c6\u89c9\u4e00\u81f4\u6027\u4e0e\u7f16\u8f91\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6a21\u6001\u6545\u4e8b\u751f\u6210\u5de5\u5177\u867d\u7136\u80fd\u81ea\u52a8\u751f\u6210\u63d2\u753b\u6545\u4e8b\u4e66\uff0c\u4f46\u7531\u4e8e\u6545\u4e8b\u7684\u89d2\u8272\u3001\u4e16\u754c\u8bbe\u5b9a\u7b49\u72b6\u6001\u4fe1\u606f\u4ec5\u9690\u542b\u4e8e\u8f93\u5165\uff0c\u7528\u6237\u65e0\u6cd5\u7ec6\u81f4\u8c03\u6574\u6545\u4e8b\u7ec6\u8282\uff0c\u7f16\u8f91\u7c92\u5ea6\u8f83\u7c97\u4e14\u6613\u7834\u574f\u8de8\u9875\u4e00\u81f4\u6027\u3002", "method": "StoryState\u4f5c\u4e3a\u65e0\u987b\u91cd\u65b0\u8bad\u7ec3\u7684\u6587\u672c\u751f\u6210\u56fe\u50cf\u65b9\u6cd5\u7684\u4e2d\u4ecb\uff0c\u57fa\u4e8eLLM\u6784\u5efa\u4ee3\u7406\uff0c\u663e\u5f0f\u8868\u793a\u89d2\u8272\u8868\u3001\u4e16\u754c\u8bbe\u5b9a\u548c\u5206\u9875\u9762\u7ea6\u675f\uff0c\u5e76\u81ea\u52a8\u7ef4\u62a4\u8fd9\u4e9b\u72b6\u6001\uff0c\u901a\u8fc7prompt\u751f\u6210\u6216\u7f16\u8f91\u6bcf\u4e00\u9875\u5185\u5bb9\uff0c\u4ece\u800c\u5bf9\u5404\u79cd\u751f\u6210\u540e\u7aef\u4fdd\u6301\u517c\u5bb9\u6027\u3002", "result": "\u5728\u7cfb\u7edf\u7ea7\u591a\u9875\u7f16\u8f91\u5b9e\u9a8c\u4e2d\uff0cStoryState\u80fd\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u672c\u5730\u9875\u9762\u7f16\u8f91\uff0c\u63d0\u5347\u4e86\u591a\u9875\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u6539\u52a8\u3001\u4ea4\u4e92\u6b21\u6570\u53ca\u7f16\u8f91\u65f6\u95f4\uff0c\u6548\u679c\u4f18\u4e8e1Prompt1Story\uff0c\u5e76\u63a5\u8fd1Gemini Storybook\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "StoryState\u4e3a\u6545\u4e8b\u4e66\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u7f16\u8f91\u3001\u9ad8\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u521b\u4f5c\u4f53\u9a8c\uff0c\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.02266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02266", "abs": "https://arxiv.org/abs/2602.02266", "authors": ["Tan Sang Nguyen", "Muhammad Reza Qorib", "Hwee Tou Ng"], "title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data", "comment": null, "summary": "Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u771f\u6b63\u5f00\u6e90\u7684\u4e1c\u5357\u4e9a\u5927\u8bed\u8a00\u6a21\u578bOpenSeal\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u5229\u7528\u5e73\u884c\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u53ef\u4ee5\u9ad8\u6548\u62d3\u5c55LLM\u5bf9\u65b0\u8bed\u8a00\u7684\u652f\u6301\uff0c\u6027\u80fd\u5ab2\u7f8e\u540c\u7b49\u89c4\u6a21\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u591a\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\u3002\u867d\u7136\u5df2\u6709\u4e00\u4e9b\u4e1c\u5357\u4e9a\u5730\u533a\u7684\u6a21\u578b\uff0c\u4f46\u90fd\u6ca1\u6709\u771f\u6b63\u505a\u5230\u5f00\u6e90\uff0c\u6ca1\u6709\u516c\u5f00\u8bad\u7ec3\u6570\u636e\uff0c\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u7814\u7a76\u6027\u3002\u7814\u7a76\u4eba\u5458\u4e9f\u9700\u4e00\u4e2a\u5168\u9762\u516c\u5f00\u4e14\u652f\u6301\u591a\u79cd\u4e1c\u5357\u4e9a\u8bed\u8a00\u7684\u9ad8\u6027\u80fdLLM\u3002", "method": "\u4f5c\u8005\u53d7\u5e73\u884c\u6570\u636e\u63d0\u5347\u591a\u8bed\u8a00\u8868\u73b0\u7684\u6700\u65b0\u7814\u7a76\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e86\u6709\u5bf9\u7167\u548c\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u4e13\u6ce8\u4e8e\u5728\u5927\u6a21\u578b\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u5f15\u5165\u5e73\u884c\u6570\u636e\uff0c\u4ec5\u752834.7B\u6807\u8bb0\u7684\u5e73\u884c\u6570\u636e\uff0c\u57288\u5757NVIDIA H200 GPU\u4e0a\u8bad\u7ec3180\u5c0f\u65f6\u3002\u5e76\u516c\u5f00\u5168\u90e8\u8bad\u7ec3\u6570\u636e\u548c\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4f7f\u7528\u4ec5\u7531\u5e73\u884c\u6570\u636e\u7ec4\u6210\u7684\u8bed\u6599\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u662f\u62d3\u5c55\u6a21\u578b\u652f\u6301\u65b0\u8bed\u8a00\u6700\u6709\u6548\u7684\u65b9\u5f0f\u3002\u6240\u8bad\u7ec3\u7684OpenSeal\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u540c\u89c4\u6a21\u4e3b\u6d41\u4e1c\u5357\u4e9a\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "OpenSeal\u4f5c\u4e3a\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u4e1c\u5357\u4e9a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u540c\u65f6\u4fc3\u8fdb\u900f\u660e\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u6539\u8fdb\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.01306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01306", "abs": "https://arxiv.org/abs/2602.01306", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Mohd Yamani Idna Idris"], "title": "DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling", "comment": null, "summary": "Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory", "AI": {"tldr": "DeCorStory\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u9664\u7684\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u6587\u672c\u5230\u56fe\u50cf\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u6bd4\u73b0\u6709\u8bad\u7ec3\u514d\u9664\u65b9\u6cd5\u53d6\u5f97\u66f4\u4f18\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u514d\u9664\u7684\u65b9\u6cd5\u5728\u5c06\u6240\u6709\u63d0\u793a\u62fc\u63a5\u4e3a\u5355\u4e00\u5e8f\u5217\u65f6\uff0c\u4f1a\u5bfc\u81f4\u5e27\u95f4\u5d4c\u5165\u76f8\u5173\u6027\u8fc7\u5f3a\uff0c\u8fdb\u800c\u51fa\u73b0\u989c\u8272\u6cc4\u6f0f\u3001\u80cc\u666f\u6df7\u5408\u3001\u89d2\u8272\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u4fdd\u6301\u8de8\u5e27\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8fde\u7eed\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faDeCorStory\u6846\u67b6\uff0c\u901a\u8fc7Gram-Schmidt\u6b63\u4ea4\u5316\u51cf\u5c11\u5e27\u95f4\u63d0\u793a\u5d4c\u5165\u7684\u76f8\u5173\u6027\uff0c\u5e76\u7528\u5947\u5f02\u503c\u91cd\u52a0\u6743\u52a0\u5f3a\u63d0\u793a\u7279\u5b9a\u4fe1\u606f\uff0c\u540c\u65f6\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u8eab\u4efd\u4fdd\u7559\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u4fdd\u8bc1\u89d2\u8272\u4e00\u81f4\u6027\u3002\u6574\u4e2a\u6d41\u7a0b\u65e0\u9700\u6a21\u578b\u4fee\u6539\u6216\u5fae\u8c03\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5165\u73b0\u6709\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDeCorStory\u5728\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50\u3001\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u8bad\u7ec3\u514d\u9664\u65b9\u6cd5\uff0c\u5e76\u5728\u65e0\u8bad\u7ec3\u57fa\u7ebf\u4e2d\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "DeCorStory\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u6545\u4e8b\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u9664\u3001\u6548\u679c\u4f18\u8d8a\u4e14\u5b9e\u73b0\u4fbf\u6377\u7684\u65b0\u65b9\u6848\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5e27\u95f4\u8bed\u4e49\u4e0e\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u4fc3\u8fdb\u76f8\u5173\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2602.02270", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02270", "abs": "https://arxiv.org/abs/2602.02270", "authors": ["El Batoul Bechiri", "Dihia Lanasri"], "title": "dziribot: rag based intelligent conversational agent for algerian arabic dialect", "comment": null, "summary": "The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.", "AI": {"tldr": "\u672c\u6587\u63a8\u51fa\u4e86DziriBOT\uff0c\u5bf9\u963f\u5c14\u53ca\u5229\u4e9aDarja\u65b9\u8a00\u4e2d\u7684\u5ba2\u670d\u5bf9\u8bdd\u63d0\u4f9b\u667a\u80fd\u652f\u6301\uff0c\u5e76\u4ee5\u6700\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5ba2\u6237\u670d\u52a1\u6570\u5b57\u5316\u52a0\u901f\uff0c\u9700\u8981\u80fd\u591f\u5e94\u5bf9Algerian Darja\u65b9\u8a00\u590d\u6742\u6027\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002Darja\u5b58\u5728\u62fc\u5199\u65e0\u6807\u51c6\u3001\u6cd5\u8bed\u6df7\u6742\u53ca\u963f\u62c9\u4f2f\u5b57\u6bcd\u4e0e\u62c9\u4e01\u5b57\u6bcd\u6df7\u7528\u7b49\u96be\u9898\uff0c\u73b0\u6709\u7cfb\u7edf\u9002\u7528\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u4f53\u7cfb\u7ed3\u6784\u7ed3\u5408\u4e13\u7528NLU\u4e0eRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\uff0c\u5e76\u7cfb\u7edf\u5bf9\u6bd4\u4e86\u57fa\u4e8eRasa\u7a00\u758f\u7279\u5f81\u3001\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u57fa\u4e8etransformer\u7684\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u5fae\u8c03\u540e\u7684DziriBERT\u6a21\u578b\u5728\u5904\u7406\u62fc\u5199\u6df7\u4e71\u548c\u7f55\u89c1\u610f\u56fe\u65b9\u9762\uff0c\u5747\u5927\u5e45\u8d85\u8fc7\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6548\u679c\u3002", "conclusion": "DziriBOT\u4e3aAlgerian\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u8a00\u81ea\u52a8\u5316\u5ba2\u670d\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u9762\u5411\u590d\u6742\u65b9\u8a00\u5e02\u573a\u7684\u667a\u80fd\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u84dd\u672c\u3002"}}
{"id": "2602.01329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01329", "abs": "https://arxiv.org/abs/2602.01329", "authors": ["Divya Jyoti Bajpai", "Shubham Agarwal", "Apoorv Saxena", "Kuldeep Kulkarni", "Subrata Mitra", "Manjesh Kumar Hanawal"], "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching", "comment": "Accepted at International Conference on Learning Representations (ICLR 2026)", "summary": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u80fd\u5927\u5e45\u52a0\u901fFlow Matching (FM) \u89c6\u89c9\u751f\u6210\u6a21\u578b\u63a8\u7406\u7684\u65b0\u6846\u67b6FlowCast\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u4e0b\u7684\u663e\u8457\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u6d41\u5339\u914d\uff08FM\uff09\u65b9\u6cd5\u867d\u7136\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u9ad8\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6781\u6162\uff0c\u4e25\u91cd\u5236\u7ea6\u5176\u5b9e\u7528\u6027\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6848\u65e0\u6cd5\u517c\u987e\u6548\u679c\u3001\u5f00\u9500\u53ca\u901a\u7528\u6027\u3002\u56e0\u6b64\u9700\u8981\u65b0\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u65e2\u80fd\u4fdd\u8bc1\u6548\u679c\u53c8\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u5404\u79cdFM\u6a21\u578b\u3002", "method": "FlowCast\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u75ab\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528FM\u6a21\u578b\u8bad\u7ec3\u65f6\u4fdd\u6301\u901f\u5ea6\u6052\u5b9a\u7684\u7279\u6027\uff0c\u901a\u8fc7\u7ebf\u6027\u5916\u63a8\u5f53\u524d\u901f\u5ea6\uff0c\u9884\u6d4b\u672a\u6765\u6b65\u957f\uff0c\u5e76\u7ed3\u5408\u5747\u65b9\u8bef\u5dee\u95e8\u63a7\u673a\u5236\u5224\u65ad\u662f\u5426\u91c7\u7eb3\u3002\u8fd9\u6837\u5728\u72b6\u6001\u7a33\u5b9a\u65f6\u53ef\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\uff0c\u5728\u590d\u6742\u533a\u57df\u4fdd\u6301\u7cbe\u786e\u3002\u6846\u67b6\u65e0\u9700\u989d\u5916\u795e\u7ecf\u7f51\u7edc\uff0c\u4efb\u610fFM\u6a21\u578b\u5747\u53ef\u76f4\u63a5\u5d4c\u5165\u3002", "result": "\u7406\u8bba\u5206\u6790\u7ed9\u51fa\u4e86FlowCast\u5e26\u6765\u7684\u8f68\u8ff9\u504f\u5dee\u4e0a\u754c\u3002\u5b9e\u9a8c\u663e\u793a\uff0cFlowCast\u5728\u56fe\u50cf\u751f\u6210\u3001\u89c6\u9891\u751f\u6210\u53ca\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc72.5\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u4e0e\u6807\u51c6\u5b8c\u6574\u6d41\u7a0b\u65e0\u5dee\u522b\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u52a0\u901f\u57fa\u7ebf\u3002", "conclusion": "FlowCast\u65e0\u9700\u8bad\u7ec3\u3001\u6613\u4e8e\u96c6\u6210\uff0c\u53ef\u5927\u5e45\u52a0\u901fFM\u89c6\u89c9\u751f\u6210\u6a21\u578b\u63a8\u7406\uff0c\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u548c\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u5c06FM\u6a21\u578b\u5e94\u7528\u4e8e\u5b9e\u65f6\u6216\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2602.02276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02276", "abs": "https://arxiv.org/abs/2602.02276", "authors": ["Kimi Team", "Tongtong Bai", "Yifan Bai", "Yiping Bao", "S. H. Cai", "Yuan Cao", "Y. Charles", "H. S. Che", "Cheng Chen", "Guanduo Chen", "Huarong Chen", "Jia Chen", "Jiahao Chen", "Jianlong Chen", "Jun Chen", "Kefan Chen", "Liang Chen", "Ruijue Chen", "Xinhao Chen", "Yanru Chen", "Yanxu Chen", "Yicun Chen", "Yimin Chen", "Yingjiang Chen", "Yuankun Chen", "Yujie Chen", "Yutian Chen", "Zhirong Chen", "Ziwei Chen", "Dazhi Cheng", "Minghan Chu", "Jialei Cui", "Jiaqi Deng", "Muxi Diao", "Hao Ding", "Mengfan Dong", "Mengnan Dong", "Yuxin Dong", "Yuhao Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Lingxiao Du", "Yulun Du", "Yu Fan", "Shengjun Fang", "Qiulin Feng", "Yichen Feng", "Garimugai Fu", "Kelin Fu", "Hongcheng Gao", "Tong Gao", "Yuyao Ge", "Shangyi Geng", "Chengyang Gong", "Xiaochen Gong", "Zhuoma Gongque", "Qizheng Gu", "Xinran Gu", "Yicheng Gu", "Longyu Guan", "Yuanying Guo", "Xiaoru Hao", "Weiran He", "Wenyang He", "Yunjia He", "Chao Hong", "Hao Hu", "Jiaxi Hu", "Yangyang Hu", "Zhenxing Hu", "Ke Huang", "Ruiyuan Huang", "Weixiao Huang", "Zhiqi Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yu Jing", "Guokun Lai", "Aidi Li", "C. Li", "Cheng Li", "Fang Li", "Guanghe Li", "Guanyu Li", "Haitao Li", "Haoyang Li", "Jia Li", "Jingwei Li", "Junxiong Li", "Lincan Li", "Mo Li", "Weihong Li", "Wentao Li", "Xinhang Li", "Xinhao Li", "Yang Li", "Yanhao Li", "Yiwei Li", "Yuxiao Li", "Zhaowei Li", "Zheming Li", "Weilong Liao", "Jiawei Lin", "Xiaohan Lin", "Zhishan Lin", "Zichao Lin", "Cheng Liu", "Chenyu Liu", "Hongzhang Liu", "Liang Liu", "Shaowei Liu", "Shudong Liu", "Shuran Liu", "Tianwei Liu", "Tianyu Liu", "Weizhou Liu", "Xiangyan Liu", "Yangyang Liu", "Yanming Liu", "Yibo Liu", "Yuanxin Liu", "Yue Liu", "Zhengying Liu", "Zhongnuo Liu", "Enzhe Lu", "Haoyu Lu", "Zhiyuan Lu", "Junyu Luo", "Tongxu Luo", "Yashuo Luo", "Long Ma", "Yingwei Ma", "Shaoguang Mao", "Yuan Mei", "Xin Men", "Fanqing Meng", "Zhiyong Meng", "Yibo Miao", "Minqing Ni", "Kun Ouyang", "Siyuan Pan", "Bo Pang", "Yuchao Qian", "Ruoyu Qin", "Zeyu Qin", "Jiezhong Qiu", "Bowen Qu", "Zeyu Shang", "Youbo Shao", "Tianxiao Shen", "Zhennan Shen", "Juanfeng Shi", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Pengwei Song", "Tianhui Song", "Xiaoxi Song", "Hongjin Su", "Jianlin Su", "Zhaochen Su", "Lin Sui", "Jinsong Sun", "Junyao Sun", "Tongyu Sun", "Flood Sung", "Yunpeng Tai", "Chuning Tang", "Heyi Tang", "Xiaojuan Tang", "Zhengyang Tang", "Jiawen Tao", "Shiyuan Teng", "Chaoran Tian", "Pengfei Tian", "Ao Wang", "Bowen Wang", "Chensi Wang", "Chuang Wang", "Congcong Wang", "Dingkun Wang", "Dinglu Wang", "Dongliang Wang", "Feng Wang", "Hailong Wang", "Haiming Wang", "Hengzhi Wang", "Huaqing Wang", "Hui Wang", "Jiahao Wang", "Jinhong Wang", "Jiuzheng Wang", "Kaixin Wang", "Linian Wang", "Qibin Wang", "Shengjie Wang", "Shuyi Wang", "Si Wang", "Wei Wang", "Xiaochen Wang", "Xinyuan Wang", "Yao Wang", "Yejie Wang", "Yipu Wang", "Yiqin Wang", "Yucheng Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhaowei Wang", "Zhengtao Wang", "Zhexu Wang", "Zihan Wang", "Zizhe Wang", "Chu Wei", "Ming Wei", "Chuan Wen", "Zichen Wen", "Chengjie Wu", "Haoning Wu", "Junyan Wu", "Rucong Wu", "Wenhao Wu", "Yuefeng Wu", "Yuhao Wu", "Yuxin Wu", "Zijian Wu", "Chenjun Xiao", "Jin Xie", "Xiaotong Xie", "Yuchong Xie", "Yifei Xin", "Bowei Xing", "Boyu Xu", "Jianfan Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinbo Xu", "Xinran Xu", "Yangchuan Xu", "Yichang Xu", "Yuemeng Xu", "Zelai Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Guangyao Yang", "Hao Yang", "Junwei Yang", "Kai Yang", "Ningyuan Yang", "Ruihan Yang", "Xiaofei Yang", "Xinlong Yang", "Ying Yang", "Yi Yang", "Yi Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Dan Ye", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Chengzhen Yu", "Longhui Yu", "Tao Yu", "Tianxiang Yu", "Enming Yuan", "Mengjie Yuan", "Xiaokun Yuan", "Yang Yue", "Weihao Zeng", "Dunyuan Zha", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Jin Zhang", "Puqi Zhang", "Qiao Zhang", "Rui Zhang", "Xiaobin Zhang", "Y. Zhang", "Yadong Zhang", "Yangkun Zhang", "Yichi Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yushun Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Chenguang Zhao", "Feifan Zhao", "Jinxiang Zhao", "Shuai Zhao", "Xiangyu Zhao", "Yikai Zhao", "Zijia Zhao", "Huabin Zheng", "Ruihan Zheng", "Shaojie Zheng", "Tengyang Zheng", "Junfeng Zhong", "Longguang Zhong", "Weiming Zhong", "M. Zhou", "Runjie Zhou", "Xinyu Zhou", "Zaida Zhou", "Jinguo Zhu", "Liya Zhu", "Xinhao Zhu", "Yuxuan Zhu", "Zhen Zhu", "Jingze Zhuang", "Weiyu Zhuang", "Ying Zou", "Xinxing Zu"], "title": "Kimi K2.5: Visual Agentic Intelligence", "comment": "Kimi K2.5 tech report", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6587\u672c\u4e0e\u89c6\u89c9\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u81ea\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u5e76\u884c\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u65b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u9762\u4e34\u591a\u6a21\u6001\u7406\u89e3\u548c\u590d\u6742\u4efb\u52a1\u5206\u89e3\u7684\u6311\u6218\uff0c\u9700\u8981\u6a21\u578b\u540c\u65f6\u9ad8\u6548\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u80fd\u81ea\u52a8\u7ec4\u7ec7\u548c\u5e76\u884c\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "method": "Kimi K2.5\u91c7\u7528\u4e86\u8054\u5408\u7684\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9\u76d1\u7763\u5fae\u8c03\u3001\u6587\u672c-\u89c6\u89c9\u8054\u5408\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u79cd\u6280\u672f\uff1b\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u201cAgent Swarm\u201d\u591a\u667a\u80fd\u4f53\u5e76\u884c\u8c03\u5ea6\u6846\u67b6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u52a8\u6001\u5206\u89e3\u5e76\u540c\u65f6\u89e3\u51b3\u5f02\u8d28\u5b50\u95ee\u9898\u3002", "result": "Kimi K2.5\u5728\u4ee3\u7801\u751f\u6210\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u7b49\u5404\u9886\u57df\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u8868\u73b0\u3002Agent Swarm\u5e76\u884c\u6846\u67b6\u4f7f\u5ef6\u8fdf\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u5927\u5e45\u964d\u4f4e\uff08\u6700\u9ad84.5\u500d\uff09\u3002", "conclusion": "Kimi K2.5\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8054\u5408\u4f18\u5316\u548c\u667a\u80fd\u4f53\u5e76\u884c\u673a\u5236\u5bf9\u901a\u7528\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u6781\u4e3a\u6709\u6548\u3002\u4f5c\u8005\u8fd8\u5f00\u653e\u4e86\u6a21\u578b\u4ee5\u63a8\u52a8\u9886\u57df\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.01334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01334", "abs": "https://arxiv.org/abs/2602.01334", "authors": ["Yan Ma", "Weiyu Zhang", "Tianle Li", "Linge Du", "Xuyang Shen", "Pengfei Liu"], "title": "What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom", "comment": "code: https://github.com/GAIR-NLP/Med", "summary": "Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.", "AI": {"tldr": "\u9019\u7bc7\u8ad6\u6587\u5206\u6790\u4e86\u8996\u89ba\u5de5\u5177\u4f7f\u7528\u5f37\u5316\u5b78\u7fd2\uff08RL\uff09\u5c0d\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\u6027\u80fd\u7684\u5f71\u97ff\uff0c\u767c\u73fe\u5176\u63d0\u5347\u4e3b\u8981\u4f86\u81ea\u6a21\u578b\u672c\u8eab\u80fd\u529b\u7684\u589e\u5f37\uff0c\u800c\u975e\u5584\u7528\u5de5\u5177\u3002", "motivation": "\u904e\u53bb\u7528\u8996\u89ba\u5de5\u5177\uff08\u5982\u88c1\u526a\u3001\u7e2e\u653e\uff09\u7684RL\u8b93VLM\u8868\u73fe\u63d0\u5347\uff0c\u4f46\u4e0d\u6e05\u695a\u9019\u7a2e\u63d0\u5347\u662f\u4f86\u81ea\u5de5\u5177\u7684\u6709\u6548\u5229\u7528\u9084\u662f\u6a21\u578b\u672c\u8eab\u7684\u9032\u6b65\uff0c\u56e0\u6b64\u4f5c\u8005\u60f3\u89e3\u958b\u9019\u4e00\u73fe\u8c61\u3002", "method": "\u4f5c\u8005\u63d0\u51faMED\u6846\u67b6\uff0c\u5206\u70ba\u5ea6\u91cf\u3001\u89e3\u91cb\u548c\u8a3a\u65b7\u4e09\u6b65\uff0c\u80fd\u5340\u5206\u5167\u5728\u80fd\u529b\u8b8a\u5316\u8207\u5de5\u5177\u5e36\u4f86\u7684\u6548\u61c9\uff0c\u4e26\u7d30\u5206\u5de5\u5177\u5f71\u97ff\u7684\u201c\u589e\u76ca\u201d\u548c\u201c\u50b7\u5bb3\u201d\uff0c\u9032\u4e00\u6b65\u5206\u6790\u5176\u6f14\u5316\u6a5f\u5236\u3002\u5be6\u9a57\u6db5\u84cb\u5169\u7a2e\u5de5\u5177\u5148\u9a57\u4e0d\u540c\u7684VLM\uff0c\u4ee5\u53ca\u516d\u500b\u57fa\u6e96\u6e2c\u8a66\u9ede\u3002", "result": "\u5be6\u9a57\u767c\u73fe\u5728\u4e0d\u540c\u6aa2\u67e5\u9ede\u4e0a\uff0c\u6a21\u578b\u9032\u6b65\u4e3b\u8981\u4f86\u81ea\u81ea\u8eab\u5b78\u7fd2\uff0c\u5de5\u5177RL\u4e3b\u8981\u5728\u964d\u4f4e\u5de5\u5177\u672c\u8eab\u5f15\u5165\u7684\u932f\u8aa4\uff0c\u6bd4\u5982\u6e1b\u5c11\u56e0\u8abf\u7528\u51fa\u932f\u548c\u5de5\u5177\u8a8d\u77e5\u5e72\u64fe\uff0c\u5c0d\u65bc\u85c9\u52a9\u5de5\u5177\u7cfe\u6b63\u6a21\u578b\u5931\u8aa4\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u73fe\u6709\u7684\u8996\u89ba\u5de5\u5177\u4f7f\u7528RL\u65b9\u6cd5\uff0c\u662f\u5b78\u6703\u5b89\u5168\u5730\u8207\u5de5\u5177\u5171\u5b58\uff0c\u9084\u7121\u6cd5\u771f\u6b63\u99d5\u99ad\u548c\u7cbe\u901a\u5de5\u5177\u3002"}}
{"id": "2602.02287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02287", "abs": "https://arxiv.org/abs/2602.02287", "authors": ["Isaac Chung", "Linda Freienthal"], "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages", "comment": "First Workshop on Multilingual Multicultural Evaluation, co-located with EACL 2026", "summary": "Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.\n  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u8de8\u8bed\u8a00\u5927\u6a21\u578b\u8bc4\u4f30\uff0c\u53d1\u73b0\u8bc4\u6d4b\u65f6\u6a21\u578b\u5f97\u5206\u6392\u540d\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u4f1a\u51fa\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u9700\u8981\u8bed\u7528\u5224\u65ad\uff08\u5982\u8fde\u8d2f\u6027\u548c\u6307\u4ee4\u9075\u5faa\u7b49\uff09\u65f6\u3002\u8fd9\u53cd\u6620\u51fa\u81ea\u52a8\u8bc4\u6d4b\u548cLLM\u5224\u5b98\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8de8\u8bed\u8a00\u8bc4\u6d4b\u5e38\u5e38\u5c06\u6a21\u578b\u771f\u5b9e\u8868\u73b0\u548c\u6d4b\u91cf\u4e0d\u7a33\u5b9a\u6027\u6df7\u6dc6\uff0c\u7f3a\u4e4f\u8bca\u65ad\u6027\u5206\u6790\uff0c\u5c24\u5176\u5728\u5bcc\u5f62\u6001\u53d8\u5316\u7684\u8bed\u8a00\uff08\u5982\u82ac\u5170\u8bed\u7cfb\uff09\u4e2d\u66f4\u5bb9\u6613\u51fa\u73b0\u8bc4\u6d4b\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63a7\u5236\u751f\u6210\u6761\u4ef6\uff0c\u5728\u7231\u6c99\u5c3c\u4e9a\u8bed\u3001\u82ac\u5170\u8bed\u548c\u5308\u7259\u5229\u8bed\u4e0a\u5408\u6210\u5ba2\u670d\u5bf9\u8bdd\uff0c\u5e76\u7528\u76f8\u540c\u81ea\u52a8\u6307\u6807\u53caLLM\u5224\u5b98\u6cd5\u8bc4\u4f30\u8868\u73b0\uff0c\u7ed3\u5408\u6bcd\u8bed\u8005\u4eba\u5de5\u6807\u6ce8\u5206\u6790\u6392\u540d\u7a33\u5b9a\u6027\u3002", "result": "\u8868\u9762\u5c42\u7ea7\u6307\u6807\uff08\u5982\u8bcd\u6c47\u591a\u6837\u6027\u3001\u8868\u9762\u53ca\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff09\u5728\u591a\u8bed\u8a00\u95f4\u8f83\u4e3a\u7a33\u5b9a\uff0c\u4f46\u6d89\u53ca\u8bed\u7528\u7684\u5224\u65ad\uff08\u5982\u8fde\u8d2f\u6027\u3001\u6307\u4ee4\u9075\u5faa\uff09\u6392\u540d\u8868\u73b0\u4e0d\u7a33\u751a\u81f3\u76f8\u5173\u6027\u51e0\u4e4e\u4e3a\u96f6\u3002\u8fd9\u4e9b\u5dee\u5f02\u5e76\u975e\u6a21\u578b\u80fd\u529b\u5b9e\u9645\u5dee\u8ddd\uff0c\u800c\u662f\u8bc4\u4ef7\u65b9\u5f0f\u53d7\u8bed\u8a00\u5f71\u54cd\u3002", "conclusion": "\u73b0\u6709\u8bc4\u4ef7\u65b9\u6cd5\uff08\u5c24\u5176LLM\u5224\u5b98\u6cd5\uff09\u96be\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u5728\u5f62\u6001\u590d\u6742\u8bed\u8a00\u4e0b\u3002\u96f6\u6837\u672c\u5224\u522b\u8fc1\u79fb\u5728\u7bc7\u7ae0\u7ea7\u8bc4\u4f30\u4e0a\u96be\u4ee5\u53ef\u9760\uff0c\u9700\u8981\u9488\u5bf9\u5177\u4f53\u8bed\u8a00\u5b9a\u6807\u548c\u4ee5\u4eba\u5de5\u57fa\u51c6\u6821\u51c6\u3002\u5728\u6587\u672b\u516c\u5e03\u4e86\u590d\u73b0\u534f\u8bae\u548c\u6570\u636e\uff0c\u4ee5\u63a8\u52a8\u591a\u8bed\u79cd\u7814\u7a76\u3002"}}
{"id": "2602.01335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01335", "abs": "https://arxiv.org/abs/2602.01335", "authors": ["Yu Xu", "Yuxin Zhang", "Juan Cao", "Lin Gao", "Chunyu Wang", "Oliver Deussen", "Tong-Yee Lee", "Fan Tang"], "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning", "comment": "11 pages, 10 figures", "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u9690\u55bb\u8fc1\u79fb\uff08VMT\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u53ca\u8ba4\u77e5\u7406\u8bba\uff0c\u5c06\u62bd\u8c61\u521b\u610f\u4ece\u53c2\u8003\u56fe\u50cf\u8fc1\u79fb\u5230\u65b0\u7684\u76ee\u6807\uff0c\u5b9e\u73b0\u9ad8\u9636\u89c6\u89c9\u9690\u55bb\u751f\u6210\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u5f0fAI\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7f3a\u4e4f\u5bf9\u9690\u55bb\u6027\u3001\u62bd\u8c61\u903b\u8f91\u7684\u7406\u89e3\u548c\u521b\u9020\u80fd\u529b\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u5177\u6709\u521b\u610f\u7684\u89c6\u89c9\u9690\u55bb\u751f\u6210\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u8981\u7a81\u7834\u73b0\u6709AI\u53ea\u80fd\u5904\u7406\u50cf\u7d20\u548c\u8868\u9762\u7279\u5f81\u7684\u5c40\u9650\uff0c\u4f7fAI\u80fd\u591f\u81ea\u52a8\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u521b\u9020\u672c\u8d28\u5e76\u8fc1\u79fb\u5230\u65b0\u7684\u4e3b\u9898\u5bf9\u8c61\u4e0a\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u8ba4\u77e5\u7406\u8bba\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u878d\u5408\u7406\u8bba\uff08CBT\uff09\u548c\u521b\u65b0\u7684Schema Grammar\uff0c\u5c06\u5173\u7cfb\u4e0d\u53d8\u6027\u4e0e\u5177\u4f53\u89c6\u89c9\u5143\u7d20\u89e3\u8026\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u611f\u77e5\u667a\u80fd\u4f53\uff08\u63d0\u53d6\u53c2\u8003\u56fe\u50cf\u7684schema\uff09\u3001\u8fc1\u79fb\u667a\u80fd\u4f53\uff08\u4fdd\u8bc1\u901a\u7528\u7a7a\u95f4\u4e0d\u53d8\u6027\uff09\u3001\u751f\u6210\u667a\u80fd\u4f53\uff08\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\uff09\u53ca\u8bca\u65ad\u667a\u80fd\u4f53\uff08\u6a21\u62df\u4e13\u5bb6\u8bc4\u5ba1\u3001\u95ed\u73af\u7ea0\u9519\uff09\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9690\u55bb\u4e00\u81f4\u6027\u3001\u7c7b\u6bd4\u9002\u5207\u6027\u548c\u89c6\u89c9\u521b\u610f\u6027\u7b49\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5e7f\u544a\u4e0e\u5a92\u4f53\u7b49\u9ad8\u5f71\u54cd\u529b\u521b\u610f\u573a\u666f\u4e2d\u7684\u81ea\u52a8\u5316\u89c6\u89c9\u9690\u55bb\u751f\u6210\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6280\u672f\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u521b\u610f\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002\u6e90\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.02290", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02290", "abs": "https://arxiv.org/abs/2602.02290", "authors": ["Alex Argese", "Pasquale Lisena", "Rapha\u00ebl Troncy"], "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?", "comment": null, "summary": "Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.", "AI": {"tldr": "\u63d0\u51faStoryScore\u8bc4\u4ef7\u751f\u6210\u5f0fAI\u64b0\u5199\u79d1\u5b66\u6545\u4e8b\u7684\u8d28\u91cf\uff0c\u7ed3\u5408\u591a\u7ef4\u5ea6\u6307\u6807\u8bc4\u4f30\u53d9\u4e8b\u80fd\u529b\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6458\u8981\u8bc4\u4ef7\u6307\u6807\u96be\u4ee5\u6355\u6349\u53d9\u4e8b\u62bd\u8c61\u5316\u3001\u7b80\u5316\u548c\u521b\u65b0\u6559\u5b66\u884c\u4e3a\uff0c\u5bf9\u751f\u6210\u578bAI\u8bb2\u8ff0\u79d1\u5b66\u6545\u4e8b\u7684\u8d28\u91cf\u8bc4\u4ef7\u5b58\u5728\u5c40\u9650\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u4e8b\u5b9e\u5e7b\u89c9\u68c0\u6d4b\u5668\u5e38\u5c06\u5408\u89c4\u7684\u53d9\u4e8b\u91cd\u6784\u8bef\u5224\u4e3a\u9519\u8bef\uff0c\u5c24\u5176\u5728\u521b\u610f\u5185\u5bb9\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faStoryScore\u7efc\u5408\u6307\u6807\uff0c\u5305\u62ec\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u8bcd\u6c47\u843d\u5730\u3001\u53d9\u4e8b\u63a7\u5236\u3001\u7ed3\u6784\u4fdd\u771f\u3001\u5197\u4f59\u907f\u514d\u548c\u5b9e\u4f53\u7ea7\u4e8b\u5b9e\u5e7b\u89c9\u68c0\u6d4b\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u591a\u9762\u8bc4\u4f30AI\u751f\u6210\u79d1\u5b66\u6545\u4e8b\u3002", "result": "\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u8bb8\u591a\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u521b\u65b0\u53d9\u4e8b\u548c\u4e8b\u5b9e\u9519\u8bef\u3002\u867d\u7136\u81ea\u52a8\u6307\u6807\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4ef7\u6709\u6548\uff0c\u5374\u96be\u4ee5\u5168\u9762\u8861\u91cf\u53d9\u8ff0\u548c\u53d9\u4e8b\u63a7\u5236\u3002", "conclusion": "StoryScore\u4e3aAI\u751f\u6210\u79d1\u5b66\u6545\u4e8b\u7684\u8bc4\u6d4b\u63d0\u4f9b\u66f4\u9002\u5408\u7684\u591a\u7ef4\u5de5\u5177\uff0c\u6307\u51fa\u73b0\u6709\u81ea\u52a8\u6307\u6807\u5728\u5904\u7406\u53d9\u4e8b\u521b\u65b0\u4e0e\u4e8b\u5b9e\u5e7b\u89c9\u5e73\u8861\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u79d1\u5b66\u4f20\u64ad\u548cAI\u5185\u5bb9\u751f\u6210\u7684\u79d1\u5b66\u8bc4\u4ef7\u3002"}}
{"id": "2602.01340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01340", "abs": "https://arxiv.org/abs/2602.01340", "authors": ["Yubo Dong", "Linchao Zhu"], "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness", "comment": null, "summary": "Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u56fa\u5b9a\u538b\u7f29\u7387\u7684VAE\u8f6c\u6362\u4e3a\u652f\u6301\u591a\u7ea7\u65f6\u57df\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u89c6\u9891\u5904\u7406\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709Latent Video Diffusion Models\uff08LVDMs\uff09\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u6548\u7387\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u589e\u52a0\u9690\u85cf\u901a\u9053\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\u6dfb\u52a0\u989d\u5916\u91c7\u6837\u5c42\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u652f\u6301\u66f4\u9ad8\u4f46\u7075\u6d3b\u7684\u89c6\u9891\u538b\u7f29\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u6027\u80fd\u635f\u5931\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u56fa\u5b9a\u538b\u7f29\u7387\u7684VAE\u8f6c\u5316\u4e3a\u652f\u6301\u591a\u7ea7\u65f6\u57df\u538b\u7f29\u7684\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5e94\u5bf9\u9ad8\u538b\u7f29\u7387\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u4e5f\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u5bf9\u6a21\u578b\u5728\u591a\u6837\u89c6\u9891\u7247\u6bb5\u4e0a\u7684\u8868\u73b0\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u8870\u9000\uff0c\u5e76\u4e14\u4e0e\u6269\u6563\u6a21\u578bDiT\u826f\u597d\u517c\u5bb9\uff0c\u53ef\u4ee5\u5b9e\u73b0\u540c\u65f6\u8bad\u7ec3\u3002\u5728\u591a\u6837\u7279\u6027\u7684\u89c6\u9891\u7247\u6bb5\u4e0a\u8868\u73b0\u51fa\u8f83\u597d\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u7ea7\u65f6\u57df\u538b\u7f29VAE\u80fd\u591f\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5904\u7406\u80fd\u529b\u548c\u7075\u6d3b\u6027\uff0c\u517c\u5bb9\u73b0\u6709\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u548c\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.02301", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02301", "abs": "https://arxiv.org/abs/2602.02301", "authors": ["Min Cai", "Yu Liang", "Longzheng Wang", "Yan Wang", "Yueyang Zhang", "Long Xia", "Zhiyuan Sun", "Xi Ye", "Daiting Shi"], "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery", "comment": "Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io", "summary": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u68af\u5ea6\u624b\u672f\uff08MGS\uff09\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u591a\u9886\u57df\u8bad\u7ec3\u65f6\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u591a\u4efb\u52a1\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u8fdb\u5c55\u4e2d\u8d77\u5230\u6838\u5fc3\u4f5c\u7528\uff0c\u4f46\u662f\u5728\u591a\u9886\u57df\u4e0a\u8bad\u7ec3\u901a\u7528\u6a21\u578b\u4f9d\u7136\u9762\u4e34\u9886\u57df\u5f02\u8d28\u6027\u5e26\u6765\u7684\u56f0\u96be\u3002\u73b0\u6709\u987a\u5e8f\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u90fd\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u8de8\u57df\u5e72\u6270\uff0c\u9650\u5236\u6027\u80fd\u63d0\u5347\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u5206\u6790\u4e86\u987a\u5e8f\u5f3a\u5316\u5b66\u4e60\u548c\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5e26\u6765\u7684\u5e72\u6270\uff0c\u5e76\u63d0\u51fa\u5728transformer\u5185\u90e8\u6a21\u5757\u7ea7\u5904\u7406\u68af\u5ea6\u51b2\u7a81\u7684\u6a21\u5757\u5316\u68af\u5ea6\u624b\u672f\uff08MGS\uff09\u65b9\u6cd5\uff0c\u5c06\u68af\u5ea6\u77db\u76fe\u5206\u89e3\u5230\u5404\u4e2a\u6a21\u5757\u8fdb\u884c\u5355\u72ec\u8c03\u8282\u3002", "result": "\u5728Llama\u548cQwen\u7b49\u6a21\u578b\u4e0a\uff0cMGS\u5728\u6570\u5b66\u3001\u804a\u5929\u3001\u6307\u4ee4\u8ddf\u968f\u4e09\u4e2a\u9886\u57df\u76f8\u6bd4\u6807\u51c6\u591a\u4efb\u52a1RL\u5e73\u5747\u63d0\u53474.3\u5206\uff0816.6%\uff09\u548c4.5\u5206\uff0811.1%\uff09\uff1b\u6b64\u5916\uff0cMGS\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u4e0b\u4f9d\u7136\u7a33\u5b9a\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u660e\u786e\u4e86\u591a\u57dfRL\u5e72\u6270\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u6a21\u5757\u7ea7\u68af\u5ea6\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\uff0c\u4e3a\u901a\u7528\u578b\u5927\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.01345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01345", "abs": "https://arxiv.org/abs/2602.01345", "authors": ["Yu Zhang", "Jingyi Liu", "Feng Liu", "Duoqian Miao", "Qi Zhang", "Kexue Fu", "Changwei Wang", "Longbing Cao"], "title": "Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis", "comment": "11 pages, 8 figures", "summary": "Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.", "AI": {"tldr": "NOVA\u662f\u4e00\u79cd\u5728\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u71b5\u5206\u6790\u5b9e\u73b0VAR\u6a21\u578b\u81ea\u9002\u5e94token\u88c1\u526a\u52a0\u901f\u7684\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u5efa\u6a21\u7531\u4e8etoken\u6570\u91cf\u5de8\u5927\u5bfc\u81f4\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u6602\uff0c\u800c\u73b0\u6709token\u88c1\u526a\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u9636\u6bb5\u5212\u5206\u8fc7\u4e8e\u7ecf\u9a8c\u3001\u88c1\u526a\u8ba1\u5212\u4e0d\u81ea\u9002\u5e94\u53ca\u52a0\u901f\u8303\u56f4\u6709\u9650\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u4e86\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e9btoken\u88c1\u526a\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "NOVA\u63d0\u51fa\u57fa\u4e8e\u71b5\u53d8\u5316\u5728\u7ebf\u68c0\u6d4bVAR\u6a21\u578b\u63a8\u7406\u65f6\u5efa\u6a21\u52a8\u6001\u7684\u8f6c\u53d8\u70b9\uff0c\u636e\u6b64\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u4e0d\u540clayer\u548cscale\u7684token\u88c1\u526a\u6bd4\u7387\uff0c\u5e76\u52a8\u6001\u8c03\u6574scale-linkage\u548clayer-linkage\uff0c\u5b9e\u73b0\u6309\u9700\u88c1\u526a\u4f4e\u71b5token\uff1b\u540c\u65f6\u590d\u7528\u4e0a\u4e00scale\u7684\u6b8b\u5dee\u7f13\u5b58\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u73b0\u6709VAR\u6a21\u578b\uff0c\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u7684\u52a0\u901f\u3002", "result": "NOVA\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u591f\u6839\u636e\u71b5\u53d8\u5316\u52a8\u6001\u8c03\u6574token\u88c1\u526a\uff0c\u81ea\u9002\u5e94\u5730\u63d0\u5347\u63a8\u7406\u52a0\u901f\u6bd4\u4e14\u517c\u987e\u751f\u6210\u8d28\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86NOVA\u4f5c\u4e3a\u65e0\u8bad\u7ec3\u52a0\u901fVAR\u6a21\u578b\u7684\u7b80\u5355\u6709\u6548\u6027\u3002", "conclusion": "NOVA\u6846\u67b6\u901a\u8fc7\u71b5\u5206\u6790\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VAR\u52a0\u901f\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u3001\u9ad8\u6548\u4e14\u8d28\u91cf\u53ef\u63a7\u7684token\u51cf\u88c1\u52a0\u901f\uff0c\u4e3aVAR\u6a21\u578b\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02315", "abs": "https://arxiv.org/abs/2602.02315", "authors": ["Rapha\u00ebl Sarfati", "Eric Bigelow", "Daniel Wurgaft", "Jack Merullo", "Atticus Geiger", "Owen Lewis", "Tom McGrath", "Ekdeep Singh Lubana"], "title": "The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors", "comment": null, "summary": "Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved \"belief manifolds\" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.", "AI": {"tldr": "\u672c\u8bba\u6587\u5206\u6790LLM\uff08\u4ee5Llama-3.2\u4e3a\u4f8b\uff09\u5728\u63a8\u7406\u5206\u5e03\u53c2\u6570\u65f6\uff0c\u5176\u4fe1\u5ff5\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u63a2\u8ba8\u51e0\u79cd\u5e72\u9884\u4e0e\u64cd\u63a7\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136LLM\u80fd\u591f\u6839\u636e\u8f93\u5165\u751f\u6210\u6761\u4ef6\u5316\u4fe1\u5ff5\uff0c\u4f46\u6211\u4eec\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u4fe1\u5ff5\u5728\u6a21\u578b\u5185\u90e8\u5982\u4f55\u7f16\u7801\u3001\u5982\u4f55\u968f\u65b0\u8bc1\u636e\u53d8\u5316\u53ca\u5982\u4f55\u6709\u6548\u5730\u64cd\u63a7\u6216\u5e72\u9884\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53d7\u63a7\u5b9e\u9a8c\uff0c\u8ba9Llama-3.2\u4ec5\u51ed\u4e0a\u4e0b\u6587\u4e2d\u7684\u6837\u672c\u6570\u636e\u63a8\u65ad\u6b63\u6001\u5206\u5e03\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u5e76\u89c2\u5bdf\u5176\u5185\u90e8\u8868\u5f81\u7ed3\u6784\u3002\u901a\u8fc7\u5bf9\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u7ebf\u6027\u64cd\u63a7\u548c\u51e0\u4f55\u611f\u77e5\u64cd\u63a7\u7b49\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "1. \u53d1\u73b0\u6a21\u578b\u5185\u90e8\u4f1a\u81ea\u53d1\u5f62\u6210\u8868\u793a\u4fe1\u5ff5\u7684\u201c\u66f2\u9762\u6d41\u5f62\u201d\uff1b2. \u5f53\u5206\u5e03\u7a81\u53d8\u65f6\uff0c\u6a21\u578b\u80fd\u591f\u76f8\u5e94\u5730\u9002\u5e94\uff0c\u4f46\u7ebf\u6027\u64cd\u63a7\u5f80\u5f80\u5bfc\u81f4\u4fe1\u5ff5\u8f6c\u79fb\u5230\u6d41\u5f62\u4e4b\u5916\uff0c\u4ece\u800c\u51fa\u73b0\u5f02\u5e38\u8868\u5f81\uff1b3. \u57fa\u4e8e\u51e0\u4f55\u6216\u573a\u611f\u77e5\u7684\u64cd\u63a7\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u4fe1\u5ff5\u7684\u5408\u7406\u53d8\u5316\u3002", "conclusion": "LLM\u5185\u90e8\u81ea\u53d1\u51fa\u73b0\u590d\u6742\u7684\u51e0\u4f55\u4fe1\u5ff5\u8868\u5f81\uff0c\u4e5f\u5373\u4ec5\u7528\u7ebf\u6027\u65b9\u6cd5\u7406\u89e3\u6216\u5e72\u9884\u6a21\u578b\u4fe1\u5ff5\u662f\u8fdc\u8fdc\u4e0d\u591f\u7684\uff0c\u66f4\u590d\u6742\u7684\u7ed3\u6784\u548c\u65b9\u6cd5\u624d\u80fd\u51c6\u786e\u53cd\u6620\u548c\u64cd\u63a7\u8fd9\u4e9b\u4fe1\u5ff5\u3002"}}
{"id": "2602.01352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01352", "abs": "https://arxiv.org/abs/2602.01352", "authors": ["Xingzu Zhan", "Chen Xie", "Honghang Chen", "Yixun Lin", "Xiaochun Mai"], "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation", "comment": "8 pages,5 figures", "summary": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5T2M Mamba\uff0c\u901a\u8fc7\u878d\u5408\u52a8\u4f5c\u5468\u671f\u6027\u4e0e\u5173\u952e\u5e27\u91cd\u8981\u6027\uff0c\u5e76\u589e\u5f3a\u6587\u672c\u4e0e\u52a8\u4f5c\u8868\u8fbe\u5bf9\u9f50\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u52a8\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u52a8\u4f5c\u5468\u671f\u6027\u4e0e\u5173\u952e\u5e27\u663e\u8457\u6027\u88ab\u72ec\u7acb\u5904\u7406\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u8026\u5408\u5173\u7cfb\uff0c\u5bfc\u81f4\u957f\u5e8f\u5217\u751f\u6210\u65f6\u52a8\u4f5c\u6f02\u79fb\uff1b\u4e8c\u662f\u6a21\u578b\u5bf9\u8bed\u4e49\u7b49\u6548\u4f46\u63aa\u8f9e\u4e0d\u540c\u7684\u63cf\u8ff0\u4e0d\u9c81\u68d2\uff0c\u8f7b\u5fae\u540c\u4e49\u66ff\u6362\u4f1a\u5f15\u8d77\u52a8\u4f5c\u8f93\u51fa\u5f02\u5e38\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Periodicity-Saliency Aware Mamba\u6a21\u578b\uff0c\u5229\u7528\u5f3a\u5316\u7684\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b\u7b97\u6cd5\u5bf9\u5173\u952e\u5e27\u8d4b\u6743\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eFFT\u7684\u81ea\u52a8\u76f8\u5173\u65b9\u6cd5\u6709\u6548\u4f30\u7b97\u52a8\u4f5c\u5468\u671f\u6027\uff0c\u4ece\u800c\u6355\u6349\u4e24\u8005\u7684\u8026\u5408\u52a8\u6001\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u5468\u671f\u6027\u5dee\u5206\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff08PDCAM\uff09\uff0c\u589e\u5f3a\u6587\u672c\u4e0e\u52a8\u4f5c\u7279\u5f81\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e860.068\u7684FID\u5206\u6570\uff0c\u5e76\u5728\u6240\u6709\u5176\u4ed6\u8bc4\u4ef7\u6307\u6807\u4e0a\u83b7\u5f97\u4e86\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "T2M Mamba\u6709\u6548\u7f13\u89e3\u4e86\u65e2\u5f80\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u7684\u4e24\u4e2a\u6838\u5fc3\u7f3a\u9677\uff0c\u63d0\u5347\u4e86\u751f\u6210\u52a8\u4f5c\u7684\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u5468\u671f\u6027-\u5173\u952e\u5e27\u8026\u5408\u5efa\u6a21\u53ca\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.02320", "categories": ["cs.CL", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2602.02320", "abs": "https://arxiv.org/abs/2602.02320", "authors": ["Feiyang Cai", "Guijuan He", "Yi Hu", "Jingjing Wang", "Joshua Luo", "Tianyu Zhu", "Srikanth Pilla", "Gang Li", "Ling Liu", "Feng Luo"], "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method", "comment": null, "summary": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5143\u6570\u636e\u6307\u5bfc\u5927\u6a21\u578b\u51c6\u786e\u751f\u6210\u5206\u5b50\u7ed3\u6784\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u9ad8\u7cbe\u5ea6\u5206\u5b50-\u63cf\u8ff0\u914d\u5bf9\u6570\u636e\u96c6\u3002", "motivation": "\u5206\u5b50\u529f\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7ed3\u6784\uff0c\u5c06\u5206\u5b50\u7ed3\u6784\u4e0e\u81ea\u7136\u8bed\u8a00\u51c6\u786e\u5bf9\u9f50\u5bf9\u4e8e\u5927\u6a21\u578b\u5904\u7406\u5316\u5b66\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4eba\u5de5\u6807\u6ce8\u9ad8\u8d28\u91cf\u7ed3\u6784\u63cf\u8ff0\u6570\u636e\u6781\u4e3a\u6602\u8d35\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5b9e\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff1a\u5229\u7528\u89c4\u5219\u9a71\u52a8\u7684\u5316\u5b66\u547d\u540d\u89e3\u6790\u5668\u89e3\u8bfbIUPAC\u540d\u8bcd\uff0c\u751f\u6210\u8be6\u7ec6\u7684\u5206\u5b50\u7ed3\u6784XML\u5143\u6570\u636e\uff0c\u518d\u6839\u636e\u8fd9\u4e9b\u5143\u6570\u636e\u5f15\u5bfc\u5927\u6a21\u578b\u751f\u6210\u7cbe\u51c6\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u7ea616.3\u4e07\u5bf9\u5206\u5b50-\u63cf\u8ff0\u6570\u636e\u3002", "result": "\u4f7f\u7528LLM\u548c\u4eba\u5de5\u4e13\u5bb6\u5bf92000\u4e2a\u5206\u5b50\u6837\u672c\u8fdb\u884c\u4e25\u683c\u8bc4\u6d4b\uff0c\u63cf\u8ff0\u51c6\u786e\u7387\u8fbe\u523098.6%\u3002", "conclusion": "\u5236\u4f5c\u51fa\u7684\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u5206\u5b50\u4e0e\u8bed\u8a00\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u9ad8\u53ef\u9760\u6027\u7684\u57fa\u7840\uff0c\u63d0\u8bae\u7684\u65b9\u6cd5\u4e5f\u6613\u4e8e\u6269\u5c55\u81f3\u66f4\u5927\u89c4\u6a21\u548c\u66f4\u5e7f\u6cdb\u7684\u7ed3\u6784\u76f8\u5173\u5316\u5b66\u4efb\u52a1\u3002"}}
{"id": "2602.01369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01369", "abs": "https://arxiv.org/abs/2602.01369", "authors": ["Songping Wang", "Qinglong Liu", "Yueming Lyu", "Ning Li", "Ziwen He", "Caifeng Shan"], "title": "Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684MoE\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5bf9\u6297\u653b\u51fb\uff08TLGA\u4e0eJ-TLGA\uff09\u548c\u9632\u5fa1\uff08J-TLAT\uff09\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5bf9\u6297\u6837\u672c\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1MoE\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5404\u7ec4\u4ef6\uff08\u5982\u8def\u7531\u5668\u4e0e\u4e13\u5bb6\u6a21\u5757\uff09\u5404\u81ea\u53ca\u534f\u540c\u7684\u8106\u5f31\u6027\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u7ec6\u7c92\u5ea6\u5206\u6790\u5e76\u63d0\u5347MoE\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bbe\u8ba1\u4e86\u9488\u5bf9MoE\u5185\u90e8\u8def\u7531\u5668\u7684\u5bf9\u6297\u653b\u51fb\uff08TLGA\uff09\uff0c\u63ed\u793a\u5176\u72ec\u7acb\u5f31\u70b9\uff1b\u968f\u540e\u53c8\u63d0\u51fa\u8054\u5408\u6270\u52a8\u8def\u7531\u5668\u4e0e\u4e13\u5bb6\u7684J-TLGA\uff0c\u66b4\u9732\u534f\u540c\u6027\u5f31\u70b9\u3002\u4e3a\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86J-TLAT\u8054\u5408\u5bf9\u6297\u8bad\u7ec3\uff0c\u63d0\u5347\u6574\u4f53\u9632\u5fa1\u80fd\u529b\u3002", "result": "\u6240\u63d0J-TLAT\u6846\u67b6\u53ef\u6709\u6548\u52a0\u5f3aMoE\u5404\u7ec4\u4ef6\u5bf9\u5bf9\u6297\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u7ed3\u6784\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff1b\u63a8\u7406\u6210\u672c\u76f8\u8f83\u7a20\u5bc6\u6a21\u578b\u51cf\u5c1160%\u4ee5\u4e0a\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86MoE\u7684\u7ec4\u4ef6\u7ea7\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u517c\u987e\u653b\u51fb\u4e0e\u9632\u5fa1\u7684\u65b0\u65b9\u6cd5\u3002J-TLAT\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u8fd8\u5177\u6709\u8f83\u4f4e\u7684\u63a8\u7406\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2602.02326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02326", "abs": "https://arxiv.org/abs/2602.02326", "authors": ["Neeraja Kirtane", "Kuan-Hao Huang"], "title": "Language Steering for Multilingual In-Context Learning", "comment": null, "summary": "While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3alanguage vectors\u7684\u65e0\u8bad\u7ec3\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfcLLM\u6fc0\u6d3b\u5411\u91cf\u63d0\u5347\u591a\u8bed\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u4e14\u80fd\u63ed\u793a\u8bed\u8a00\u7ed3\u6784\u3002", "motivation": "\u591a\u8bed\u79cd\u5927\u6a21\u578b\u5728\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0c\u82f1\u6587\u793a\u4f8b\u8f6c\u5316\u4e3a\u975e\u82f1\u8bed\u6d4b\u8bd5\u4f1a\u663e\u8457\u964d\u4f4e\u6548\u679c\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u591a\u8bed\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u4e0d\u5747\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5047\u8bbeLLM\u62e5\u6709\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u4e0d\u540c\u8bed\u8a00\u53ef\u89c6\u4f5c\u8be5\u7a7a\u95f4\u4e2d\u7684\u4e0d\u540c\u65b9\u5411\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u7528\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u6fc0\u6d3b\u5411\u91cf\u5dee\u503c\u4f5c\u4e3a\u201clanguage vectors\u201d\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c06\u5176\u52a0\u5230\u6a21\u578b\u4e2d\u95f4\u5c42\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6a21\u578b\u5f15\u5bfc\u3002", "result": "\u5728\u4e09\u79cd\u6a21\u578b\u3001\u4e09\u5957\u6570\u636e\u96c6\u300119\u79cd\u8bed\u8a00\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u548c\u8bed\u8a00\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002\u8bed\u8a00\u5411\u91cf\u95f4\u805a\u7c7b\u80fd\u63ed\u793a\u8bed\u8a00\u5bb6\u65cf\u7ed3\u6784\uff0c\u4e14\u5411\u91cf\u5177\u6709\u8de8\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "language vectors\u6280\u672f\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6539\u5584\u591a\u8bed\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u8868\u73b0\uff0c\u4e14\u6240\u5f97\u5411\u91cf\u5177\u5907\u6cdb\u5316\u6027\u548c\u8bed\u8a00\u7ed3\u6784\u89e3\u91ca\u6027\uff0c\u5bf9\u591a\u8bed\u79cd\u6a21\u578b\u5f00\u53d1\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01370", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01370", "abs": "https://arxiv.org/abs/2602.01370", "authors": ["Leonardo Brusini", "Cristian Sbrolli", "Eugenio Lomurno", "Toshihiko Yamasaki", "Matteo Matteucci"], "title": "PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles", "comment": null, "summary": "Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.", "AI": {"tldr": "PolyGen\u901a\u8fc7\u591a\u751f\u6210\u5668\u548c\u7ed3\u6784\u591a\u6837\u6027\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5927\u5e45\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u591a\u4efb\u52a1\u548c\u7ec4\u5408\u6027\u80fd\u529b\uff0c\u6548\u679c\u4f18\u4e8e\u5355\u4e00\u751f\u6210\u5668\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u653e\u5927\u5355\u4e00\u751f\u6210\u5668\uff0c\u9020\u6210\u751f\u6210\u504f\u5dee\u548c\u7279\u5f81\u5355\u4e00\uff0c\u9650\u5236\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faPolyGen\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u7ed3\u6784\u4e0d\u540c\u7684\u751f\u6210\u5668\u4ea4\u53c9\u8bad\u7ec3\uff0c\u51cf\u5c11\u6a21\u578b\u7279\u6709\u4f2a\u5f71\uff0c\u5e76\u91c7\u7528\u7a0b\u5e8f\u5316\u56f0\u96be\u8d1f\u6837\u672c\u673a\u5236\u63d0\u5347\u7ec6\u7c92\u5ea6\u5408\u6210\u80fd\u529b\u3002\u6570\u636e\u9884\u7b97\u4ece\u5355\u4e00caption\u5206\u914d\u5230\u591a\u6e90\u53d8\u4f53\uff0c\u4ee5\u63d0\u9ad8\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "PolyGen\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u6700\u4f73\u5355\u4e00\u751f\u6210\u5668\u57fa\u7ebf\uff08SynthCLIP\uff09\u9ad819%\uff0c\u5728\u7ec4\u5408\u6027\u57fa\u51c6\uff08SugarCrepe++\uff09\u9ad89.1%\u3002", "conclusion": "\u63d0\u5347\u7ed3\u6784\u591a\u6837\u6027\u6bd4\u5355\u7eaf\u589e\u5927\u5355\u4e00\u6765\u6e90\u6570\u636e\u91cf\u66f4\u6709\u6548\uff0cPolyGen\u80fd\u66f4\u9ad8\u6548\u5730\u6269\u5c55\u548c\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2602.02343", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02343", "abs": "https://arxiv.org/abs/2602.02343", "authors": ["Ziwen Xu", "Chenyan Wu", "Hengyu Sun", "Haiwen Hong", "Mengru Wang", "Yunzhi Yao", "Longtao Huang", "Hui Xue", "Shumin Deng", "Zhixuan Chu", "Huajun Chen", "Ningyu Zhang"], "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics", "comment": "Work in progress", "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u79cd\u63a7\u5236\u65b9\u6cd5\uff08\u5305\u62ec\u53c2\u6570\u5fae\u8c03\u3001LoRA\u9002\u5e94\u3001\u6fc0\u6d3b\u5e72\u9884\u7b49\uff09\u6574\u5408\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u9a71\u52a8\u7684\u52a8\u6001\u6743\u91cd\u66f4\u65b0\u6846\u67b6\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u5957\u7edf\u4e00\u7684\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u201c\u504f\u597d-\u6548\u7528\u201d\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684SPLIT\u65b9\u6cd5\u4ee5\u517c\u987e\u4e24\u8005\u3002", "motivation": "\u73b0\u6709\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u591a\u5355\u72ec\u7814\u7a76\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u65b9\u6cd5\u95f4\u7684\u8054\u7cfb\u88ab\u5ffd\u7565\u3001\u96be\u4ee5\u6a2a\u5411\u6bd4\u8f83\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6df1\u5165\u7406\u89e3\u5e76\u4f18\u5316\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u628a\u5404\u79cd\u63a7\u5236\u65b9\u6cd5\u90fd\u5efa\u6a21\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u5f15\u53d1\u7684\u52a8\u6001\u6743\u91cd\u8c03\u6574\uff0c\u5f52\u4e3a\u7edf\u4e00\u6846\u67b6\u4e0b\uff1b\u63d0\u51fa\u901a\u8fc7\u201c\u504f\u597d-\u6548\u7528\u201d\u5206\u6790\u6cd5\u6765\u5206\u522b\u91cf\u5316\u5bf9\u76ee\u6807\u6982\u5ff5\u7684\u504f\u597d\uff08preference\uff09\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u7684\u751f\u6210\u6548\u7528\uff08utility\uff09\uff0c\u5728\u540c\u4e00\u5bf9\u6bd4\u6807\u5c3a\u4e0a\u8bc4\u4f30\u4e8c\u8005\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4f5c\u8005\u5206\u6790\u4e86\u4e0d\u540c\u65b9\u6cd5\u4e0b\u4e8c\u8005\u6743\u8861\u673a\u5236\uff0c\u5e76\u7528\u201c\u6fc0\u6d3b\u6d41\u5f62\u201d\u89c6\u89d2\u8fdb\u4e00\u6b65\u89e3\u91ca\u5176\u73b0\u8c61\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86SPLIT\u65b9\u6cd5\u6539\u5584\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u89c2\u5bdf\u5230\uff1a\u6240\u6709\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u201c\u504f\u597d\u63d0\u5347\u3001\u6548\u7528\u964d\u4f4e\u201d\u7684\u6743\u8861\uff08\u5373\u63a7\u5236\u8d8a\u5f3a\uff0c\u751f\u6210\u5411\u76ee\u6807\u6982\u5ff5\u504f\u597d\u660e\u663e\uff0c\u4f46\u6548\u7528\u5982\u5408\u7406\u6027\u548c\u76f8\u5173\u6027\u4e0b\u964d\uff09\uff1bSPLIT\u65b9\u6cd5\u80fd\u66f4\u597d\u5728\u589e\u5f3a\u504f\u597d\u7684\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6548\u7528\u3002", "conclusion": "\u672c\u6587\u7edf\u4e00\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u63d0\u51fa\u5e76\u5b9e\u8bc1\u201c\u504f\u597d-\u6548\u7528\u201d\u6743\u8861\u7684\u65b0\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u901a\u8fc7\u6fc0\u6d3b\u6d41\u5f62\u5206\u6790\u589e\u5f3a\u7406\u89e3\uff0c\u5e76\u63a8\u51fa\u53ef\u517c\u987e\u4e8c\u8005\u7684SPLIT\u65b9\u6cd5\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01382", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01382", "abs": "https://arxiv.org/abs/2602.01382", "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"], "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation", "comment": null, "summary": "Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.\n  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.", "AI": {"tldr": "\u63d0\u51faPromptRL\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u57fa\u4e8e\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\uff0c\u89e3\u51b3\u73b0\u6709\u6587\u751f\u56fe\u751f\u6210\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u63d0\u793a\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u6781\u5927\u63d0\u5347\u4e86\u591a\u9879\u6307\u6807\uff0c\u4e14\u4ee3\u7801\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u6d41\u5339\u914d\u6a21\u578b\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u5956\u52b1\u76ee\u6807\uff0c\u4f46\u5b58\u5728\u751f\u6210\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u3001\u4ee5\u53ca\u5bf9\u8bad\u7ec3\u63d0\u793a\u8bcd\u8fc7\u5ea6\u8bb0\u5fc6\uff08\u63d0\u793a\u8fc7\u62df\u5408\uff09\uff0c\u5728\u9762\u5bf9\u98ce\u683c\u53d8\u5316\u4f46\u8bed\u4e49\u7b49\u4ef7\u63d0\u793a\u65f6\u8868\u73b0\u4e25\u91cd\u4e0b\u964d\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6837\u672c\u5229\u7528\u7387\u5e76\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faPromptRL\u65b9\u6cd5\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u8bcd\u6539\u5199\u5668\uff0c\u76f4\u63a5\u5d4c\u5165RL\u6d41\u7a0b\u4e2d\uff0c\u4f18\u5316\u65f6\u534f\u540c\u8bad\u7ec3\u6a21\u578b\u4e0e\u63d0\u793a\u6539\u5199\u5668\uff0c\u52a8\u6001\u91cd\u5199\u63d0\u793a\uff0c\u63d0\u5347 RL \u591a\u6837\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "result": "PromptRL\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\uff1aGenEval 0.97\uff0cOCR 0.98\uff0cPickScore 24.05\u3002\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\uff0c\u7528\u6781\u5c11\u91c7\u6837\uff080.06M\uff09\u5c31\u5c06FLUX.1-Kontext\u7684EditReward\u4ece1.19\u63d0\u5347\u52301.43\uff0c\u8d85\u8d8aGemini 2.5 Flash Image\uff081.37\uff09\uff0c\u5ab2\u7f8e\u66f4\u590d\u6742\u6240\u9700\u5fae\u8c03\u548c\u6ce8\u91ca\u7684\u6570\u636e\u65b9\u6848\uff08ReasonNet 1.44\uff09\u3002\u6bd4\u666e\u901aRL\u91c7\u6837\u6548\u7387\u9ad8\u51fa2\u500d\u4ee5\u4e0a\u3002", "conclusion": "PromptRL\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u6027\u80fd\u4e0e\u91c7\u6837\u6548\u7387\uff0c\u7f13\u89e3\u4e86\u8fc7\u62df\u5408\u4e0e\u6570\u636e\u5229\u7528\u95ee\u9898\uff0c\u663e\u793a\u5728\u6d41\u6a21\u578bRL\u6d41\u7a0b\u4e2d\u5f15\u5165\u63d0\u793a\u91cd\u5199\u6709\u5de8\u5927\u4ef7\u503c\u3002"}}
{"id": "2602.02360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02360", "abs": "https://arxiv.org/abs/2602.02360", "authors": ["Ryan Huynh", "Frank Guerin", "Alison Callwood"], "title": "Automated Multiple Mini Interview (MMI) Scoring", "comment": "18 pages, 2 figures", "summary": "Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u8f6f\u6280\u80fd\uff08\u5982\u5171\u60c5\u3001\u4f26\u7406\u5224\u65ad\u548c\u6c9f\u901a\uff09\uff0c\u81ea\u52a8\u5316\u9762\u8bd5\u6253\u5206\u6548\u679c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u5728\u9009\u62d4\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u8f6f\u6280\u80fd\u5f88\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u6253\u5206\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u504f\u89c1\u3002\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5c3d\u7ba1\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\uff0c\u4f46\u5176\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u8ff7\u4f60\u9762\u8bd5\uff08MMI\uff09\u4e2d\u590d\u6742\u4e14\u62bd\u8c61\u7684\u60c5\u5883\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u63d0\u793a\uff08multi-agent prompting\uff09\u6846\u67b6\uff0c\u5c06MMI\u7684\u8bc4\u4ef7\u8fc7\u7a0b\u5206\u4e3a\u8f6c\u5f55\u6587\u672c\u4f18\u5316\u548c\u57fa\u4e8e\u5177\u4f53\u6807\u51c6\u6253\u5206\u4e24\u4e2a\u9636\u6bb5\uff0c\u91c7\u7528\u6709\u4e09\u8f6e\u793a\u4f8b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u5927\u578b\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MMI\u4efb\u52a1\u4e0a\uff0c\u5e73\u5747QWK\uff080.62\uff09\u8fdc\u8d85\u4e13\u95e8\u5fae\u8c03\u7684\u57fa\u7ebf\u6a21\u578b\uff080.32\uff09\uff0c\u5728ASAP\u57fa\u51c6\u4e0a\u4e5f\u63a5\u8fd1\u9886\u57df\u4e13\u7528SOTA\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4e3b\u89c2\u63a8\u7406\u4efb\u52a1\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u6216\u53ef\u66ff\u4ee3\u6570\u636e\u91cf\u5927\u4f46\u9ad8\u6210\u672c\u7684\u5fae\u8c03\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u6539\u53d8LLM\u5728\u81ea\u52a8\u5316\u8bc4\u4f30\u573a\u666f\u7684\u5e94\u7528\u65b9\u5f0f\u3002"}}
{"id": "2602.01391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01391", "abs": "https://arxiv.org/abs/2602.01391", "authors": ["Xiaoyan Xing", "Xiao Zhang", "Sezer Karaoglu", "Theo Gevers", "Anand Bhattad"], "title": "Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics", "comment": "Project page: https:\\\\augmented-latent-intrinsics.github.io", "summary": "Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\\\augmented-latent-intrinsics.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u91cd\u5149\u7167\u65b9\u6cd5\uff08ALI\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5177\u6709\u50cf\u7d20\u5bf9\u9f50\u7684\u89c6\u89c9\u5148\u9a8c\u548c\u81ea\u76d1\u7763\u7b56\u7565\uff0c\u5728\u590d\u6742\u6750\u8d28\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5149\u7167\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u91cd\u5149\u7167\u65b9\u6cd5\u5728\u5904\u7406\u91d1\u5c5e\u3001\u73bb\u7483\u7b49\u590d\u6742\u6750\u8d28\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c3d\u7ba1\u5f15\u5165\u4e86\u66f4\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u53cd\u800c\u4f1a\u964d\u4f4e\u91cd\u5149\u7167\u7684\u4fdd\u771f\u5ea6\uff0c\u5b58\u5728\u8bed\u4e49\u62bd\u8c61\u4e0e\u5149\u7167\u771f\u5b9e\u611f\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u8bed\u4e49\u62bd\u8c61\u4e0e\u5149\u7167\u4fe1\u606f\u8868\u8fbe\u7684\u77db\u76fe\uff0c\u63d0\u51fa\u4e86Augmented Latent Intrinsics\uff08ALI\uff09\u6846\u67b6\uff1a\u901a\u8fc7\u50cf\u7d20\u5bf9\u9f50\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u6f5c\u5728\u56fa\u6709\u5c5e\u6027\u8868\u793a\u878d\u5408\uff0c\u5728\u81ea\u76d1\u7763\u7cbe\u7ec6\u5316\u8bad\u7ec3\u7b56\u7565\u4e0b\uff0c\u5b9e\u73b0\u5728\u65e0\u6807\u6ce8\u771f\u5b9e\u56fe\u50cf\u5bf9\u4e0a\u8bad\u7ec3\uff0c\u540c\u65f6\u5f15\u5165\u81f4\u5bc6\u7684\u50cf\u7d20\u7ea7\u89c6\u89c9\u5148\u9a8c\uff0c\u63d0\u9ad8\u5149\u7167\u91cd\u73b0\u8d28\u91cf\u3002", "result": "ALI\u65b9\u6cd5\u5728\u4ec5\u57fa\u4e8e\u672a\u6807\u6ce8\u7684\u771f\u5b9e\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\u7684\u6761\u4ef6\u4e0b\uff0c\u5728\u5404\u7c7b\u573a\u666f\uff0c\u7279\u522b\u662f\u91d1\u5c5e\u3001\u73bb\u7483\u7b49\u9ad8\u5149\u590d\u6742\u6750\u8d28\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u9884\u8bad\u7ec3\u89c6\u89c9\u5148\u9a8c\u5e76\u4e0d\u603b\u80fd\u63d0\u5347\u56fe\u50cf\u91cd\u5149\u7167\u8d28\u91cf\u3002ALI\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u5168\u5c40\u8bed\u4e49\u4e0e\u5c40\u90e8\u50cf\u7d20\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u590d\u6742\u6750\u8d28\u4e0a\u53d6\u5f97\u66f4\u9ad8\u6027\u80fd\uff0c\u4e3a\u67ef\u89e3\u51b3\u76f8\u5173\u9886\u57df\u7684\u4e00\u5927\u74f6\u9888\u3002"}}
{"id": "2602.02377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02377", "abs": "https://arxiv.org/abs/2602.02377", "authors": ["Haotong Yang", "Zitong Wang", "Shijia Kang", "Siqi Yang", "Wenkai Yu", "Xu Niu", "Yike Sun", "Yi Hu", "Zhouchen Lin", "Muhan Zhang"], "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof", "comment": "Under review", "summary": "While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality \"**question-proof-check**\" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u548c\u9a8c\u8bc1\u6570\u5b66\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u8bad\u7ec3\u51fa\u9ad8\u8d28\u91cf\u7684\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u51c6\u786e\u6027\u4e0e\u901a\u7528\u6027\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u957f\u8db3\u8fdb\u6b65\uff0c\u4f46\u9762\u5bf9\u57fa\u4e8e\u8bc1\u660e\u7684\u95ee\u9898\u65f6\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u7b54\u6848\u5339\u914d\u7684\u65b9\u6cd5\u65e0\u6cd5\u9a8c\u8bc1\u8bc1\u660e\u7684\u771f\u5b9e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u80fd\u81ea\u52a8\u8bc4\u4f30\u5b8c\u6574\u8bc1\u660e\u8fc7\u7a0b\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\u7684\uff0c\u53ef\u6269\u5c55\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff0c\u5229\u7528LLM\u751f\u6210\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u201c\u9898\u76ee-\u8bc1\u660e-\u68c0\u67e5\u201d\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u6db5\u76d6\u591a\u79cd\u96be\u5ea6\u3001\u8bed\u8a00\u98ce\u683c\u548c\u9519\u8bef\u7c7b\u578b\u3002\u751f\u6210\u7684\u6570\u636e\u7ecf\u8fc7\u5206\u5c42\u4eba\u5de5\u5ba1\u6838\u786e\u4fdd\u6807\u6ce8\u4e00\u81f4\u6027\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u5e73\u8861\u4e0e\u8fc7\u7a0b\u5956\u52b1\u673a\u5236\u8f85\u52a9\u6a21\u578b\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5956\u52b1\u6a21\u578b\u5728\u5956\u52b1\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u7684\u5f15\u5bfc\u80fd\u529b\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\u548c\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff0c\u4e3a\u63d0\u5347LLM\u6570\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5bf9\u81ea\u52a8\u5316\u96be\u9898\u8bc1\u660e\u4e0e\u68c0\u9a8c\u5177\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2602.01418", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01418", "abs": "https://arxiv.org/abs/2602.01418", "authors": ["Christoffer Koo \u00d8hrstr\u00f8m", "Rafael I. Cabral Muchacho", "Yifei Dong", "Filippos Moumtzidellis", "Ronja G\u00fcldenring", "Florian T. Pokorny", "Lazaros Nalpantidis"], "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas", "comment": null, "summary": "We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u629b\u7269\u7ebf\u7684\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08PaPE\uff09\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u591a\u6e90\u81ea\u4e8e1D\u6587\u672c\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u89c6\u89c9\u7684\u7a7a\u95f4\u7279\u6027\u548c\u9700\u6c42\uff0c\u5982\u5e73\u79fb\u3001\u65cb\u8f6c\u4e0d\u53d8\u6027\u7b49\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8868\u73b0\u3002\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u66f4\u7b26\u5408\u89c6\u89c9\u7279\u8d28\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "method": "\u63d0\u51faPaPE\uff08\u629b\u7269\u7ebf\u578b\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u57fa\u4e8e\u5e73\u79fb\u4e0d\u53d8\u6027\u3001\u65cb\u8f6c\u4e0d\u53d8\u6027\uff08PaPE-RI\uff09\u3001\u968f\u8ddd\u79bb\u8870\u51cf\u3001\u65b9\u5411\u6027\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7b49\u539f\u5219\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u89c6\u89c9\u6a21\u6001\uff0c\u5982\u56fe\u50cf\u3001\u70b9\u4e91\u3001\u89c6\u9891\u7b49\u3002", "result": "\u57284\u79cd\u6a21\u6001\u30018\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPaPE\u548cPaPE-RI\u57287/8\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002\u5728ImageNet-1K\u8de8\u5206\u8fa8\u7387\u6d4b\u8bd5\u4e2d\uff0cPaPE\u6bd4\u6b21\u4f18\u65b9\u6cd5\u7edd\u5bf9\u63d0\u5347\u53ef\u8fbe10.5%\u3002", "conclusion": "PaPE\u80fd\u591f\u663e\u8457\u63d0\u5347\u6ce8\u610f\u529b\u6a21\u578b\u7684\u89c6\u89c9\u4efb\u52a1\u8868\u73b0\uff0c\u5728\u591a\u6a21\u6001\u3001\u591a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u548c\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2602.02378", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02378", "abs": "https://arxiv.org/abs/2602.02378", "authors": ["Raunak Jain", "Mudita Khurana", "John Stephens", "Srinivas Dharmasanam", "Shankar Venkataraman"], "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making", "comment": null, "summary": "As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.", "AI": {"tldr": "\u73b0\u6709\u5927\u6a21\u578b\uff08LLM\uff09\u4ece\u5355\u7eaf\u52a9\u624b\u6269\u5c55\u5230\u51b3\u7b56\u652f\u6301\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u65e0\u6761\u4ef6\u9644\u548c\u3001\u5224\u65ad\u4e0d\u51c6\u7684\u5371\u9669\u8d8b\u52bf\u3002\u672c\u6587\u63d0\u51fa\u8981\u4ece\u751f\u6210\u7b54\u6848\u8f6c\u5411\u5bf9\u5173\u952e\u524d\u63d0\u7684\u534f\u4f5c\u6cbb\u7406\uff0c\u786e\u4fdd\u4eba\u673a\u5408\u4f5c\u66f4\u53ef\u9760\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\uff08\u76ee\u6807\u5b58\u5728\u4e89\u8bae\u4e14\u51b3\u7b56\u53cd\u8f6c\u4ee3\u4ef7\u9ad8\uff09\u7684\u573a\u666f\u4e2d\u5e94\u7528\uff0c\u73b0\u6709\u6a21\u578b\u5bb9\u6613\u673a\u68b0\u9644\u548c\uff0c\u63a9\u76d6\u9519\u8bef\u524d\u63d0\uff0c\u628a\u9a8c\u8bc1\u6210\u672c\u63a8\u7ed9\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e26\u6765\u51b3\u7b56\u98ce\u9669\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5dee\u5f02\u9a71\u52a8\u63a7\u5236\u56de\u8def\u201d\uff0c\u57fa\u4e8e\u77e5\u8bc6\u5e95\u5ea7\u6355\u6349\u5e76\u5b9a\u4f4d\u524d\u63d0\u51b2\u7a81\uff0c\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u5206\u6b67\uff08\u76ee\u7684\u6027\u3001\u8ba4\u77e5\u6027\u3001\u7a0b\u5e8f\u6027\uff09\uff0c\u5e76\u9488\u5bf9\u51b3\u7b56\u5173\u952e\u70b9\u5206\u7247\u8fdb\u884c\u6709\u9650\u534f\u5546\u3002\u5f15\u5165\u4e86\u201c\u627f\u8bfa\u95e8\u63a7\u201d\uff0c\u5bf9\u5173\u952e\u4f46\u672a\u627f\u8bfa\u7684\u524d\u63d0\u963b\u65ad\u81ea\u52a8\u884c\u52a8\uff0c\u5fc5\u8981\u65f6\u6709\u98ce\u9669\u65e5\u5fd7\u8986\u76d6\u3002", "result": "\u901a\u8fc7\u5bb6\u6559\uff08tutoring\uff09\u573a\u666f\u8bf4\u660e\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u5b9e\u73b0\u524d\u63d0\u53ef\u5ba1\u8ba1\u3001\u8bc1\u636e\u6807\u51c6\u53ef\u9a8c\u8bc1\u7684\u4fe1\u4efb\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u5bf9\u8bdd\u6d41\u7545\u6027\u3002\u63d0\u51fa\u4e86\u53ef\u8bc1\u4f2a\u7684\u8bc4\u4ef7\u6807\u51c6\u3002", "conclusion": "\u8981\u5b9e\u73b0\u53ef\u9760\u7684\u4eba\u673a\u4f19\u4f34\u5173\u7cfb\uff0c\u9700\u628a\u5173\u6ce8\u70b9\u4ece\u667a\u80fd\u4f53\u7684\u6d41\u7545\u56de\u7b54\u8f6c\u5411\u53ef\u5ba1\u8ba1\u7684\u524d\u63d0\u6cbb\u7406\uff0c\u53ea\u6709\u8fd9\u6837\uff0cAI\u624d\u80fd\u5728\u590d\u6742\u4e0d\u786e\u5b9a\u51b3\u7b56\u4e2d\u6210\u4e3a\u8d1f\u8d23\u4efb\u7684\u5408\u4f5c\u8005\u3002"}}
{"id": "2602.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01435", "abs": "https://arxiv.org/abs/2602.01435", "authors": ["Soumyaroop Nandi", "Prem Natarajan"], "title": "BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images", "comment": null, "summary": "We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet", "AI": {"tldr": "BioTamperNet\u662f\u4e00\u79cd\u9762\u5411\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u4eb2\u548c\u529b\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u5bf9\u4f2a\u9020\u5185\u5bb9\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u7684\u56fe\u50cf\u53d6\u8bc1\u6a21\u578b\u5927\u591a\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bf9\u4e8e\u7ec6\u5fae\u4f2a\u9020\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u6709\u6548\u6027\u53d7\u5230\u5f71\u54cd\u3002\u4e9f\u9700\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7684\u7be1\u6539\u68c0\u6d4b\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBioTamperNet\uff0c\u5229\u7528\u4eb2\u548c\u529b\u5f15\u5bfc\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6355\u6349\u56fe\u50cf\u5185\u90e8\u76f8\u4f3c\u6027\uff0c\u5e76\u5229\u7528\u4eb2\u548c\u529b\u5f15\u5bfc\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5efa\u6a21\u8de8\u56fe\u50cf\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8be5\u6a21\u578b\u53d7\u5230State Space Model\uff08SSM\uff09\u8fd1\u4f3c\u65b9\u6cd5\u542f\u53d1\uff0c\u96c6\u6210\u4e86\u8f7b\u91cf\u7ea7\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u9ad8\u6548\u5b9e\u73b0\u7cbe\u7ec6\u5b9a\u4f4d\u3002\u6574\u4f53\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7be1\u6539\u533a\u57df\u53ca\u5176\u6e90\u4f4d\u7f6e\u3002", "result": "\u5728\u4e3b\u6d41\u751f\u7269\u53d6\u8bc1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u4e0e\u591a\u79cd\u4e3b\u6d41\u65b9\u6cd5\u5bf9\u6bd4\uff0cBioTamperNet\u5728\u51c6\u786e\u68c0\u6d4b\u590d\u5236\u533a\u57df\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "BioTamperNet\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e2d\u7ec6\u5fae\u4f2a\u9020\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u5b9e\u9a8c\u6570\u636e\u7684\u53ef\u4fe1\u6027\u4fdd\u9a7e\u62a4\u822a\u3002"}}
{"id": "2602.02382", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02382", "abs": "https://arxiv.org/abs/2602.02382", "authors": ["Ziyan Zhang", "Chao Wang", "Zhuo Chen", "Chiyi Li", "Kai Song"], "title": "ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs", "comment": null, "summary": "Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ROG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u548c\u5927\u8bed\u8a00\u6a21\u578b\u94fe\u5f0f\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u56de\u7b54\u4e00\u9636\u903b\u8f91\u590d\u6742\u67e5\u8be2\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u9ad8\u590d\u6742\u5ea6\u548c\u5426\u5b9a\u7c7b\u67e5\u8be2\u4e0a\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u4e00\u9636\u903b\u8f91\u67e5\u8be2\u96be\u5ea6\u8f83\u5927\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u6295\u5f71\u3001\u4ea4\u96c6\u3001\u5e76\u96c6\u548c\u5426\u5b9a\u7b49\u591a\u8fd0\u7b97\u7b26\u590d\u6742\u67e5\u8be2\u7ed3\u6784\u65f6\uff0c\u73b0\u6709\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u8fc7\u7a0b\u7d2f\u79ef\u8bef\u5dee\u5927\u3001\u5bf9\u590d\u6742\u67e5\u8be2\u9c81\u68d2\u6027\u5dee\u7b49\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u6709\u6548\u7684\u67e5\u8be2\u5206\u89e3\u4e0e\u8bc1\u636e\u68c0\u7d22\u65b9\u6cd5\u514b\u670d\u8fd9\u4e9b\u74f6\u9888\u3002", "method": "\u63d0\u51faROG\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u591a\u8fd0\u7b97\u7b26\u67e5\u8be2\u5206\u89e3\u4e3a\u5355\u4e00\u8fd0\u7b97\u7b26\u5b50\u67e5\u8be2\uff0c\u4f9d\u6b21\u5904\u7406\u6bcf\u4e2a\u5b50\u67e5\u8be2\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u68c0\u7d22\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u7d27\u51d1\u90bb\u57df\u8bc1\u636e\uff0c\u91c7\u7528\u5927\u6a21\u578b\u8fdb\u884c\u94fe\u5f0f\u903b\u8f91\u63a8\u7406\u3002\u540c\u65f6\uff0c\u4fdd\u5b58\u5e76\u590d\u7528\u4e2d\u95f4\u7ed3\u679c\uff0c\u63d0\u5347\u63a8\u7406\u4e00\u81f4\u6027\u4e0e\u51c6\u786e\u6027\u3002", "result": "\u5728\u6807\u51c6\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cROG\u5728\u6574\u4f53\u51c6\u786e\u7387\u53ca\u590d\u6742\u5ea6\u9ad8\u3001\u5305\u542b\u5426\u5b9a\u64cd\u4f5c\u7684\u67e5\u8be2\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5f3a\u6709\u529b\u7684\u5d4c\u5165\u57fa\u7ebf\u65b9\u6cd5\u3002\u901a\u8fc7\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u7d2f\u79ef\uff0c\u63d0\u5347\u4e86\u9ad8\u590d\u6742\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "ROG\u4e3a\u5d4c\u5165\u5f0f\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4e3a\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u68c0\u7d22\u652f\u6491\u3001\u9010\u6b65\u63a8\u7406\u4ee5\u53ca\u4e2d\u95f4\u7b54\u6848\u7f13\u5b58\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u590d\u6742\u67e5\u8be2\u7ed3\u6784\u7684\u8981\u6c42\uff0c\u662f\u5904\u7406\u590d\u6742\u548c\u5426\u5b9a\u7c7b\u67e5\u8be2\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.01452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01452", "abs": "https://arxiv.org/abs/2602.01452", "authors": ["Penghao Deng", "Jidong J. Yang", "Jiachen Bian"], "title": "Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles", "comment": "21 pages, 15 figures, 3 tables", "summary": "Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a \"part-versus-whole\" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8f66\u8f86\u524d\u89c6\u6444\u50cf\u5934\u83b7\u53d6\u9053\u8def\u573a\u666f\uff0c\u5206\u6790\u9a7e\u9a76\u65f6\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\u4e0e\u573a\u666f\u8bed\u4e49\u5bf9\u8c61\u7684\u5339\u914d\uff0c\u4e3a\u8f85\u52a9\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "motivation": "\u4e86\u89e3\u9a7e\u9a76\u5458\u5728\u884c\u9a76\u8fc7\u7a0b\u4e2d\u7684\u89c6\u7ebf\u5206\u5e03\u5bf9\u63d0\u5347\u8f85\u52a9\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u548c\u9053\u8def\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u5c06\u6ce8\u89c6\u70b9\u4e0e\u573a\u666f\u4e2d\u5177\u4f53\u8bed\u4e49\u5bf9\u8c61\u5173\u8054\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u66f4\u597d\u5730\u7406\u89e3\u9a7e\u9a76\u5458\u5173\u6ce8\u91cd\u70b9\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\uff1a1\uff09\u76f4\u63a5\u5bf9\u8c61\u68c0\u6d4b\uff08YOLOv13\uff09\uff1b2\uff09\u5206\u5272\u8f85\u52a9\u5206\u7c7b\uff08SAM2+EfficientNetV2\u4e0eYOLOv13\u5bf9\u6bd4\uff09\uff1b3\uff09\u57fa\u4e8e\u67e5\u8be2\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-7b\u4e0eQwen2.5-VL-32b\u5bf9\u6bd4\uff09\uff0c\u7528\u4e8e\u8bc6\u522b\u6ce8\u89c6\u5bf9\u8c61\u3002\u6bd4\u8f83\u65b9\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "YOLOv13\u548c\u5927\u89c4\u6a21VLM\uff08Qwen2.5-VL-32b\uff09\u6548\u679c\u6700\u4f73\uff0cMacro F1\u5206\u6570\u8d85\u8fc70.84\uff0c\u5c24\u5176Qwen2.5-VL-32b\u5728\u591c\u95f4\u53ca\u68c0\u6d4b\u5c0f\u578b\u5b89\u5168\u5173\u952e\u7269\u4f53\u65f6\u8868\u73b0\u66f4\u7a33\u5065\u3002\u5206\u5272\u8f85\u52a9\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u201c\u90e8\u5206-\u6574\u4f53\u201d\u8bed\u4e49\u5dee\u5f02\uff0c\u53ec\u56de\u7387\u4f4e\u3002", "conclusion": "\u5b58\u5728\u5b9e\u65f6\u68c0\u6d4b\u6548\u7387\u4e0e\u5927\u6a21\u578b\u8bed\u5883\u7406\u89e3\u548c\u9c81\u68d2\u6027\u7684\u6743\u8861\u3002\u7814\u7a76\u4e3a\u672a\u6765\u80fd\u7406\u89e3\u9a7e\u9a76\u5458\u52a8\u6001\u5173\u6ce8\u7684\u667a\u80fd\u9a7e\u9a76\u76d1\u63a7\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u548c\u5b9e\u8df5\u53c2\u8003\u3002"}}
{"id": "2602.02414", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02414", "abs": "https://arxiv.org/abs/2602.02414", "authors": ["Joshua Mitton", "Prarthana Bhattacharyya", "Digory Smith", "Thomas Christie", "Ralph Abboud", "Simon Woodhead"], "title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank", "comment": "21 pages, 8 figures, 8 tables. Joshua Mitton and Prarthana Bhattacharyya contributed equally to this paper", "summary": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u8bc6\u522b\u5b66\u751f-\u5bfc\u5e08\u5bf9\u8bdd\u4e2d\u7684\u5b66\u4e60\u8bef\u89e3\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u53ca\u65f6\u8bc6\u522b\u5b66\u751f\u8bef\u89e3\u5bf9\u63d0\u5347\u5b66\u4e60\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u6559\u5e08\u81ea\u8eab\u7684\u52aa\u529b\u548c\u76f4\u89c9\uff0c\u5b58\u5728\u5c40\u9650\u3002\u8bba\u6587\u65e8\u5728\u7528LLM\u81ea\u52a8\u5316\u3001\u7cbe\u51c6\u5730\u53d1\u73b0\u5b66\u751f\u8bef\u89e3\uff0c\u51cf\u8f7b\u6559\u5e08\u8d1f\u62c5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u7528\u5fae\u8c03\u540e\u7684LLM\u751f\u6210\u5b66\u751f\u53ef\u80fd\u7684\u8bef\u89e3\uff1b2\uff09\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u4ece\u4e2d\u68c0\u7d22\u6700\u76f8\u5173\u7684\u8bef\u89e3\u5019\u9009\u9879\uff1b3\uff09\u518d\u7528\u53e6\u4e00\u4e2a\u5fae\u8c03LLM\u8fdb\u884c\u8bc4\u4f30\u548c\u6392\u5e8f\u3002\u6bd4\u8f83\u4e86\u4e0d\u540c\u57fa\u7840LLM\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u6559\u80b2\u5e73\u53f0\u7684\u5bf9\u8bdd\u6570\u636e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u548c\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1b\u5fae\u8c03\u80fd\u63d0\u5347\u751f\u6210\u8bef\u89e3\u7684\u8d28\u91cf\u5e76\u6709\u65f6\u8d85\u8fc7\u66f4\u5927\u95ed\u6e90\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u751f\u6210\u4e0e\u91cd\u6392\u5e8f\u6b65\u9aa4\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5229\u7528LLM\u68c0\u6d4b\u5b66\u4e60\u8bef\u89e3\u7684\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u8f85\u52a9\u751a\u81f3\u63d0\u5347\u6559\u5e08\u5728\u5b66\u751f\u8bef\u89e3\u53d1\u73b0\u548c\u5e72\u9884\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2602.01459", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01459", "abs": "https://arxiv.org/abs/2602.01459", "authors": ["Joey Kuang", "Alexander Wong"], "title": "Understanding vision transformer robustness through the lens of out-of-distribution detection", "comment": "Accepted to JCVIS 2025", "summary": "Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e3b\u6d41\u5c0f\u578b\u89c6\u89c9Transformer\u5728\u91cf\u5316\uff08\u964d\u4f4e\u8ba1\u7b97\u7cbe\u5ea6\uff09\u6761\u4ef6\u4e0b\uff0c\u9762\u5bf9\u5206\u5e03\u5916\u6570\u636e\uff08OOD\uff09\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u53cd\u800c\u524a\u5f31\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89c6\u89c9Transformer\u5c3d\u7ba1\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9650\u5236\u5176\u5b9e\u65f6\u548c\u666e\u53ca\u5316\u5e94\u7528\u3002\u91cf\u5316\u6280\u672f\u80fd\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u5bb9\u6613\u635f\u5931\u6027\u80fd\uff0c\u4e14\u5f53\u524d\u4e3b\u8981\u9488\u5bf9\u5206\u5e03\u5185\u4efb\u52a1\uff0c\u8f83\u5c11\u5173\u6ce8\u5206\u5e03\u5916\u60c5\u5883\u3002\u5982\u4f55\u8ba9\u91cf\u5316\u6a21\u578b\u517c\u5177\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\uff0c\u662f\u8be5\u7814\u7a76\u5173\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5bf9DeiT\u3001DeiT3\u548cViT\u7b49\u4e3b\u6d41Vision Transformer\u5728\u5e38\u89c1OOD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4f4e\u6bd4\u7279\uff08\u5c24\u5176\u662f4\u6bd4\u7279\uff09\u91cf\u5316\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5206\u6790\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u89c4\u6a21\uff08ImageNet-1k vs ImageNet-22k\uff09\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "(1) 4\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u5b58\u5728\u521d\u59cb\u4e0d\u7a33\u5b9a\uff0c\u66f4\u5927\u89c4\u6a21\uff08ImageNet-22k\uff09\u9884\u8bad\u7ec3\u4f7f\u5f97DeiT3\u6a21\u578b\u91cf\u5316\u540e\u7684\u4e0b\u964d\u5e45\u5ea6\u66f4\u5927\uff08\u6027\u80fd\u964d\u5e45\u9ad8\u8fbe17%\uff09\uff1b(2) \u9884\u8bad\u7ec3\u89c4\u6a21\u8d8a\u5927\uff0c\u91cf\u5316\u540e\u8bc6\u522b\u5206\u5e03\u5916\u6837\u672c\u7684\u80fd\u529b\u4e0b\u964d\u5f97\u8d8a\u5389\u5bb3\uff08ViT\u548cDeiT3\u5728AUPR-out\u6307\u6807\u964d\u5e45\u5206\u522b\u8fbe15%\u548c19.2%\uff09\uff1b(3) \u4f7f\u7528\u8f83\u5c0f\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u964d\u5e45\u8f83\u5c0f\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9884\u8bad\u7ec3\u867d\u7136\u63d0\u5347\u4e86\u5168\u7cbe\u5ea6\u8868\u73b0\uff0c\u4f46\u5bf9\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u68c0\u6d4b\u4e0a\u7684\u9c81\u68d2\u6027\u6709\u526f\u4f5c\u7528\u3002\u63d0\u5347\u4f4e\u6bd4\u7279\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65f6\uff0c\u5e94\u66f4\u5173\u6ce8\u6570\u636e\u589e\u5f3a\u800c\u4e0d\u662f\u5355\u7eaf\u6269\u5927\u9884\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3002"}}
{"id": "2602.02440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02440", "abs": "https://arxiv.org/abs/2602.02440", "authors": ["Nishat Raihan", "Sadiya Sayara Chowdhury Puspo", "Ana-Maria Bucur", "Stevie Chancellor", "Marcos Zampieri"], "title": "Large Language Models for Mental Health: A Multilingual Evaluation", "comment": null, "summary": "Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u4e86LLMs\u5728\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u5bf9\u5176\u6027\u80fd\u7684\u5f71\u54cd\u3002\u90e8\u5206LLM\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u6c34\u5e73\uff0c\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u4e14\u4e0d\u540c\u8bed\u8a00\u7684\u4e0b\u964d\u5e45\u5ea6\u4e0d\u540c\u3002", "motivation": "\u5f53\u524dLLMs\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u548c\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4e0a\u3002\u9274\u4e8e\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5bf9\u51c6\u786e\u7406\u89e3\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7684\u9ad8\u9700\u6c42\uff0c\u63a2\u7a76LLMs\u7684\u591a\u8bed\u8a00\u9002\u5e94\u6027\u548c\u7ffb\u8bd1\u5f71\u54cd\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u57288\u4e2a\u4e0d\u540c\u8bed\u8a00\u7684\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\uff08\u5305\u62ec\u539f\u59cb\u548c\u673a\u5668\u7ffb\u8bd1\u7248\u672c\uff09\u4e0a\uff0c\u9488\u5bf9\u4e13\u6709\u548c\u5f00\u6e90LLMs\u8fdb\u884czero-shot\u3001few-shot\u548c\u5fae\u8c03\u8bc4\u6d4b\uff0c\u5e76\u4e0e\u4f20\u7edfNLP\u65b9\u6cd5\u505a\u5bf9\u6bd4\uff1b\u8fd8\u5206\u6790\u4e86\u673a\u5668\u7ffb\u8bd1\u5728\u4e0d\u540c\u8bed\u8a00\u65cf\u548c\u7c7b\u578b\u4e0b\u7684\u8d28\u91cf\u5bf9LLM\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u4e13\u6709LLMs\u4e0e\u7ecf\u8fc7\u5fae\u8c03\u7684\u5f00\u6e90LLMs\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u5f53\u524d\u6700\u4f73\u7684F1\u5206\u6570\u3002\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4e0a\uff0c\u6574\u4f53\u6027\u80fd\u4e0b\u6ed1\uff0c\u964d\u5e45\u968f\u8bed\u8a00\u548c\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "LLMs\u5728\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u80fd\u529b\uff0c\u90e8\u5206\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5982\u679c\u673a\u5668\u7ffb\u8bd1\u5b58\u5728\u7ed3\u6784\u6216\u8bcd\u6c47\u4e0d\u5339\u914d\u65f6\uff0c\u6027\u80fd\u4f1a\u53d7\u5f71\u54cd\u3002\u591a\u8bed\u8a00\u5e94\u7528\u65f6\uff0c\u9700\u5173\u6ce8\u7ffb\u8bd1\u8d28\u91cf\u5e26\u6765\u7684\u5c40\u9650\u3002"}}
{"id": "2602.01530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01530", "abs": "https://arxiv.org/abs/2602.01530", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Preserving Localized Patch Semantics in VLMs", "comment": null, "summary": "Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word \"cat\"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08Logit Lens Loss, LLL\uff09\uff0c\u65e8\u5728\u63d0\u5347Logit Lens\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u4fdd\u6301\u89c6\u89c9token\u7684\u5c40\u90e8\u4fe1\u606f\uff0c\u4f7f\u5176\u4ea7\u751f\u66f4\u6709\u610f\u4e49\u7684\u70ed\u529b\u56fe\u3002", "motivation": "\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u89c6\u89c9token\u7684\u5185\u5bb9\u5f88\u5bb9\u6613\u5728\u8bed\u8a00\u5904\u7406\u8fc7\u7a0b\u4e2d\u4e27\u5931\u5176\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u73b0\u6709Logit Lens\u7684\u53ef\u89c6\u5316\u5de5\u5177\u96be\u4ee5\u89e3\u91ca\u548c\u5b9a\u4f4d\u539f\u59cb\u89c6\u89c9\u5185\u5bb9\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u5e76\u5bf9\u9f50\u89c6\u89c9token\u4e0e\u5176\u5b9e\u9645\u56fe\u50cf\u6982\u5ff5\u7684\u5173\u7cfb\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Logit Lens Loss\uff08LLL\uff09\uff0c\u4f5c\u4e3a\u5bf9\u4f20\u7edf\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\uff08NTP\uff09\u7684\u8865\u5145\u635f\u5931\uff0c\u4f7f\u89c6\u89c9token\u7684\u5d4c\u5165\u66f4\u597d\u5730\u4e0e\u63cf\u8ff0\u5176\u56fe\u50cf\u533a\u57df\u7684\u6587\u672c\u6982\u5ff5\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u6216\u8fdb\u884c\u5927\u89c4\u6a21\u518d\u8bad\u7ec3\uff0c\u4ec5\u901a\u8fc7\u6dfb\u52a0\u635f\u5931\u51fd\u6570\u7ea6\u675fself-attention\u4e2d\u56fe\u50cf\u4e0e\u6587\u672ctoken\u7684\u6df7\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLL\u4e0d\u4ec5\u80fd\u591f\u4f7fLogit Lens\u751f\u6210\u6709\u610f\u4e49\u7684\u76ee\u6807\u7f6e\u4fe1\u70ed\u529b\u56fe\uff0c\u8fd8\u5728\u4e0d\u589e\u52a0\u7279\u6b8a\u7ed3\u6784\uff08\u5982\u5206\u5272\u5934\uff09\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e86\u5206\u5272\u7b49\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "Logit Lens Loss\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9token\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u589e\u5f3aVLMs\u5728\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u53ca\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u4fbf\u9002\u7528\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02462", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02462", "abs": "https://arxiv.org/abs/2602.02462", "authors": ["Gabriele Maraia", "Marco Valentino", "Fabio Massimo Zanzotto", "Leonardo Ranaldi"], "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u62bd\u8c61\u6307\u5bfc\u63a8\u7406\u201d\u6846\u67b6\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u6bb5\u8bba\u63a8\u7406\u4e2d\u53d7\u8bed\u4e49\u5185\u5bb9\u5e72\u6270\u5bfc\u81f4\u7684\u5224\u65ad\u504f\u5dee\uff0c\u589e\u5f3a\u5176\u5f62\u5f0f\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u6bb5\u8bba\u7b49\u5f62\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5e38\u5e38\u53d7\u5185\u5bb9\u6548\u5e94\u5f71\u54cd\uff0c\u5373\u5c06\u8bed\u4e49\u5408\u7406\u6027\u4e0e\u903b\u8f91\u6709\u6548\u6027\u6df7\u6dc6\u3002\u5df2\u6709\u65b9\u6cd5\u5c1a\u96be\u5f7b\u5e95\u6d88\u9664\u8fd9\u79cd\u8bed\u4e49\u5e72\u6270\uff0c\u9700\u8981\u65b0\u7684\u673a\u5236\u6765\u5206\u79bb\u5f62\u5f0f\u63a8\u7406\u4e0e\u8bcd\u6c47\u8bed\u4e49\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5185\u5bb9\u4e30\u5bcc\u4e0e\u62bd\u8c61\u5f62\u5f0f\u914d\u5bf9\u7684\u4e09\u6bb5\u8bba\u9898\u76ee\uff0c\u5e76\u5728\u62bd\u8c61\u8f93\u5165\u4e0a\u7684\u6a21\u578b\u6fc0\u6d3b\u5b9a\u4e49\u4e86\u201c\u62bd\u8c61\u63a8\u7406\u7a7a\u95f4\u201d\u3002\u8fdb\u800c\u8bbe\u8ba1\u201c\u62bd\u8c61\u5668\u201d\uff0c\u6839\u636e\u5e26\u5185\u5bb9\u7684\u6fc0\u6d3b\u9884\u6d4b\u8be5\u62bd\u8c61\u7a7a\u95f4\u8868\u5f81\uff0c\u901a\u8fc7\u591a\u5c42\u5e72\u9884\u5f71\u54cd\u6a21\u578b\u524d\u5411\u4f20\u64ad\u3002\u8fd8\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u68c0\u9a8c\u4e86\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u901a\u8fc7\u8fd9\u79cd\u201c\u6fc0\u6d3b\u5bf9\u9f50\u62bd\u8c61\u201d\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u5185\u5bb9\u9a71\u52a8\u7684\u63a8\u7406\u9519\u8bef\uff0c\u63d0\u9ad8\u5bf9\u903b\u8f91\u6709\u6548\u6027\u7684\u654f\u611f\u5ea6\u548c\u63a8\u7406\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5176\u5bf9\u8bed\u4e49\u5e72\u6270\u7684\u6297\u6027\uff0c\u5e76\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u6307\u660e\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.01533", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01533", "abs": "https://arxiv.org/abs/2602.01533", "authors": ["Zhe Ling", "Sicheng Yu", "Danyu Yang"], "title": "Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units", "comment": null, "summary": "Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\\pm 180^{\\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\\%$, $96.67\\%$, and $94.33\\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u8def\u5f84\u7b7e\u540d\uff08SW-PS\uff09\u548c\u7ebf\u6027\u9012\u5f52\u5355\u5143\uff08LRU\uff09\u7684\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u65cb\u8f6c\u4e0d\u53d8\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u5c3d\u7ba1\u4f7f\u7528\u52a8\u6001\u7b14\u753b\u7b49\u7279\u5f81\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u65cb\u8f6c\u53d8\u5f62\u4f1a\u663e\u8457\u964d\u4f4e\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u56e0\u6b64\u5982\u4f55\u63d0\u53d6\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u4ecd\u662f\u6311\u6218\u3002", "method": "\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u8def\u5f84\u7b7e\u540d\uff08SW-PS\uff09\u63d0\u53d6\u5b57\u7b26\u7684\u5c40\u90e8\u7ed3\u6784\u7279\u5f81\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u7ebf\u6027\u9012\u5f52\u5355\u5143\uff08LRU\uff09\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u7ed3\u5408RNN\u7684\u5feb\u901f\u9012\u589e\u5904\u7406\u548cSSM\u7684\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\uff0c\u9002\u5e94\u7b14\u753b\u52a8\u6001\u53d8\u5316\u3002\u540c\u65f6\u5728\u4e0d\u540c\u5b57\u7b26\u96c6\uff08\u6570\u5b57\u3001\u5927\u5199\u5b57\u6bcd\u3001\u6c49\u5b57\u90e8\u9996\uff09\u4e0a\u8fdb\u884c\u968f\u673a\u65cb\u8f6c\uff08\u00b1180\u00b0\uff09\u7684\u5b9e\u9a8c\u3002", "result": "\u5728CASIA-OLHWDB1.1\u6570\u636e\u96c6\u7684\u4e09\u79cd\u5b50\u96c6\u4e0a\uff0c\u96c6\u6210\u5b66\u4e60\u540e\u7684\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230\u4e8699.62%\uff08\u6570\u5b57\uff09\u300196.67%\uff08\u82f1\u6587\u5b57\u6bcd\uff09\u300194.33%\uff08\u6c49\u5b57\u90e8\u9996\uff09\uff0c\u8d85\u8fc7\u5176\u4ed6\u5bf9\u6bd4\u6a21\u578b\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "SW-PS+LRU\u65b9\u6cd5\u5728\u65cb\u8f6c\u4e0d\u53d8\u6027\u3001\u51c6\u786e\u7387\u548c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02464", "abs": "https://arxiv.org/abs/2602.02464", "authors": ["Or Shafran", "Shaked Ronen", "Omri Fahn", "Shauli Ravfogel", "Atticus Geiger", "Mor Geva"], "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry", "comment": null, "summary": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u56e0\u5b50\u5206\u6790\u5668\uff08MFA\uff09\u4f5c\u4e3a\u65e0\u76d1\u7763\u5206\u6790\u5927\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u6355\u6349\u590d\u6742\u548c\u975e\u7ebf\u6027\u6982\u5ff5\u3002", "motivation": "\u4ee5\u5f80\u7528\u4e8e\u6fc0\u6d3b\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u8fc7\u4e8e\u4f9d\u8d56\u7ebf\u6027\u53ef\u5206\u7684\u5355\u4e00\u5168\u5c40\u65b9\u5411\uff0c\u96be\u4ee5\u63cf\u8ff0\u590d\u6742\u7684\u975e\u7ebf\u6027\u6216\u591a\u7ef4\u6982\u5ff5\u3002\u5982\u4f55\u63d0\u5347\u5bf9\u8fd9\u4e9b\u7ed3\u6784\u7684\u523b\u753b\u80fd\u529b\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u6982\u5ff5\u53d1\u73b0\uff0c\u662f\u8be5\u5de5\u4f5c\u7684\u51fa\u53d1\u70b9\u3002", "method": "\u4f5c\u8005\u91c7\u7528Mixture of Factor Analyzers\uff08MFA\uff09\uff0c\u5c06\u6fc0\u6d3b\u7a7a\u95f4\u8868\u793a\u4e3a\u591a\u4e2a\u9ad8\u65af\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u6709\u672c\u5730\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u8d28\u5fc3\u548c\u5c40\u90e8\u53d8\u5316\u4e24\u90e8\u5206\uff0c\u5e76\u5728Llama-3.1-8B\u548cGemma-2-2B\u6a21\u578b\u4e0a\u8bad\u7ec3\u5927\u89c4\u6a21MFA\u3002", "result": "MFA\u80fd\u6355\u6349\u6fc0\u6d3b\u7a7a\u95f4\u7684\u590d\u6742\u975e\u7ebf\u6027\u7ed3\u6784\u3002\u5728\u5b9a\u4f4d\u548c\u64cd\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMFA\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u57fa\u7ebf\uff0c\u4e14\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u8d85\u8d8a\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u751a\u81f3\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u4e0e\u6709\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u5b50\u7a7a\u95f4\u8868\u8fbe\u7684\u672c\u5730\u51e0\u4f55\u7ed3\u6784\uff0c\u662f\u5206\u6790\u5927\u6a21\u578b\u6fc0\u6d3b\u548c\u8fdb\u884c\u6a21\u578b\u64cd\u63a7\u7684\u6709\u6548\u5355\u5143\uff0c\u80fd\u591f\u8865\u8db3\u7ebf\u6027\u65b9\u5411\u65e0\u6cd5\u53d1\u73b0\u7684\u590d\u6742\u6982\u5ff5\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u6982\u5ff5\u53d1\u73b0\u548c\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.01538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01538", "abs": "https://arxiv.org/abs/2602.01538", "authors": ["Youliang Zhang", "Zhengguang Zhou", "Zhentao Yu", "Ziyao Huang", "Teng Hu", "Sen Liang", "Guozhen Zhang", "Ziqiao Peng", "Shunkai Li", "Yi Chen", "Zixiang Zhou", "Yuan Zhou", "Qinglin Lu", "Xiu Li"], "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars", "comment": null, "summary": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InteractAvatar\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u80fd\u591f\u4e0e\u5468\u56f4\u7269\u4f53\u4ea4\u4e92\u7684\u53ef\u8bf4\u8bdd\u865a\u62df\u4eba\uff08Talking Avatar\uff09\u81ea\u52a8\u751f\u6210\u3002\u5176\u521b\u65b0\u70b9\u5728\u4e8e\u540c\u65f6\u5b9e\u73b0\u4e86\u73af\u5883\u611f\u77e5\u548c\u9ad8\u8d28\u91cf\u52a8\u4f5c\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4eba-\u7269\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u5907\u7b80\u5355\u52a8\u4f5c\u7684\u5168\u8eab\u8bf4\u8bdd\u865a\u62df\u4eba\uff0c\u4f46\u8ba9\u5176\u6267\u884c\u4e0e\u5468\u56f4\u5177\u4f53\u7269\u4f53\u76f8\u5173\u3001\u4e14\u4e0e\u6587\u672c\u8bed\u4e49\u5bf9\u9f50\u7684\u4ea4\u4e92\u52a8\u4f5c\u4ecd\u7136\u662f\u672a\u89e3\u51b3\u96be\u70b9\u3002\u6311\u6218\u4e3b\u8981\u5728\u4e8e\u865a\u62df\u4eba\u9700\u8981\u5177\u5907\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u751f\u6210\u52a8\u4f5c\u65f6\u517c\u987e\u53ef\u63a7\u6027\u4e0e\u89c6\u9891\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86Dual-Stream\uff08\u53cc\u6d41\uff09\u6846\u67b6\u2018InteractAvatar\u2019\uff0c\u901a\u8fc7\u5c06\u611f\u77e5\u4e0e\u89c4\u5212\u4ece\u89c6\u9891\u5408\u6210\u4e2d\u89e3\u8026\uff0c\u63d0\u9ad8\u4e86\u5bf9\u4eba-\u7269\u4ea4\u4e92\u7684\u5904\u7406\u3002\u5177\u4f53\u5305\u62ec\uff1a\u4f7f\u7528\u68c0\u6d4b\uff08Detection\uff09\u63d0\u5347\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u5f15\u5165\u611f\u77e5\u4e0e\u4ea4\u4e92\u6a21\u5757\uff08PIM\uff09\u751f\u6210\u6587\u672c\u76f8\u5173\u52a8\u4f5c\uff1b\u63d0\u51fa\u97f3\u9891-\u4ea4\u4e92\u611f\u77e5\u751f\u6210\u6a21\u5757\uff08AIM\uff09\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\uff1b\u5e76\u8bbe\u8ba1\u52a8\u4f5c\u5230\u89c6\u9891\u5bf9\u9f50\u5668\uff0c\u652f\u6301\u6a21\u5757\u7684\u5e76\u884c\u5171\u751f\uff0c\u7f13\u89e3\u63a7\u5236\u4e0e\u8d28\u91cf\u77db\u76fe\u3002", "result": "\u8bbe\u7acb\u4e86\u65b0\u7684\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\u57fa\u51c6\u6570\u636e\u96c6GroundedInter\uff0c\u5f00\u5c55\u4e86\u5927\u91cf\u5b9e\u9a8c\u4e0e\u5bf9\u6bd4\uff0c\u7ed3\u679c\u8868\u660eInteractAvatar\u65b9\u6cd5\u5728\u751f\u6210\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\u65b9\u9762\u6548\u679c\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684InteractAvatar\u80fd\u6709\u6548\u751f\u6210\u80fd\u591f\u4e0e\u7269\u4f53\u4ea4\u4e92\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\u52a8\u753b\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u63a8\u52a8\u4e86\u4eba-\u7269\u4ea4\u4e92\u5408\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.02467", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02467", "abs": "https://arxiv.org/abs/2602.02467", "authors": ["Noam Steinmetz Yalon", "Ariel Goldstein", "Liad Mudrik", "Mor Geva"], "title": "Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models", "comment": null, "summary": "Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u662f\u5426\u5177\u5907\u4ee5\u4fe1\u5ff5\u9a71\u52a8\u7684\u81ea\u4e3b\u6027\u4e0e\u5143\u8ba4\u77e5\u76d1\u63a7\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u610f\u8bc6\u6307\u6807\u7814\u7a76\u63d0\u4f9b\u5b9e\u8bc1\u6570\u636e\u3002", "motivation": "\u968f\u7740LLM\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\uff0c\u5b66\u754c\u5f00\u59cb\u63a2\u8ba8\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u5177\u6709\u67d0\u79cd\u5f62\u5f0f\u7684\u2018\u610f\u8bc6\u2019\uff0c\u4e3a\u6b64\u9700\u8981\u5b9e\u8bc1\u6307\u6807\u6765\u8bc4\u4f30\u4eba\u5de5\u7cfb\u7edf\u7684\u610f\u8bc6\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u91c7\u7528Butlin\u7b49\u4eba\u63d0\u51fa\u7684\u4eba\u5de5\u7cfb\u7edf\u610f\u8bc6\u5224\u636e\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807HOT-3\uff0c\u5177\u4f53\u65b9\u6cd5\u662f\u5c06\u6a21\u578b\u7684\u4fe1\u5ff5\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8868\u5f81\uff0c\u901a\u8fc7\u8bbe\u5b9a\u91cf\u5316\u6307\u6807\uff0c\u5206\u6790\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3b\u5bfc\u4fe1\u5ff5\u7684\u53d8\u5316\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e0b\u8003\u67e5\u4fe1\u5ff5\u95f4\u52a8\u6001\u5173\u7cfb\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a(1) \u5916\u90e8\u64cd\u4f5c\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u5f71\u54cd\u6a21\u578b\u5185\u90e8\u7684\u4fe1\u5ff5\u5f62\u6210\u8fc7\u7a0b\uff1b(2) \u4fe1\u5ff5\u7684\u5f62\u6210\u5177\u6709\u56e0\u679c\u6027\u5730\u51b3\u5b9a\u6a21\u578b\u7684\u884c\u52a8\u9009\u62e9\uff1b(3) \u6a21\u578b\u80fd\u591f\u76d1\u6d4b\u5e76\u62a5\u544a\u81ea\u8eab\u7684\u4fe1\u5ff5\u72b6\u6001\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660eLLM\u5177\u5907\u4fe1\u5ff5\u9a71\u52a8\u7684\u81ea\u4e3b\u6027\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\u80fd\u529b\uff0c\u4e3a\u63a2\u7a76LLM\u4e2d\u81ea\u4e3b\u6027\u3001\u4fe1\u5ff5\u53ca\u5143\u8ba4\u77e5\u7b49\u610f\u8bc6\u76f8\u5173\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2602.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01540", "abs": "https://arxiv.org/abs/2602.01540", "authors": ["Yuehai Chen"], "title": "FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training", "comment": null, "summary": "Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u7fa4\u8ba1\u6570\u7f51\u7edcFSCA-Net\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u79bb\u548c\u4ea4\u4e92\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u5e94\u5bf9\u591a\u6570\u636e\u96c6\u8d1f\u8fc1\u79fb\uff0c\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u5728\u4eba\u7fa4\u8ba1\u6570\u9886\u57df\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8de8\u73af\u5883\u65f6\u53d7\u57df\u5dee\u5f02\u5f71\u54cd\u4e25\u91cd\uff0c\u76f4\u63a5\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u89e3\u51b3\u7279\u5f81\u6df7\u6dc6\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "FSCA-Net\u660e\u786e\u5730\u5c06\u7279\u5f81\u5206\u4e3a\u57df\u4e0d\u53d8\u548c\u57df\u7279\u5b9a\u4e24\u7c7b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u5730\u5efa\u6a21\u4e8c\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002\u540c\u65f6\u8bbe\u8ba1\u4e92\u4fe1\u606f\u4f18\u5316\u76ee\u6807\uff0c\u4fc3\u4f7f\u57df\u4e0d\u53d8\u7279\u5f81\u4e00\u81f4\u3001\u57df\u7279\u5b9a\u7279\u5f81\u4e92\u8865\u3002", "result": "\u5728\u591a\u4e2a\u4eba\u7fa4\u8ba1\u6570\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc1\u660e\uff0cFSCA-Net\u663e\u8457\u51cf\u5f31\u4e86\u8d1f\u8fc1\u79fb\uff0c\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FSCA-Net\u4e3a\u591a\u57df\u4eba\u7fa4\u8ba1\u6570\u63d0\u51fa\u4e86\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6848\uff0c\u6709\u6548\u7f13\u89e3\u8d1f\u8fc1\u79fb\u3001\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02474", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02474", "abs": "https://arxiv.org/abs/2602.02474", "authors": ["Haozhen Zhang", "Quanyu Long", "Jianzhu Bao", "Tao Feng", "Weizhi Zhang", "Haodong Yue", "Wenya Wang"], "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents", "comment": "Code is available at https://github.com/ViktorAxelsen/MemSkill", "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.", "AI": {"tldr": "MemSkill\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7406\u5ff5\uff0c\u5c06LLM agent\u7684\u8bb0\u5fc6\u64cd\u4f5c\u8bbe\u8ba1\u4e3a\u53ef\u5b66\u4e60\u3001\u53ef\u8fdb\u5316\u7684\u201c\u8bb0\u5fc6\u6280\u80fd\u201d\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u5177\u5907\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09agent\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u9759\u6001\u3001\u4eba\u5de5\u8bbe\u8ba1\u7684\u64cd\u4f5c\uff0c\u8fd9\u79cd\u65b9\u5f0f\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4e14\u5728\u957f\u5386\u53f2\u6570\u636e\u4e0b\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u79cd\u66f4\u81ea\u9002\u5e94\u3001\u667a\u80fd\u5316\u7684\u8bb0\u5fc6\u7ba1\u7406\u65b9\u5f0f\u3002", "method": "MemSkill\u5c06\u4f20\u7edf\u7684\u9759\u6001\u8bb0\u5fc6\u64cd\u4f5c\u91cd\u6784\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u590d\u7528\u7684\u201c\u8bb0\u5fc6\u6280\u80fd\u201d\u3002\u5176\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u63a7\u5236\u5668\uff0c\u5b66\u4f1a\u6311\u9009\u5c11\u91cf\u76f8\u5173\u6280\u80fd\uff1b2\uff09\u6267\u884c\u5668\uff0c\u7531LLM\u9a71\u52a8\uff0c\u751f\u6210\u6280\u80fd\u5f15\u5bfc\u7684\u8bb0\u5fc6\u5185\u5bb9\uff1b3\uff09\u8bbe\u8ba1\u5668\uff0c\u5b9a\u671f\u5ba1\u89c6\u6280\u80fd\u5931\u8bef\u6848\u4f8b\uff0c\u5e76\u66f4\u65b0\u548c\u4f18\u5316\u6280\u80fd\u96c6\uff0c\u5f62\u6210\u5c01\u95ed\u5faa\u73af\u3001\u6301\u7eed\u8fdb\u5316\u7684\u673a\u5236\u3002", "result": "\u5728LoCoMo\u3001LongMemEval\u3001HotpotQA\u548cALFWorld\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMemSkill\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MemSkill\u5b9e\u73b0\u4e86\u6280\u80fd\u9009\u62e9\u7b56\u7565\u4e0e\u6280\u80fd\u96c6\u672c\u8eab\u7684\u53cc\u91cd\u81ea\u8fdb\u5316\uff0c\u63a8\u52a8LLM\u667a\u80fd\u4f53\u8bb0\u5fc6\u7ba1\u7406\u66f4\u5177\u9002\u5e94\u6027\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c2\u5bdf\u4e0e\u601d\u8def\u3002"}}
{"id": "2602.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01541", "abs": "https://arxiv.org/abs/2602.01541", "authors": ["Boyi Li", "Yifan Shen", "Yuanzhe Liu", "Yifan Xu", "Jiateng Liu", "Xinzhuo Li", "Zhengyuan Li", "Jingyuan Zhu", "Yunhan Zhong", "Fangzhou Lan", "Jianguo Cao", "James M. Rehg", "Heng Ji", "Ismini Lourentzou", "Xu Cao"], "title": "Toward Cognitive Supersensing in Multimodal Large Language Model", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCognitive Supersensing\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u8d4b\u4e88\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7c7b\u4eba\u89c6\u89c9\u610f\u8c61\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u8ba4\u77e5\u578bVQA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dMLLMs\u867d\u5728\u611f\u77e5\u7c7b\u4efb\u52a1\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u62bd\u8c61\u89c6\u89c9\u8bb0\u5fc6\u7684\u590d\u6742\u8ba4\u77e5\u95ee\u9898\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u5ffd\u89c6\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u89c6\u7a7a\u95f4\u753b\u677f\u548c\u89c6\u89c9\u610f\u8c61\u7684\u89c6\u89c9\u63a8\u7406\u673a\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51faCognitive Supersensing\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u6f5c\u5728\u89c6\u89c9\u610f\u8c61\u9884\u6d4b\uff08LVIP\uff09\u5934\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u8ba4\u77e5\u6f5c\u5728\u5e8f\u5217\uff0c\u5e76\u4e0e\u7b54\u6848\u5bf9\u9f50\uff0c\u5f62\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u5185\u90e8\u63a8\u7406\u94fe\u3002\u540c\u65f6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u5229\u7528\u89c6\u89c9\u6f5c\u5728\u4fe1\u606f\u4f18\u5316\u6587\u672c\u63a8\u7406\u8def\u5f84\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86CogSense-Bench\u6570\u636e\u96c6\u8bc4\u6d4bMLLMs\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528Cognitive Supersensing\u8bad\u7ec3\u7684MLLMs\u5728CogSense-Bench\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u57df\u5916\u6570\u5b66\u548c\u79d1\u5b66VQA\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5185\u5728\u89c6\u89c9\u610f\u8c61\u80fd\u529b\u53ef\u80fd\u662f\u8fde\u63a5\u611f\u77e5\u8bc6\u522b\u548c\u8ba4\u77e5\u7406\u89e3\u7684\u5173\u952e\u3002CogSense-Bench\u6570\u636e\u96c6\u53ca\u76f8\u5173\u6a21\u578b\u6743\u91cd\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.02477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02477", "abs": "https://arxiv.org/abs/2602.02477", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Zhenghao Lin", "Eric Hancheng Jiang", "Hengyuan Zhang", "Yelong Shen", "Kai-Wei Chang", "Ying Nian Wu", "Yeyun Gong", "Weizhu Chen"], "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5206\u800c\u6cbb\u4e4b\uff08DAC\uff09\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u9010\u6b65\u6df1\u5165\uff0c\u4f20\u7edf\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5728\u6a21\u578b\u80fd\u529b\u6781\u9650\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u5e76\u4e14\u5176\u4e25\u683c\u7684\u987a\u5e8f\u7279\u6027\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u7684\u6269\u5c55\u6027\u3002\u5206\u800c\u6cbb\u4e4b\u63a8\u7406\u6709\u671b\u5f25\u8865\u8fd9\u4e9b\u7f3a\u9677\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u4e0eDAC\u63a8\u7406\u5728\u63a8\u7406\u63a8\u65ad\u65f6\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u5176\u6548\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06DAC\u63a8\u7406\u4e0e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u3002\u5728\u6bcf\u4e00\u6b65\u7531\u7b56\u7565\u7f51\u7edc\u5c06\u590d\u6742\u95ee\u9898\u62c6\u5206\u4e3a\u82e5\u5e72\u5b50\u95ee\u9898\uff0c\u4f9d\u6b21\u6c42\u89e3\u540e\u7ed3\u5408\u5b50\u95ee\u9898\u7b54\u6848\u89e3\u51b3\u539f\u59cb\u95ee\u9898\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e0e\u6c42\u89e3\u8fc7\u7a0b\u6574\u5408\u8fdbRL\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u540c\u7b49\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c\u8be5DAC-RL\u6846\u67b6\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6bd4\u8d5b\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u8d85\u8d8aCoT\u65b9\u6cd5\uff0cPass@1 \u63d0\u53478.6%\uff0cPass@32\u63d0\u53476.3%\u3002", "conclusion": "\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684DAC\u63a8\u7406\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u4e0a\u9650\u548c\u6269\u5c55\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684CoT\u63a8\u7406\u3002"}}
{"id": "2602.01559", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.01559", "abs": "https://arxiv.org/abs/2602.01559", "authors": ["Libo Zhu", "Zihan Zhou", "Zhiyi Zhou", "Yiyang Qu", "Weihang Zhang", "Keyu Shi", "Yifan Fu", "Yulun Zhang"], "title": "Combined Flicker-banding and Moire Removal for Screen-Captured Images", "comment": null, "summary": "Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moir\u00e9 patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moir\u00e9 patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moir\u00e9 patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLEAR\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u5c4f\u5e55\u62cd\u6444\u56fe\u7247\u4e2d\u83ab\u5c14\u6761\u7eb9\u548c\u9891\u95ea\u5e26\u8054\u5408\u53bb\u9664\u7684\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5173\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u8bbe\u5907\u62cd\u6444\u5c4f\u5e55\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u6210\u50cf\u4e2d\u5e38\u89c1\u7684\u83ab\u5c14\u6761\u7eb9\u548c\u9891\u95ea\u5e26\u4f1a\u4e25\u91cd\u5f71\u54cd\u56fe\u7247\u8d28\u91cf\u3002\u7531\u4e8e\u8fd9\u4e24\u79cd\u9000\u5316\u7684\u9ad8\u5ea6\u8026\u5408\uff0c\u5355\u4e00\u9000\u5316\u7684\u53bb\u9664\u65b9\u6cd5\u65e0\u6cd5\u80dc\u4efb\u3002\u672c\u7814\u7a76\u65e8\u5728\u5e94\u5bf9\u8fd9\u4e00\u590d\u5408\u9000\u5316\u95ee\u9898\u3002", "method": "1. \u63d0\u51faCLEAR\u7edf\u4e00\u4fee\u590d\u6846\u67b6\uff1b2. \u6784\u5efa\u540c\u65f6\u5305\u542b\u83ab\u5c14\u6761\u7eb9\u548c\u9891\u95ea\u5e26\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b3. \u5f15\u5165ISP\u4e3a\u57fa\u7840\u7684\u9891\u95ea\u6a21\u62df\u7ba1\u7ebf\uff0c\u6269\u5c55\u9000\u5316\u7c7b\u578b\u5e76\u5e2e\u52a9\u6a21\u578b\u8bad\u7ec3\uff1b4. \u8bbe\u8ba1\u9891\u57df\u5206\u89e3\u91cd\u7ec4\u6a21\u5757\u548c\u8f68\u8ff9\u5bf9\u9f50\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u590d\u5408\u9000\u5316\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u548c\u590d\u6742\u5b9e\u9645\u573a\u666f\u4e0b\uff0c\u5747\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002", "conclusion": "CLEAR\u6846\u67b6\u662f\u9996\u4e2a\u6709\u6548\u5e94\u5bf9\u5c4f\u6444\u56fe\u50cf\u590d\u5408\u9000\u5316\uff08\u83ab\u5c14+\u9891\u95ea\u5e26\uff09\u7684\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u663e\u8457\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02486", "abs": "https://arxiv.org/abs/2602.02486", "authors": ["Jialiang Zhu", "Gongrui Zhang", "Xiaolong Ma", "Lin Xu", "Miaosen Zhang", "Ruiqi Yang", "Song Wang", "Kai Qiu", "Zhirong Wu", "Qi Dai", "Ruichun Ma", "Bei Liu", "Yifan Yang", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Xin Geng", "Baining Guo"], "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents", "comment": null, "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.", "AI": {"tldr": "Re-TRAC\u662f\u4e00\u79cd\u65b0\u578b\u57fa\u4e8eLLM\u7684\u7814\u7a76\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u8f68\u8ff9\u53cd\u601d\u4e0e\u7ed3\u6784\u5316\u72b6\u6001\u603b\u7ed3\uff0c\u5b9e\u73b0\u6bd4ReAct\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u66f4\u4f18\u7684\u641c\u7d22\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eReAct\u6846\u67b6\u7684LLM\u7814\u7a76\u4ee3\u7406\u91c7\u7528\u7ebf\u6027\u6d41\u7a0b\uff0c\u96be\u4ee5\u590d\u7528\u5386\u53f2\u72b6\u6001\u3001\u63a2\u7d22\u591a\u5206\u652f\u8def\u5f84\u6216\u7ef4\u6301\u5168\u5c40\u610f\u8bc6\uff0c\u5bfc\u81f4\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3001\u5197\u4f59\u63a2\u7d22\u548c\u4f4e\u6548\u641c\u7d22\u3002", "method": "Re-TRAC\u901a\u8fc7\u5728\u6bcf\u6761\u641c\u7d22\u8f68\u8ff9\u7ed3\u675f\u540e\u751f\u6210\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\uff0c\u6db5\u76d6\u8bc1\u636e\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u5931\u8d25\u548c\u540e\u7eed\u8ba1\u5212\uff0c\u5e76\u5c06\u5176\u7528\u4e8e\u6307\u5bfc\u540e\u7eed\u8f68\u8ff9\uff0c\u5b9e\u73b0\u8de8\u8f68\u8ff9\u7684\u4fe1\u606f\u6574\u5408\u548c\u53cd\u601d\u3002\u5bf9\u4e8e\u5c0f\u6a21\u578b\uff0c\u8fd8\u63d0\u51faRe-TRAC\u611f\u77e5\u7684\u6709\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cRe-TRAC\u5728BrowseComp\u4efb\u52a1\u4e0a\u6bd4ReAct\u6846\u67b6\u63d0\u534715-20%\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u5c0f\u6a21\u578b\uff0c\u5229\u7528Re-TRAC\u5fae\u8c03\u4e5f\u8fbe\u5230\u4e86\u540c\u89c4\u6a21\u4e0b\u7684SOTA\u3002Re-TRAC\u8fd8\u80fd\u663e\u8457\u51cf\u5c11\u5de5\u5177\u8c03\u7528\u548ctoken\u4f7f\u7528\uff0c\u5b9e\u73b0\u66f4\u6709\u9488\u5bf9\u6027\u7684\u9010\u6b65\u63a2\u7d22\u3002", "conclusion": "Re-TRAC\u521b\u65b0\u6027\u5730\u5c06\u7814\u7a76\u4ee3\u7406\u7684\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u8fed\u4ee3\u53cd\u601d\u548c\u5168\u5c40\u89c4\u5212\uff0c\u514b\u670d\u7ebf\u6027\u6846\u67b6\u7684\u5c40\u9650\uff0c\u6709\u6548\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u7ed3\u679c\u8d28\u91cf\u3002"}}
{"id": "2602.01561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01561", "abs": "https://arxiv.org/abs/2602.01561", "authors": ["Yejin Son", "Saejin Kim", "Dongjun Min", "Younjae Yu"], "title": "Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd", "comment": "24 pages", "summary": "Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MUN\u591a\u6a21\u6001\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\uff0c\u4ee5\u53ca\u4e00\u79cd\u63d0\u5347\u5c0f\u6a21\u578b\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u7684R-ICL\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u975e\u5e38\u89c4\u3001\u53cd\u76f4\u89c9\u573a\u666f\u4e0b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u7f3a\u4e4f\u5bf9\u2018\u53cd\u5e38\u8bc6\u2019\u60c5\u51b5\u7684\u6d4b\u8bd5\u5de5\u5177\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMUN\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u573a\u666f\u4e0e\u51fa\u4eba\u610f\u6599\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u914d\u5bf9\u8bc4\u6d4b\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51faR-ICL\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u68c0\u7d22\u5668MER\uff0c\u5b9e\u73b0\u5927\u6a21\u578b\u5411\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0R-ICL\u65b9\u6cd5\u5728\u4f4e\u9891\u3001\u975e\u5e38\u89c4\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u5e38\u89c4ICL\u63d0\u5347\u4e86\u5e73\u57478.3%\u7684\u8868\u73b0\u3002", "conclusion": "MUN\u6570\u636e\u96c6\u548cR-ICL\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u3001\u6587\u5316\u591a\u6837\u53ca\u975e\u5178\u578b\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u4e0e\u9002\u5e94\u6027\u8bc4\u4f30\u4e0e\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.02495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02495", "abs": "https://arxiv.org/abs/2602.02495", "authors": ["Peter Chen", "Xiaopeng Li", "Xi Chen", "Tianyi Lin"], "title": "Reward-free Alignment for Conflicting Objectives", "comment": "27 pages", "summary": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5904\u7406\u591a\u76ee\u6807\u51b2\u7a81\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u65b0\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u4e0d\u540cLLM\u4e0a\uff0c\u90fd\u80fd\u53d6\u5f97\u4f18\u8d8a\u7684\u5e15\u7d2f\u6258\u6743\u8861\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684LLM\u5bf9\u9f50\u901a\u5e38\u9762\u5bf9\u591a\u4e2a\u5f7c\u6b64\u51b2\u7a81\u7684\u7528\u6237\u76ee\u6807\uff0c\u7b80\u5355\u7684\u52a0\u6743\u6216\u805a\u5408\u5e38\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u4e0e\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u800c\u73b0\u6709\u591a\u76ee\u6807\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u5e26\u6765\u4e86\u590d\u6742\u5ea6\u548c\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faReward-free Alignment framework for Conflicted Objectives (RACO)\uff0c\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u521b\u65b0\u5730\u7528clipped conflict-averse gradient descent\u65b9\u6cd5\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u5e76\u7ed9\u51fa\u6536\u655b\u5230Pareto\u5173\u952e\u70b9\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u8fd8\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5e76\u5728\u591a\u76ee\u6807\u6587\u672c\u6458\u8981\u548c\u5b89\u5168\u5bf9\u9f50\u4efb\u52a1\u4e0a\u505a\u4e86\u5b9e\u9a8c\u3002", "result": "\u5728Qwen 3\u3001Llama 3\u548cGemma 3\u7b49\u591a\u4e2aLLM\u53ca\u591a\u4efb\u52a1\u4e0a\uff0cRACO\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u6bd4\u73b0\u6709\u591a\u76ee\u6807\u5bf9\u9f50\u57fa\u7ebf\u53d6\u5f97\u66f4\u4f18\u5e15\u7d2f\u6258\u6743\u8861\u3002", "conclusion": "RACO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807LLM\u5bf9\u9f50\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u3001\u5177\u5907\u826f\u597d\u7406\u8bba\u6027\u8d28\uff0c\u80fd\u591f\u8de8\u6a21\u578b\u548c\u4efb\u52a1\u666e\u9002\u63d0\u5347\u591a\u76ee\u6807\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2602.01570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01570", "abs": "https://arxiv.org/abs/2602.01570", "authors": ["Yiwen Jia", "Hao Wei", "Yanhui Zhou", "Chenyang Ge"], "title": "One-Step Diffusion for Perceptual Image Compression", "comment": null, "summary": "Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u4e00\u6b65\u6269\u6563\u8fc7\u7a0b\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u89e3\u7801\u901f\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u5224\u522b\u5668\u589e\u5f3a\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u76f8\u5f53\u7684\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u6709\u66f4\u9ad8\u6548\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5355\u6b65\u6269\u6563\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u9700\u4e00\u6b21\u53bb\u566a\u6781\u5927\u63d0\u9ad8\u89e3\u7801\u6548\u7387\u3002\u4e3a\u63d0\u5347\u91cd\u5efa\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5f15\u5165\u4f5c\u7528\u4e8e\u7279\u5f81\u8868\u793a\uff08\u800c\u975e\u539f\u59cb\u50cf\u7d20\uff09\u7684\u5224\u522b\u5668\u4ee5\u66f4\u597d\u6355\u6349\u9ad8\u5c42\u6b21\u7ed3\u6784\u548c\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6548\u679c\u4e0a\u53ef\u4e0e\u6700\u65b0\u6269\u6563\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u63d0\u534746\u500d\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5f0f\u56fe\u50cf\u538b\u7f29\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u9009\u62e9\u3002\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u793e\u533a\u9a8c\u8bc1\u548c\u5e94\u7528\u3002"}}
{"id": "2602.01574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01574", "abs": "https://arxiv.org/abs/2602.01574", "authors": ["Haobo Wang", "Weiqi Luo", "Xiaojun Jia", "Xiaochun Cao"], "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models", "comment": null, "summary": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SGHA-Attack\uff0c\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08VLMs\uff09\u7684\u8bed\u4e49\u5f15\u5bfc\u5206\u5c42\u5bf9\u9f50\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u63d0\u5347\u6709\u76ee\u6807\u6027\u7684\u8f6c\u79fb\u653b\u51fb\u80fd\u529b\u5e76\u589e\u5f3a\u5bf9\u6297\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9VLMs\u7684\u6709\u76ee\u6807\u8f6c\u79fb\u653b\u51fb\u65b9\u6cd5\u5728\u4ee3\u7406\u6a21\u578b\u7279\u5f81\u7a7a\u95f4\u6613\u8fc7\u62df\u5408\uff0c\u4ec5\u5173\u6ce8\u6700\u7ec8\u5c42\u5bf9\u9f50\uff0c\u5ffd\u89c6\u4e86\u4e2d\u95f4\u8bed\u4e49\u7684\u534f\u540c\uff0c\u5bfc\u81f4\u8de8\u6a21\u578b\u653b\u51fb\u8f6c\u79fb\u6027\u8f83\u5dee\u3002", "method": "\u65b9\u6cd5\u521b\u65b0\u70b9\u5728\u4e8e\uff1a1\uff09\u57fa\u4e8e\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91c7\u6837\uff0c\u751f\u6210\u548c\u76ee\u6807\u63d0\u793a\u8bcd\u76f8\u5173\u7684\u591a\u53c2\u8003\u89c6\u89c9\u951a\u70b9\uff0c\u7cbe\u9009\u6700\u76f8\u5173Top-K\u7528\u4e8e\u52a0\u6743\u4f18\u5316\uff1b2\uff09\u5728\u89c6\u89c9\u7279\u5f81\u7684\u591a\u4e2a\u5c42\u6b21\u3001\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u4e0a\u5b9e\u65bd\u4e2d\u95f4\u5c42\u5bf9\u9f50\uff1b3\uff09\u5728\u4e2d\u95f4\u5c42\u8de8\u6a21\u6001\uff08\u89c6\u89c9\u4e0e\u6587\u672c\uff09\u7a7a\u95f4\u8fdb\u884c\u540c\u6b65\uff0c\u5f3a\u5316\u65e9\u671f\u8de8\u6a21\u6001\u4fe1\u53f7\u6307\u5bfc\uff0c\u8d85\u8d8a\u5355\u4e00\u6700\u7ec8\u5c42\u7279\u5f81\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSGHA-Attack\u5728\u591a\u4e2a\u5f00\u6e90\u548c\u5546\u4e1a\u9ed1\u76d2VLM\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u76ee\u6807\u653b\u51fb\u8f6c\u79fb\u80fd\u529b\uff0c\u5e76\u80fd\u62b5\u6297\u9884\u5904\u7406\u548c\u51c0\u5316\u7c7b\u9632\u5fa1\u3002", "conclusion": "\u5f15\u5165\u591a\u951a\u70b9\u548c\u5206\u5c42\u5bf9\u9f50\u673a\u5236\u53ef\u6709\u6548\u63d0\u5347VLMs\u8f6c\u79fb\u653b\u51fb\u7684\u9488\u5bf9\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u4e0e\u9632\u5fa1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01586", "abs": "https://arxiv.org/abs/2602.01586", "authors": ["Wencan Cheng", "Gim Hee Lee"], "title": "HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation", "comment": "AAAI accepted", "summary": "3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHandMCM\u7684\u65b0\u578b3D\u624b\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u5bf9\u906e\u6321\u573a\u666f\u4e0b\u624b\u90e8\u5173\u952e\u70b9\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u7684\u5b66\u4e60\uff0c\u5b9e\u73b0\u5bf9\u624b\u90e8\u4e09\u7ef4\u5173\u952e\u70b9\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3D\u624b\u52bf\u4f30\u8ba1\u5bf9\u4e8e\u589e\u5f3a\u73b0\u5b9e\u7b49\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u624b\u90e8\u81ea\u906e\u6321\u53ca\u4e0e\u7269\u4f53\u4ea4\u4e92\u65f6\u7684\u906e\u6321\uff0c\u4f7f\u5f97\u7cbe\u786e\u4f30\u8ba1\u4e00\u76f4\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b0\u65b9\u6cd5HandMCM\uff0c\u5728\u7ed3\u6784\u4e0a\u878d\u5408\u4e86\u5c40\u90e8\u4fe1\u606f\u6ce8\u5165/\u8fc7\u6ee4\u6a21\u5757\u548c\u5173\u952e\u70b9\u5bf9\u5e94\u5efa\u6a21\u6a21\u5757\u3002\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u8f93\u5165\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5065\u58ee\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHandMCM\u5728\u4e25\u91cd\u906e\u6321\u7b49\u590d\u6742\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HandMCM\u65b9\u6cd5\u63d0\u5347\u4e863D\u624b\u52bf\u4f30\u8ba1\u5728\u590d\u6742\u906e\u6321\u73af\u5883\u4e0b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u6709\u671b\u63a8\u52a8\u5176\u5b9e\u7528\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2602.02014", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02014", "abs": "https://arxiv.org/abs/2602.02014", "authors": ["Hongxin Xiang", "Pengsen Ma", "Yunkang Cao", "Di Yu", "Haowen Chen", "Xinyu Yang", "Xiangxiang Zeng"], "title": "Rethinking Genomic Modeling Through Optical Character Recognition", "comment": null, "summary": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.", "AI": {"tldr": "OpticalDNA\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57fa\u56e0\u7ec4\u5efa\u6a21\u8f6c\u5316\u4e3a\u7c7b\u4f3cOCR\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u56e0\u7ec4\u5927\u6a21\u578b\u591a\u628aDNA\u4f5c\u4e3a\u4e00\u7ef4\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u57fa\u56e0\u4fe1\u606f\u672c\u8d28\u4e0a\u7a00\u758f\u548c\u975e\u8fde\u7eed\u3001\u7ed3\u6784\u590d\u6742\uff0c\u5bfc\u81f4\u65e0\u6548\u8ba1\u7b97\u548c\u4e0a\u4e0b\u6587\u538b\u7f29\u56f0\u96be\u3002", "method": "\u63d0\u51faOpticalDNA\u6846\u67b6\uff0c\u5c06DNA\u6e32\u67d3\u4e3a\u7ed3\u6784\u5316\u89c6\u89c9\u5e03\u5c40\uff0c\u8bad\u7ec3\u5305\u542b\u89c6\u89c9DNA\u7f16\u7801\u5668\u548c\u6587\u6863\u89e3\u7801\u5668\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u628a\u57fa\u56e0\u7ec4\u4efb\u52a1\u8f6c\u5316\u4e3a\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\uff0c\u5b9e\u73b0\u89c6\u89c9Token\u7ea7\u9ad8\u4fdd\u771f\u538b\u7f29\u3002", "result": "\u5728\u591a\u9879\u57fa\u56e0\u7ec4\u4efb\u52a1\u57fa\u51c6\u4e0a\uff0cOpticalDNA\u5927\u5e45\u8d85\u8d8a\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff1b\u5904\u7406\u6700\u957f\u8fbe45\u4e07\u78b1\u57fa\u5e8f\u5217\u65f6\uff0c\u7528\u6781\u5c11Token\u6570\u53d6\u5f97\u6700\u4f18\u8868\u73b0\uff0c\u6240\u9700\u53ef\u8bad\u7ec3\u53c2\u6570\u8fdc\u4f4e\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5f00\u542f\u4e86\u57fa\u56e0\u7ec4\u89c6\u89c9\u8868\u5f81\u4e0e\u7406\u89e3\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u957f\u5e8f\u5217\u8868\u5f81\u3002"}}
{"id": "2602.01591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01591", "abs": "https://arxiv.org/abs/2602.01591", "authors": ["Zhixiong Yue", "Zixuan Ni", "Feiyang Ye", "Jinshan Zhang", "Sheng Shen", "Zhenpeng Mi"], "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages", "comment": null, "summary": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAFS GRPO\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u57fa\u4e8eRL\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u91cc\u6b65\u9aa4\u5c11\u3001\u5bf9\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6ce8\u5165\u548c\u5956\u52b1\u51fd\u6570\u521b\u65b0\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u504f\u597d\u5bf9\u9f50\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6d41\u5339\u914d\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u65f6\u9700\u8981\u8bb8\u591a\u53bb\u566a\u6b65\u9aa4\uff0c\u4e14\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u4e14\u4e0d\u7cbe\u786e\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTAFS GRPO\u6846\u67b6\uff1a\u8fed\u4ee3\u6027\u5730\u5bf9\u4e00\u6b65\u91c7\u6837\u7ed3\u679c\u6ce8\u5165\u81ea\u9002\u5e94\u7684\u65f6\u95f4\u566a\u58f0\uff0c\u53cd\u590d\u9000\u706b\u91c7\u6837\u7ed3\u679c\uff0c\u65e2\u589e\u52a0\u91c7\u6837\u8fc7\u7a0b\u7684\u968f\u673a\u6027\uff0c\u53c8\u4fdd\u7559\u6bcf\u5f20\u56fe\u7247\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u540c\u65f6\uff0c\u5f15\u5165\u6b65\u957f\u611f\u77e5\u7684\u4f18\u52bf\u96c6\u6210\u673a\u5236\u7ed3\u5408\u4e86GRPO\uff0c\u5b9e\u73b0\u65e0\u9700\u5956\u52b1\u51fd\u6570\u53ef\u5fae\uff0c\u53c8\u80fd\u4e3a\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u5bc6\u96c6\u4e14\u6709\u9488\u5bf9\u6027\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "TAFS GRPO\u5728\u6587\u751f\u56fe\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684few-step\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u7247\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u5f02\u3002", "conclusion": "TAFS GRPO\u4e3a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u5e26\u6765\u4e86\u66f4\u9ad8\u6548\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u6709\u5b9e\u9645\u8fd0\u7528\u6f5c\u529b\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5373\u5c06\u5f00\u6e90\uff0c\u4fbf\u4e8e\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.02185", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02185", "abs": "https://arxiv.org/abs/2602.02185", "authors": ["Yu Zeng", "Wenxuan Huang", "Zhen Fang", "Shuang Chen", "Yufan Shen", "Yishuo Cai", "Xiaoman Wang", "Zhenfei Yin", "Lin Chen", "Zehui Chen", "Shiting Huang", "Yiming Zhao", "Yao Hu", "Philip Torr", "Wanli Ouyang", "Shaosheng Cao"], "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VDR-Bench\u57fa\u51c6\uff0c\u7528\u4ee5\u66f4\u771f\u5b9e\u548c\u4e25\u683c\u5730\u8bc4\u6d4b\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u7684\u68c0\u7d22\u4e0e\u95ee\u7b54\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u7b56\u7565\u63d0\u5347\u89c6\u89c9\u68c0\u7d22\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u4e0e\u641c\u7d22\u5f15\u64ce\u7ed3\u5408\u7684\u590d\u6742\u89c6\u89c9-\u6587\u672c\u4fe1\u606f\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u7f3a\u4e4f\u4ee5\u89c6\u89c9\u68c0\u7d22\u4e3a\u6838\u5fc3\u7684\u8bbe\u95ee\uff0c\u4e8c\u662f\u8bc4\u6d4b\u573a\u666f\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u68c0\u7d22\u96be\u5ea6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u6d4b\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u63d0\u51faVDR-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b2,000\u4e2a\u7ecf\u591a\u9636\u6bb5\u7b56\u5212\u548c\u4e13\u5bb6\u8bc4\u5ba1\u7684VQA\u5b9e\u4f8b\uff0c\u65e8\u5728\u4f53\u73b0\u771f\u5b9e\u4e16\u754c\u4e0b\u7684\u590d\u6742\u591a\u6a21\u6001\u68c0\u7d22\u9700\u6c42\uff0c\u5e76\u7528\u591a\u8f6e\u88c1\u526a-\u68c0\u7d22\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u89c6\u89c9\u68c0\u7d22\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u5e94\u7528VDR-Bench\u548c\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u89c9\u68c0\u7d22\u80fd\u529b\u5f97\u5230\u4e86\u6709\u6548\u63d0\u5347\uff0c\u4f53\u73b0\u4e86\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6df1\u5ea6\u68c0\u7d22\u7cfb\u7edf\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u66f4\u8d34\u5408\u5b9e\u9645\u7684\u8bc4\u6d4b\u6807\u51c6\u548c\u65b9\u6cd5\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u80fd\u529b\u63d0\u5347\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002\u4ee3\u7801\u53ca\u6570\u636e\u96c6\u5c06\u516c\u5f00\uff0c\u4fbf\u4e8e\u5b66\u754c\u590d\u73b0\u548c\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01593", "abs": "https://arxiv.org/abs/2602.01593", "authors": ["Wenzhuo Zhao", "Keren Fu", "Jiahao He", "Xiaohong Liu", "Qijun Zhao", "Guangtao Zhai"], "title": "Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework", "comment": null, "summary": "Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the \"task-specific\" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u67b6\u6784Samba\u4ee5\u53ca\u591a\u4efb\u52a1\u901a\u7528\u7248\u672cSamba+\uff0c\u5728\u516d\u7c7bSOD\u4efb\u52a1\u548c22\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8eCNN\u611f\u53d7\u91ce\u6709\u9650\u53caTransformer\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u800c\u65b0\u5174Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u53ef\u517c\u987e\u5168\u5c40\u611f\u53d7\u91ce\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9a71\u52a8\u4e86\u672c\u5de5\u4f5c\u7684\u63d0\u51fa\u3002", "method": "1\uff09\u521b\u65b0\u6027\u5730\u5c06Mamba\u7528\u4e8e\u591a\u4e2aSOD\u4efb\u52a1\uff1b2\uff09\u63d0\u51faSGMB\u6a21\u5757\uff0c\u5176\u4e2d\u7a7a\u95f4\u90bb\u57df\u626b\u63cf\u7b97\u6cd5\u4fdd\u8bc1\u663e\u8457\u533a\u57df\u7a7a\u95f4\u8fde\u7eed\u6027\uff1b3\uff09\u63d0\u51faCAU\u4fc3\u8fdb\u5206\u5c42\u7279\u5f81\u7684\u4e0a\u4e0b\u6587\u5bf9\u9f50\u805a\u5408\uff1b4\uff09Samba+\u901a\u8fc7\u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u5b9e\u73b0\u8de8\u6a21\u6001\u7edf\u4e00\uff0c\u5e76\u5f15\u5165HGA\u6a21\u5757\u5b9e\u73b0\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\uff0cMACL\u7b56\u7565\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "Samba\u5728\u516d\u7c7b\u663e\u8457\u6027\u68c0\u6d4b\u4efb\u52a1\u300122\u4e2a\u6570\u636e\u96c6\u4e0a\u5355\u72ec\u5747\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0cSamba+\u4f5c\u4e3a\u5355\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u53ca\u4efb\u52a1\u4e0a\u8868\u73b0\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "Samba\u53ca\u5176\u591a\u4efb\u52a1\u6269\u5c55Samba+\u6a21\u578b\u5728SOD\u9886\u57df\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7cbe\u5ea6\u3001\u66f4\u4f18\u8ba1\u7b97\u6548\u7387\u548c\u66f4\u597d\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.01594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01594", "abs": "https://arxiv.org/abs/2602.01594", "authors": ["Wenzhuo Liu", "Qiannan Guo", "Zhen Wang", "Wenshuo Wang", "Lei Yang", "Yicheng Qiao", "Lening Wang", "Zhiwei Li", "Chen Lv", "Shanghang Zhang", "Junqiang Xi", "Huaping Liu"], "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception", "comment": null, "summary": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u901a\u7528\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff08UV-M3TL\uff09\uff0c\u80fd\u540c\u65f6\u8bc6\u522b\u9a7e\u9a76\u5458\u884c\u4e3a\u3001\u60c5\u7eea\u3001\u8f66\u8f86\u884c\u4e3a\u548c\u4ea4\u901a\u73af\u5883\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u9700\u8981\u540c\u65f6\u7406\u89e3\u9a7e\u9a76\u5458\u884c\u4e3a\u53ca\u73af\u5883\uff0c\u7136\u800c\u591a\u4efb\u52a1\u8054\u5408\u5b66\u4e60\u5e38\u4f1a\u5bfc\u81f4\u4efb\u52a1\u95f4\u8d1f\u8fc1\u79fb\uff0c\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002\u4f5c\u8005\u52a8\u673a\u662f\u5728\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u7f13\u89e3\u4efb\u52a1\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51faUV-M3TL\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5206\u652f\u7a7a\u95f4\u901a\u9053\u591a\u6a21\u6001\u5d4c\u5165\uff08DB-SCME\uff09\u548c\u81ea\u9002\u5e94\u7279\u5f81\u89e3\u8026\u591a\u4efb\u52a1\u635f\u5931\uff08AFD-Loss\uff09\u3002DB-SCME\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u5206\u522b\u5efa\u6a21\u5171\u4eab\u7279\u5f81\u4e0e\u7279\u5b9a\u4efb\u52a1\u7279\u5f81\uff0c\u589e\u5f3a\u8de8\u4efb\u52a1\u77e5\u8bc6\u4f20\u9012\u5e76\u51cf\u5c11\u51b2\u7a81\u3002AFD-Loss\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u52a0\u6743\u53ca\u7279\u5f81\u89e3\u8026\uff0c\u4fc3\u8fdb\u591a\u4efb\u52a1\u4f18\u5316\u7a33\u5b9a\u5e76\u5b66\u4e60\u5dee\u5f02\u5316\u8868\u8fbe\u3002", "result": "\u5728AIDE\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\uff0cUV-M3TL\u5728\u9a7e\u9a76\u5458\u884c\u4e3a\u3001\u60c5\u611f\u3001\u8f66\u8f86\u884c\u4e3a\u548c\u4ea4\u901a\u73af\u5883\u56db\u9879\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97SOTA\u6027\u80fd\uff1b\u5728BDD100K\u3001CityScapes\u3001NYUD-v2\u3001PASCAL-Context\u7b49\u516c\u5f00\u591a\u4efb\u52a1\u611f\u77e5\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5927\u591a\u6570\u4efb\u52a1\u83b7\u5f97SOTA\u3002", "conclusion": "UV-M3TL\u6846\u67b6\u80fd\u591f\u6709\u6548\u7f13\u89e3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u591a\u4efb\u52a1\u611f\u77e5\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u5f3a\u901a\u7528\u6027\u548c\u9886\u5148\u6027\u80fd\uff0c\u6709\u671b\u63d0\u5347ADAS\u7b49\u5b9e\u9645\u5e94\u7528\u7cfb\u7edf\u7684\u8868\u73b0\u3002"}}
{"id": "2602.01609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01609", "abs": "https://arxiv.org/abs/2602.01609", "authors": ["Junqing Lin", "Xingyu Zheng", "Pei Cheng", "Bin Fu", "Jingwei Sun", "Guangzhong Sun"], "title": "Token Pruning for In-Context Generation in Diffusion Transformers", "comment": "20 pages", "summary": "In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86ToPi\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684Diffusion Transformer\uff08DiT\uff09In-context\u751f\u6210\u4e2d\u9ad8\u6548\u7684Token\u526a\u679d\u65b9\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u901f\u5ea6\u4e14\u4fdd\u8bc1\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "In-context\u751f\u6210\u9700\u8981\u62fc\u63a5\u53c2\u8003\u56fe\u50cf\u548c\u76ee\u6807\uff0c\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u663e\u8457\u589e\u52a0\uff0c\u5e26\u6765\u8ba1\u7b97\u74f6\u9888\u3002\u800c\u73b0\u6709\u7684Token\u51cf\u5c11\u65b9\u6cd5\u662f\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u8bbe\u8ba1\u7684\uff0c\u672a\u8003\u8651\u4e0a\u4e0b\u6587\u548c\u76ee\u6807Token\u5728\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u529f\u80fd\u4e0a\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u5bf9\u672c\u4efb\u52a1\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86ToPi\u6846\u67b6\uff1a\uff081\uff09\u5229\u7528\u79bb\u7ebf\u6821\u51c6\u654f\u611f\u6027\u5206\u6790\uff0c\u5bfb\u627e\u5173\u952e\u7684\u6ce8\u610f\u529b\u5c42\uff0c\u4f5c\u4e3a\u526a\u679d\u5197\u4f59\u4fe1\u606f\u7684\u4f9d\u636e\uff1b\uff082\uff09\u57fa\u4e8e\u8fd9\u4e9b\u6ce8\u610f\u529b\u5c42\uff0c\u5b9a\u4e49\u4e86\u65b0\u7684\u5f71\u54cd\u529b\u5ea6\u91cf\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u4e0a\u4e0b\u6587Token\u5bf9\u751f\u6210\u7ed3\u679c\u7684\u8d21\u732e\uff0c\u5e76\u636e\u6b64\u6709\u9009\u62e9\u5730\u526a\u679dToken\uff1b\uff083\uff09\u63d0\u51fa\u4e86\u65f6\u5e8f\u66f4\u65b0\u7b56\u7565\uff0c\u52a8\u6001\u9002\u5e94\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684Token\u91cd\u8981\u6027\u53d8\u5316\u3002\u6574\u4e2a\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eToPi\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5728\u4e0d\u635f\u5931\u7ed3\u6784\u4fdd\u771f\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u63a8\u7406\u901f\u5ea6\u53ef\u63d0\u534730%\u4ee5\u4e0a\u3002", "conclusion": "ToPi\u6709\u6548\u7a81\u7834\u4e86\u73b0\u6709DiT In-context\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4ee5\u7075\u6d3b\u3001\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548Token\u526a\u679d\uff0c\u4e3a\u53ef\u63a7\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.01623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01623", "abs": "https://arxiv.org/abs/2602.01623", "authors": ["Susan Liang", "Chao Huang", "Filippos Bellos", "Yolo Yunlong Tang", "Qianxiang Shen", "Jing Bi", "Luchuan Song", "Zeliang Zhang", "Jason Corso", "Chenliang Xu"], "title": "Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?", "comment": null, "summary": "State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86Omni-Judge\u7cfb\u7edf\uff0c\u8bc4\u4f30omni-LLM\u662f\u5426\u80fd\u4f5c\u4e3a\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u7684\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u5224\u5de5\u5177\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4ef7\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5f53\u524d\u6a21\u578b\u53ef\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u5e26\u540c\u6b65\u97f3\u9891\u7684\u89c6\u9891\uff0c\u4f46\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u8fd9\u4e9b\u4e09\u6a21\u6001\uff08\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\uff09\u8f93\u51fa\u4ecd\u672a\u89e3\u51b3\u3002\u4eba\u5de5\u8bc4\u4ef7\u53ef\u9760\u4f46\u96be\u4ee5\u6269\u5c55\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u6307\u6807\u5bf9\u590d\u6742\u573a\u666f\u548c\u591a\u6a21\u6001\u89e3\u91ca\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u6025\u9700\u66f4\u667a\u80fd\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86Omni-Judge\u7cfb\u7edf\uff0c\u5229\u7528\u65b0\u5174\u7684omni-LLMs\uff08\u5168\u6a21\u6001\u5927\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u4e5d\u9879\u611f\u77e5\u4e0e\u5bf9\u9f50\u6307\u6807\uff0c\u7cfb\u7edf\u6027\u8bc4\u6d4b\u5176\u5bf9\u6587\u672c\u9a71\u52a8\u97f3\u89c6\u9891\u751f\u6210\u7684\u8bc4\u4ef7\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u81ea\u52a8\u6307\u6807\u5bf9\u6bd4\u3002\u7279\u522b\u5173\u6ce8\u4e8e\u97f3\u9891-\u6587\u672c\u3001\u89c6\u9891-\u6587\u672c\u3001\u97f3\u89c6\u9891-\u6587\u672c\u4e09\u8005\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "Omni-Judge\u5728\u591a\u9879\u8bc4\u4ef7\u6307\u6807\u4e0a\u76f8\u5173\u6027\u4e0e\u4f20\u7edf\u6307\u6807\u76f8\u5f53\uff0c\u4e14\u5728\u590d\u6742\u8bed\u4e49\u4efb\u52a1\uff08\u5982\u4e09\u6a21\u6001\u4e00\u81f4\u6027\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4f46\u5728\u9ad8\u5e27\u7387\u89c6\u9891\u8d28\u91cf\u3001\u89c6\u542c\u540c\u6b65\u7b49\u7eaf\u611f\u77e5\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5f52\u56e0\u4e8eomni-LLM\u5bf9\u65f6\u5e8f\u4fe1\u606f\u628a\u63a7\u6709\u9650\u3002\u540c\u65f6\u80fd\u8f93\u51fa\u89e3\u91ca\u6027\u5f3a\u7684\u8bc4\u8bed\uff0c\u5229\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "Omni-LLM\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u5668\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5df2\u5177\u5907\u8f83\u597d\u7684\u4eba\u7c7b\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\uff0c\u4f46\u65f6\u5e8f\u5206\u8fa8\u7387\u7b49\u5c40\u9650\u6027\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2602.01624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01624", "abs": "https://arxiv.org/abs/2602.01624", "authors": ["Minh-Quan Le", "Gaurav Mittal", "Cheng Zhao", "David Gu", "Dimitris Samaras", "Mei Chen"], "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards", "comment": null, "summary": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u6587\u672c\u751f\u6210\u89c6\u9891\uff08T2V\uff09\u540e\u8bad\u7ec3\u7b97\u6cd5PISCES\uff0c\u5229\u7528\u65b0\u9896\u7684\u53cc\u91cd\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u5956\u52b1\u673a\u5236\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u4e0e\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709T2V\u751f\u6210\u7684\u5956\u52b1\u540e\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u504f\u597d\u6807\u6ce8\u6216\u7528\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5f97\u5230\u7684\u6709\u504f\u77e9\u9635\uff0c\u5bfc\u81f4\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u4e14\u76d1\u7763\u6548\u679c\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5b9e\u73b0\u65e0\u9700\u6807\u6ce8\u4e14\u5bf9\u9f50\u4eba\u7c7b\u5224\u65ad\u7684\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u89c6\u9891\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u3002", "method": "PISCES\u5229\u7528\u53cc\u91cd\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u5956\u52b1\u6a21\u5757\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u5206\u5e03\u5f0fOT\u5bf9\u9f50\u6574\u4f53\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff08Distributional OT-aligned Quality Reward\uff09\uff0c\u53e6\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u79bb\u6563token\u7ea7OT\u5bf9\u9f50\uff0c\u5f3a\u5316\u8bed\u4e49\u4e0e\u65f6\u7a7a\u5bf9\u5e94\u5173\u7cfb\uff08Discrete Token-level OT-aligned Semantic Reward\uff09\u3002\u8fd9\u79cd\u5956\u52b1\u673a\u5236\u9002\u914d\u591a\u79cd\u4f18\u5316\u8303\u5f0f\uff0c\u5982\u53cd\u5411\u4f20\u64ad\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u5728\u77ed\u89c6\u9891\u548c\u957f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\uff0cPISCES\u5728VBench\u7b49\u57fa\u51c6\u7684\u8d28\u91cf\u4e0e\u8bed\u4e49\u5206\u6570\u4e0a\u90fd\u4f18\u4e8e\u5df2\u6709\u4eba\u5de5\u6807\u6ce8\u548c\u65e0\u6807\u6ce8\u65b9\u6cd5\u3002\u4eba\u7c7b\u504f\u597d\u5b9e\u9a8c\u4e5f\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u751f\u6210\u6548\u679c\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "PISCES\u901a\u8fc7\u53cc\u91cdOT\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u7684\u751f\u6210\u540e\u8bad\u7ec3\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u7ebf\u3002"}}
{"id": "2602.01630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01630", "abs": "https://arxiv.org/abs/2602.01630", "authors": ["Bohan Zeng", "Kaixin Zhu", "Daili Hua", "Bozhou Li", "Chengzhuo Tong", "Yuran Wang", "Xinyi Huang", "Yifan Dai", "Zixiang Zhang", "Yifan Yang", "Zhou Liu", "Hao Liang", "Xiaochen Ma", "Ruichuan An", "Tianyi Bai", "Hongcheng Gao", "Junbo Niu", "Yang Shi", "Xinlong Chen", "Yue Ding", "Minglei Shi", "Kai Zeng", "Yiwen Tang", "Yuanxing Zhang", "Pengfei Wan", "Xintao Wang", "Wentao Zhang"], "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "comment": "13 pages, 4 figures", "summary": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f53\u524d\u4e16\u754c\u6a21\u578b\u7684\u7814\u7a76\u73b0\u72b6\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6307\u51fa\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u548c\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e16\u754c\u6a21\u578b\u8bbe\u8ba1\u89c4\u8303\uff0c\u4ee5\u63a8\u52a8\u66f4\u52a0\u6574\u4f53\u548c\u7cfb\u7edf\u7684\u4e16\u754c\u5efa\u6a21\u3002", "motivation": "\u5f53\u524d\u4e16\u754c\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u9884\u6d4b\u3001\u4e09\u7ef4\u4f30\u8ba1\u3001\u7b26\u53f7\u7406\u89e3\u7b49\uff09\u6ce8\u5165\u7269\u7406\u6216\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u6574\u4f53\u6027\uff0c\u96be\u4ee5\u5b9e\u73b0\u9762\u5411\u590d\u6742\u73af\u5883\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u68b3\u7406\u5e76\u5206\u6790\u4e86\u73b0\u6709\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7edf\u4e00\u7684\u8bbe\u8ba1\u89c4\u8303\uff0c\u5f3a\u8c03\u4e16\u754c\u6a21\u578b\u9700\u8981\u96c6\u6210\u4ea4\u4e92\u3001\u611f\u77e5\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7a7a\u95f4\u8868\u5f81\u7b49\u80fd\u529b\uff0c\u6210\u4e3a\u6709\u539f\u5219\u6027\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u800c\u975e\u529f\u80fd\u6742\u7cc5\u7684\u96c6\u5408\u3002", "result": "\u672c\u6587\u7406\u8bba\u6027\u5730\u63d0\u51fa\u4e86\u4e16\u754c\u6a21\u578b\u5e94\u8be5\u8986\u76d6\u7684\u6838\u5fc3\u8981\u7d20\u548c\u7edf\u4e00\u8bbe\u8ba1\u8981\u6c42\uff0c\u4e3a\u4eca\u540e\u7684\u7814\u7a76\u5efa\u6a21\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u65b9\u5411\u6307\u5f15\u3002", "conclusion": "\u6587\u7ae0\u8ba4\u4e3a\u7edf\u4e00\u4e14\u89c4\u8303\u7684\u8bbe\u8ba1\u6846\u67b6\u662f\u5b9e\u73b0\u9c81\u68d2\u4e14\u901a\u7528\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\uff0c\u4e3a\u540e\u7eed\u66f4\u7cfb\u7edf\u548c\u5b8c\u5584\u7684\u4e16\u754c\u5efa\u6a21\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.01633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01633", "abs": "https://arxiv.org/abs/2602.01633", "authors": ["Xinyuan Zhao", "Yihang Wu", "Ahmad Chaddad", "Tareef Daqqaq", "Reem Kateb"], "title": "Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification", "comment": "Accepted in Knowledge-Based Systems", "summary": "While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\\% to 41.69\\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\uff08DAFL\uff09\u4e0e\u5ba2\u6237\u7aef\u611f\u77e5\u805a\u5408\u7b56\u7565\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u4e0d\u540c\u5ba2\u6237\u6570\u636e\u4e0d\u5747\u8861\u60c5\u51b5\u4e0b\u9762\u90e8\u56fe\u50cf\u5206\u7c7b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ViT\uff09\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u3002\u4f46\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u4e86\u83b7\u53d6\uff0c\u5c24\u5176\u662f\u533b\u7597\u5f71\u50cf\u3002\u8054\u90a6\u5b66\u4e60\u867d\u53ef\u907f\u514d\u6570\u636e\u4ea4\u6362\uff0c\u4f46\u5f02\u6784\u6027\u4e0e\u7c7b\u522b\u4e0d\u5747\u8861\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFL\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\uff08DAFL\uff09\u5904\u7406\u7c7b\u522b\u4e0d\u5747\u8861\u2014\u2014\u901a\u8fc7\u5ba2\u6237\u6570\u636e\u5206\u5e03\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u6743\u91cd\uff0c\u786e\u4fdd\u5c11\u6570\u7c7b\u4e0d\u88ab\u5ffd\u7565\u3002\u540c\u65f6\uff0c\u5f15\u5165\u57fa\u4e8e\u6570\u636e\u89c4\u6a21\u4e0e\u7279\u6027\u7684\u52a0\u6743\u805a\u5408\u7b56\u7565\uff0c\u9002\u5e94\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u3002", "result": "\u5728ISIC\u3001Ocular Disease\u53caRSNA-ICH\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8eDenseNet121\u3001ResNet50\u3001ViT\u7b49\u591a\u4e2a\u4e3b\u6d41\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad80.98%~41.69%\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b0\u635f\u5931\u51fd\u6570\u548c\u805a\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u4e0d\u540c\u5ba2\u6237\u7aef\u548c\u7c7b\u522b\u4e0d\u5747\u8861\u60c5\u5f62\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5bf9\u672a\u6765\u57fa\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5177\u6709\u8f83\u5927\u501f\u9274\u4ef7\u503c\u3002"}}
{"id": "2602.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01639", "abs": "https://arxiv.org/abs/2602.01639", "authors": ["Tianyu Yang", "ChenWei He", "Xiangzhao Hao", "Tianyue Wang", "Jiarui Guo", "Haiyun Guo", "Leigang Qu", "Jinqiao Wang", "Tat-Seng Chua"], "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ebReCALL\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4ee5\u89e3\u51b3\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u5728\u9002\u914d\u4e3a\u5224\u522b\u5f0f\u68c0\u7d22\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7531\u56fe\u7247\u548c\u6587\u672c\u7ec4\u5408\u67e5\u8be2\u7684\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\uff08CIR\uff09\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7ec4\u5408\u5f0f\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u9700\u8981\u6a21\u578b\u7406\u89e3\u548c\u878d\u5408\u56fe\u7247\u4e0e\u6587\u672c\u4e24\u79cd\u6a21\u6001\u7684\u4fe1\u606f\u3002\u4f20\u7edf\u53cc\u5854\u6a21\u578b\u96be\u4ee5\u5b8c\u6210\u590d\u6742\u7684\u6a21\u6001\u5408\u6210\u63a8\u7406\uff1b\u800c\u76f4\u63a5\u5c06\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u6a21\u578b\u7528\u4e8e\u5224\u522b\u5f0f\u68c0\u7d22\u5219\u5f15\u53d1\u80fd\u529b\u9000\u5316\uff08\u7cbe\u7ec6\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff09\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u65b9\u6cd5\u5e94\u5bf9\u8fd9\u4e00\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86ReCALL\u6846\u67b6\uff0c\u5305\u542b\u201c\u8bca\u65ad-\u751f\u6210-\u7cbe\u70bc\u201d\u4e09\u6b65\uff1a\uff081\uff09\u8bca\u65adretriever\u6a21\u578b\u7684\u8ba4\u77e5\u76f2\u70b9\uff0c\u901a\u8fc7\u81ea\u5f15\u5bfc\u7684\u5b9e\u4f8b\u6316\u6398\u53d1\u73b0\u6a21\u578b\u5f31\u70b9\uff1b\uff082\uff09\u5229\u7528\u57fa\u7840MLLM\u94fe\u5f0f\u63d0\u793a\u751f\u6210\u77eb\u6b63\u6307\u5bfc\u548c\u8bad\u7ec3\u4e09\u5143\u7ec4\uff0c\u5e76\u501f\u52a9VQA\u8fdb\u884c\u4e00\u81f4\u6027\u7b5b\u9009\uff1b\uff083\uff09\u901a\u8fc7\u5206\u7ec4\u5bf9\u6bd4\u8bad\u7ec3\u6cd5\uff0c\u7528\u751f\u6210\u7684\u4e09\u5143\u7ec4\u6301\u7eed\u8bad\u7ec3retriever\uff0c\u4f7f\u5176\u91cd\u83b7\u7cbe\u7ec6\u63a8\u7406\u4e0e\u5224\u522b\u7a7a\u95f4\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u67b6\u6784\u65e0\u5173\u3002", "result": "\u5728CIRR\u548cFashionIQ\u57fa\u51c6\u4e0a\uff0cReCALL\u6301\u7eed\u4fee\u6b63\u5224\u522b\u6a21\u578b\u7684\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "ReCALL\u80fd\u591f\u6709\u6548\u77eb\u6b63MLLM\u5728\u68c0\u7d22\u9002\u914d\u4e2d\u7684\u80fd\u529b\u635f\u5931\uff0c\u5728\u7ec4\u5408\u5f0f\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01649", "abs": "https://arxiv.org/abs/2602.01649", "authors": ["Yinchao Ma", "Qiang Zhou", "Zhibin Wang", "Xianing Chen", "Hanqing Yang", "Jun Song", "Bo Zheng"], "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning", "comment": "This paper is accepted by AAAI2026", "summary": "Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \\textbf{C}ontribution-\\textbf{a}ware token \\textbf{Co}mpression algorithm for \\textbf{VID}eo understanding (\\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video LLM\uff09token\u538b\u7f29\u7b97\u6cd5CaCoVID\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u53c8\u4fdd\u8bc1\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u5927\u91cf\u5197\u4f59\u89c6\u9891token\uff0c\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002\u867d\u7136\u6709\u7b97\u6cd5\u901a\u8fc7\u4fdd\u7559\u9ad8attention\u5206\u6570\u7684token\u5c1d\u8bd5\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46attention\u5206\u6570\u4e0e\u9884\u6d4b\u6b63\u786e\u6027\u7684\u76f4\u63a5\u5173\u8054\u5b58\u5728\u7591\u95ee\u3002\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u66f4\u79d1\u5b66\u7684\u538b\u7f29\u65b9\u5f0f\uff0c\u65e0\u635f\u6216\u6700\u5c0f\u635f\u5931\u4e0b\u51cf\u5c11token\u6570\u3002", "method": "\u63d0\u51faCaCoVID\u7b97\u6cd5\uff0c\u9996\u5148\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u7b56\u7565\u7f51\u7edc\uff0c\u4e3b\u52a8\u9009\u62e9\u6700\u6709\u8d21\u732e\u7684token\u7ec4\u5408\u3002\u8be5\u65b9\u6cd5\u91cd\u70b9\u5173\u6ce8token\u5bf9\u6700\u7ec8\u6b63\u786e\u9884\u6d4b\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u800c\u975e\u7b80\u5355\u4f9d\u8d56attention\u5206\u6570\u3002\u5176\u6b21\uff0c\u5f15\u5165\u7ec4\u5408\u5f0f\u7b56\u7565\u4f18\u5316\u4e0e\u5728\u7ebf\u91c7\u6837\uff0c\u6781\u5927\u51cf\u5c11token\u7ec4\u5408\u641c\u7d22\u7a7a\u95f4\uff0c\u52a0\u5feb\u4f18\u5316\u6536\u655b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cCaCoVID\u663e\u8457\u51cf\u5c11token\u6570\u91cf\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "CaCoVID\u4ee5\u5b9e\u9645\u8d21\u732e\u4e3a\u57fa\u7840\u4f18\u5316token\u9009\u62e9\u7b56\u7565\uff0c\u6781\u5927\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u5173\u4ee3\u7801\u5373\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.01661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01661", "abs": "https://arxiv.org/abs/2602.01661", "authors": ["Xingyu Miao", "Junting Dong", "Qin Zhao", "Yuhang Yang", "Junhao Chen", "Yang Long"], "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction", "comment": null, "summary": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u5e8f\u5217\u4e2d\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u5bc6\u96c6\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u771f\u5b9e\u6027\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u5e76\u5f15\u5165\u65f6\u5e8f\u6807\u6ce8\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u867d\u80fd\u5728\u5355\u5e27\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5728\u8fd0\u52a8\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u4e0b\u65f6\u5e8f\u4e00\u81f4\u6027\u5dee\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u5408\u591a\u5bc6\u96c6\u4efb\u52a1\u4e0e\u65f6\u5e8f\u6570\u636e\u7684\u6807\u6ce8\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u751f\u6210\u5177\u6709\u50cf\u7d20\u7ea7\u51c6\u786e\u6807\u7b7e\u7684\u4eba\u4f53\u5e27\u4e0e\u8fd0\u52a8\u5bf9\u9f50\u5e8f\u5217\uff0c\u5e76\u63d0\u4f9b\u7a7a\u95f4\u4e0e\u65f6\u5e8f\u5c42\u7ea7\u7684\u76d1\u7763\u3002\u63d0\u51fa\u57fa\u4e8eViT\u7684\u7edf\u4e00\u5bc6\u96c6\u9884\u6d4b\u5668\uff0c\u7ed3\u5408CSE\u5d4c\u5165\u6ce8\u5165\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u878d\u5408\u540e\u7684\u901a\u9053\u91cd\u52a0\u6743\u673a\u5236\u63d0\u5347\u51e0\u4f55\u7279\u5f81\u53ef\u9760\u6027\u3002\u6a21\u578b\u8bad\u7ec3\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u5148\u9759\u6001\u9884\u8bad\u7ec3\u83b7\u5f97\u7a7a\u95f4\u8868\u5f81\uff0c\u518d\u901a\u8fc7\u52a8\u6001\u56fe\u5e8f\u5217\u76d1\u7763\u4f18\u5316\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728THuman2.1\u548cHi4D\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f18\u7ed3\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u91ce\u5916\u89c6\u9891\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f15\u5165\u5408\u6210\u65f6\u5e8f\u76d1\u7763\u4e0e\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u9884\u6d4b\u4efb\u52a1\u7684\u7a7a\u95f4\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u4e3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u591a\u4efb\u52a1\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01666", "abs": "https://arxiv.org/abs/2602.01666", "authors": ["Yan Wang", "Partho Hassan", "Samiha Sadeka", "Nada Soliman", "M M Sayeef Abdullah", "Sabit Hassan"], "title": "Moonworks Lunara Aesthetic II: An Image Variation Dataset", "comment": null, "summary": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Lunara Aesthetic II\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u5b66\u4e60\uff0c\u5177\u6709\u9ad8\u5ba1\u7f8e\u8d28\u91cf\u53ca\u8eab\u4efd\u4fdd\u6301\u7279\u6027\u3002", "motivation": "\u76ee\u524d\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u9886\u57df\uff0c\u5bf9\u4e8e\u4e0a\u4e0b\u6587\u53d8\u5316\uff08\u5982\u5149\u7167\u3001\u8272\u8c03\u3001\u6784\u56fe\u7b49\uff09\u4e0b\u5982\u4f55\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u4e0d\u53d8\u7684\u80fd\u529b\u8bc4\u4f30\u548c\u8bad\u7ec3\u624b\u6bb5\u6709\u9650\uff0c\u540c\u65f6\u516c\u4f17\u53ef\u7528\u3001\u8d28\u91cf\u9ad8\u3001\u4f26\u7406\u6765\u6e90\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u4f9b\u4e00\u4e2a\u5177\u5907\u8eab\u4efd\u4fdd\u6301\u4e14\u652f\u6301\u591a\u6837\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9ad8\u5ba1\u7f8e\u6570\u636e\u96c6\uff0c\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "method": "Lunara Aesthetic II\u7531Moonworks\u521b\u4f5c\u7684\u539f\u521b\u827a\u672f\u548c\u7167\u7247\u751f\u6210\uff0c\u901a\u8fc7\u5bf9\u539f\u59cb\u56fe\u50cf\u5e94\u7528\u4e00\u7cfb\u5217\u4e0a\u4e0b\u6587\u8f6c\u6362\uff08\u5982\u5929\u6c14\u3001\u89c6\u89d2\u3001\u8272\u8c03\u3001\u6c1b\u56f4\u7b49\uff09\uff0c\u5f97\u52302,854\u5bf9\u951a\u70b9-\u53d8\u4f53\u56fe\u50cf\u914d\u5bf9\uff0c\u5728\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u4e0d\u53d8\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u53d8\u5316\uff0c\u6570\u636e\u96c6\u514d\u8d39\u7ebf\u4e0a\u516c\u5f00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6570\u636e\u96c6\u65e0\u8bba\u5728\u8eab\u4efd\u7a33\u5b9a\u6027\u3001\u76ee\u6807\u5c5e\u6027\u53d8\u5316\u5b9e\u73b0\u6548\u679c\u8fd8\u662f\u5ba1\u7f8e\u8868\u73b0\u65b9\u9762\u5747\u4f18\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u6293\u53d6\u6570\u636e\u96c6\u3002", "conclusion": "Lunara Aesthetic II\u6570\u636e\u96c6\u4e3a\u8bc4\u6d4b\u548c\u8bad\u7ec3\u56fe\u50cf\u751f\u6210\u3001\u7f16\u8f91\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u3001\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u53ca\u7f16\u8f91\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u516c\u5f00\u4e14\u9ad8\u8d28\u91cf\u7684\u8d44\u6e90\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u548c\u5173\u7cfb\u76d1\u7763\uff0c\u6709\u52a9\u4e8e\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2602.01674", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.01674", "abs": "https://arxiv.org/abs/2602.01674", "authors": ["Hail Song", "Boram Yoon", "Seokhwan Yang", "Seoyoung Kang", "Hyunjeong Kim", "Henning Metzmacher", "Woontack Woo"], "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR", "comment": "Accepted as an IEEE TVCG paper at IEEE VR 2026 (journal track)", "summary": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VRGaussianAvatar\u7cfb\u7edf\uff0c\u80fd\u4ec5\u901a\u8fc7\u5934\u6234\u663e\u793a\u5668\u8ffd\u8e2a\u4fe1\u53f7\u5b9e\u73b0\u5b9e\u65f6\u5168\u8eab3D\u9ad8\u65af\u5934\u50cf\u865a\u62df\u73b0\u5b9e\u91cd\u5efa\uff0c\u5177\u5907\u9ad8\u6027\u80fd\u4e0e\u9ad8\u771f\u5b9e\u611f\u3002", "motivation": "\u76ee\u524d\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5168\u8eab\u5316\u8eab\u5927\u591a\u4f9d\u8d56\u591a\u6444\u50cf\u5934\u6216\u4f20\u611f\u5668\uff0c\u800c\u4ec5\u51edHMD\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f63D\u91cd\u5efa\u4ecd\u6709\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6e32\u67d3\u6548\u7387\u548c\u8868\u73b0\u771f\u5b9e\u611f\u65b9\u9762\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u524d\u540e\u7aef\u5e76\u884c\u7ba1\u7ebf\uff1a\u524d\u7aef\u901a\u8fc7\u9006\u5411\u8fd0\u52a8\u5b66\u4f30\u7b97\u5168\u8eab\u59ff\u6001\u5e76\u4f20\u9001\u5230\u540e\u7aef\uff0c\u540e\u7aef\u7528\u53cc\u76ee\u6e32\u67d3\u6280\u672f\u5c06\u5355\u5f20\u56fe\u7247\u91cd\u5efa\u76843D\u9ad8\u65af\u5934\u50cf\u8fdb\u884c\u9ad8\u6548\u6e32\u67d3\u3002\u5f15\u5165\u4e86Binocular Batching\u6280\u672f\uff0c\u4ee5\u5355\u6b21\u6279\u5904\u7406\u540c\u65f6\u8ba1\u7b97\u5de6\u53f3\u773c\u89c6\u56fe\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u4f18\u5316VR\u663e\u793a\u6548\u7387\u3002", "result": "\u901a\u8fc7\u6027\u80fd\u6d4b\u8bd5\u548c\u4e3b\u89c2\u7528\u6237\u7814\u7a76\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u7cfb\u7edf\u80fd\u6301\u7eed\u652f\u6301\u4ea4\u4e92\u5f0fVR\u6027\u80fd\uff0c\u5728\u5916\u89c2\u76f8\u4f3c\u6027\u3001\u5316\u8eab\u611f\u548c\u53ef\u4fe1\u5ea6\u4e0a\u4f18\u4e8e\u56fe\u50cf\u53ca\u89c6\u9891\u57fa\u7840\u7684mesh\u5f62\u8c61\u5bf9\u6bd4\u65b9\u6848\u3002", "conclusion": "VRGaussianAvatar\u5b9e\u73b0\u4e86\u57fa\u4e8eHMD\u8ffd\u8e2a\u4fe1\u53f7\u7684\u9ad8\u6548\u3001\u771f\u5b9e\u3001\u4f53\u9a8c\u4f18\u79c0\u76843DGS\u5168\u8eab\u865a\u62df\u5f62\u8c61\u65b9\u6848\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5b9e\u65f6\u5168\u8eab\u5316\u8eab\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2602.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01677", "abs": "https://arxiv.org/abs/2602.01677", "authors": ["Yinchao Ma", "Dengqing Yang", "Zhangyu He", "Wenfei Yang", "Tianzhu Zhang"], "title": "SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking", "comment": "This paper is accepted by IEEE TIP", "summary": "Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4f4e\u7b97\u529b\u9700\u6c42\u7684\u89c6\u89c9\u76ee\u6807\u8ffd\u8e2a\u6a21\u578b\uff08SMTrack\uff09\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u6709\u6548\u5730\u5efa\u6a21\u957f\u65f6\u5e8f\u7684\u65f6\u7a7a\u4f9d\u8d56\u3002", "motivation": "\u5728\u52a8\u6001\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u89c6\u89c9\u8ffd\u8e2a\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5efa\u6a21\u957f\u65f6\u6027\u4f9d\u8d56\uff0c\u5f15\u5165\u65f6\u5e8f\u4fe1\u606f\u901a\u5e38\u9700\u590d\u6742\u6a21\u5757\u6216\u9ad8\u8ba1\u7b97\u6d88\u8017\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8ffd\u8e2a\u7684\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u4ee3\u4ef7\u3002", "method": "\u4f5c\u8005\u501f\u9274\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u63d0\u51faState-aware Mamba Tracker\uff08SMTrack\uff09\uff0c\u5176\u4e2d\u5e94\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u9009\u62e9\u6027\u72b6\u6001\u611f\u77e5\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u72b6\u6001\u4f20\u64ad\u548c\u66f4\u65b0\u5b9e\u73b0\u5e27\u95f4\u9ad8\u6548\u957f\u8ddd\u79bb\u4ea4\u4e92\uff0c\u8bad\u7ec3\u671f\u95f4\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMTrack\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6d88\u8017\u7684\u540c\u65f6\uff0c\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "SMTrack\u4e3a\u89c6\u89c9\u8ffd\u8e2a\u4e2d\u7684\u65f6\u5e8f\u5efa\u6a21\u63d0\u4f9b\u4e86\u7b80\u6d01\u3001\u9ad8\u6548\u7684\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u517c\u987e\u4e86\u6027\u80fd\u4e0e\u7b97\u529b\u5f00\u9500\u3002"}}
{"id": "2602.01683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01683", "abs": "https://arxiv.org/abs/2602.01683", "authors": ["Kangcong Li", "Peng Ye", "Lin Zhang", "Chao Wang", "Huafeng Qin", "Tao Chen"], "title": "FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding", "comment": null, "summary": "Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical \"gist\"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u8bb0\u5fc6\u7f51\u7edcFreshMem\uff0c\u4ee5\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5728\u7ebf\u89c6\u9891\u6d41\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8fde\u7eed\u611f\u77e5\u548c\u957f\u65f6\u6bb5\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u7531\u79bb\u7ebf\u8f6c\u81f3\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u65f6\uff0c\u5b58\u5728\u9002\u5e94\u6027\u4e0d\u8db3\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u548c\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u957f\u65f6\u89c6\u9891\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faFreshMem\uff0c\u4e00\u79cd\u53d7\u5927\u8111\u611f\u77e5\u4e0e\u8bb0\u5fc6\u6a21\u578b\u542f\u53d1\u7684\u201c\u9891\u7387-\u7a7a\u95f4\u6df7\u5408\u8bb0\u5fc6\u7f51\u7edc\u201d\u3002\u5176\u7531\u591a\u5c3a\u5ea6\u9891\u7387\u8bb0\u5fc6\uff08MFM\uff09\u6a21\u5757\u5c06\u6ea2\u51fa\u7684\u5e27\u6295\u5f71\u4e3a\u9891\u57df\u7cfb\u6570\u5e76\u7ed3\u5408\u6b8b\u5dee\u7ec6\u8282\u5e2e\u52a9\u6062\u590d\u5168\u5c40\u80cc\u666f\uff0c\u4ee5\u53ca\u7a7a\u95f4\u7f29\u7565\u56fe\u8bb0\u5fc6\uff08STM\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u5c06\u89c6\u9891\u6d41\u5212\u5206\u4e3a\u9ad8\u5bc6\u5ea6\u7a7a\u95f4\u7f29\u7565\u56fe\u6765\u5b58\u50a8\u5386\u53f2\u4fe1\u606f\u3002", "result": "\u5728StreamingBench\u3001OV-Bench\u548cOVO-Bench\u4e09\u4e2a\u4e3b\u6d41\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u8bc4\u6d4b\u4e2d\uff0cFreshMem\u5728\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4e3aQwen2-VL\u57fa\u7840\u6a21\u578b\u5e26\u67655.20%\u30014.52%\u30012.34%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u591a\u79cd\u5168\u91cf\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "FreshMem\u4e3a\u957f\u65f6\u6bb5\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee5\u5f80\u7ec6\u8282\u9057\u5931\u4e0e\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u5bf9\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6301\u7eed\u611f\u77e5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.01696", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01696", "abs": "https://arxiv.org/abs/2602.01696", "authors": ["Jiaming Cui", "Shuai Zhou", "Wenqiang Li", "Ruifeng Qin", "Feng Shen"], "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection", "comment": null, "summary": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CMAFNet\uff0c\u4e00\u79cd\u878d\u5408RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u65b0\u578b\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u8f93\u7535\u7ebf\u8def\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u5c0f\u76ee\u6807\u3001\u590d\u6742\u80cc\u666f\u548c\u5149\u7167\u53d8\u5316\u7b49\u96be\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728TLRGBD\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8f93\u7535\u7ebf\u8def\u7f3a\u9677\u68c0\u6d4b\u4e2d\uff0c\u81ea\u52a8\u5316\u65e0\u4eba\u673a\u68c0\u6d4b\u7531\u4e8e\u7f3a\u9677\u5c0f\u3001\u80cc\u666f\u590d\u6742\u548c\u5149\u7167\u53d8\u5316\u5927\u7b49\u95ee\u9898\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8f83\u4f4e\u3002\u4f20\u7edf\u4f9d\u8d56RGB\u56fe\u50cf\u7684\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u5f62\u6001\u5fae\u5c0f\u7684\u7f3a\u9677\u548c\u76f8\u4f3c\u80cc\u666f\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u878d\u5408\u5176\u4ed6\u4fe1\u606f\uff08\u5982\u6df1\u5ea6\uff09\u4ee5\u63d0\u5347\u7ec6\u7c92\u5ea6\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86CMAFNet\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\uff081\uff09\u57fa\u4e8e\u5b57\u5178\u7684\u8bed\u4e49\u91cd\u7ec4\u6a21\u5757\uff0c\u5bf9\u7279\u5f81\u8fdb\u884c\u51c0\u5316\uff0c\u6291\u5236\u4e0d\u540c\u6a21\u6001\u7684\u566a\u58f0\uff0c\u7a81\u51fa\u4e0e\u7f3a\u9677\u76f8\u5173\u7684\u4fe1\u606f\uff1b\uff082\uff09\u4e0a\u4e0b\u6587\u8bed\u4e49\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5168\u5c40\u7a7a\u95f4\u4f9d\u8d56\uff0c\u5f3a\u5316\u7ed3\u6784\u8bed\u4e49\u63a8\u7406\u3002\u540c\u65f6\uff0c\u5728\u51c0\u5316\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u4f4d\u7f6e\u7684\u5f52\u4e00\u5316\uff0c\u5b9e\u73b0\u6df1\u5ea6\u4e0eRGB\u7279\u5f81\u7684\u7edf\u8ba1\u4e00\u81f4\u6027\u548c\u5bf9\u9f50\uff0c\u4f7f\u5f97\u7279\u5f81\u878d\u5408\u524d\u8de8\u6a21\u6001\u517c\u5bb9\u3002", "result": "\u5728TLRGBD\u6570\u636e\u96c6\uff0894.5%\u4e3a\u5c0f\u76ee\u6807\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCMAFNet\u83b7\u5f9732.2%\u7684mAP@50\u548c12.5%\u7684\u5c0f\u76ee\u6807APs\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad89.8\u548c4.0\u4e2a\u767e\u5206\u70b9\u3002\u8f7b\u91cf\u5316\u7248\u672c\u4e5f\u8fbe\u523024.8%\u7684mAP@50\u548c228 FPS\uff0c\u53c2\u6570\u4ec54.9M\uff0c\u4f18\u4e8e\u6240\u6709YOLO\u65b9\u6cd5\uff0c\u5e76\u80fd\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5ab2\u7f8eTransformer\u65b9\u6cd5\u3002", "conclusion": "CMAFNet\u901a\u8fc7\u6709\u6548\u878d\u5408RGB\u4e0e\u6df1\u5ea6\u4fe1\u606f\uff0c\u5728\u5c0f\u76ee\u6807\u8f93\u7535\u7ebf\u8def\u7f3a\u9677\u68c0\u6d4b\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u517c\u987e\u4e86\u901f\u5ea6\u548c\u53c2\u6570\u91cf\uff0c\u5bf9\u5b9e\u9645\u65e0\u4eba\u673a\u5de1\u68c0\u573a\u666f\u5177\u6709\u8f83\u5927\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01710", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01710", "abs": "https://arxiv.org/abs/2602.01710", "authors": ["Salma Zahran", "Zhou Ao", "Zhengyang Zhang", "Chen Chi", "Chenchen Yuan", "Yanming Wang"], "title": "Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis", "comment": null, "summary": "Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u62df\u4e0e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08CycleGAN\uff09\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u663e\u5fae\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u663e\u5fae\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\uff0c\u4f46\u9ad8\u6602\u6210\u672c\u548c\u4e3b\u89c2\u6027\u4e25\u91cd\u9650\u5236\u4e86\u81ea\u52a8\u5316\u5e94\u7528\u3002\u867d\u7136\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u53ef\u5927\u91cf\u63d0\u4f9b\u6570\u636e\uff0c\u4f46\u4eff\u771f\u6570\u636e\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u3002", "method": "1\uff09\u5229\u7528\u76f8\u573a\u6a21\u62df\u751f\u6210\u5927\u91cf\u5e26\u6709\u771f\u5b9e\u63a9\u6a21\u7684\u5fae\u89c2\u7ed3\u6784\u56fe\u50cf\uff1b2\uff09\u91c7\u7528CycleGAN\u5c06\u4eff\u771f\u56fe\u50cf\u65e0\u76d1\u7763\u8f6c\u6362\u4e3a\u903c\u771f\u7684SEM\u5b9e\u9a8c\u56fe\u50cf\uff1b3\uff09\u7528\u8fd9\u4e9b\u9ad8\u4fdd\u771f\u7684\u5408\u6210\u56fe\u50cf\u8bad\u7ec3U-Net\u5206\u5272\u7f51\u7edc\uff0c\u5e76\u5728\u771f\u5b9e\u5b9e\u9a8c\u56fe\u50cf\u4e0a\u68c0\u9a8c\u6548\u679c\u3002", "result": "\u4ec5\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684U-Net\u5728\u771f\u5b9e\u5b9e\u9a8c\u56fe\u50cf\u4e0a\u53d6\u5f97\u4e86\u5e73\u5747\u8fb9\u754cF1\u5206\u65700.90\u3001IOU\u4e3a0.88\u7684\u4f18\u5f02\u6210\u7ee9\uff1bt-SNE\u7279\u5f81\u6295\u5f71\u548c\u9999\u519c\u71b5\u5206\u6790\u5747\u8868\u660e\uff0c\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5728\u7edf\u8ba1\u548c\u7279\u5f81\u5206\u5e03\u4e0a\u65e0\u663e\u8457\u5dee\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u663e\u5fae\u56fe\u50cf\u5206\u5272\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u9ad8\u901a\u91cf\u5206\u6790\u548c\u81ea\u52a8\u5316\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01723", "abs": "https://arxiv.org/abs/2602.01723", "authors": ["Yikun Ma", "Yiqing Li", "Jingwen Ye", "Zhongkai Wu", "Weidong Zhang", "Lin Gao", "Zhi Jin"], "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization", "comment": null, "summary": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFastPhysGS\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u57fa\u4e8e\u7269\u7406\u7684\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u6a21\u62df\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7269\u7406\u76843D\u573a\u666f\u6a21\u62df\u4f9d\u8d56\u4e8e\u7e41\u7410\u7684\u53c2\u6570\u8c03\u4f18\u6216\u5bf9\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a8\u6001\u7684\u84b8\u998f\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u548c\u4f18\u5316\u6548\u7387\u53d7\u9650\u3002\u540c\u65f6\uff0c\u76f4\u63a5\u5229\u7528LLMs/VLMs\u5b58\u5728\u611f\u77e5\u504f\u5dee\uff0c\u65e0\u6cd5\u7cbe\u786e\u8fd8\u539f\u7269\u7406\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u65b9\u6cd5\u5ffd\u89c6\u4e863DGS\u7684\u8868\u9762\u7ed3\u6784\uff0c\u5bfc\u81f4\u52a8\u6001\u6a21\u62df\u4e0d\u5408\u7406\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u5177\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u548c\u7269\u7406\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "FastPhysGS\u6846\u67b6\u5305\u542b\u4e24\u5927\u6838\u5fc3\u6280\u672f\uff1a(1) \u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u91cd\u8981\u6027\u91c7\u6837\u7684\u5b9e\u4f8b\u611f\u77e5\u7c92\u5b50\u586b\u5145\uff08Instance-aware Particle Filling, IPF\uff09\uff0c\u9ad8\u6548\u586b\u5145\u5185\u90e8\u7c92\u5b50\u7684\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\uff1b(2) \u53cc\u5411\u56fe\u89e3\u8026\u4f18\u5316\uff08Bidirectional Graph Decoupling Optimization, BGDO\uff09\uff0c\u81ea\u9002\u5e94\u5730\u5feb\u901f\u4f18\u5316VLM\u9884\u6d4b\u7684\u6750\u6599\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cFastPhysGS\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u7269\u7406\u6a21\u62df\uff0c1\u5206\u949f\u5185\u5373\u53ef\u5b8c\u6210\u6a21\u62df\uff0c\u8fd0\u884c\u5185\u5b58\u4ec5\u97007GB\uff0c\u7269\u7406\u884c\u4e3a\u548c\u6e32\u67d3\u8d28\u91cf\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FastPhysGS\u5728\u7269\u7406\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u7528\u6027\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e3D\u7269\u7406\u52a8\u753b\u3001\u865a\u62df\u73b0\u5b9e\u7b49\u9886\u57df\uff0c\u63a8\u52a8\u4e86\u52a8\u60013D\u5185\u5bb9\u7684\u9ad8\u6548\u751f\u6210\u548c\u6a21\u62df\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2602.01724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01724", "abs": "https://arxiv.org/abs/2602.01724", "authors": ["Tushar Anand", "Maheswar Bora", "Antitza Dantcheva", "Abhijit Das"], "title": "DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation", "comment": "IEEE International Conference on Robotics and Automation 2026", "summary": "In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Mamba\u5757DenVisCoM\u4ee5\u53ca\u6df7\u5408\u5f0f\u67b6\u6784\uff0c\u53ef\u9ad8\u6548\u3001\u5b9e\u65f6\u5730\u8054\u5408\u4f30\u7b97\u5149\u6d41\u548c\u89c6\u5dee\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u517c\u987e\u51c6\u786e\u6027\u4e0e\u5b9e\u65f6\u6027\u3002", "motivation": "\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u8fd0\u52a8\u5206\u6790\u4efb\u52a1\u5173\u7cfb\u5bc6\u5207\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u80fd\u540c\u65f6\u9ad8\u6548\u89e3\u51b3\u4e8c\u8005\u7684\u7edf\u4e00\u6a21\u578b\u3002\u56e0\u800c\u9700\u8981\u4e00\u79cd\u53ef\u5b9e\u65f6\u3001\u51c6\u786e\u8054\u5408\u5b8c\u6210\u4efb\u52a1\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eDenVisCoM\u7684Mamba\u5757\u53ca\u7ed3\u5408Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4ee5\u8054\u5408\u4f30\u7b97\u8fd0\u52a8\uff08\u5149\u6d41\uff09\u4e0e\u4e09\u7ef4\u611f\u77e5\u4efb\u52a1\uff08\u89c6\u5dee\uff09\uff0c\u4f18\u5316\u5b9e\u65f6\u63a8\u7406\u3001\u5185\u5b58\u5360\u7528\u4e0e\u51c6\u786e\u7387\u95f4\u7684\u6743\u8861\u3002", "result": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u4f30\u7b97\u5149\u6d41\u548c\u89c6\u5dee\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\uff0c\u5e76\u5177\u5907\u8f83\u4f4e\u7684\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "DenVisCoM\u6df7\u5408\u67b6\u6784\u80fd\u591f\u6709\u6548\u3001\u5b9e\u65f6\u4e14\u51c6\u786e\u5730\u89e3\u51b3\u5149\u6d41\u548c\u89c6\u5dee\u7684\u8054\u5408\u4f30\u7b97\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002\u4f5c\u8005\u5df2\u5f00\u6e90\u5168\u90e8\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2602.01738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01738", "abs": "https://arxiv.org/abs/2602.01738", "authors": ["Yue Zhou", "Xinan He", "Kaiqing Lin", "Bing Fan", "Feng Ding", "Bin Li"], "title": "Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models", "comment": null, "summary": "While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578bFrozen\u7279\u5f81\u7684\u7b80\u5355\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff08AIGI\uff09\uff0c\u4e0d\u4ec5\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fd8\u5728\u73b0\u5b9e\u5206\u5e03\u573a\u666f\u4e2d\u5927\u5927\u8d85\u8d8a\u73b0\u6709\u4e13\u7528\u68c0\u6d4b\u5668\u3002", "motivation": "\u73b0\u6709\u4e13\u7528AIGI\u68c0\u6d4b\u5668\u5728\u4eba\u5de5\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u5ea6\u9ad8\uff0c\u4f46\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u8868\u73b0\u4e25\u91cd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u73b0\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982Perception Encoder\u3001MetaCLIP 2\u3001DINOv3\uff09\u7684\u51bb\u7ed3\u7279\u5f81\uff0c\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u3002\u5bf9\u5176\u5728\u6807\u51c6\u6570\u636e\u96c6\u3001\u672a\u77e5\u751f\u6210\u5668\u548c\u73b0\u5b9e\u5206\u5e03\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u5168\u9762\u8bc4\u6d4b\u3002", "result": "\u8be5\u7b80\u5355\u6a21\u578b\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u80fd\u5339\u914d\u4e13\u7528\u68c0\u6d4b\u5668\u6548\u679c\uff0c\u5e76\u5728in-the-wild\u573a\u666f\u4e2d\u51c6\u786e\u7387\u63d0\u5347\u8d8530%\u3002\u5206\u6790\u53d1\u73b0\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u663e\u5f0f\u5b66\u4e60\u5230\u4f2a\u9020\u8bed\u4e49\uff0cSSL\u6a21\u578b\u5219\u9690\u5f0f\u83b7\u5f97\u53d6\u8bc1\u5224\u522b\u529b\u3002\u4f46\u5728\u91cd\u62cd\u3001\u4f20\u8f93\u3001VAE\u91cd\u5efa\u548c\u5c40\u90e8\u7f16\u8f91\u4e0b\u4ecd\u6709\u9650\u3002", "conclusion": "\u4f5c\u8005\u547c\u5401AIGI\u53d6\u8bc1\u9700\u4ece\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u6570\u636e\u96c6\u8f6c\u5411\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e0d\u65ad\u8fdb\u5316\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4ee5\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.01741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01741", "abs": "https://arxiv.org/abs/2602.01741", "authors": ["Sicheng Pan", "Chen Tang", "Shuzhao Xie", "Ke Yang", "Weixiang Zhang", "Jiawei Li", "Bin Chen", "Shu-Tao Xia", "Zhi Wang"], "title": "Tail-Aware Post-Training Quantization for 3D Geometry Models", "comment": null, "summary": "The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a3D\u51e0\u4f55\u6a21\u578b\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5TAPTQ\uff0c\u5927\u5e45\u63d0\u5347\u91cf\u5316\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3D\u51e0\u4f55\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u8d8a\u6765\u8d8a\u5927\uff0c\u4f7f\u5f97\u5176\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709PTQ\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf92D\u89c6\u89c9Transformer\u4f18\u5316\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e3D\u6a21\u578b\uff0c\u4e14\u6821\u51c6\u5f00\u9500\u8fc7\u5927\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u53d1\u5c55\u80fd\u66f4\u9ad8\u6548\u9002\u5e943D\u51e0\u4f55\u5b66\u4e60\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "1\uff09\u63d0\u51fa\u5206\u9636\u6bb5\u7c97\u5230\u7ec6\u7684\u6821\u51c6\u5b50\u96c6\u6784\u5efa\u7b56\u7565\uff0c\u517c\u987e\u6570\u636e\u7eaf\u51c0\u5ea6\u548c\u51e0\u4f55\u4ee3\u8868\u6027\uff0c\u7f13\u89e33D\u6570\u636e\u96c6\u7684\u6570\u636e\u89c4\u6a21\u74f6\u9888\u30022\uff09\u5c06\u91cf\u5316\u533a\u95f4\u9009\u62e9\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u57fa\u4e8e\u4e09\u5206\u641c\u7d22\u7684\u65b0\u7b97\u6cd5\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(N)\u964d\u81f3O(logN)\u30023\uff09\u63d0\u51fa\u57fa\u4e8e\u5c3e\u76f8\u5bf9\u8bef\u5dee\uff08TRE\uff09\u7684\u9010\u6a21\u5757\u8865\u507f\u673a\u5236\uff0c\u81ea\u52a8\u68c0\u6d4b\u5e76\u4fee\u590d\u5bf9\u6fc0\u6d3b\u5c3e\u90e8\u5f02\u5e38\u503c\u654f\u611f\u7684\u6a21\u5757\uff0c\u51cf\u5c0f\u91cf\u5316\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728VGGT\u548cPi3\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTAPTQ\u5728\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684PTQ\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6821\u51c6\u65f6\u95f4\u3002", "conclusion": "TAPTQ\u4e3a3D\u51e0\u4f55\u6a21\u578b\u63d0\u4f9b\u4e86\u4e13\u7528\u4e14\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u90e8\u7f72\u6548\u7387\uff0c\u5177\u5907\u5f3a\u5e94\u7528\u4ef7\u503c\u3002\u4ee3\u7801\u5373\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.01753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01753", "abs": "https://arxiv.org/abs/2602.01753", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings", "comment": null, "summary": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6a21\u578bObjEmbed\uff0c\u9488\u5bf9\u7ec6\u7c92\u5ea6\u7269\u4f53\u4e0e\u6587\u672c\u5bf9\u9f50\u95ee\u9898\uff0c\u80fd\u9ad8\u6548\u652f\u6301\u591a\u79cd\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u572818\u9879\u57fa\u51c6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u66f4\u591a\u805a\u7126\u5168\u5c40\u56fe\u6587\u5bf9\u9f50\uff0c\u96be\u4ee5\u5904\u7406\u56fe\u50cf\u533a\u57df\u4e0e\u7279\u5b9a\u6587\u672c\u77ed\u8bed\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u9700\u6c42\u3002\u7ec6\u81f4\u533a\u57df\u7684\u5bf9\u9f50\u5bf9\u4e8e\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u5982\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u5c40\u90e8\u68c0\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "ObjEmbed\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u751f\u6210\u5bf9\u5e94\u7269\u4f53\u7684\u5d4c\u5165\uff0c\u540c\u65f6\u8fd8\u751f\u6210\u5168\u5c40\u5d4c\u5165\u3002\u6bcf\u4e2a\u533a\u57df\u88ab\u8d4b\u4e88\u8bed\u4e49\u5d4c\u5165\u548c\u53ef\u5b9a\u4f4d\u6027\u7684IoU\u5d4c\u5165\uff0c\u4e24\u8005\u7ed3\u5408\u540e\u7528\u4e8e\u7269\u4f53\u4e0e\u6587\u672c\u7684\u5339\u914d\u3002\u6a21\u578b\u5728\u5355\u6b21\u524d\u5411\u63a8\u7406\u5185\u540c\u65f6\u5bf9\u5168\u90e8\u5bf9\u8c61\u548c\u56fe\u7247\u6574\u4f53\u8fdb\u884c\u7f16\u7801\uff0c\u65e2\u517c\u987e\u5206\u533a\u57df\uff0c\u53c8\u652f\u6301\u6574\u4f53\u68c0\u7d22\u3002", "result": "ObjEmbed\u5728\u89c6\u89c9\u5b9a\u4f4d\u3001\u5c40\u90e8\u53ca\u5168\u5c40\u56fe\u7247\u68c0\u7d22\u7b49\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u6d4b\uff0c\u572818\u4e2a\u516c\u5f00\u57fa\u51c6\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u8bed\u4e49\u533a\u5206\u80fd\u529b\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "ObjEmbed\u6709\u6548\u63d0\u5347\u4e86\u7269\u4f53-\u6587\u672c\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6027\u80fd\uff0c\u5177\u5907\u9ad8\u9002\u7528\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u5bf9\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01754", "abs": "https://arxiv.org/abs/2602.01754", "authors": ["Gustavo P. C. P. da Luz", "Alvaro M. Aspilcueta Narvaez", "Tiago Godoi Bannwart", "Gabriel Massuyoshi Sato", "Luis Fernando Gomez Gonzalez", "Juliana Freitag Borin"], "title": "Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration", "comment": "Submitted to Journal of Internet Services and Applications, 27 pages, 20 figures, 3 tables", "summary": "Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u667a\u80fd\u505c\u8f66\u7cfb\u7edf\uff0c\u5728\u505c\u8f66\u4f4d\u7ea7\u522b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76d1\u6d4b\uff0c\u5e76\u901a\u8fc7\u8f6f\u786c\u4ef6\u521b\u65b0\u589e\u5f3a\u667a\u6167\u57ce\u5e02\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u4f30\u7b97\u533a\u57df\u5185\u5269\u4f59\u8f66\u4f4d\u6570\u91cf\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5355\u4e2a\u8f66\u4f4d\u7684\u5360\u7528\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u4e30\u5bcc\u5e94\u7528\u548c\u6d1e\u5bdf\u529b\u3002\u89e3\u51b3\u66f4\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u3001\u652f\u6301\u66f4\u590d\u6742\u5e94\u7528\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e86\u57fa\u4e8e\u8ddd\u79bb\u611f\u77e5\u7684\u5339\u914d\u65b9\u6cd5\uff08\u5e26\u6709\u7a7a\u95f4\u5bb9\u5dee\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u754c\u6846\u5206\u5272\u65b9\u6cd5\u63d0\u5347\u5bf9\u96be\u8bc6\u522b\u8f66\u4f4d\u7684\u5904\u7406\u80fd\u529b\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7YOLOv11m\u6a21\u578b\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u540c\u65f6\u5f15\u5165\u4e86\u6570\u5b57\u5f71\u5b50\uff08Digital Shadow\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u6539\u9020\u7535\u89c6\u76d2\u7684\u5e94\u7528\u652f\u6301\u670d\u52a1\u5668\u5b9e\u73b0\u786c\u4ef6\u590d\u7528\u548c\u6570\u636e\u53ef\u89c6\u5316\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8f66\u4f4d\u7ea7\u76d1\u6d4b\u4e0a\u8fbe\u5230\u4e8698.8%\u7684\u5e73\u8861\u7cbe\u5ea6\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a8\u79d2\uff0c\u4e14\u7cfb\u7edf\u90e8\u7f72\u4e8e\u4ec540.5 MB\u7684\u8f7b\u91cf\u5316\u6a21\u578b\u53ca\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u3002\u5f15\u5165\u7684\u65b0\u7ec4\u4ef6\u786e\u4fdd\u4e86\u53ef\u6269\u5c55\u6027\u4e0e\u6301\u7eed\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u505c\u8f66\u4f4d\u7ea7\u522b\u7684\u7cbe\u7ec6\u76d1\u63a7\uff0c\u5e76\u63a8\u5e7f\u4e86\u786c\u4ef6\u53ef\u6301\u7eed\u6027\u601d\u8def\uff0c\u6709\u529b\u5730\u63a8\u8fdb\u4e86\u667a\u6167\u57ce\u5e02\u667a\u80fd\u505c\u8f66\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01756", "abs": "https://arxiv.org/abs/2602.01756", "authors": ["Jun He", "Junyan Ye", "Zilong Huang", "Dongzhi Jiang", "Chenjue Zhang", "Leqi Zhu", "Renrui Zhang", "Xiang Zhang", "Weijia Li"], "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation", "comment": "36 pages, 24 figures", "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6 Mind-Brush\uff0c\u7528\u52a8\u6001\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u65b9\u5f0f\u4ee3\u66ff\u4f20\u7edf\u9759\u6001\u89e3\u7801\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u7528\u6237\u610f\u56fe\u548c\u590d\u6742\u77e5\u8bc6\u63a8\u7406\u7684\u7406\u89e3\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u8bc4\u6d4b\u57fa\u51c6 Mind-Bench\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5bf9\u9690\u542b\u7528\u6237\u610f\u56fe\u4e0e\u590d\u6742\u77e5\u8bc6\u63a8\u7406\u7684\u7406\u89e3\uff0c\u4e14\u5185\u90e8\u5148\u9a8c\u9759\u6001\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u53d8\u5316\uff0c\u4e9f\u9700\u66f4\u52a0\u667a\u80fd\u3001\u7075\u6d3b\u7684\u751f\u6210\u6846\u67b6\u3002", "method": "Mind-Brush\u901a\u8fc7\u6a21\u62df\u201c\u601d\u8003-\u67e5\u8bc1-\u521b\u4f5c\u201d\u5de5\u4f5c\u6d41\uff0c\u4e3b\u52a8\u68c0\u7d22\u591a\u6a21\u6001\u8bc1\u636e\u652f\u6301\u65b0\u9896\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u63a8\u7406\u5de5\u5177\u89e3\u51b3\u590d\u6742\u7684\u89c6\u89c9\u7ea6\u675f\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u56fe\u50cf\u751f\u6210\u3002\u540c\u65f6\u63d0\u51faMind-Bench\u57fa\u51c6\uff0c\u5168\u9762\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u9886\u57df\u548c\u65b0\u9896\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "result": "Mind-Brush\u5728\u65b0\u5efa\u7acb\u7684Mind-Bench\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4f7fQwen-Image\u7b49\u57fa\u7ebf\u6a21\u578b\u80fd\u529b\u83b7\u5f970\u52301\u7684\u7a81\u7834\uff0c\u5e76\u5728WISE\u3001RISE\u7b49\u6743\u5a01\u8bc4\u6d4b\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6210\u7ee9\uff0c\u663e\u793a\u51fa\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Mind-Brush\u6781\u5927\u5730\u63d0\u5347\u4e86\u7edf\u4e00\u751f\u6210\u6a21\u578b\u5bf9\u590d\u6742\u4efb\u52a1\u548c\u52a8\u6001\u77e5\u8bc6\u7684\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01760", "abs": "https://arxiv.org/abs/2602.01760", "authors": ["Hao Zhang", "Yanping Zha", "Zizhuo Li", "Meiqi Gong", "Jiayi Ma"], "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement", "comment": null, "summary": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5355\u5e45\u56fe\u50cf\u878d\u5408\u6846\u67b6MagicFuse\uff0c\u80fd\u591f\u5728\u53ea\u6709\u4f4e\u8d28\u91cf\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u8fd1\u4f3c\u591a\u6a21\u6001\u878d\u5408\u7684\u6548\u679c\u3002", "motivation": "\u5728\u5b9e\u9645\u4e25\u82db\u73af\u5883\u4e2d\uff0c\u7ea2\u5916\u7b49\u591a\u6a21\u6001\u4f20\u611f\u5668\u53ef\u80fd\u4e0d\u53ef\u7528\uff0c\u5982\u4f55\u4ec5\u5229\u7528\u53ef\u89c1\u5149\u56fe\u50cf\u6700\u5927\u7a0b\u5ea6\u6316\u6398\u548c\u878d\u5408\u573a\u666f\u4fe1\u606f\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMagicFuse\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53ef\u89c1\u5149\u77e5\u8bc6\u5f3a\u5316\u5206\u652f\u548c\u8de8\u5149\u8c31\u77e5\u8bc6\u751f\u6210\u5206\u652f\uff0c\u5e76\u901a\u8fc7\u591a\u57df\u77e5\u8bc6\u878d\u5408\u5206\u652f\u6574\u5408\u4e24\u8005\u4fe1\u606f\uff0c\u8f85\u4ee5\u89c6\u89c9\u548c\u8bed\u4e49\u7ea6\u675f\u4f18\u5316\u573a\u666f\u8868\u8fbe\u3002", "result": "MagicFuse\u5728\u4ec5\u7528\u53d7\u635f\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u73b0\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u4f9d\u8d56\u591a\u6a21\u6001\u8f93\u5165\u7684\u4e3b\u6d41\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "MagicFuse\u5b9e\u73b0\u4e86\u5355\u4e00\u4f4e\u8d28\u91cf\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u8de8\u5149\u8c31\u7279\u5f81\u91cd\u5efa\u548c\u878d\u5408\uff0c\u663e\u8457\u62d3\u5c55\u4e86\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5728\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.01764", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01764", "abs": "https://arxiv.org/abs/2602.01764", "authors": ["Dennis Basile", "Dennis Sprute", "Helene D\u00f6rksen", "Holger Flatt"], "title": "GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data", "comment": "Accepted at 19th CIRP Conference on Intelligent Computation in Manufacturing Engineering", "summary": "The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMEMS-LiDAR\u70b9\u4e91\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u65b0\u578b\u5165\u4fb5\u8005\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u6cd5\u89c4\u3002", "motivation": "\u5de5\u4e1a\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u9632\u6b62\u672a\u6388\u6743\u4eba\u5458\u5165\u4fb5\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u6df1\u5ea6\u5b66\u4e60\u8bc6\u522b\u65b9\u6cd5\u5bb9\u6613\u53d7\u5149\u7167\u5f71\u54cd\u4e14\u6d89\u53ca\u9690\u79c1\u95ee\u9898\uff0c\u800c\u4e14\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u83b7\u53d6\u548c\u5236\u4f5c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u91c7\u7528MEMS-LiDAR\u91c7\u96c6\u533f\u540d\u53163D\u70b9\u4e91\uff0c\u907f\u514d\u6355\u6349\u4e2a\u4eba\u8eab\u4efd\u7279\u5f81\u3002\u901a\u8fc7CARLA\u4eff\u771f\u5408\u6210\u5927\u91cf\u573a\u666f\u70b9\u4e91\u6570\u636e\uff0c\u4e0e\u771f\u5b9e\u91c7\u96c6\u7684\u70b9\u4e91\u6df7\u5408\uff0c\u5927\u5e45\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u91c7\u96c6\u548c\u6807\u6ce8\u7684\u6210\u672c\u3002", "result": "\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u70b9\u4e91\u6570\u636e\u540e\uff0c\u5165\u4fb5\u8005\u68c0\u6d4b\u7684\u5e73\u5747\u7cbe\u5ea6\u63d0\u5347\u4e8644\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u964d\u4f4e50%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u969cGDPR\u9690\u79c1\u5408\u89c4\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5de5\u4e1a\u573a\u666f\u4eba\u5458\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01783", "abs": "https://arxiv.org/abs/2602.01783", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "title": "Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation", "comment": null, "summary": "Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95\u00b0 and 2.20\u00b0 in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3\u00b0.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u5730\u4e0b\u77ff\u4e95\u88f8\u9732\u5ca9\u9762\u7ed3\u6784\u4e0d\u8fde\u7eed\u9762\u7ec4\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u77ff\u4e95\u73af\u5883\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5730\u4e0b\u77ff\u4e95\u5ca9\u9762\u7ed3\u6784\u4e0d\u8fde\u7eed\u9762\u7ec4\u7684\u8868\u5f81\u5bf9\u5ca9\u4f53\u7a33\u5b9a\u6027\u8bc4\u4f30\u548c\u8fd0\u8425\u5b89\u5168\u6781\u4e3a\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u5728\u5b9e\u9645\u5c01\u95ed\u77ff\u4e95\u73af\u5883\u4e0b\u81ea\u52a8\u9ad8\u6548\u8bc6\u522b\u7684\u6280\u672f\u5c1a\u4e0d\u5b8c\u5584\u3002", "method": "\u672c\u65b9\u6cd5\u5305\u62ec\u4e09\u5927\u521b\u65b0\u70b9\uff1a\uff081\uff09\u91c7\u7528\u5355\u6b21\u6ee4\u6ce2\u7b56\u7565\uff0c\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u6280\u672f\u4e00\u6b21\u6027\u63d0\u53d6\u5e73\u9762\u533a\u57df\u5e76\u6291\u5236\u566a\u58f0\u548c\u9ad8\u66f2\u7387\u4f2a\u5f71\uff1b\uff082\uff09\u63d0\u51fa\u5faa\u73af\u65b9\u4f4d\u53d8\u6362\u65b9\u6848\uff0c\u5c06\u6781\u5750\u6807\u4e0b\u7684\u8d70\u5411\u548c\u503e\u89d2\u51c6\u786e\u8f6c\u6362\u5230\u7b1b\u5361\u5c14\u7a7a\u95f4\uff0c\u514b\u670d\u4f20\u7edf\u7b1b\u5361\u5c14\u805a\u7c7b\u5bf9\u6781\u5750\u6807\u6570\u636e\u7684\u5c40\u9650\uff1b\uff083\uff09\u91c7\u7528\u5c42\u6b21\u805a\u7c7b\u65b9\u6cd5\uff0c\u65e0\u9700\u7528\u6237\u9884\u8bbe\u9762\u7ec4\u6570\uff0c\u81ea\u52a8\u8bc6\u522b\u4e0d\u540c\u5bc6\u5ea6\u5206\u5e03\u7684\u9762\u7ec4\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u77ff\u4e95\u5df7\u9053\u7684\u6570\u636e\u4e0a\u4e0e\u624b\u5de5\u9009\u53d6\u548c\u4e3b\u6d41\u81ea\u52a8\u7ed3\u6784\u6d4b\u7ed8\u6280\u672f\u6bd4\u5bf9\uff0c\u8868\u73b0\u51fa\u5728\u4e0d\u8fde\u7eed\u9762\u8d70\u5411\u4e0e\u503e\u89d2\u8bc6\u522b\u4e0a\u7684\u6700\u4f4e\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u5206\u522b\u4e3a1.95\u00b0\u548c2.20\u00b0\uff0c\u805a\u7c7b\u79bb\u6563\u5ea6\u8bef\u5dee\u5747\u4f4e\u4e8e3\u00b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u52a8\u7ed3\u6784\u4e0d\u8fde\u7eed\u9762\u7ec4\u8bc6\u522b\u65b9\u6cd5\u4e0d\u4ec5\u81ea\u52a8\u5316\u7a0b\u5ea6\u9ad8\uff0c\u4e14\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u9645\u77ff\u4e95\u5c01\u95ed\u7a7a\u95f4\u7684\u7ed3\u6784\u8868\u5f81\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u9ad8\u6548\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2602.01799", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01799", "abs": "https://arxiv.org/abs/2602.01799", "authors": ["Ido Faran", "Nathan S. Netanyahu", "Maxim Shoshany"], "title": "Spatio-Temporal Transformers for Long-Term NDVI Forecasting", "comment": null, "summary": "Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5f02\u8d28\u5730\u8868\u957f\u671f\u536b\u661f\u5f71\u50cf\u65f6\u5e8f\u5206\u6790\u7684\u65b0\u6a21\u578b STT-LTF\uff0c\u901a\u8fc7\u7a7a\u95f4\u4e0e\u65f6\u95f4\u7edf\u4e00\u5efa\u6a21\uff0c\u5728\u5730\u4e2d\u6d77\u590d\u6742\u666f\u89c2\u4e0b\u83b7\u5f97\u66f4\u4f18\u9884\u6d4b\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u591a\u5c3a\u5ea6\u7a7a\u95f4\u5f02\u8d28\u6027\u4e0e\u957f\u671f\u5e8f\u5217\u7279\u5f81\uff0c\u4e14\u5730\u4e2d\u6d77\u5730\u533a\u7684\u590d\u6742\u6027\u53ca\u6570\u636e\u957f\u671f\u7f3a\u4e4f\u6807\u6ce8\uff0c\u5bfc\u81f4\u9884\u6d4b\u957f\u671f\u751f\u6001\u52a8\u6001\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa Spatio-Temporal Transformer for Long Term Forecasting\uff08STT-LTF\uff09\u6846\u67b6\uff0c\u5728Transformer\u7ed3\u6784\u4e2d\u878d\u5408\u7a7a\u95f4patch\u3001\u65f6\u95f4\u5e8f\u5217\u3001\u5730\u7406\u5750\u6807\uff0c\u5b9e\u73b0\u7a7a\u95f4-\u65f6\u95f4\u8054\u5408\u5efa\u6a21\u3002\u91c7\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u63a9\u7801\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u652f\u6301\u4efb\u610f\u672a\u6765\u65f6\u95f4\u70b9\u7684\u76f4\u63a5\u9884\u6d4b\u3002", "result": "STT-LTF \u5728 1984-2024 \u5e74Landsat\u536b\u661f\u5f71\u50cf\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u53d6\u5f97MAE \u4e3a0.0328\uff0cR^2\u4e3a0.8412\uff0c\u663e\u8457\u4f18\u4e8e\u7edf\u8ba1\u6a21\u578b\u3001CNN\u3001LSTM\u53ca\u6807\u51c6Transformer\u7b49\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "STT-LTF \u53ef\u6709\u6548\u5904\u7406\u65f6\u7a7a\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u4e0d\u540c\u9884\u6d4b\u533a\u95f4\uff0c\u9002\u7528\u4e8e\u5730\u4e2d\u6d77\u7b49\u5f02\u8d28\u5feb\u901f\u53d8\u5316\u533a\u57df\u7684\u957f\u671f\u5f71\u50cf\u5206\u6790\uff0c\u4e3a\u5927\u5c3a\u5ea6\u751f\u6001\u76d1\u6d4b\u4e0e\u53d8\u5316\u9884\u6d4b\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.01801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01801", "abs": "https://arxiv.org/abs/2602.01801", "authors": ["Dvir Samuel", "Issar Tzachor", "Matan Levy", "Micahel Green", "Gal Chechik", "Rami Ben-Ari"], "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention", "comment": "Project Page: https://dvirsamuel.github.io/fast-auto-regressive-video/", "summary": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5KV\u7f13\u5b58\u81a8\u80c0\u5bfc\u81f4\u5ef6\u8fdf\u548cGPU\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65e0\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u52a0\u901f\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u753b\u8d28\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u957f\u671f\u7a33\u5b9a\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u652f\u6301\u6d41\u5f0f\u751f\u6210\uff0c\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u957f\u89c6\u9891\u5408\u6210\u548c\u4ea4\u4e92\u5f0f\u795e\u7ecf\u5f15\u64ce\uff0c\u4f46\u968f\u7740\u751f\u6210\u6b65\u6570\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u6a21\u5757KV\u7f13\u5b58\u5360\u7528\u5feb\u901f\u589e\u957f\uff0c\u5e26\u6765\u5ef6\u8fdf\u548c\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u548c\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u6548\u7387\u548c\u6269\u5c55\u6027\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u5256\u6790\u4e86\u5197\u4f59\u6765\u6e90\uff0c\u63d0\u51fa\uff1a1\uff09TempCache\uff0c\u57fa\u4e8e\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\u538b\u7f29KV\u7f13\u5b58\uff0c\u6291\u5236\u7f13\u5b58\u81a8\u80c0\uff1b2\uff09AnnCA\uff0c\u5229\u7528\u8fd1\u4f3c\u6700\u8fd1\u90bb\uff08ANN\uff09\u5feb\u901f\u6311\u9009\u4e0e\u5e27\u76f8\u5173\u7684\u63d0\u793atoken\uff0c\u52a0\u901f\u8de8\u6ce8\u610f\u529b\uff1b3\uff09AnnSA\uff0c\u901a\u8fc7ANN\u9650\u5236\u81ea\u6ce8\u610f\u529b\u6bcf\u4e2aquery\u4ec5\u4e0e\u8bed\u4e49\u5339\u914d\u7684key\u4ea4\u4e92\uff0c\u8fdb\u4e00\u6b65\u7a00\u758f\u5316\u8ba1\u7b97\u3002\u8be5\u6846\u67b6\u514d\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u73b0\u6709\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b05-10\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u51e0\u4e4e\u4fdd\u6301\u76f8\u540c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u4e14\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u6709\u6548\u7a33\u5b9a\u63a8\u7406\u901f\u5ea6\u548cGPU\u5cf0\u503c\u5185\u5b58\uff0c\u4e0d\u50cf\u4ee5\u5f80\u65b9\u6cd5\u90a3\u6837\u968f\u751f\u6210\u6b65\u6570\u6076\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u65e0\u5173\u6ce8\u610f\u529b\u538b\u7f29\u4e0e\u52a0\u901f\u65b9\u6848\u6709\u6548\u7f13\u89e3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u7684\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u957f\u65f6\u5e8f\u7684\u751f\u6210\u540c\u6b65\u5e26\u6765\u4e86\u663e\u8457\u7684\u6548\u7387\u548c\u8d44\u6e90\u6539\u8fdb\uff0c\u6709\u671b\u62d3\u5c55\u76f8\u5173\u6a21\u578b\u7684\u5b9e\u9645\u573a\u666f\u5e94\u7528\u3002"}}
{"id": "2602.01805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01805", "abs": "https://arxiv.org/abs/2602.01805", "authors": ["Menglin Han", "Zhangkai Ni"], "title": "FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing", "comment": null, "summary": "Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5FlowBypass\uff0c\u901a\u8fc7\u98de\u8d8a\u5f0f\u8f68\u8ff9\u8fde\u63a5\uff0c\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u8f68\u8ff9\u957f\u5ea6\u4e0a\u5b58\u5728\u4e24\u96be\uff1a\u957f\u8f68\u8ff9\u79ef\u7d2f\u8bef\u5dee\u964d\u4f4e\u4fdd\u771f\u5ea6\uff0c\u77ed\u8f68\u8ff9\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u7f16\u8f91\u8981\u6c42\u3002\u4e14\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u9aa8\u5e72\u7f51\u7edc\u7279\u5f81\u64cd\u4f5c\uff0c\u901a\u7528\u6027\u5dee\u3002\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u63a2\u7d22\u65e0\u7279\u5f81\u64cd\u4f5c\u7684\u65b0\u578b\u7f16\u8f91\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eRectified Flow\u7684FlowBypass\u6846\u67b6\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7684\u53cd\u6f14\u2014\u91cd\u5efa\u8f68\u8ff9\uff0c\u901a\u8fc7\u7cbe\u786e\u63a8\u5bfc\u4e24\u6761\u8f68\u8ff9\u53ca\u5176\u6570\u503c\u89e3\u6cd5\uff0c\u5f62\u6210\u9ad8\u6548\u7f16\u8f91\u8def\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u7279\u5f81\u64cd\u63a7\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlowBypass\u5728\u7f16\u8f91\u6307\u4ee4\u5bf9\u9f50\u5ea6\u548c\u65e0\u5173\u533a\u57df\u7ec6\u8282\u4fdd\u771f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7f16\u7f09\u8d28\u91cf\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "FlowBypass\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e24\u96be\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u666e\u9002\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u65e0\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\uff0c\u4e3a\u4eca\u540e\u901a\u7528\u578b\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01812", "abs": "https://arxiv.org/abs/2602.01812", "authors": ["Cheng Wang", "Qiyu Gao", "Fandong Zhang", "Shu Zhang", "Yizhou Yu"], "title": "LDRNet: Large Deformation Registration Model for Chest CT Registration", "comment": null, "summary": "Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u80f8\u90e8CT\u56fe\u50cf\u5927\u53d8\u5f62\u914d\u51c6\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5LDRNet\uff0c\u76f8\u8f83\u4e8e\u8111\u90e8\u914d\u51c6\uff0c\u80f8\u90e8\u914d\u51c6\u66f4\u5177\u6311\u6218\u4e14\u53d8\u5f62\u66f4\u5927\u3002\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u914d\u51c6\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u914d\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u8111\u90e8\u56fe\u50cf\uff0c\u80f8\u90e8CT\u56e0\u53d8\u5f62\u5927\u4e14\u7ed3\u6784\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u914d\u51c6\u7b97\u6cd5\u3002", "method": "\u63d0\u51faLDRNet\u6a21\u578b\uff0c\u5148\u9884\u6d4b\u7c97\u914d\u51c6\u573a\uff0c\u518d\u8fdb\u884c\u7c97\u5230\u7ec6\u7684\u9010\u6b65\u7cbe\u7ec6\u5316\u3002\u5176\u4e3b\u8981\u521b\u65b0\u4e3a\uff1a1) refine block\u7528\u4e8e\u4e0d\u540c\u5206\u8fa8\u7387\u4e0a\u7684\u914d\u51c6\u573a\u7cbe\u5316\uff1b2) rigid block\u5bf9\u9ad8\u9636\u7279\u5f81\u5b66\u4e60\u521a\u6027\u53d8\u6362\u77e9\u9635\u3002\u6a21\u578b\u5728\u79c1\u6709\u548cSegTHOR\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4e3b\u6d41\u914d\u51c6\u65b9\u6cd5\u4f5c\u5bf9\u6bd4\u3002", "result": "LDRNet\u5728\u5927\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\uff08state-of-the-art\uff09\uff0c\u4e14\u901f\u5ea6\u660e\u663e\u5feb\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff08\u5982VoxelMorph\u3001RCN\u3001LapIRN\u7b49\uff09\u3002", "conclusion": "LDRNet\u4e3a\u80f8\u90e8CT\u914d\u51c6\u7684\u9ad8\u6548\u3001\u7cbe\u786e\u7b97\u6cd5\uff0c\u514b\u670d\u4e86\u5927\u53d8\u5f62\u7b49\u96be\u9898\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u3002"}}
{"id": "2602.01814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01814", "abs": "https://arxiv.org/abs/2602.01814", "authors": ["Xiao Liang", "Yunzhu Zhang", "Linchao Zhu"], "title": "GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation", "comment": null, "summary": "Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a0\u901f\u89c6\u9891\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u2014\u2014Guided Progressive Distillation\uff08GPD\uff09\uff0c\u80fd\u591f\u5728\u6781\u5927\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u7684\u540c\u65f6\u7ef4\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u53bb\u566a\u6b65\u9aa4\u8ba1\u7b97\u91cf\u5de8\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u867d\u53ef\u51cf\u5c11\u6b65\u9aa4\uff0c\u4f46\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u6613\u5bfc\u81f4\u663e\u8457\u753b\u8d28\u4e0b\u964d\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u517c\u5177\u9ad8\u6548\u4e0e\u9ad8\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "GPD\u65b9\u6cd5\u901a\u8fc7\u201c\u6559\u5e08-\u5b66\u751f\u201d\u7b56\u7565\uff0c\u8ba9\u6559\u5e08\u6a21\u578b\u9010\u6b65\u5f15\u5bfc\u5b66\u751f\u6a21\u578b\u4ee5\u66f4\u5927\u6b65\u957f\u8fd0\u884c\uff0c\u5305\u542b\uff1a\uff081\uff09\u5728\u7ebf\u751f\u6210\u8bad\u7ec3\u76ee\u6807\u4ee5\u7b80\u5316\u4f18\u5316\u5e76\u63d0\u5347\u6548\u7387\uff1b\uff082\uff09\u5728\u6f5c\u7a7a\u95f4\u5f15\u5165\u9891\u57df\u7ea6\u675f\u4ee5\u4fdd\u6301\u7ec6\u8282\u548c\u65f6\u95f4\u52a8\u6001\u3002", "result": "GPD\u65b9\u6cd5\u5728Wan2.1\u6a21\u578b\u4e0a\u5c06\u91c7\u6837\u6b65\u6570\u4ece48\u6b65\u51cf\u5c11\u52306\u6b65\uff0c\u540c\u65f6\u5728VBench\u8bc4\u6d4b\u4e0a\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "GPD\u76f8\u8f83\u4e8e\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u6cd5\uff0c\u5728\u6d41\u7a0b\u7b80\u4fbf\u6027\u548c\u753b\u8d28\u4fdd\u7559\u65b9\u9762\u5747\u5c55\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u662f\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a0\u901f\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01816", "abs": "https://arxiv.org/abs/2602.01816", "authors": ["Wenjin Hou", "Wei Liu", "Han Hu", "Xiaoxiao Sun", "Serena Yeung-Levy", "Hehe Fan"], "title": "Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVIA-Bench\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9762\u5bf9\u89c6\u89c9\u9519\u89c9\u548c\u89c6\u89c9\u5f02\u5e38\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u867d\u7136MLLMs\u5728\u5e38\u89c4\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fd9\u4e9b\u6d4b\u8bd5\u591a\u4e3a\u5206\u5e03\u5185\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5728\u201c\u53cd\u5e38\u60c5\u5883\u201d\u4e0b\u7684\u7a33\u5065\u6027\u8bc4\u4ef7\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u6765\u63a2\u7d22\u6a21\u578b\u9762\u5bf9\u975e\u5e38\u89c4\u89c6\u89c9\u4fe1\u606f\u65f6\u7684\u80fd\u529b\u5f31\u70b9\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86VIA-Bench\uff0c\u5305\u542b\u516d\u5927\u7c7b\uff08\u5982\u989c\u8272\u9519\u89c9\u3001\u8fd0\u52a8\u9519\u89c9\u3001\u683c\u5f0f\u5854\u9519\u89c9\u3001\u51e0\u4f55\u7a7a\u95f4\u9519\u89c9\u3001\u4e00\u822c\u89c6\u89c9\u9519\u89c9\u548c\u89c6\u89c9\u5f02\u5e38\uff09\u5171\u8ba11000\u4f59\u7ec4\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff0c\u901a\u8fc7\u201c\u4eba\u7c7b\u5ba1\u6838\u201d\u786e\u4fdd\u9898\u76ee\u8d28\u91cf\uff0c\u5e76\u7528\u5176\u7cfb\u7edf\u6d4b\u8bd5\u4e86\u8d85\u8fc720\u4e2a\u5148\u8fdbMLLMs\uff08\u542b\u95ed\u6e90\u3001\u5f00\u6e90\u548c\u63a8\u7406\u589e\u5f3a\u578b\u6a21\u578b\uff09\uff0c\u91cd\u70b9\u8003\u5bdf\u5b83\u4eec\u5bf9\u9519\u89c9\u523a\u6fc0\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709MLLMs\u5728VIA-Bench\u6d4b\u8bd5\u4e2d\u666e\u904d\u8106\u5f31\uff0c\u5373\u4fbf\u662f\u5f15\u5165Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u6280\u5de7\u4e5f\u5e76\u672a\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u53cd\u800c\u5728\u9762\u5bf9\u9519\u89c9\u523a\u6fc0\u65f6\u51fa\u73b0\u63a8\u7406\u903b\u8f91\u5d29\u6e83\u7684\u201c\u6d77\u5e02\u8703\u697c\u578b\u201d\u9519\u8bef\uff0c\u66b4\u9732\u51fa\u6a21\u578b\u4e0e\u4eba\u7c7b\u611f\u77e5\u95f4\u7684\u6839\u672c\u6027\u5dee\u5f02\u3002", "conclusion": "\u6a21\u578b\u5728\u89c6\u89c9\u9519\u89c9\u548c\u5f02\u5e38\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u7684\u8106\u5f31\u6027\u8868\u660e\uff0cMLLMs\u5728\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u4e4b\u8def\u4e0a\u5b58\u5728\u611f\u77e5\u80fd\u529b\u74f6\u9888\uff0c\u672a\u6765\u9700\u8981\u6709\u9488\u5bf9\u6027\u5730\u89e3\u51b3\u611f\u77e5\u548c\u8ba4\u77e5\u5c42\u9762\u7684\u5dee\u8ddd\u3002\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5c06\u5bf9\u5916\u516c\u5f00\uff0c\u4fbf\u4e8e\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.01836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01836", "abs": "https://arxiv.org/abs/2602.01836", "authors": ["Yin Wu", "Daniel Slieter", "Carl Esselborn", "Ahmed Abouelazm", "Tsung Yuan Tseng", "J. Marius Z\u00f6llner"], "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery", "comment": null, "summary": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.", "AI": {"tldr": "\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e0d\u540c\u56fd\u5bb6\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u6cd5\u89c4\u4e0e\u4ea4\u901a\u73af\u5883\u5dee\u5f02\u5bfc\u81f4\u6570\u636e\u9886\u57df\u8f6c\u79fb\u3001\u611f\u77e5\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u8857\u666f\u56fe\u50cf\u8f85\u52a9\u91c7\u96c6\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u5e76\u4f4e\u6210\u672c\u9009\u53d6\u4ee3\u8868\u6027\u5730\u70b9\uff0c\u63d0\u9ad8\u8de8\u56fd\u6a21\u578b\u9002\u5e94\u6548\u7387\u3002", "motivation": "\u8de8\u56fd\u90e8\u7f72\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u65f6\uff0c\u4f20\u7edf\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u5b9e\u9645\u9053\u8def\u884c\u9a76\uff0c\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u4e14\u96be\u4ee5\u7cfb\u7edf\u7b5b\u9009\u6570\u636e\u7684\u4ee3\u8868\u6027\u5730\u70b9\uff0c\u56e0\u6b64\u4e9f\u9700\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u81ea\u52a8\u5316\u91c7\u96c6\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u8857\u666f\u56fe\u50cf\u5f15\u5bfc\u7684\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u3002\u9996\u5148\u5229\u7528\u516c\u5f00\u8857\u666f\u56fe\u50cf\uff0c\u901a\u8fc7KNN\u7279\u5f81\u8ddd\u79bb\u548c\u89c6\u89c9-\u8bed\u8a00\u8054\u5408\u5c5e\u6027\u4e24\u79cd\u8bc4\u5206\u65b9\u5f0f\uff0c\u7b5b\u9009\u611f\u5174\u8da3\u5730\u70b9\u3002\u91c7\u7528collect-detect\u534f\u8bae\uff0c\u901a\u8fc7Zenseact Open Dataset\u4e0eMapillary\u6570\u636e\u96c6\u914d\u5bf9\uff0c\u6784\u5efa\u8de8\u9886\u57df\u5171\u5740\u6570\u636e\u96c6\u3002\u4ee5\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u4e3a\u5b9e\u9a8c\u4efb\u52a1\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u5728\u4ea4\u901a\u6807\u5fd7\u8de8\u56fd\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7528\u4e00\u534a\u76ee\u6807\u57df\u6570\u636e\u5c31\u53ef\u8fbe\u5230\u4e0e\u968f\u673a\u91c7\u6837\u63a5\u8fd1\u7684\u6027\u80fd\u3002\u5bf9\u6574\u4e2a\u56fd\u5bb6\u7ea7\u522b\u5206\u6790\u8fdb\u884c\u6210\u672c\u4f30\u7b97\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u8857\u666f\u56fe\u50cf\u5904\u7406\u7ecf\u6d4e\u53ef\u884c\u3002", "conclusion": "\u8857\u666f\u56fe\u50cf\u5f15\u5bfc\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\u80fd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u5730\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u8de8\u56fd\u9002\u5e94\u6027\uff0c\u4e3a\u89e3\u51b3\u9886\u57df\u8f6c\u79fb\u96be\u9898\u5e26\u6765\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01843", "abs": "https://arxiv.org/abs/2602.01843", "authors": ["Qian Xu", "Xi Li", "Fei Gao", "Jie Guo", "Haojuan Yuan", "Shuaipeng Fan", "Mingjin Zhang"], "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u548c\u7269\u7406\u77e5\u8bc6\u7684\u63d2\u4ef6\u7ed3\u6784SPIRIT\uff0c\u7528\u4e8e\u7edf\u4e00\u89e3\u51b3\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u5728\u5355\u5e27\u4e0e\u591a\u5e27\uff08\u89c6\u9891\uff09\u573a\u666f\u4e0b\u7684\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7a7a\u95f4\u4e0e\u65f6\u95f4\u8054\u5408\u7684\u5904\u7406\u673a\u5236\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u5728\u65e9\u671f\u9884\u8b66\u548c\u76d1\u63a7\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u4f46\u7531\u4e8e\u7ea2\u5916\u6570\u636e\u7a00\u7f3a\u53ca\u5176\u4fe1\u53f7\u5f31\u3001\u8bed\u4e49\u7ebf\u7d22\u5c11\uff0c\u76f4\u63a5\u5229\u7528\u4f9d\u8d56\u53ef\u89c1\u5149\u8bed\u4e49\u7684\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u5916\u89c2\u7684\u5e27\u95f4\u5173\u8054\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u5229\u7528VFM\u4f18\u52bf\uff0c\u53c8\u80fd\u9002\u5e94\u7ea2\u5916\u7279\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPIRIT\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u7269\u7406\u77e5\u8bc6\u63d2\u4ef6\u5bf9VFM\u8fdb\u884c\u9002\u914d\u3002\u7a7a\u95f4\u4e0a\uff0cPIFR\u6a21\u5757\u501f\u9274\u79e9\u7a00\u758f\u5206\u89e3\uff0c\u6291\u5236\u80cc\u666f\u3001\u589e\u5f3a\u7a00\u758f\u76ee\u6807\u4fe1\u53f7\uff1b\u65f6\u95f4\u4e0a\uff0cPGMA\u6a21\u5757\u5c06\u5386\u53f2\u5e27\u4fe1\u606f\u901a\u8fc7\u8f6f\u7a7a\u95f4\u5148\u9a8c\u5f15\u5165\u8de8\u5e27\u8bb0\u5fc6\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u89c6\u9891\u68c0\u6d4b\uff0c\u5e76\u5728\u65e0\u65f6\u95f4\u4e0a\u4e0b\u6587\u65f6\u81ea\u52a8\u9000\u5316\u4e3a\u5355\u5e27\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSPIRIT\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86VFM \u57fa\u7ebf\u548c\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff08SOTA\uff09\u7684\u6027\u80fd\u3002", "conclusion": "SPIRIT \u6846\u67b6\u6709\u6548\u514b\u670d\u4e86\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u6a21\u6001\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u7269\u7406\u77e5\u8bc6\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u7684\u9ad8\u6548\u68c0\u6d4b\u548c\u9002\u5e94\uff0c\u53ef\u63a8\u5e7f\u81f3\u5b9e\u9645\u76d1\u63a7\u4e0e\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2602.01844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01844", "abs": "https://arxiv.org/abs/2602.01844", "authors": ["Yuliang Zhan", "Jian Li", "Wenbing Huang", "Wenbing Huang", "Yang Liu", "Hao Sun"], "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions", "comment": "ICLR 2026", "summary": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5df2\u77e5\u7269\u7406\u5c5e\u6027\uff0c\u5373\u53ef\u4ece\u591a\u89c6\u89d2\u89c6\u89c9\u89c2\u6d4b\u4e2d\u65e0\u76d1\u7763\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u7684\u65b0\u65b9\u6cd5\uff08CloDS\uff09\uff0c\u5728\u4fdd\u6301\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u80fd\u6709\u6548\u5730\u4ece\u89c6\u89c9\u6570\u636e\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u6a21\u62df\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u5df2\u77e5\u7684\u7269\u7406\u5c5e\u6027\u6765\u8fdb\u884c\u76d1\u7763\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u7684\u5b9e\u7528\u6027\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5728\u7f3a\u4e4f\u7269\u7406\u5c5e\u6027\u5148\u9a8c\u77e5\u8bc6\u4e0b\uff0c\u5982\u4f55\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u3002", "method": "\u4f5c\u8005\u5236\u5b9a\u4e86\u5e03\u6599\u52a8\u529b\u5b66\u5b66\u4e60\u7684\u65b0\u573a\u666f\uff08CDG\uff09\uff0c\u5e76\u63d0\u51fa\u4e86CloDS\u6846\u67b6\u3002CloDS\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u8fdb\u884c\u89c6\u9891\u5230\u51e0\u4f55\u7684\u5bf9\u9f50\uff08grounding\uff09\uff0c\u7136\u540e\u5728\u83b7\u5f97\u7684\u7f51\u683c\u4e0a\u8bad\u7ec3\u52a8\u529b\u5b66\u6a21\u578b\uff1b\u540c\u65f6\u5728\u5bf9\u9f50\u9636\u6bb5\u5f15\u5165\u4e86\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\uff08Gaussian splatting\uff09\u7684\u53cc\u4f4d\u7f6e\u4e0d\u900f\u660e\u5ea6\u8c03\u8282\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u5c3a\u5ea6\u975e\u7ebf\u6027\u53d8\u5f62\u548c\u81ea\u906e\u6321\u95ee\u9898\uff0c\u652f\u63012D\u52303D\u7684\u53cc\u5411\u6620\u5c04\uff0c\u5e76\u540c\u65f6\u8003\u8651\u9ad8\u65af\u7ec4\u4ef6\u7684\u7edd\u5bf9\u4e0e\u76f8\u5bf9\u4f4d\u7f6e\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cCloDS\u80fd\u591f\u6709\u6548\u4ece\u89c6\u89c9\u6570\u636e\u5b66\u4e60\u51fa\u5e03\u6599\u52a8\u529b\u5b66\uff0c\u5bf9\u4e8e\u672a\u89c1\u8fc7\u7684\u65b0\u914d\u7f6e\u540c\u6837\u6709\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CloDS\u8bc1\u660e\u4e86\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u4ec5\u4ece\u591a\u89c6\u89d2\u89c6\u89c9\u6570\u636e\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\uff0c\u6709\u671b\u62d3\u5c55\u52a8\u529b\u5b66\u4eff\u771f\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.01850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01850", "abs": "https://arxiv.org/abs/2602.01850", "authors": ["Pei Li", "Jiaxi Yin", "Lei Ouyang", "Shihan Pan", "Ge Wang", "Han Ding", "Fei Wang"], "title": "WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?", "comment": "Under Review. 28 pages, 9 figures, 6 tables", "summary": "IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWS-IMUBench\uff0c\u5bf9IMU\u8bbe\u5907\u4e0b\u7684\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff08WS-IMU-TAL\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u57fa\u51c6\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e2d\u9762\u4e34\u65b0\u6311\u6218\uff0c\u5e76\u7ed9\u51fa\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eIMU\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u591a\u4e3a\u5206\u7c7b\uff0c\u4e0d\u80fd\u53cd\u6620\u65e5\u5e38\u884c\u4e3a\u7684\u4e30\u5bcc\u65f6\u95f4\u7ed3\u6784\u3002\u800c\u73b0\u6709\u7684\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff08IMU-TAL\uff09\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u7684\u5e27\u7ea7\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u4f5c\u8005\u5173\u6ce8\u53ea\u7528\u5e8f\u5217\u7ea7\u6807\u7b7e\u7684\u5f31\u76d1\u7763IMU-TAL\uff0c\u5e76\u5e0c\u671b\u8bc4\u4f30\u5df2\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u6ca1\u6709\u63d0\u51fa\u65b0\u7684\u7b97\u6cd5\uff0c\u800c\u662f\u6784\u5efa\u4e86WS-IMUBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u8bc4\u6d4b\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\u9886\u57df\u4e03\u79cd\u5178\u578b\u5f31\u76d1\u7763\u5b9a\u4f4d\u65b9\u6cd5\u5728IMU-TAL\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e03\u4e2aIMU\u516c\u5f00\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u8d85\u8fc73,540\u6b21\u6a21\u578b\u8bad\u7ec3\u548c7,080\u6b21\u63a8\u65ad\u8bc4\u6d4b\uff0c\u5e76\u56f4\u7ed5\u4e09\u5927\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u8fc1\u79fb\u6027\u80fd\u4f9d\u8d56\u4e8e\u6570\u636e\u6a21\u6001\uff0c\u65f6\u95f4\u57df\u5f31\u76d1\u7763\u65b9\u6cd5\u666e\u904d\u6bd4\u57fa\u4e8e\u56fe\u50cf\u65b9\u6848\u66f4\u7a33\u5b9a\u3002\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\uff08\u5982\u957f\u65f6\u52a8\u4f5c\u3001\u9ad8\u7ef4\u4f20\u611f\uff09\u4e0b\uff0c\u5f31\u76d1\u7763\u8868\u73b0\u63a5\u8fd1\u6709\u76d1\u7763\uff0c\u4f46\u5728\u5904\u7406\u77ed\u52a8\u4f5c\u3001\u65f6\u95f4\u6a21\u7cca\u53ca\u5efa\u8bae\u6846\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u7a81\u51fa\u77ed\u677f\u3002", "conclusion": "WS-IMUBench\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u6d4b\u6d41\u7a0b\u548c\u5206\u6790\u8303\u4f8b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8WS-IMU-TAL\u9886\u57df\u7684\u8fdb\u6b65\u3002\u4f5c\u8005\u63d0\u51fa\u672a\u6765\u53ef\u5173\u6ce8IMU\u4e13\u5c5e\u63d0\u6848\u751f\u6210\u3001\u8fb9\u754c\u611f\u77e5\u76ee\u6807\u548c\u66f4\u5f3a\u65f6\u5e8f\u5efa\u6a21\u7b49\u65b9\u5411\u3002"}}
{"id": "2602.01851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01851", "abs": "https://arxiv.org/abs/2602.01851", "authors": ["Huanyu Zhang", "Xuehai Bai", "Chengzu Li", "Chen Liang", "Haochen Tian", "Haodong Li", "Ruichuan An", "Yifan Zhang", "Anna Korhonen", "Zhang Zhang", "Liang Wang", "Tieniu Tan"], "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing", "comment": "https://vibe-benchmark.github.io/", "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u7684\u89c6\u89c9\u6307\u4ee4\u57fa\u51c6\uff08VIBE\uff09\uff0c\u7cfb\u7edf\u8bc4\u6d4b\u591a\u79cd\u6a21\u578b\u5728\u8ddf\u968f\u4e0d\u540c\u590d\u6742\u7a0b\u5ea6\u89c6\u89c9\u6307\u4ee4\u65f6\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u548c\u8bc4\u6d4b\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u6307\u4ee4\uff0c\u800c\u4eba\u7c7b\u4ea4\u6d41\u901a\u5e38\u5305\u542b\u89c6\u89c9\u4fe1\u606f\uff08\u5982\u8349\u56fe\uff09\uff0c\u7f3a\u4e4f\u76f8\u5e94\u7684\u89c6\u89c9\u591a\u6a21\u6001\u6307\u4ee4\u8bc4\u6d4b\u673a\u5236\u3002", "method": "\u642d\u5efa\u4e86VIBE\u57fa\u51c6\uff0c\u6784\u5efa\u4e09\u5c42\u89c6\u89c9\u6307\u4ee4\u5c42\u7ea7\uff08\u6307\u793a\u6027\u3001\u5f62\u6001\u64cd\u4f5c\u3001\u56e0\u679c\u63a8\u7406\uff09\uff0c\u6db5\u76d6\u4e0d\u540c\u590d\u6742\u7a0b\u5ea6\u7684\u4e00\u7cfb\u5217\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u6837\u4f8b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5927\u6a21\u578b\u201c\u6cd5\u5b98\u201d+\u4efb\u52a1\u4e13\u5c5e\u6307\u6807\u7684\u8bc4\u6d4b\u6846\u67b6\u3002", "result": "\u8bc4\u6d4b\u4e8617\u4e2a\u4e3b\u6d41\u5f00\u6e90\u4e0e\u4e13\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u5728\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u521d\u5177\u80fd\u529b\u5e76\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4f46\u5728\u4efb\u52a1\u96be\u5ea6\u5347\u9ad8\u65f6\u6240\u6709\u6a21\u578b\u6027\u80fd\u5747\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u89c6\u89c9\u6307\u4ee4\u65f6\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\uff0c\u65b0\u57fa\u51c6\u548c\u8bc4\u6d4b\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.01854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01854", "abs": "https://arxiv.org/abs/2602.01854", "authors": ["A S M Sharifuzzaman Sagar", "Mohammed Bennamoun", "Farid Boussaid", "Naeha Sharif", "Lian Xu", "Shaaban Sahmoud", "Ali Kishk"], "title": "Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection", "comment": null, "summary": "In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5728\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\uff0c\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u5bf9\u56fe\u7247\u6587\u672c\u8054\u5408\u63a8\u7406\u4ef7\u503c\u6709\u9650\uff0c\u53cd\u800c\u53ef\u80fd\u964d\u4f4e\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u5f80\u5f80\u901a\u8fc7\u56fe\u6587\u914d\u5bf9\u8868\u8fbe\u865a\u5047\u8bed\u4e49\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u4e3b\u8981\u5173\u6ce8\u50cf\u7d20\u7ea7\u7be1\u6539\uff0c\u5c1a\u4e0d\u6e05\u695a\u5176\u5bf9\u5b9e\u9645\u4e8b\u5b9e\u6838\u67e5\u7684\u8d21\u732e\u548c\u6f5c\u5728\u8d1f\u9762\u4f5c\u7528\u3002\u4e3a\u79d1\u5b66\u8bc4\u4f30\u8be5\u7c7b\u5de5\u5177\u5728\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u4e2d\u7684\u5b9e\u9645\u6548\u76ca\uff0c\u63d0\u51fa\u8be5\u9879\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u57fa\u51c6\u6570\u636e\u96c6\uff08MMFakeBench\u548cDGM4\uff09\uff0c\u5bf9\u6bd4\u8bc4\u4f30\u4e3b\u6d41\u56fe\u7247\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u3001\u4ee5\u8bc1\u636e\u9a71\u52a8\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff08\u5305\u542bMCTS\u68c0\u7d22\u548cMAD\u63a8\u7406\uff09\u3001\u4ee5\u53ca\u878d\u5408\u68c0\u6d4b\u5668\u7ed3\u679c\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u5e76\u7cfb\u7edf\u6027\u5bf9\u6bd4\u5404\u6a21\u578b\u6548\u679c\u3002", "result": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u81ea\u8eab\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684F1\u5206\u6570\u4ec50.26-0.53\uff08MMFakeBench\uff09\u548c0.33-0.49\uff08DGM4\uff09\uff1b\u5176\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\u65f6\u53cd\u800c\u9020\u6210\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u6574\u4f53F1\u4e0b\u964d0.04-0.08\u3002\u76f8\u8f83\u4e4b\u4e0b\uff0c\u8bc1\u636e\u9a71\u52a8\u7684\u6838\u67e5\u7cfb\u7edf\u6548\u679c\u6700\u4f73\uff0cF1\u9ad8\u8fbe0.81\uff08MMFakeBench\uff09\u548c0.55\uff08DGM4\uff09\u3002", "conclusion": "\u8054\u5408\u56fe\u6587\u865a\u5047\u4fe1\u606f\u67e5\u8bc1\u4e3b\u8981\u4f9d\u8d56\u5bf9\u8bed\u4e49\u548c\u5916\u90e8\u8bc1\u636e\u7684\u7406\u89e3\uff0c\u50cf\u7d20\u7ea7\u4f2a\u9020\u4fe1\u53f7\u5bf9\u63a8\u7406\u8d21\u732e\u6709\u9650\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u53cd\u800c\u524a\u5f31\u5bf9\u771f\u5b9e\u865a\u5047\u4fe1\u606f\u7684\u5224\u522b\u80fd\u529b\u3002"}}
{"id": "2602.01864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01864", "abs": "https://arxiv.org/abs/2602.01864", "authors": ["Yuan Wang", "Yuhao Wan", "Siming Zheng", "Bo Li", "Qibin Hou", "Peng-Tao Jiang"], "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling", "comment": "26 pages, 19 figures. Accepted to ICLR 2026", "summary": "Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a \"Trust but Verify\" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u53c2\u8003\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5Ada-RefSR\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u964d\u8d28\u60c5\u51b5\u4e0b\u53c2\u8003\u56fe\u50cf\u4fe1\u606f\u5229\u7528\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u53c2\u8003\u56fe\u50cf\u53ef\u63d0\u5347\u6548\u679c\uff0c\u4f46\u771f\u5b9e\u573a\u666f\u4e0b\u4f4e\u8d28\u8f93\u5165\u4e0e\u53c2\u8003\u56fe\u4e4b\u95f4\u7684\u5173\u8054\u6027\u5f31\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u53ef\u9760\u5229\u7528\u53c2\u8003\u4fe1\u606f\uff0c\u6613\u5bfc\u81f4\u4fe1\u606f\u9519\u8bef\u878d\u5408\u6216\u6709\u7528\u7ebf\u7d22\u7684\u5ffd\u7565\u3002", "method": "\u63d0\u51fa\u4e86Ada-RefSR\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u81ea\u9002\u5e94\u9690\u5f0f\u76f8\u5173\u95e8\u63a7\uff08AICG\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6458\u8981token\u63d0\u53d6\u4e3b\u8981\u7684\u53c2\u8003\u56fe\u6848\uff0c\u5e76\u4e0e\u4f4e\u8d28\u7279\u5f81\u4e4b\u95f4\u9690\u5f0f\u5efa\u6a21\u76f8\u5173\u6027\uff0c\u96c6\u6210\u4e8e\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\uff0c\u4ee5\u8f7b\u91cf\u7ea7\u65b9\u5f0f\u52a8\u6001\u8c03\u8282\u53c2\u8003\u4fe1\u606f\u7684\u5f15\u5165\u3002", "result": "Ada-RefSR\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u987e\u4e86\u8d85\u5206\u8fa8\u7387\u7684\u4fdd\u771f\u5ea6\u3001\u81ea\u7136\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e14\u5728\u53c2\u8003\u56fe\u5bf9\u9f50\u7a0b\u5ea6\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "Ada-RefSR\u5b9e\u73b0\u4e86\u5bf9\u53c2\u8003\u4fe1\u606f\u53ef\u63a7\u3001\u53ef\u9760\u7684\u5229\u7528\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u56fe\u50cf\u590d\u539f\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b0\u9014\u5f84\uff0c\u53ef\u63a8\u5e7f\u81f3\u66f4\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.01881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01881", "abs": "https://arxiv.org/abs/2602.01881", "authors": ["Ye Chen", "Yupeng Zhu", "Xiongzhen Zhang", "Zhewen Wan", "Yingzhe Li", "Wenjun Zhang", "Bingbing Ni"], "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding", "comment": null, "summary": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u4ee3\u7406\u7684\u53c2\u6570\u5316\u56fe\u50cf\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u548c\u9ad8\u4fdd\u771f\u7684\u56fe\u50cf/\u89c6\u9891\u7f16\u8f91\uff0c\u5bf9\u6bd4\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u5177\u5907\u66f4\u4f4e\u53c2\u6570\u91cf\u548c\u66f4\u76f4\u89c2\u7cbe\u7ec6\u7684\u64cd\u63a7\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u50cf\u8868\u793a\u65b9\u6cd5\uff08\u5982\u6805\u683c\u56fe\u50cf\u3001\u9ad8\u65af\u539f\u8bed\u3001\u9690\u5f0f\u8868\u793a\uff09\u5b58\u5728\u8868\u793a\u5197\u4f59\u6216\u7f3a\u4e4f\u8bed\u4e49\u5bf9\u5e94\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u624b\u52a8\u7f16\u8f91\u7e41\u7410\u6216\u96be\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u63a7\u5236\u3002\u9650\u5236\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7684\u9ad8\u6548\u3001\u53ef\u63a7\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5206\u79bb\u8bed\u4e49\u3001\u51e0\u4f55\u3001\u7eb9\u7406\u53c2\u6570\u7684\u5206\u5c42\u4ee3\u7406\u8868\u793a\u3002\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u56fe\u50cf\u5206\u89e3\uff0c\u81ea\u9002\u5e94Bezier\u62df\u5408\u548c\u533a\u57df\u7ec6\u5206\u6784\u5efa\u5206\u5c42\u51e0\u4f55\u4ee3\u7406\u7ed3\u6784\uff0c\u5e76\u5728\u4ee3\u7406\u8282\u70b9\u5d4c\u5165\u591a\u5c3a\u5ea6\u9690\u5f0f\u7eb9\u7406\u53c2\u6570\uff0c\u8fd8\u5f15\u5165\u533a\u57df\u81ea\u9002\u5e94\u7279\u5f81\u7d22\u5f15\u673a\u5236\u4ee5\u589e\u5f3a\u7a7a\u95f4\u7eb9\u7406\u8fde\u7eed\u6027\uff0c\u65e0\u9700\u751f\u6210\u6a21\u578b\u53ef\u505a\u9ad8\u8d28\u91cf\u80cc\u666f\u8865\u5168\u3002\u540c\u65f6\u7ed3\u5408\u57fa\u4e8e\u4f4d\u7f6e\u7684\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7269\u7406\u52a8\u753b\u3002", "result": "\u5728ImageNet\u3001OIR-Bench\u3001HumanEdit\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u56fe\u50cf\u91cd\u5efa\u4e0e\u7f16\u8f91\u7684\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u6e32\u67d3\u4fdd\u771f\u5ea6\u3001\u53c2\u6570\u91cf\u3001\u7f16\u8f91\u4ea4\u4e92\u6027\u548c\u7269\u7406\u771f\u5b9e\u611f\u4e0a\u5747\u8fbe\u5230\u6216\u8d85\u8fc7SOTA\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5927\u5e45\u51cf\u5c11\u4e86\u53c2\u6570\u7528\u91cf\uff0c\u63d0\u5347\u4e86\u7f16\u8f91\u76f4\u89c2\u6027\u4e0e\u7075\u6d3b\u6027\uff0c\u8fd8\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u9ad8\u4e00\u81f4\u6027\u7684\u7269\u7406\u52a8\u753b\u6e32\u67d3\uff0c\u662f\u9ad8\u6548\u53ef\u63a7\u56fe\u50cf/\u89c6\u9891\u8868\u793a\u4e0e\u7f16\u8f91\u7684\u6709\u529b\u65b9\u6848\u3002"}}
{"id": "2602.01901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01901", "abs": "https://arxiv.org/abs/2602.01901", "authors": ["Jiedong Zhuang", "Lu Lu", "Ming Dai", "Rui Hu", "Jian Chen", "Qiang Liu", "Haoji Hu"], "title": "Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model", "comment": "Accepted by AAAI26", "summary": "Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLazy Attention\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u8de8\u5c42\u5171\u4eab\u76f8\u4f3c\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u548c\u7f13\u5b58\u5f00\u9500\u3002", "motivation": "\u73b0\u6709MLLMs\u7531\u4e8e\u89c6\u89c9\u7f16\u7801\u5668\u4ea7\u751f\u6d77\u91cf\u89c6\u89c9token\uff0c\u5bfc\u81f4\u8ba1\u7b97\u91cf\u548cKV\u7f13\u5b58\u9700\u6c42\u5c45\u9ad8\u4e0d\u4e0b\u3002\u867d\u7136\u5df2\u6709\u5404\u79cdtoken\u526a\u679d\u65b9\u6cd5\u964d\u4f4e\u5f00\u9500\uff0c\u4f46\u5e38\u635f\u5bb3KV\u7f13\u5b58\u5b8c\u6574\u6027\u3001\u5f71\u54cd\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\uff0c\u56e0\u6b64\u4f5c\u8005\u5bfb\u6c42\u65b0\u7684\u4f18\u5316\u9014\u5f84\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u6df1\u5165\u5206\u6790\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d1\u73b0\u8d85\u8fc7\u4e00\u534a\u7684\u89e3\u7801\u5c42\u95f4\u6ce8\u610f\u529b\u9ad8\u5ea6\u8bed\u4e49\u76f8\u4f3c\uff0c\u63d0\u51fa\u8de8\u5c42\u7ee7\u627f\u6ce8\u610f\u529b\u4fe1\u606f\u7684\u65b9\u6cd5Lazy Attention\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u3001\u53ef\u4e0e\u73b0\u6709\u63a8\u7406\u6846\u67b6\u517c\u5bb9\u7684Q Cache\uff0c\u7528\u4e8e\u5c42\u95f4\u67e5\u8be2\u7684\u590d\u7528\uff0c\u5e76\u53ef\u4e0e\u5df2\u6709token\u526a\u679d\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u53ef\u51cf\u5c1135%\u4ee5\u4e0a\u7684KV\u7f13\u5b58\u4f7f\u7528\uff0c\u63d0\u53471.5\u500d\u63a8\u7406\u541e\u5410\uff0c\u4ec5\u8f7b\u5fae\uff08\u7ea61%\uff09\u964d\u4f4e\u6027\u80fd\uff0c\u4e14\u8f83\u5148\u8fdbtoken\u526a\u679d\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4fdd\u6301\u4e0a\u66f4\u4f18\u3002", "conclusion": "Lazy Attention\u901a\u8fc7\u5c42\u95f4\u6ce8\u610f\u529b\u5171\u4eab\uff0c\u6709\u6548\u964d\u4f4e\u4e86MLLMs\u63a8\u7406\u6210\u672c\uff0c\u5e76\u5728\u5927\u5e45\u51cf\u5c11\u7f13\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u517c\u5bb9\u6027\u5f3a\u4e14\u5b9e\u7528\u4ef7\u503c\u9ad8\u3002"}}
{"id": "2602.01905", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01905", "abs": "https://arxiv.org/abs/2602.01905", "authors": ["Theodore Zhengde Zhao", "Sid Kiblawi", "Jianwei Yang", "Naoto Usuyama", "Reuben Tan", "Noel C Codella", "Tristan Naumann", "Hoifung Poon", "Mu Wei"], "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization", "comment": null, "summary": "Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.", "AI": {"tldr": "STELLAR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u89c6\u89c9\u7279\u5f81\uff0c\u517c\u987e\u4e86\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u50cf\u7d20\u7ea7\u91cd\u5efa\uff0c\u5f25\u5408\u4e86\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u81ea\u76d1\u7763\u5b66\u4e60\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u56fe\u50cf\u91cd\u5efa\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u51b2\u7a81\uff0c\u4e24\u8005\u96be\u4ee5\u517c\u5f97\uff1a\u5982DINO\u820d\u5f03\u7a7a\u95f4\u4fe1\u606f\u4e13\u6ce8\u8bed\u4e49\uff0c\u800cMAE\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u5374\u7f3a\u4e4f\u62bd\u8c61\u8868\u8fbe\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u4e24\u8005\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTELLAR\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u7279\u5f81\u5206\u89e3\u4e3a\u8bed\u4e49\u6982\u5ff5\u548c\u7a7a\u95f4\u5206\u5e03\u77e9\u9635\u7684\u4f4e\u79e9\u4e58\u79ef\u3002\u901a\u8fc7\u8fd9\u79cd\u89e3\u8026\uff0c\u540c\u65f6\u5bf9\u8bed\u4e49token\u505a\u589e\u5f3a\u5bf9\u9f50\uff08\u7c7b\u4f3cDINO\uff09\uff0c\u53c8\u80fd\u901a\u8fc7\u7a7a\u95f4\u77e9\u9635\u4fdd\u6301\u50cf\u7d20\u7ea7\u7684\u91cd\u5efa\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u4ec516\u4e2a\u7a00\u758ftoken\uff0c\u5728\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u56fe\u50cf\u91cd\u5efa\uff08FID=2.60\uff09\u7684\u540c\u65f6\uff0c\u5728ImageNet\u4e0a\u8fbe\u5230\u4e0e\u5bc6\u96c6backbone\u76f8\u5f53\u7684\u8bed\u4e49\u5206\u7c7b\u6027\u80fd\uff08\u51c6\u786e\u738779.10%\uff09\u3002", "conclusion": "STELLAR\u80fd\u6709\u6548\u6865\u63a5\u751f\u6210\u5f0f\u4e0e\u5224\u522b\u5f0f\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u901a\u8fc7\u8bed\u4e49\u4e0e\u7a7a\u95f4\u4fe1\u606f\u5206\u79bb\uff0c\u6210\u4e3a\u4e00\u79cd\u517c\u5177\u8bed\u4e49\u7406\u89e3\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u9ad8\u6548\u7a00\u758f\u89c6\u89c9\u8868\u793a\u3002"}}
{"id": "2602.01906", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01906", "abs": "https://arxiv.org/abs/2602.01906", "authors": ["Farhan Ullah", "Irfan Ullah", "Khalil Khan", "Giovanni Pau", "JaKeoung Koo"], "title": "DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification", "comment": null, "summary": "Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u6362\u5668\u6a21\u578bDSXFormer\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u6c60\u5316\u8c31\u7279\u5f81\u538b\u7f29\u6269\u5c55\u6a21\u5757\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5149\u8c31\u533a\u5206\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u53d7\u9650\u4e8e\u9ad8\u7ef4\u5149\u8c31\u4fe1\u606f\u3001\u590d\u6742\u7684\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u6807\u6ce8\u6837\u672c\u6709\u9650\uff0c\u73b0\u6709\u53d8\u6362\u5668\u6a21\u578b\u96be\u4ee5\u5728\u6548\u7387\u548c\u5149\u8c31\u5224\u522b\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faDSXFormer\u6a21\u578b\uff0c\u5305\u62ec\uff1a1\uff09\u5229\u7528\u53cc\u6c60\u5316\uff08\u5168\u5c40\u5747\u503c\u548c\u6700\u5927\u6c60\u5316\uff09\u8c31\u538b\u7f29\u6269\u5c55\u6a21\u5757\u81ea\u9002\u5e94\u589e\u5f3a\u5149\u8c31\u7279\u5f81\u533a\u5206\u6027\u548c\u6ce2\u6bb5\u95f4\u4f9d\u8d56\u5efa\u6a21\uff1b2\uff09\u5728\u7a97\u53e3\u578b\u53d8\u6362\u5668\u4e2d\u96c6\u6210\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u673a\u5236\uff0c\u52a8\u6001\u6355\u83b7\u5c40\u90e8\u8c31-\u7a7a\u95f4\u5173\u7cfb\u5e76\u964d\u4f4e\u8ba1\u7b97\u91cf\uff1b3\uff09\u91c7\u7528\u8865\u4e01\u63d0\u53d6\u3001\u5d4c\u5165\u53ca\u5408\u5e76\u4fc3\u8fdb\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728Salinas\u3001Indian Pines\u3001Pavia University\u548cKennedy Space Center\u56db\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\uff0cDSXFormer\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523099.95%\u300198.91%\u300199.85%\u548c98.52%\uff0c\u5747\u8d85\u8fc7\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DSXFormer\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u5206\u7c7b\u7684\u5149\u8c31\u533a\u5206\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01951", "abs": "https://arxiv.org/abs/2602.01951", "authors": ["Shuyang Wu", "Yifu Qiu", "Ines P. Nearchou", "Sandrine Prost", "Jonathan A Fallowfield", "Hakan Bilen", "Timothy J Kendall"], "title": "Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network", "comment": null, "summary": "Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7f51\u7edc\uff08MSPN\uff09\uff0c\u53ef\u4ee5\u7b80\u4fbf\u5730\u96c6\u6210\u8fdb\u73b0\u6709\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u75c5\u7406\u4efb\u52a1\uff0c\u5e76\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u7684MIL\u65b9\u6cd5\u5728\u878d\u5408\u591a\u653e\u5927\u500d\u6570\u4fe1\u606f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u6bd4\u5982\u53ea\u80fd\u5728\u6700\u540e\u878d\u5408\u7279\u5f81\u3001\u5ffd\u7565\u5c3a\u5ea6\u95f4\u5173\u8054\uff0c\u5e76\u4e14\u4f9d\u8d56\u8bbe\u5907\u5382\u5546\u5b9a\u4e49\u7684\u653e\u5927\u500d\u6570\uff0c\u5bfc\u81f4\u65b9\u6cd5\u4e0d\u7075\u6d3b\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63d0\u51fa\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u65b9\u5f0f\uff0c\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u4fbf\u5229\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7f51\u7edc\uff08MSPN\uff09\uff0c\u5176\u4e3b\u8981\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u7f51\u683c\u7684\u91cd\u6620\u5c04\uff0c\u5229\u7528\u9ad8\u653e\u5927\u500d\u6570\u7279\u5f81\u63a8\u5bfc\u51fa\u4f4e\u5206\u8fa8\u7387\u7684\u7c97\u7565\u7279\u5f81\uff1b2\uff09\u7c97\u5c3a\u5ea6\u5f15\u5bfc\u7f51\u7edc\uff08CGN\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u7c97\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002MSPN\u6a21\u5757\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u96c6\u6210\u5230\u4e3b\u6d41\u57fa\u4e8e\u6ce8\u610f\u529b\u7684MIL\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u591a\u5c3a\u5ea6\u5206\u6790\u3002", "result": "\u57284\u79cd\u6ce8\u610f\u529bMIL\u6846\u67b6\u30014\u4e2a\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\u548c3\u79cd\u57fa\u7840\u6a21\u578b\uff08\u5305\u62ec\u9884\u8bad\u7ec3MIL\uff09\u4e0a\uff0c\u4f5c\u8005\u5c06MSPN\u4f5c\u4e3a\u6a21\u5757\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aMSPN\u65e0\u8bba\u914d\u7f6e\u5982\u4f55\uff0c\u90fd\u80fd\u7a33\u5b9a\u63d0\u5347MIL\u7684\u8868\u73b0\u3002", "conclusion": "MSPN\u662f\u4e00\u79cd\u8f7b\u91cf\u3001\u6613\u7528\u4e14\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u5206\u6790\u6a21\u5757\uff0c\u80fd\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u6ce8\u610f\u529b\u7684MIL\u65b9\u6cd5\u5728\u8ba1\u7b97\u75c5\u7406\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01954", "abs": "https://arxiv.org/abs/2602.01954", "authors": ["Shuai Yang", "Ziyue Huang", "Jiaxin Chen", "Qingjie Liu", "Yunhong Wang"], "title": "Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images", "comment": null, "summary": "Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RS-MPOD\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63d0\u793a\uff0c\u63d0\u5347\u4e86\u9065\u611f\u9886\u57df\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u7c7b\u522b\u6307\u5b9a\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u4e0d\u660e\u786e\u6216\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\uff0c\u76f8\u8f83\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u7684\u65b9\u6cd5\u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u9065\u611f\u573a\u666f\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5f80\u5f80\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u6765\u6307\u5b9a\u68c0\u6d4b\u7c7b\u522b\uff0c\u4f46\u7531\u4e8e\u9065\u611f\u4efb\u52a1\u7c7b\u522b\u5177\u6709\u8f83\u5f3a\u7684\u4efb\u52a1\u548c\u5e94\u7528\u76f8\u5173\u8bed\u4e49\uff0c\u9884\u8bad\u7ec3\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u80fd\u529b\u4e0d\u603b\u662f\u53ef\u9760\uff0c\u5bfc\u81f4\u7c7b\u522b\u6307\u5b9a\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faRS-MPOD\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u4e0d\u518d\u5c40\u9650\u4e8e\u6587\u672c\u63d0\u793a\uff0c\u65b0\u589e\u89c6\u89c9\u63d0\u793a\u7f16\u7801\u5668\uff0c\u80fd\u4ece\u6837\u672c\u5b9e\u4f8b\u4e2d\u63d0\u53d6\u57fa\u4e8e\u5916\u89c2\u7684\u7c7b\u522b\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u6587\u672c\u7684\u7c7b\u522b\u6307\u5b9a\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u5728\u6587\u672c\u4e0e\u89c6\u89c9\u63d0\u793a\u5747\u6709\u65f6\u5bf9\u5176\u8fdb\u884c\u6709\u6548\u6574\u5408\u3002", "result": "\u5728\u591a\u79cd\u9065\u611f\u6570\u636e\u96c6\u548c\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff1a\u5728\u7c7b\u522b\u8bed\u4e49\u6a21\u7cca\u548c\u6570\u636e\u5206\u5e03\u53d8\u5316\u60c5\u51b5\u4e0b\uff0c\u89c6\u89c9\u63d0\u793a\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u7c7b\u522b\u6307\u5b9a\uff0c\u591a\u6a21\u6001\u63d0\u793a\u5219\u5728\u6587\u672c\u8bed\u4e49\u826f\u597d\u65f6\u8868\u73b0\u540c\u6837\u4f18\u5f02\u3002", "conclusion": "\u5c06\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63d0\u793a\u5f15\u5165\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u53ef\u6709\u6548\u63d0\u5347\u9065\u611f\u9886\u57df\u7c7b\u522b\u6307\u5b9a\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u7684\u4f20\u7edf\u505a\u6cd5\u3002"}}
{"id": "2602.01973", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01973", "abs": "https://arxiv.org/abs/2602.01973", "authors": ["Muli Yang", "Gabriel James Goenawan", "Henan Wang", "Huaiyuan Qin", "Chenghao Xu", "Yanhua Yang", "Fen Fang", "Ying Sun", "Joo-Hwee Lim", "Hongyuan Zhu"], "title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated", "comment": "AAAI 2026. Code: https://github.com/muliyangm/AIGI-Det-Calib", "summary": "Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u7684\u540e\u7f6e\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6a21\u578b\u7684logits\u8fdb\u884c\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u4fee\u6b63\uff0c\u4ee5\u589e\u5f3aAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u5206\u5e03\u8f6c\u79fb\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5373\u4fbf\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6d4b\u8bd5\u65f6\u4ecd\u5e38\u56e0\u5206\u5e03\u8f6c\u79fb\u6216\u9690\u542b\u5148\u9a8c\uff0c\u5bfc\u81f4\u5c06\u4f2a\u9020\u56fe\u50cf\u8bef\u5224\u4e3a\u771f\u5b9e\u3002\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u8fc7\u62df\u5408\u4e8e\u67d0\u4e9b\u8868\u5c42\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u7684\u4e0d\u5177\u5907\u6cdb\u5316\u6027\uff0c\u5bfc\u81f4\u51b3\u7b56\u9608\u503c\u9519\u4f4d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u7684\u540e\u7f6e\u6821\u51c6\u6846\u67b6\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5728\u76ee\u6807\u5206\u5e03\u7684\u4e00\u4e2a\u5c0f\u578b\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u4f18\u5316\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6807\u91cf\u53c2\u6570\u6765\u4fee\u6b63\u6a21\u578b\u8f93\u51fa\u7684logits\u53c2\u6570\uff0c\u4e3b\u5e72\u7f51\u7edc\u4fdd\u6301\u51bb\u7ed3\uff0c\u4e14\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u3002\u901a\u8fc7\u8fd9\u4e00\u53c2\u6570\u6821\u51c6\uff0c\u53ef\u4ee5\u8865\u507f\u56e0\u5206\u5e03\u504f\u79fb\u9020\u6210\u7684\u6a21\u578b\u8f93\u51fa\u5931\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u65b9\u6cd5\u8f7b\u91cf\u4e14\u9ad8\u6548\u3002", "conclusion": "\u8bba\u6587\u65b9\u6cd5\u4e3a\u5f00\u653e\u73af\u5883\u4e0bAI\u751f\u6210\u56fe\u50cf\u7684\u68c0\u6d4b\u5e26\u6765\u4e86\u53ef\u9760\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b0\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5e94\u5bf9\u5206\u5e03\u8f6c\u79fb\uff0c\u63d0\u9ad8\u68c0\u6d4b\u5668\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2602.01984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01984", "abs": "https://arxiv.org/abs/2602.01984", "authors": ["Minyoung Lee", "Yeji Park", "Dongjun Hwang", "Yejin Kim", "Seong Joon Oh", "Junsuk Choe"], "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling", "comment": "Accepted at ICLR 2026", "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7f29\u653e\u5206\u9694\u7b26token\u7684\u9690\u85cf\u72b6\u6001\u6765\u6539\u8fdb\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u963b\u6b62\u4e86\u8de8\u56fe\u50cf\u4fe1\u606f\u6cc4\u6f0f\uff0c\u63d0\u5347\u4e86\u5728\u591a\u56fe\u50cf\u4e0e\u591a\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LVLM\u5728\u5904\u7406\u591a\u5f20\u56fe\u7247\u65f6\u8868\u73b0\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u533a\u5206\u4e0d\u540c\u56fe\u7247\u7684\u4fe1\u606f\uff0c\u51fa\u73b0\u8de8\u56fe\u7247\u7684\u4fe1\u606f\u6cc4\u6f0f\u73b0\u8c61\u3002\u867d\u7136\u5df2\u4f7f\u7528\u5206\u9694\u7b26\u6807\u6ce8\u56fe\u7247\uff0c\u4f46\u5206\u9694\u7b26\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u5bf9\u5206\u9694\u7b26token\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u7f29\u653e\uff0c\u4ee5\u52a0\u5f3a\u540c\u4e00\u56fe\u7247\u5185\u90e8\u7684\u4fe1\u606f\u4ea4\u4e92\uff0c\u6291\u5236\u4e0d\u540c\u56fe\u7247\u4e4b\u95f4\u7684\u65e0\u5173\u4ea4\u4e92\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4fdd\u62a4\u6bcf\u5f20\u56fe\u7247\u7684\u7279\u6709\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u4e0d\u589e\u52a0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728Mantis\u3001MuirBench\u3001MIRB\u548cQBench2\u7b49\u591a\u56fe\u50cf\u57fa\u51c6\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1b\u5728TQABench\u3001MultiNews\u548cWCEP-10\u7b49\u591a\u6587\u6863\u3001\u591a\u8868\u683c\u4efb\u52a1\u4e0a\u4e5f\u6709\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5206\u9694\u7b26token\uff0cLVLM\u80fd\u66f4\u51c6\u786e\u5730\u533a\u5206\u548c\u63a8\u7406\u591a\u56fe\u7247\u3001\u591a\u6587\u6863\u573a\u666f\u7684\u4fe1\u606f\uff0c\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u53ef\u5e7f\u6cdb\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2602.01991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01991", "abs": "https://arxiv.org/abs/2602.01991", "authors": ["Pablo Domingo-Gregorio", "Javier Ruiz-Hidalgo"], "title": "Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models", "comment": null, "summary": "Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u5bf9\u56fe\u50cf\u5c40\u90e8\u533a\u57df\u7ec6\u81f4\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u63d0\u5347\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u4ec5\u4f9d\u9760\u6587\u672c\u6307\u4ee4\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u7ec6\u8282\u7684\u7cbe\u51c6\u63a7\u5236\u975e\u5e38\u56f0\u96be\uff0c\u76ee\u524d\u5df2\u6709\u65b9\u6cd5\u591a\u4e3a\u6574\u4f53\u65bd\u52a0\u6761\u4ef6\uff0c\u5bf9\u5c40\u90e8\u7ec6\u8282\u7684\u4e2a\u6027\u5316\u63a7\u5236\u975e\u5e38\u6709\u9650\uff0c\u4e0d\u80fd\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a9\u819c\u7279\u5f81\uff08masking features\uff09\u548c\u65b0\u7684\u635f\u5931\u9879\uff0c\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u5bf9\u56fe\u50cf\u7684\u6307\u5b9a\u533a\u57df\u8fdb\u884c\u5355\u72ec\u63a7\u5236\uff0c\u800c\u5176\u4f59\u533a\u57df\u5219\u7531\u6269\u6563\u6a21\u578b\u6839\u636e\u539f\u59cb\u6587\u672c\u81ea\u52a8\u751f\u6210\u3002\u635f\u5931\u9879\u589e\u5f3a\u4e86\u4efb\u610f\u6269\u6563\u6b65\u7684\u5f53\u524d\u72b6\u6001\u4e0e\u6700\u7ec8\u6f5c\u5728\u7a7a\u95f4\u6837\u672c\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5c40\u90e8\u533a\u57df\u53ef\u63a7\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u5408\u6210\u51fa\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u66f4\u7cbe\u786e\u7684\u533a\u57df\u63a7\u5236\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u6ee1\u8db3\u4e2a\u6027\u5316\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u751f\u6210\u9700\u6c42\u3002"}}
{"id": "2602.02000", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02000", "abs": "https://arxiv.org/abs/2602.02000", "authors": ["Bing He", "Jingnan Gao", "Yunuo Chen", "Ning Cao", "Gang Chen", "Zhengxue Cheng", "Li Song", "Wenjun Zhang"], "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors", "comment": "ICLR 2026", "summary": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e2D\u9ad8\u65af\u6e85\u5c04\uff082DGS\uff09\u7684\u65b9\u6cd5SurfSplat\uff0c\u5b9e\u73b0\u4e86\u4ece\u7a00\u758f\u56fe\u50cf\u9ad8\u4fdd\u771f\u91cd\u5efa3D\u573a\u666f\uff0c\u5e76\u6709\u6548\u514b\u670d\u4e86\u73b0\u67093DGS\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u51fa\u73b0\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5b9e\u73b0\u7684\u901a\u75283D\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u751f\u6210\u8fde\u7eed\u8868\u9762\uff0c\u7ecf\u5e38\u5bfc\u81f4\u79bb\u6563\u3001\u504f\u8272\u7684\u70b9\u4e91\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7ec6\u8282\u8868\u73b0\u5dee\u3002\u4e3a\u63d0\u9ad8\u51e0\u4f55\u8fde\u7eed\u6027\u4e0e\u91cd\u5efa\u8d28\u91cf\uff0c\u4e9f\u9700\u66f4\u4f18\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSurfSplat\uff0c\u4e00\u79cd\u4ee52D\u9ad8\u65af\u6e85\u5c04\u4e3a\u57fa\u7840\u7684\u524d\u9988\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8868\u9762\u8fde\u7eed\u6027\u5148\u9a8c\u4e0e\u5f3a\u5236alpha\u6df7\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u51e0\u4f55\u7cbe\u5ea6\u548c\u8868\u9762\u8fde\u8d2f\u6027\u91cd\u5efa\u3002\u8fd8\u63d0\u51fa\u4e86\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u4e00\u81f4\u6027\uff08HRRC\uff09\u65b0\u8bc4\u4ef7\u6307\u6807\uff0c\u7528\u4ee5\u91cf\u5316\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728RealEstate10K\u3001DL3DV\u548cScanNet\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSurfSplat\u5728\u4f20\u7edf\u6307\u6807\u548cHRRC\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5df2\u67093DGS\u65b9\u6cd5\u3002", "conclusion": "SurfSplat\u4e3a\u57fa\u4e8e\u7a00\u758f\u56fe\u50cf\u7684\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u51e0\u4f55\u548c\u7eb9\u7406\u7684\u8fd8\u539f\u8d28\u91cf\u3002"}}
{"id": "2602.02002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02002", "abs": "https://arxiv.org/abs/2602.02002", "authors": ["Guosheng Zhao", "Yaozeng Wang", "Xiaofeng Wang", "Zheng Zhu", "Tingdong Yu", "Guan Huang", "Yongchen Zai", "Ji Jiao", "Changliang Xue", "Xiaole Wang", "Zhen Yang", "Futang Zhu", "Xingang Wang"], "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "comment": "16 pages, 7 figures", "summary": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream", "AI": {"tldr": "UniDriveDreamer\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u7684\u7edf\u4e00\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u591a\u6444\u50cf\u5934\u89c6\u9891\u548cLiDAR\u5e8f\u5217\uff0c\u6bd4\u73b0\u6709\u5355\u4e00\u6a21\u6001\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u65b9\u6cd5\u591a\u96c6\u4e2d\u5728\u5355\u4e00\u6a21\u6001\uff08\u5982\u89c6\u9891\u6216LiDAR\uff09\uff0c\u7f3a\u4e4f\u540c\u65f6\u5904\u7406\u591a\u6a21\u6001\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u591a\u4f20\u611f\u5668\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86UniDriveDreamer\u6846\u67b6\uff0c\u5305\u62ec\u4e3aLiDAR\u5e8f\u5217\u8bbe\u8ba1\u7684VAE\u3001\u4e3a\u591a\u6444\u50cf\u5934\u89c6\u9891\u8bbe\u8ba1\u7684VAE\uff0c\u5e76\u901a\u8fc7Unified Latent Anchoring\uff08ULA\uff09\u5bf9\u4e24\u79cd\u6a21\u6001\u7684\u6f5c\u5728\u5411\u91cf\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\u3002\u968f\u540e\u5c06\u5bf9\u9f50\u540e\u7684\u7279\u5f81\u878d\u5408\uff0c\u5e76\u7528\u6269\u6563Transformer\u5efa\u6a21\u5176\u51e0\u4f55\u4e0e\u65f6\u5e8f\u5173\u7cfb\uff0c\u518d\u878d\u5408\u7ed3\u6784\u5316\u573a\u666f\u4fe1\u606f\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u6307\u5bfc\u751f\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniDriveDreamer\u65e0\u8bba\u5728\u89c6\u9891\u8fd8\u662fLiDAR\u751f\u6210\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "UniDriveDreamer\u80fd\u591f\u7edf\u4e00\u751f\u6210\u591a\u6a21\u6001\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\uff0c\u63d0\u5347\u6570\u636e\u5408\u6210\u8d28\u91cf\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u5e26\u6765\u8fdb\u6b65\u3002"}}
{"id": "2602.02004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02004", "abs": "https://arxiv.org/abs/2602.02004", "authors": ["Gongli Xi", "Kun Wang", "Zeming Gao", "Huahui Yi", "Haolang Lu", "Ye Tian", "Wendong Wang"], "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning", "comment": "20 pages, 7 figures", "summary": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\uff08hallucination\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86ClueRecall\u5ea6\u91cf\u548cClueTracer\u63d2\u4ef6\uff0c\u6709\u6548\u6291\u5236\u6a21\u578b\u5e7b\u89c9\u5e76\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u751f\u6210\u672a\u88ab\u8f93\u5165\u56fe\u50cf\u6216\u95ee\u9898\u652f\u6301\u7684\u5185\u5bb9\uff08\u5373\u5e7b\u89c9\uff09\u3002\u7814\u7a76\u53d1\u73b0\u539f\u56e0\u4e4b\u4e00\u662f\u63a8\u7406\u6f02\u79fb\uff08reasoning drift\uff09\uff1a\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u9010\u6b65\u8131\u79bb\u89c6\u89c9\u8bc1\u636e\u3002\u4ee5\u5f80\u9488\u5bf9\u975e\u63a8\u7406\u6a21\u578b\u7684\u5b9a\u4f4d\u6216\u5e72\u9884\u65b9\u6cd5\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faClueRecall\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u89c6\u89c9\u7ebf\u7d22\u68c0\u7d22\u80fd\u529b\u3002\u540c\u65f6\u63d0\u51faClueTracer\u63d2\u4ef6\uff1a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u65e0\u53c2\u6570\u3001\u67b6\u6784\u65e0\u5173\uff0c\u80fd\u591f\u8ffd\u8e2a\u6a21\u578b\u63a8\u7406\u8def\u5f84\u4e2d\u5173\u952e\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u4ece\u800c\u7cbe\u51c6\u5730\u5b9a\u4f4d\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u56fe\u50cf\u533a\u57df\uff0c\u6291\u5236\u5bf9\u65e0\u5173\u533a\u57df\u7684\u6ce8\u610f\u3002\u8be5\u65b9\u6cd5\u4ee5\u95ee\u9898\u51fa\u53d1\uff0c\u8d2f\u7a7f\u8f93\u51fa\u548c\u89c6\u89c9Token\u4e4b\u95f4\u7684\u63a8\u7406\u94fe\u8def\u3002", "result": "ClueTracer\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u73b0\u6709\u5404\u7c7b\u63a8\u7406\u6a21\u578b\uff08\u5982R1-OneVision\u3001Ocean-R1\u3001MM-Eureka\u7b49\uff09\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u8868\u73b0\u5e73\u5747\u63d0\u53471.21\u500d\uff0c\u5728\u975e\u63a8\u7406\u4efb\u52a1\u4e0b\u4e5f\u67091.14\u500d\u7684\u6da8\u5e45\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684ClueTracer\u65b9\u6cd5\u4e0eClueRecall\u6307\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8fd8\u5177\u5907\u5e7f\u6cdb\u7684\u517c\u5bb9\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02033", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02033", "abs": "https://arxiv.org/abs/2602.02033", "authors": ["Shuo Lu", "Haohan Wang", "Wei Feng", "Weizhen Wang", "Shen Zhang", "Yaoyu Li", "Ao Ma", "Zheng Zhang", "Jingjing Lv", "Junjie Shen", "Ching Law", "Bing Zhan", "Yuan Xu", "Huizai Yao", "Yongcan Yu", "Chenyang Si", "Jian Liang"], "title": "One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation", "comment": null, "summary": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.", "AI": {"tldr": "\u63d0\u51faOSMF\u6846\u67b6\uff0c\u6839\u636e\u7528\u6237\u5206\u7ec4\u7279\u5f81\u751f\u6210\u5b9a\u5236\u5e7f\u544a\u56fe\u7247\uff0c\u663e\u8457\u63d0\u5347\u5404\u7fa4\u4f53\u7684CTR\uff0c\u89e3\u51b3\u4e86\u5e7f\u544a\u751f\u6210\u4e2d\u7684\u201c\u4e00\u5200\u5207\u201d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5e7f\u544a\u56fe\u7247\u751f\u6210\u65b9\u6cd5\u53ea\u5173\u6ce8\u6574\u4f53CTR\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u95f4\u7684\u504f\u597d\u5dee\u5f02\uff0c\u4ece\u800c\u5236\u7ea6\u4e86\u7cbe\u51c6\u8425\u9500\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5206\u7ec4\u504f\u597d\u672a\u88ab\u6709\u6548\u5229\u7528\u5bfc\u81f4\u7684\u5e7f\u544a\u6295\u653e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "1\uff09\u5148\u8fdb\u884c\u9762\u5411\u4ea7\u54c1\u7684\u81ea\u9002\u5e94\u7528\u6237\u5206\u7ec4\uff0c\u6574\u5408\u7fa4\u4f53\u504f\u597d\u7279\u5f81\uff1b2\uff09\u57fa\u4e8e\u7fa4\u4f53\u7279\u5f81\uff0c\u5229\u7528\u7fa4\u4f53\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08G-MLLM\uff09\u8fdb\u884c\u6709\u6761\u4ef6\u7684\u5e7f\u544a\u56fe\u7247\u751f\u6210\uff1b3\uff09\u63d0\u51faGroup-DPO\u65b9\u6cd5\uff0c\u5fae\u8c03G-MLLM\uff0c\u5b9e\u73b0\u7fa4\u4f53\u7279\u5b9a\u504f\u597d\u7684\u4f18\u5316\uff1b4\uff09\u6784\u5efa\u4e86GAIP\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\uff0c50\u4e07\u7ec4\u7fa4\u4f53\u504f\u597d\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u4e0e\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684OSMF\u6846\u67b6\u5728\u5e7f\u544a\u56fe\u7247\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u79bb\u7ebf\u8fd8\u662f\u5728\u7ebf\uff0c\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5404\u4e2a\u5206\u7ec4\u7fa4\u4f53\u7684CTR\u4e0a\u5747\u5b9e\u73b0\u63d0\u5347\uff0c\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002", "conclusion": "OSMF\u6846\u67b6\u89e3\u51b3\u4e86\u5e7f\u544a\u56fe\u7247\u751f\u6210\u4e2d\u7fa4\u4f53\u504f\u597d\u591a\u6837\u6027\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7fa4\u4f53CTR\uff0c\u662f\u9488\u5bf9\u4e2a\u6027\u5316\u5728\u7ebf\u5e7f\u544a\u6295\u653e\u7684\u91cd\u8981\u8fdb\u5c55\u3002GAIP\u6570\u636e\u96c6\u7684\u53d1\u5e03\u4e5f\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2602.02043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02043", "abs": "https://arxiv.org/abs/2602.02043", "authors": ["Cristian Sbrolli", "Matteo Matteucci", "Toshihiko Yamasaki"], "title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models", "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).", "AI": {"tldr": "\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7ec4\u5408\u63a8\u7406\u4e0a\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4f1a\u6df7\u6dc6\u989c\u8272\u4e0e\u7269\u4f53\u914d\u5bf9\uff08\u5982\u201c\u7ea2\u8272\u65b9\u5757\u548c\u84dd\u8272\u7403\u4f53\u201d\u4e0e\u201c\u84dd\u8272\u65b9\u5757\u548c\u7ea2\u8272\u7403\u4f53\u201d\uff09\u3002\u4f5c\u8005\u63d0\u51faAuto-Comp\u81ea\u52a8\u5408\u6210\u57fa\u51c6\u8bc4\u6d4b\u7ba1\u9053\uff0c\u80fd\u7cbe\u51c6\u5206\u6790VLMs\u5728\u989c\u8272\u7ed1\u5b9a\u548c\u7a7a\u95f4\u5173\u7cfb\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5b58\u5728\u666e\u904d\u4e14\u66f4\u4e3a\u6df1\u5c42\u7684\u7ec4\u5408\u5931\u8d25\u95ee\u9898\u3002", "motivation": "VLM\u7ecf\u5e38\u5728\u7ec6\u7c92\u5ea6\u7684\u7ec4\u5408\u63a8\u7406\u60c5\u666f\u4e0b\u51fa\u9519\uff0c\u5c24\u5176\u662f\u5c5e\u6027\u4e0e\u7269\u4f53\u914d\u5bf9\u65f6\u6613\u6df7\u6dc6\u3002\u73b0\u6709\u8bc4\u6d4b\u7f3a\u4e4f\u53ef\u63a7\u6027\uff0c\u96be\u4ee5\u5256\u6790\u5177\u4f53\u662f\u89c6\u89c9\u8fd8\u662f\u8bed\u8a00\u8868\u5f81\u5bfc\u81f4\u7684\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u53ef\u63a7\u3001\u7cfb\u7edf\u7684\u8bc4\u6d4b\u5de5\u5177\uff0c\u5e2e\u52a9\u793e\u533a\u7406\u89e3\u5e76\u6539\u8fdb\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faAuto-Comp\u81ea\u52a8\u5408\u6210\u8bc4\u6d4b\u7ba1\u9053\uff0c\u901a\u8fc7\u6700\u5c0f\u63cf\u8ff0\uff08\u5c11\u91cf\u5143\u7d20\u7cbe\u786e\u8868\u8fbe\uff0c\u5982\u201c\u76d1\u89c6\u5668\u5728\u81ea\u884c\u8f66\u5de6\u8fb9\u201d\uff09\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u63cf\u8ff0\uff08\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u573a\u666f\u53e5\uff09\uff0c\u81ea\u52a8\u914d\u5bf9\u751f\u6210\u56fe\u50cf\u53ca\u6587\u672c\u57fa\u51c6\uff0c\u5b9e\u73b0\u53ef\u63a7A/B\u6d4b\u8bd5\uff0c\u5256\u6790\u6a21\u578b\u5bf9\u5c5e\u6027\u3001\u7a7a\u95f4\u5173\u7cfb\u7684\u63a8\u7406\u80fd\u529b\u3002\u5f15\u5165\u201cConfusion Benchmark\u201d\u5206\u6790\u6a21\u578b\u5bf9\u4f4e\u71b5\u5e72\u6270\uff08\u5982\u91cd\u590d\u989c\u8272\u6216\u7269\u4f53\uff09\u7684\u654f\u611f\u6027\u3002", "result": "\u7528Auto-Comp\u7cfb\u7edf\u5bf920\u79cd\u4e3b\u6d41VLMs\uff08\u5982CLIP\u3001SigLIP\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u6d4b\u8bd5\uff0c\u53d1\u73b0\u989c\u8272\u7ed1\u5b9a\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u666e\u904d\u5931\u8d25\uff0c\u4e0d\u4ec5\u9650\u4e8e\u7b80\u5355\u5c5e\u6027\u4e92\u6362\uff0c\u9762\u5bf9\u4f4e\u71b5\u5e72\u6270\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002\u540c\u65f6\u53d1\u73b0\u4e0a\u4e0b\u6587\u4e30\u5bcc\u573a\u666f\u63cf\u8ff0\u867d\u7136\u6709\u52a9\u4e8e\u6574\u4f53\u7a7a\u95f4\u63a8\u7406\uff0c\u5374\u4f1a\u56e0\u89c6\u89c9\u5e72\u6270\u5bfc\u81f4\u5c40\u90e8\u5c5e\u6027\u7ed1\u5b9a\u66f4\u5f31\u3002", "conclusion": "\u4e3b\u6d41VLMs\u7ec4\u5408\u63a8\u7406\u5b58\u5728\u4e25\u91cd\u74f6\u9888\uff0c\u5f53\u524d\u8bbe\u8ba1\u672a\u80fd\u89e3\u51b3\u89c6\u89c9\u4e0e\u8bed\u8a00\u5c5e\u6027\u7ed1\u5b9a\u548c\u7a7a\u95f4\u7ec4\u5408\u3002\u4e30\u5bcc\u4e0a\u4e0b\u6587\u867d\u53ef\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u4f46\u4f1a\u52a0\u91cd\u89c6\u89c9\u5e72\u6270\u3002Auto-Comp\u5c06\u63a8\u52a8\u793e\u533a\u6df1\u5165\u8bc4\u6d4b\u548c\u6539\u8fdbVLM\u7ec4\u5408\u6cdb\u5316\u3002\u6e90\u7801\u4e0e\u57fa\u51c6\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.02067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02067", "abs": "https://arxiv.org/abs/2602.02067", "authors": ["Nikola Cenikj", "\u00d6zg\u00fcn Turgut", "Alexander M\u00fcller", "Alexander Steger", "Jan Kehrer", "Marcus Brugger", "Daniel Rueckert", "Eimo Martens", "Philip M\u00fcller"], "title": "Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data", "comment": null, "summary": "Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u89c6\u89d2\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6SegmentMIL\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u89c6\u89d2\u7ea7\u6807\u6ce8\u7684\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u5206\u7c7b\uff0c\u5e76\u4e14\u5728\u5185\u90e8\u548c\u5916\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u79c0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6602\u8d35\u7684\u89c6\u89d2\u7ea7\u6807\u6ce8\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u591a\u89c6\u89d2\u4e4b\u95f4\u7684\u65f6\u5e8f\u52a8\u6001\u548c\u4f9d\u8d56\u6027\uff0c\u8fd9\u4e9b\u90fd\u662f\u5b9e\u9645\u4e34\u5e8a\u8bca\u65ad\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u3002", "method": "\u63d0\u51fa\u4e86SegmentMIL\u6846\u67b6\uff0c\u57fa\u4e8eTransformer\u7f51\u7edc\u7ed3\u6784\uff0c\u91c7\u7528\u60a3\u8005\u7ea7\u76d1\u7763\uff0c\u65e0\u9700\u89c6\u89d2\u7ea7\u6807\u6ce8\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u72ed\u7a84\u7684\u5206\u7c7b\u548c\u60a3\u75c5\u533a\u57df\uff08\u5de6\u53f3\u51a0\u72b6\u52a8\u8109\u53ca\u5176\u5206\u6bb5\uff09\u7684\u5b9a\u4f4d\u3002", "result": "SegmentMIL\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u65b9\u6cd5\u4e0e\u7ecf\u5178\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u57fa\u7ebf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5355\u89c6\u89d2\u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "SegmentMIL\u4e0d\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u5177\u5907\u8f83\u597d\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u5e94\u7528\u524d\u666f\uff0c\u4e3a\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02089", "abs": "https://arxiv.org/abs/2602.02089", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Xiuping Liang", "Tongfei Chen", "Shuwei Shao", "Linlin Yang", "Huobin Tan", "Baochang Zhang"], "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction", "comment": "ICLR 2026", "summary": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UrbanGS\u6846\u67b6\uff0c\u9488\u5bf93D Gaussian Splatting\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u591a\u9879\u6539\u8fdb\u4ee5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u67093D Gaussian Splatting\u65b9\u6cd5\u5728\u5c0f\u573a\u666f\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u57ce\u5e02\u7ea7\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u66b4\u9732\u51fa\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u5185\u5b58\u6d88\u8017\u5927\u548c\u6269\u5c55\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u4ee5\u63a8\u8fdb\u57ce\u5e02\u7ea7\u91cd\u5efa\u5e94\u7528\u3002", "method": "UrbanGS\u5f15\u5165\u4e86\u6df1\u5ea6\u4e00\u81f4D-Normal\u6b63\u5219\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u6df1\u5ea6\u76d1\u7763\uff0c\u5b9e\u73b0\u6240\u6709\u51e0\u4f55\u53c2\u6570\u7684\u5168\u65b9\u4f4d\u66f4\u65b0\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u7f6e\u4fe1\u52a0\u6743\u63d0\u5347\u591a\u89c6\u89d2\u6df1\u5ea6\u914d\u51c6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u4e3a\u63d0\u5347\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u51fa\u4e86\u7a7a\u95f4\u81ea\u9002\u5e94\u9ad8\u65af\u526a\u679d\u7b56\u7565(SAGP)\uff0c\u6309\u573a\u666f\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u5bc6\u5ea6\uff1b\u8fd8\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u7a7a\u95f4\u5206\u533a\u4e0e\u89c6\u70b9\u5206\u914d\uff0c\u6d88\u9664\u5206\u754c\u4f2a\u5f71\u5e76\u4f18\u5316\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5728\u591a\u4e2a\u57ce\u5e02\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660eUrbanGS\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u3001\u5185\u5b58\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u5176\u7cfb\u7edf\u6027\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "UrbanGS\u6709\u6548\u514b\u670d\u4e863DGS\u5728\u57ce\u5e02\u7ea7\u573a\u666f\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02092", "abs": "https://arxiv.org/abs/2602.02092", "authors": ["FSVideo Team", "Qingyu Chen", "Zhiyuan Fang", "Haibin Huang", "Xinwei Huang", "Tong Jin", "Minxuan Lin", "Bo Liu", "Celong Liu", "Chongyang Ma", "Xing Mei", "Xiaohui Shen", "Yaojie Shen", "Fuwen Tan", "Angtian Wang", "Xiao Yang", "Yiding Yang", "Jiamin Yuan", "Lingxi Zhang", "Yuxin Zhang"], "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space", "comment": "Project Page: https://kingofprank.github.io/fsvideo/", "summary": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\\times64\\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.", "AI": {"tldr": "FSVideo\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u6548\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u517c\u5177\u9ad8\u901f\u5ea6\u4e0e\u9ad8\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u901f\u5ea6\u548c\u89c6\u9891\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1\uff09\u8bbe\u8ba1\u4e86\u65b0\u7684\u9ad8\u538b\u7f29\u89c6\u9891\u81ea\u7f16\u7801\u5668\uff0c\u6709\u6548\u964d\u4f4e\u65f6\u7a7a\u5197\u4f59\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff1b2\uff09\u63d0\u51fa\u5e26\u5c42\u8bb0\u5fc6\u7684\u65b0\u578b\u6269\u6563Transformer\uff08DIT\uff09\uff0c\u589e\u5f3a\u5c42\u95f4\u4fe1\u606f\u6d41\u4e0e\u4e0a\u4e0b\u6587\u590d\u7528\uff1b3\uff09\u91c7\u7528\u591a\u5206\u8fa8\u7387\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u5c11\u6b65DIT\u4e0a\u91c7\u6837\u5668\u63d0\u5347\u89c6\u9891\u4fdd\u771f\u5ea6\u3002\u6700\u7ec8\u6a21\u578b\u5305\u542b14B\u53c2\u6570\u7684DIT\u5e95\u5ea7\u548c\u4e0a\u91c7\u6837\u5668\u3002", "result": "FSVideo\u5728\u4fdd\u6301\u4e0e\u4e3b\u6d41\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u751f\u6210\u8d28\u91cf\u4e0b\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u5347\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff1b\u6a21\u578b\u517c\u5177\u9ad8\u6548\u548c\u9ad8\u4fdd\u771f\u7279\u6027\u3002", "conclusion": "FSVideo\u5728\u901f\u5ea6\u548c\u6548\u679c\u4e0a\u5747\u5b9e\u73b0\u7a81\u7834\uff0c\u663e\u8457\u4fc3\u8fdb\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u662f\u751f\u6210\u5f0f\u89c6\u9891\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02107", "abs": "https://arxiv.org/abs/2602.02107", "authors": ["Yu Wang", "Chuanguang Yang", "Zhulin An", "Weilun Feng", "Jiarui Zhao", "Chengqing Yu", "Libo Huang", "Boyu Diao", "Yongjun Xu"], "title": "Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model", "comment": null, "summary": "Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5DSKD\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u6a21\u578b\u548c\u672c\u5730\u654f\u611f\u54c8\u5e0c\uff08LSH\uff09\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u5bf9\u6559\u5e08\u6a21\u578b\u77e5\u8bc6\u7684\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u591a\u6570\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u6559\u5e08\u548c\u5b66\u751f\u7684\u7279\u5f81\u4fe1\u606f\u6765\u63d0\u9ad8\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u4e8c\u8005\u7279\u5f81\u5206\u5e03\u4e0d\u540c\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u5230\u7684\u4fe1\u606f\u5e38\u5e38\u4e0e\u6559\u5e08\u6a21\u578b\u4e0d\u517c\u5bb9\uff0c\u5f71\u54cd\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6559\u5e08\u5f15\u5bfc\u7684\u5b66\u751f\u6269\u6563\u81ea\u84b8\u998f\u65b9\u6cd5\uff08DSKD\uff09\uff1a1\uff09\u7528\u6559\u5e08\u5206\u7c7b\u5668\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5bf9\u5b66\u751f\u7279\u5f81\u53bb\u566a\u91c7\u6837\uff0c\u751f\u6210\u5305\u542b\u6559\u5e08\u77e5\u8bc6\u7684\u5b66\u751f\u7279\u5f81\uff1b2\uff09\u4f7f\u7528LSH\u673a\u5236\u5728\u539f\u59cb\u5b66\u751f\u7279\u5f81\u4e0e\u53bb\u566a\u5b66\u751f\u7279\u5f81\u4e4b\u95f4\u8fdb\u884c\u7279\u5f81\u84b8\u998f\uff0c\u4ece\u800c\u4ee5\u66f4\u81ea\u7136\u7684\u65b9\u5f0f\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\u548c\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u7684\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cDSKD\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "DSKD\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u548cLSH\u673a\u5236\uff0c\u6709\u6548\u6d88\u9664\u4e86\u5e08\u751f\u7279\u5f81\u5206\u5e03\u4e0d\u4e00\u81f4\u548c\u6620\u5c04\u65b9\u5f0f\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u540e\u7eed\u77e5\u8bc6\u84b8\u998f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.02114", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02114", "abs": "https://arxiv.org/abs/2602.02114", "authors": ["Xin Ding", "Yun Chen", "Sen Zhang", "Kao Zhang", "Nenglun Chen", "Peibei Cao", "Yongwei Wang", "Fei Wu"], "title": "Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training", "comment": null, "summary": "Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \\textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\\times64$ to $256\\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u7248\u7684\u8fde\u7eed\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08iCCDM\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u8fdb\u7684EDM\u6846\u67b6\u53ca\u81ea\u9002\u5e94\u90bb\u57df\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u5e76\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684CCDM\u5728\u8fde\u7eed\u6807\u7b7e\u6761\u4ef6\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u5df2\u4f18\u4e8e\u65e9\u671f\u65b9\u6cd5\uff0c\u4f46\u91c7\u6837\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e14\u5df2\u88ab\u90e8\u5206GAN\u65b9\u6cd5\u8d85\u8d8a\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "iCCDM\u7ed3\u5408\u4e86\u6700\u65b0\u7684Elucidated Diffusion Model\uff08EDM\uff09\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u77e9\u9635\u5f62\u5f0fEDM\u8868\u8fbe\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u90bb\u57df\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u56fe\u50cf\u5206\u8fa8\u738764\u00d764\u81f3256\u00d7256\uff09\u4e0a\uff0ciCCDM\u7684\u751f\u6210\u8d28\u91cf\u548c\u91c7\u6837\u6548\u7387\u5747\u4f18\u4e8e\u5df2\u77e5\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5982Stable Diffusion 3\u7b49\u3002", "conclusion": "iCCDM\u5728\u4fdd\u8bc1\u66f4\u9ad8\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u8fde\u7eed\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u5907\u9886\u5148\u4f18\u52bf\u3002"}}
{"id": "2602.02123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02123", "abs": "https://arxiv.org/abs/2602.02123", "authors": ["Yangyi Cao", "Yuanhang Li", "Lan Chen", "Qi Mao"], "title": "MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos", "comment": null, "summary": "We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.", "AI": {"tldr": "MLV-Edit\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u57fa\u4e8e\u5149\u6d41\u7684\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u89e3\u51b3\u957f\u65f6\uff08\u5206\u949f\u7ea7\uff09\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0e\u8ba1\u7b97\u8d44\u6e90\u96be\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u6280\u672f\u4e3b\u8981\u9002\u7528\u4e8e\u77ed\u89c6\u9891\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u957f\u89c6\u9891\uff0c\u56e0\u4e3a\u957f\u89c6\u9891\u7f16\u8f91\u8ba1\u7b97\u91cf\u5927\u4e14\u96be\u4ee5\u4fdd\u8bc1\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u5206\u949f\u7ea7\u89c6\u9891\u7684\u7f16\u8f91\u96be\u9898\u3002", "method": "MLV-Edit\u8fd0\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u7b56\u7565\uff0c\u5c06\u957f\u89c6\u9891\u5212\u5206\u6210\u591a\u4e2a\u7247\u6bb5\u5206\u522b\u7f16\u8f91\u3002\u6838\u5fc3\u5305\u62ec\u4e24\u4e2a\u6a21\u5757\uff1aVelocity Blend\u5bf9\u9f50\u76f8\u90bb\u7247\u6bb5\u7684\u5149\u6d41\u573a\uff0c\u89e3\u51b3\u8fb9\u754c\u8fd0\u52a8\u4e0d\u8fde\u8d2f\u548c\u95ea\u70c1\u95ee\u9898\uff1bAttention Sink\u5219\u901a\u8fc7\u53c2\u8003\u5e27\u6291\u5236\u7ed3\u6784\u6f02\u79fb\uff0c\u4fdd\u969c\u5168\u5c40\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\uff0cMLV-Edit\u5728\u65f6\u5e8f\u7a33\u5b9a\u6027\u548c\u8bed\u4e49\u4fdd\u6301\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "MLV-Edit\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7f16\u8f91\u7684\u6311\u6218\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u53ef\u7a33\u5b9a\u7f16\u8f91\u957f\u65f6\u89c6\u9891\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02124", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02124", "abs": "https://arxiv.org/abs/2602.02124", "authors": ["Olga Graf", "Dhrupal Patel", "Peter Gro\u00df", "Charlotte Lempp", "Matthias Hein", "Fabian Heinemann"], "title": "Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies", "comment": null, "summary": "Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\\% of pathological tissue classified as healthy and 0.35\\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u556e\u9f7f\u52a8\u7269\u809d\u810f\u5168\u5207\u7247\u7ec4\u7ec7\u56fe\u50cf\u4e2d\u81ea\u52a8\u8bc6\u522b\u5065\u5eb7\u7ec4\u7ec7\u3001\u5df2\u77e5\u548c\u7f55\u89c1\u75c5\u7406\u5f02\u5e38\uff0c\u7528\u4e8e\u6bd2\u7406\u5b66\u7814\u7a76\u548c\u836f\u7269\u5b89\u5168\u6027\u7b5b\u67e5\u3002", "motivation": "\u836f\u7269\u8bf1\u5bfc\u7684\u6bd2\u6027\u53cd\u5e94\u662f\u836f\u7269\u7814\u53d1\u8fc7\u7a0b\u4e2d\u5bfc\u81f4\u5931\u8d25\u7684\u91cd\u8981\u539f\u56e0\uff0c\u73b0\u6709\u7684\u75c5\u7406\u8bc4\u4f30\u4f9d\u8d56\u4e13\u5bb6\uff0c\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u7b5b\u67e5\uff0c\u6025\u9700\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u809d\u810f\u7ec4\u7ec7\u56fe\u50cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5229\u7528DINOv2\u9884\u8bad\u7ec3\u89c6\u89c9\u53d8\u6362\u5668\u5e76\u901a\u8fc7LoRA\u5fae\u8c03\uff0c\u5b9e\u73b0\u7ec4\u7ec7\u5206\u5272\u3002\u901a\u8fc7\u9a6c\u6c0f\u8ddd\u79bb\u63d0\u53d6OOD\uff08\u5206\u5e03\u5916\uff09\u7279\u5f81\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u7c7b\u522b\u91c7\u7528\u7279\u5b9a\u9608\u503c\u6765\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u4f18\u5316\u540e\uff0c\u6846\u67b6\u5c06\u75c5\u7406\u7ec4\u7ec7\u8bef\u5224\u4e3a\u5065\u5eb7\u7684\u6bd4\u4f8b\u964d\u81f30.16%\uff0c\u5065\u5eb7\u7ec4\u7ec7\u8bef\u5224\u4e3a\u75c5\u7406\u7684\u6bd4\u4f8b\u4e3a0.35%\uff0c\u5728\u542b\u6709\u5df2\u77e5\u6bd2\u7406\u5b66\u7ed3\u679c\u7684\u5c0f\u9f20\u809d\u810f\u56fe\u50cf\u4e0a\u4e5f\u80fd\u51c6\u786e\u68c0\u6d4b\u5f02\u5e38\u548c\u7f55\u89c1\u75c5\u53d8\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u75c5\u7406\u5b66\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u6bd2\u6027\u76f8\u5173\u75c5\u53d8\u548c\u7f55\u89c1\u5f02\u5e38\uff0c\u53ef\u63d0\u5347\u524d\u671f\u7814\u53d1\u6548\u7387\u3001\u51cf\u5c11\u665a\u671f\u7814\u53d1\u5931\u8d25\uff0c\u52a9\u529b\u836f\u7269\u5b89\u5168\u6027\u8bc4\u4ef7\u3002"}}
{"id": "2602.02130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02130", "abs": "https://arxiv.org/abs/2602.02130", "authors": ["Lukas Zimmermann", "Michael Rauter", "Maximilian Schmid", "Dietmar Georg", "Barbara Kn\u00e4usl"], "title": "Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework", "comment": null, "summary": "Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\uff0c\u4f20\u7edf\u57fa\u4e8e\u914d\u51c6\u7684CBCT\u5230\u5408\u6210CT\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f1a\u88ab\u914d\u51c6\u8bef\u5dee\u5f71\u54cd\uff0c\u63d0\u51fa\u7528\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u751f\u6210\u4e25\u683c\u5bf9\u9f50\u7684\u6570\u636e\u6765\u907f\u514d\u8be5\u95ee\u9898\uff0c\u5e76\u7528\u4e0e\u8f93\u5165CBCT\u7684\u51e0\u4f55\u5bf9\u9f50\u5ea6\u800c\u975e\u5f3a\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8bc1\u660e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51e0\u4f55\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u66f4\u7b26\u5408\u4e34\u5e8a\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u7684CBCT\u8f6c\u5408\u6210CT\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e25\u683c\u914d\u51c6\u7684\u8bad\u7ec3\u5bf9\uff0c\u4f46\u5b9e\u9645\u5f88\u96be\u83b7\u5f97\u5b8c\u7f8e\u914d\u51c6\uff0c\u8fd9\u5bfc\u81f4\u8bad\u7ec3\u548c\u8bc4\u4f30\u8fc7\u7a0b\u90fd\u53d7\u5230\u914d\u51c6\u8bef\u5dee\u5f71\u54cd\uff0c\u8fdb\u800c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u66f4\u64c5\u957f\u590d\u73b0\u8fd9\u4e9b\u8bef\u5dee\u800c\u975e\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\uff0c\u5f71\u54cd\u4e34\u5e8a\u4e2d\u5bf9\u7ed3\u6784\u51c6\u786e\u7684\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7528\u57fa\u4e8e\u7269\u7406\u7684CBCT\u6a21\u62df\u751f\u6210\u5929\u7136\u914d\u51c6\u7684\u6210\u5bf9\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u8bc4\u4ef7\u6807\u51c6\u6539\u4e3a\u4e0e\u8f93\u5165CBCT\u5728\u51e0\u4f55\u4e0a\u7684\u5bf9\u9f50\u5ea6\uff08\u5982\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\uff09\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e0e\u5b58\u5728\u504f\u5dee\u914d\u51c6\u7684CT\u771f\u503c\u4e4b\u95f4\u7684\u50cf\u7d20\u503c\u5f3a\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u72ec\u7acb\u7684\u76c6\u8154\u6570\u636e\u96c6\u4e0a\uff0c\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51e0\u4f55\u5bf9\u9f50\u6307\u6807\uff08\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff080.31\u5bf90.22\uff09\uff0c\u5c3d\u7ba1\u5728\u5f3a\u5ea6\u5206\u6570\u4e0a\u7565\u4f4e\u3002\u5f3a\u5ea6\u6307\u6807\u4e0e\u4e34\u5e8a\u8bc4\u4ef7\u53cd\u76f8\u5173\uff0c\u800c\u51e0\u4f55\u5bf9\u9f50\u6307\u6807\u4e0e\u89c2\u5bdf\u8005\u4e00\u81f4\u6027\u663e\u8457\u3002\u4e34\u5e8a\u8bc4\u4f30\u4e2d\uff0c87%\u7684\u60c5\u51b5\u4e0b\u66f4\u504f\u597d\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u8f93\u51fa\u3002", "conclusion": "\u95ee\u9898\u7684\u5173\u952e\u5728\u4e8e\u914d\u51c6\u8bef\u5dee\u5f71\u54cd\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u91c7\u7528\u7269\u7406\u4eff\u771f\u53ef\u83b7\u5f97\u7ed3\u6784\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u51e0\u4f55\u5bf9\u9f50\u6307\u6807\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u66f4\u8d34\u5408\u4e34\u5e8a\u8ba4\u77e5\u9700\u6c42\u3002\u6a21\u578b\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u6bd4\u50cf\u7d20\u5f3a\u5ea6\u4e00\u81f4\u6027\u66f4\u5e94\u4f5c\u4e3a\u4e34\u5e8a\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.02154", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02154", "abs": "https://arxiv.org/abs/2602.02154", "authors": ["Sidi Wu", "Yizi Chen", "Maurizio Gribaudi", "Konrad Schindler", "Cl\u00e9ment Mallet", "Julien Perret", "Lorenz Hurni"], "title": "Deep learning enables urban change profiling through alignment of historical maps", "comment": "40 pages", "summary": "Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5927\u91cf\u5386\u53f2\u5730\u56fe\u4e2d\u7cbe\u7ec6\u5206\u6790\u57ce\u5e02\u53d8\u5316\uff0c\u5b9e\u73b0\u4e86\u5b9a\u91cf\u5316\u548c\u7cfb\u7edf\u5316\u7684\u5386\u53f2\u57ce\u5e02\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u8be6\u7ec6\u8bb0\u5f55\u4e86\u57ce\u5e02\u957f\u671f\u53d8\u8fc1\uff0c\u4f46\u5730\u56fe\u4e4b\u95f4\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u3001\u7ed8\u56fe\u98ce\u683c\u53d8\u5316\u548c\u6587\u6863\u52a3\u5316\u5bfc\u81f4\u5f88\u96be\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u548c\u5b9a\u91cf\u5316\u7684\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5168\u81ea\u52a8\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u5bc6\u96c6\u5730\u56fe\u5bf9\u9f50\u3001\u591a\u65f6\u76f8\u76ee\u6807\u68c0\u6d4b\u548c\u53d8\u5316\u5206\u6790\u6a21\u5757\uff1b\u5b9e\u73b0\u4e86\u5730\u56fe\u81ea\u52a8\u914d\u51c6\u548c\u57ce\u5e02\u5bf9\u8c61\u7684\u8de8\u65f6\u6bb5\u68c0\u6d4b\u3002", "result": "\u5728\u5df4\u9ece1868\u81f31937\u5e74\u5730\u56fe\u4f53\u7cfb\u4e0b\uff0c\u6846\u67b6\u81ea\u52a8\u68c0\u6d4b\u4e86\u57ce\u5e02\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u5f02\u8d28\u6027\u53d8\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5bf9\u9f50\u548c\u68c0\u6d4b\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u4e2d\u7684\u5386\u53f2\u57ce\u5e02\u53d8\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u5de5\u5177\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4e5f\u4fbf\u4e8e\u63a8\u5e7f\u5230\u4e0d\u540c\u7c7b\u578b\u7684\u5730\u56fe\u548c\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.02156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02156", "abs": "https://arxiv.org/abs/2602.02156", "authors": ["Wen-Jie Shu", "Xuerui Qiu", "Rui-Jie Zhu", "Harold Haodong Chen", "Yexin Liu", "Harry Yang"], "title": "LoopViT: Scaling Visual ARC with Looped Transformers", "comment": "8 pages, 11 figures", "summary": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u89c6\u89c9Transformer\u67b6\u6784Loop-ViT\uff0c\u901a\u8fc7\u53c2\u6570\u590d\u7528\u548c\u52a8\u6001\u505c\u6b62\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u5728ARC-AGI-1\u57fa\u51c6\u4e0a\u5c0f\u6a21\u578b\u8d85\u8d8a\u4e86\u66f4\u5927\u53c2\u6570\u91cf\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9Transformer\u91c7\u7528\u524d\u9988\u67b6\u6784\uff0c\u5728\u6a21\u578b\u53c2\u6570\u548c\u63a8\u7406\u6df1\u5ea6\u4e0a\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u96be\u4ee5\u6a21\u62df\u4eba\u7c7b\u5f52\u7eb3\u4e2d\u7684\u9012\u5f52\u548c\u8fed\u4ee3\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6253\u7834\u8fd9\u4e00\u7ea6\u675f\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u89c6\u89c9\u63a8\u7406\u3002", "method": "\u63d0\u51faLoop-ViT\u9012\u5f52\u67b6\u6784\uff0c\u5c06\u5377\u79ef\u4e0e\u5168\u5c40\u6ce8\u610f\u529b\u7ed3\u5408\u5f62\u6210Hybrid Block\uff0c\u5e76\u5728\u591a\u4e2a\u65f6\u523b\u590d\u7528\u540c\u4e00\u7ec4\u6743\u91cd\uff0c\u5141\u8bb8\u8fbe\u5230\u66f4\u5927\u8ba1\u7b97\u6df1\u5ea6\uff1b\u5f15\u5165\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684\u52a8\u6001\u9000\u51fa\u673a\u5236\uff0c\u6a21\u578b\u5185\u90e8\u72b6\u6001\u786e\u5b9a\u6027\u8db3\u591f\u9ad8\u65f6\u81ea\u52a8\u505c\u6b62\u8fed\u4ee3\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u81ea\u9002\u5e94\u6027\u3002", "result": "\u5728ARC-AGI-1\u4efb\u52a1\u4e0a\uff0c\u4e00\u4e2a\u53ea\u67091800\u4e07\u53c2\u6570\u7684Loop-ViT\u6a21\u578b\u83b7\u5f97\u4e8665.8%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u53c2\u6570\u91cf\u9ad8\u8fbe7300\u4e07\u7684\u5927\u578b\u96c6\u6210\u6a21\u578b\uff1b\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u9012\u5f52\u63a8\u7406\u548c\u81ea\u9002\u5e94\u505c\u6b62\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u76f8\u6bd4\u4e8e\u4e00\u5473\u589e\u52a0\u7f51\u7edc\u5bbd\u5ea6\uff0c\u91c7\u7528\u9012\u5f52\u3001\u52a8\u6001\u8ba1\u7b97\u6df1\u5ea6\u7684\u67b6\u6784\u80fd\u66f4\u9ad8\u6548\u5730\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u672a\u6765\u7684\u53d1\u5c55\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u65b9\u5411\u3002"}}
{"id": "2602.02163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02163", "abs": "https://arxiv.org/abs/2602.02163", "authors": ["Julian Wyatt", "Ronald Clark", "Irina Voiculescu"], "title": "Reg4Pru: Regularisation Through Random Token Routing for Token Pruning", "comment": "11 pages, 7 figures", "summary": "Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReg4Pru\u6b63\u5219\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8etoken\u526a\u679d\u53d8\u6362\u5668\u6a21\u578b\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u53d8\u6362\u5668\uff08Transformer\uff09\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u867d\u7136\u5e26\u6765\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u91cf\u968ftoken\u6570\u91cf\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u867d\u7136token\u526a\u679d\u7b49\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u5374\u5e26\u6765\u4e86\u7a20\u5bc6\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u66f4\u6df1\u5c42\u65f6\u66f4\u4e3a\u660e\u663e\u3002\u56e0\u6b64\u4e9f\u9700\u89e3\u51b3token\u51cf\u88c1\u5236\u5ea6\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReg4Pru\u7684\u8bad\u7ec3\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5728token\u526a\u679d\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u7279\u6b8a\u7684\u6b63\u5219\u5316\u6280\u672f\u7f13\u89e3\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u63d0\u5347\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728FIVES\u8840\u7ba1\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u5e94\u7528Reg4Pru\u540e\uff0c\u6a21\u578b\u7684\u5e73\u5747\u7cbe\u5ea6\u6bd4\u672a\u7528\u8be5\u6b63\u5219\u5316\u7684\u526a\u679d\u6a21\u578b\u63d0\u9ad8\u4e86\u7edd\u5bf946%\uff1b\u5728\u4fdd\u8bc129%\u63a8\u7406\u52a0\u901f\uff08\u76f8\u6bd4\u672a\u526a\u679d\u57fa\u7ebf\uff09\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0a\u8ff0\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0cReg4Pru\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684token\u526a\u679d\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u517c\u987e\u63a8\u7406\u6548\u7387\u4e0e\u5206\u5272\u6027\u80fd\uff0c\u5728\u9700\u8981\u51cf\u5c0f\u8ba1\u7b97\u91cf\u4f46\u53c8\u4e0d\u5e0c\u671b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u7684\u89c6\u89c9\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02171", "abs": "https://arxiv.org/abs/2602.02171", "authors": ["Lu Cao", "Xiquan He", "Junying Zeng", "Chaoyun Mai", "Min Luo"], "title": "Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks", "comment": null, "summary": "The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08TSGAN\uff09\uff0c\u901a\u8fc7\u5206\u79bb\u80ba\u7ed3\u8282\u7684\u5f62\u6001\u7ed3\u6784\u4e0e\u7eb9\u7406\u7279\u5f81\uff0c\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u4e0e\u7a7a\u95f4\u53ef\u63a7\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u63d0\u5347\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u80ba\u7ed3\u8282CT\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u6709\u9650\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u63a8\u5e7f\u6027\u548c\u8868\u73b0\u529b\u3002\u73b0\u6709\u7684\u5408\u6210\u65b9\u6cd5\u591a\u6837\u6027\u4e0e\u53ef\u63a7\u6027\u5dee\uff0c\u5bb9\u6613\u4ea7\u751f\u5355\u8c03\u7eb9\u7406\u548c\u5931\u771f\u7ed3\u6784\u3002", "method": "\u63d0\u51faTSGAN\u751f\u6210\u6a21\u578b\uff0c\u7b2c\u4e00\u9636\u6bb5\u7528StyleGAN\u751f\u6210\u8bed\u4e49\u5206\u5272\u63a9\u819c\uff0c\u7cbe\u786e\u63a7\u5236\u80ba\u7ed3\u8282\u548c\u80cc\u666f\u7ed3\u6784\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6539\u8fdb\u7248\u672cDL-Pix2Pix\u6a21\u578b\u5c06\u63a9\u819c\u8f6c\u6362\u4e3aCT\u56fe\u50cf\uff0c\u5f15\u5165\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u52a8\u6001\u6743\u91cd\u591a\u5934\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u7eb9\u7406\u4e0e\u80cc\u666f\u7279\u5f81\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728LUNA16\u6570\u636e\u96c6\u4e0a\uff0c\u91c7\u7528TSGAN\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u51c6\u786e\u7387\u63d0\u53474.6%\uff0cmAP\u63d0\u53474%\u3002", "conclusion": "TSGAN\u53ef\u63d0\u5347\u5408\u6210CT\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5e76\u4e14\u6539\u5584\u4e86\u80ba\u7ed3\u8282\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02175", "abs": "https://arxiv.org/abs/2602.02175", "authors": ["Xinquan Yu", "Wei Lu", "Xiangyang Luo"], "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization", "comment": null, "summary": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCIEC\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u4ec5\u7528\u7c97\u7c92\u5ea6\u6807\u6ce8\u8fdb\u884c\u591a\u6a21\u6001\u7be1\u6539\u5b9a\u4f4d\uff0c\u6548\u679c\u5ab2\u7f8e\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6602\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u4e0d\u6613\u5927\u89c4\u6a21\u6269\u5c55\uff0c\u56e0\u6b64\u4e9f\u9700\u4f4e\u6210\u672c\u7684\u5f31\u76d1\u7763\u65b9\u6848\u3002", "method": "CIEC\u6846\u67b6\u5305\u542b\u56fe\u50cf\u5206\u652f\u548c\u6587\u672c\u5206\u652f\u3002\u56fe\u50cf\u5206\u652f\u91c7\u7528TRPS\u6a21\u5757\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5229\u7528\u7a7a\u95f4\u5148\u9a8c\u5b9a\u4f4d\u53ef\u7591\u533a\u57df\u5e76\u6291\u5236\u5e72\u6270\u3002\u6587\u672c\u5206\u652f\u7528VCTG\u6a21\u5757\uff0c\u5173\u6ce8\u91cd\u8981\u5185\u5bb9\u8bcd\uff0c\u5229\u7528\u89c6\u89c9\u504f\u5dee\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u8f85\u4ee5\u7a00\u758f\u548c\u4e00\u81f4\u6027\u7ea6\u675f\u51cf\u5c11\u566a\u58f0\u3002\u4e24\u5206\u652f\u5747\u4ec5\u9700\u7c97\u7c92\u5ea6\u6807\u6ce8\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCIEC\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u6027\u80fd\u63a5\u8fd1\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "CIEC\u6846\u67b6\u80fd\u591f\u4ee5\u8f83\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u6709\u6548\u5b9e\u73b0\u591a\u6a21\u6001\u5f31\u76d1\u7763\u7be1\u6539\u5b9a\u4f4d\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02186", "abs": "https://arxiv.org/abs/2602.02186", "authors": ["Ziqiao Weng", "Jiancheng Yang", "Kangxian Xie", "Bo Zhou", "Weidong Cai"], "title": "Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision", "comment": "18 pages, 7 figures", "summary": "Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \\textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9488\u5bf9\u80ba\u90e8CT\u5f71\u50cf\u89e3\u5256\u6811\u7684\u62d3\u6251\u4fee\u590d\u548c\u5206\u6790\u65b9\u6cd5TopoField\uff0c\u80fd\u591f\u63d0\u5347\u62d3\u6251\u5b8c\u6574\u6027\u4e0e\u4e0b\u6e38\u5206\u6790\u6548\u7387\uff0c\u5728\u7ed3\u6784\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u5256\u6807\u6ce8\u4e0e\u5206\u5272\u3002", "motivation": "\u80ba\u90e8CT\u5f71\u50cf\u4e2d\u63d0\u53d6\u7684\u89e3\u5256\u6811\u5e38\u5b58\u5728\u7f3a\u5931\u6216\u65ad\u88c2\u5206\u652f\uff0c\u4e25\u91cd\u5f71\u54cd\u540e\u7eed\u7684\u533b\u5b66\u89e3\u5256\u5206\u6790\u548c\u5efa\u6a21\u6d41\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e9b\u62d3\u6251\u4e0d\u5b8c\u6574\u95ee\u9898\u65f6\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u6216\u5bf9\u7ed3\u6784\u635f\u574f\u4e0d\u591f\u9c81\u68d2\u7684\u7f3a\u9677\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u80ba\u90e8\u89e3\u5256\u6811\u4fee\u590d\u4e0e\u5206\u6790\u5de5\u5177\u6781\u4e3a\u8feb\u5207\u3002", "method": "TopoField\u662f\u4e00\u4e2a\u62d3\u6251\u611f\u77e5\u7684\u9690\u5f0f\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u62d3\u6251\u4fee\u590d\u4f5c\u4e3a\u4e3b\u8981\u4efb\u52a1\u5efa\u6a21\uff0c\u4e0d\u4f9d\u8d56\u5b8c\u6574\u8fde\u63a5\u6ce8\u91ca\u3002\u5176\u8f93\u5165\u4e3a\u7a00\u758f\u7684\u8868\u9762\u548c\u9aa8\u67b6\u70b9\u4e91\uff0c\u901a\u8fc7\u5bf9\u539f\u672c\u5c31\u4e0d\u5b8c\u6574\u7684\u6811\u5f15\u5165\u5408\u6210\u7ed3\u6784\u7834\u574f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u4e00\u4e2a\u652f\u6301\u7ed3\u6784\u4fee\u590d\u7684\u8fde\u7eed\u9690\u5f0f\u573a\uff0c\u5e76\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u7684\u9690\u5f0f\u51fd\u6570\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b9e\u73b0\u89e3\u5256\u6807\u6ce8\u4e0e\u80ba\u6bb5\u91cd\u5efa\u7684\u8054\u5408\u63a8\u65ad\u3002", "result": "\u5728Lung3D+\u6570\u636e\u96c6\u4e0a\uff0cTopoField\u663e\u8457\u63d0\u5347\u4e86\u80ba\u90e8\u6811\u7684\u62d3\u6251\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u4e0d\u5b8c\u6574\u7ed3\u6784\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u7684\u89e3\u5256\u6807\u6ce8\u4e0e\u80ba\u6bb5\u91cd\u5efa\u3002\u6240\u63d0\u51fa\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u6bcf\u4f8b\u5904\u7406\u4ec5\u9700\u7ea6\u4e00\u79d2\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u548c\u4e34\u5e8a\u5b9e\u65f6\u5206\u6790\u573a\u666f\u3002", "conclusion": "TopoField\u65e0\u9700\u5b8c\u6574\u8fde\u901a\u6ce8\u91ca\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u9c81\u68d2\u5730\u4fee\u590d\u548c\u5206\u6790\u62d3\u6251\u7ed3\u6784\u4e0d\u5b8c\u6574\u7684\u80ba\u90e8CT\u6811\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u5e26\u6765\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u5177\u5907\u5e7f\u6cdb\u7684\u4e34\u5e8a\u4e0e\u5de5\u7a0b\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.02193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02193", "abs": "https://arxiv.org/abs/2602.02193", "authors": ["Chen Min", "Enze Jiang", "Jishen Peng", "Zheng Ma"], "title": "SSI-DM: Singularity Skipping Inversion of Diffusion Models", "comment": null, "summary": "Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53cd\u6f14\u524d\u52a0\u5c11\u91cf\u566a\u58f0\uff0c\u8df3\u8fc7\u6269\u6563\u6a21\u578b\u53cd\u6f14\u8fc7\u7a0b\u4e2d\u7684\u5947\u5f02\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u6f14\u56fe\u50cf\u7684\u7f16\u8f91\u6027\u4e0e\u91cd\u6784\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u65f6\u9700\u8981\u5c06\u771f\u5b9e\u56fe\u50cf\u53cd\u6f14\u5230\u566a\u58f0\u7a7a\u95f4\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53cd\u6f14\u5f97\u5230\u7684\u662f\u975e\u9ad8\u65af\u5206\u5e03\u566a\u58f0\uff0c\u5bfc\u81f4\u7f16\u8f91\u6027\u5dee\uff0c\u5176\u6839\u672c\u539f\u56e0\u662f\u521d\u671f\u52a0\u566a\u58f0\u9636\u6bb5\u5b58\u5728\u6570\u5b66\u5947\u5f02\u70b9\uff0c\u4f7f\u53cd\u6f14\u95ee\u9898\u672c\u8d28\u4e0a\u4e0d\u9002\u5b9a\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SSI-DM\u65b9\u6cd5\uff0c\u5373\u5728\u6807\u51c6\u53cd\u6f14\u524d\u4eba\u4e3a\u6dfb\u52a0\u5c11\u91cf\u566a\u58f0\uff0c\u4ece\u800c\u8df3\u8fc7\u5947\u5f02\u533a\u57df\uff0c\u518d\u8fdb\u884c\u6b63\u5e38\u7684\u6269\u6563\u53cd\u6f14\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u901a\u7528\u3001\u6613\u7528\uff0c\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u7eb3\u5165\u5404\u7c7b\u578b\u6269\u6563\u6a21\u578b\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u56fe\u50cf\u91cd\u5efa\u548c\u63d2\u503c\u4efb\u52a1\u4e0a\uff0c\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u6709\u66f4\u4f18\u8868\u73b0\uff0c\u53cd\u6f14\u566a\u58f0\u5206\u5e03\u66f4\u81ea\u7136\uff0c\u91cd\u6784\u56fe\u50cf\u4fdd\u771f\u5ea6\u9ad8\uff0c\u7f16\u8f91\u6548\u679c\u597d\u3002", "conclusion": "SSI-DM\u4e3a\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53cd\u6f14\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7406\u8bba\u624e\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u91cd\u5efa\u548c\u7f16\u8f91\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02212", "abs": "https://arxiv.org/abs/2602.02212", "authors": ["Zheyuan Zhou", "Liang Du", "Zixun Sun", "Xiaoyu Zhou", "Ruimin Ye", "Qihao Chen", "Yinda Chen", "Lemiao Qiu"], "title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models", "comment": null, "summary": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MAIN-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u610f\u56fe\u4e0e\u73af\u5883\uff0c\u5bf9\u51b3\u7b56\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u4e09\u8005\u8fdb\u884c\u6df1\u5c42\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86VLA\u4efb\u52a1\u5728\u590d\u6742\u5927\u578b\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLA\u65b9\u6cd5\u5728\u9762\u5bf9\u5305\u542b\u6d77\u91cf\u5197\u4f59\u4f20\u611f\u5668\u4fe1\u606f\u548c\u5b9e\u65f6\u590d\u6742\u4ea4\u4e92\u7684\u4e09\u7ef4\u5f00\u653e\u4e16\u754c\u6216\u5927\u578bPvP\u6e38\u620f\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u5173\u952e\u52a8\u4f5c\u4fe1\u53f7\uff0c\u5bfc\u81f4\u51b3\u7b56\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMAIN-VLA\u6846\u67b6\uff0c\u5305\u542b\u610f\u56fe\u62bd\u8c61\uff08IA\uff09\uff0c\u5c06\u8be6\u7ec6\u8bed\u8a00\u6307\u4ee4\u538b\u7f29\u4e3a\u663e\u5f0f\u8bed\u4e49\u5143\uff1b\u73af\u5883\u8bed\u4e49\u62bd\u8c61\uff08ESA\uff09\uff0c\u5c06\u590d\u6742\u89c6\u89c9\u6d41\u6620\u5c04\u4e3a\u7ed3\u6784\u5316\u7684\u62d3\u6251\u53ef\u4f9b\u6027\u8868\u793a\uff1b\u8fd9\u4e24\u79cd\u62bd\u8c61\u5bf9\u9f50\u540e\uff0c\u80fd\u5728\u65e0\u53c2\u6570\u7684token-pruning\u673a\u5236\u4e0b\uff0c\u6709\u6548\u8fc7\u6ee4\u611f\u77e5\u5197\u4f59\u3002", "result": "\u5728Minecraft\u3001Game for Peace\u3001Valorant\u7b49\u5f00\u653e\u4e16\u754c\u4e0e\u5927\u578bPvP\u73af\u5883\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMAIN-VLA\u5728\u51b3\u7b56\u8d28\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u6c34\u5e73\u3002", "conclusion": "MAIN-VLA\u901a\u8fc7\u6df1\u5ea6\u8bed\u4e49\u62bd\u8c61\u4e0e\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86VLA\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u3001\u9ad8\u8d28\u3001\u5f3a\u6cdb\u5316\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.02214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02214", "abs": "https://arxiv.org/abs/2602.02214", "authors": ["Hongzhou Zhu", "Min Zhao", "Guande He", "Hang Su", "Chongxuan Li", "Jun Zhu"], "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation", "comment": "Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCausal Forcing\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5c11\u6b65\u81ea\u56de\u5f52\uff08AR\uff09\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u7406\u8bba\u4e0e\u5b9e\u9645\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e3a\u4e86\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\uff0c\u4e3b\u6d41\u65b9\u6cd5\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684\u53cc\u5411\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4f46\u4e24\u8005\u5728\u6ce8\u610f\u529b\u7ed3\u6784\u4e0a\u5b58\u5728\u67b6\u6784\u9e3f\u6c9f\uff0c\u4e14\u4f18\u826f\u7684\u7406\u8bba\u652f\u6301\u7f3a\u5931\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f5c\u8005\u6307\u51fa\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u53cc\u5411\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u521d\u59cb\u5316\u65f6\uff0c\u8fdd\u80cc\u4e86\u5e27\u7ea7\u4e00\u4e00\u6620\u5c04\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8f6c\u79fb\u5931\u771f\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u91c7\u7528\u81ea\u56de\u5f52\u578b\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u56e0\u679c\u5f3a\u5236\uff08Causal Forcing\uff09ODE\u521d\u59cb\u5316\uff0c\u4ece\u7406\u8bba\u4e0a\u8865\u9f50\u67b6\u6784\u9e3f\u6c9f\uff0c\u5e76\u91c7\u7528\u6b64\u7b56\u7565\u4f18\u5316AR\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u6d4b\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5176\u4e2d\u5728\u52a8\u6001\u5ea6\uff08Dynamic Degree\uff09\u3001\u89c6\u89c9\u5956\u52b1\uff08VisionReward\uff09\u548c\u6307\u4ee4\u9075\u5faa\uff08Instruction Following\uff09\u5206\u522b\u6bd4SOTA\u63d0\u534719.3%\u30018.7%\u548c16.7%\u3002", "conclusion": "Causal Forcing\u6709\u6548\u5f25\u5408\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u65f6\u9ad8\u8d28\u91cf\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9a8c\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u7a81\u7834\u3002"}}
{"id": "2602.02222", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02222", "abs": "https://arxiv.org/abs/2602.02222", "authors": ["Ruiqi Liu", "Manni Cui", "Ziheng Qin", "Zhiyuan Yan", "Ruoxin Chen", "Yi Han", "Zhiheng Li", "Junkai Chen", "ZhiJin Chen", "Kaiqing Lin", "Jialiang Shen", "Lubin Weng", "Jing Dong", "Yan Wang", "Shu Wu"], "title": "MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection", "comment": null, "summary": "High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the \"superhuman crossover\" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u9020\u667a\u80fd\u751f\u6210\u56fe\u50cf\uff08AIGI\uff09\u68c0\u6d4b\u65b9\u6cd5\u2014\u2014MIRROR\uff0c\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u56fe\u50cf\u6d41\u5f62\uff0c\u800c\u975e\u4f9d\u8d56\u7279\u5b9a\u4f2a\u9020\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793aMIRROR\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u8bc6\u522b\u9ad8\u4fdd\u771f\u4f2a\u9020\u56fe\u50cf\u65b9\u9762\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u4f2a\u9020\u56fe\u7247\u4e0e\u771f\u5b9e\u56fe\u7247\u8d8a\u6765\u8d8a\u96be\u4ee5\u533a\u5206\uff0c\u5a92\u4f53\u5b89\u5168\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002\u73b0\u6709AIGI\u68c0\u6d4b\u4f9d\u8d56\u4e8e\u4eba\u5de5\u7279\u5f81\uff0c\u96be\u4ee5\u9002\u5e94\u751f\u6210\u4f2a\u8ff9\u7684\u4e0d\u65ad\u53d8\u5316\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4eba\u7c7b\u662f\u901a\u8fc7\u5224\u65ad\u56fe\u7247\u4e0e\u771f\u5b9e\u4e16\u754c\u89c4\u5f8b\u7684\u4e00\u81f4\u6027\u6765\u8bc6\u522b\u4f2a\u9020\uff0c\u56e0\u6b64\u5e0c\u671b\u501f\u52a9\u8fd9\u79cd\u7a33\u5b9a\u7684\u8ba4\u77e5\u7279\u5f81\u6539\u5584\u68c0\u6d4b\u3002", "method": "\u4f5c\u8005\u5c06AIGI\u68c0\u6d4b\u89c6\u4e3a\u53c2\u8003\u5bf9\u6bd4\u4efb\u52a1\uff0c\u63d0\u51faMIRROR\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u7528\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u79bb\u6563\u8bb0\u5fc6\u5e93\u6765\u663e\u5f0f\u7f16\u7801\u73b0\u5b9e\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u7ebf\u6027\u7ec4\u5408\u5c06\u8f93\u5165\u6620\u5c04\u5230\u4e0e\u6d41\u5f62\u4e00\u81f4\u7684\u7406\u60f3\u53c2\u8003\u56fe\u50cf\uff0c\u518d\u7528\u6b8b\u5dee\u4f5c\u4e3a\u68c0\u6d4b\u4fe1\u53f7\u3002\u8bbe\u8ba1\u4e86Human-AIGI\u57fa\u51c6\uff0c\u5305\u542b\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u4f2a\u9020\u6837\u672c\uff0c\u4ee5\u8bc4\u6d4b\u6a21\u578b\u80fd\u5426\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u80fd\u529b\u3002", "result": "\u572814\u4e2a\u57fa\u51c6\u4e0a\uff0cMIRROR\u5e73\u5747\u8d85\u8d8a\u4ee5\u5f80\u65b9\u6cd52.1%\u52308.1%\u3002\u5728Human-AIGI\u57fa\u51c6\u4e0a\uff0cMIRROR\u572827\u4e2a\u751f\u6210\u5668\u4e0b\u51c6\u786e\u7387\u8fbe\u523089.6%\uff0c\u8d85\u8fc7\u666e\u901a\u7528\u6237\u548c\u89c6\u89c9\u4e13\u5bb6\uff0c\u5e76\u968f\u7740\u9884\u8bad\u7ec3\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u7ed3\u679c\u9010\u6b65\u8d34\u8fd1\u4eba\u7c7b\u611f\u77e5\u6781\u9650\u3002", "conclusion": "MIRROR\u901a\u8fc7\u5bf9\u9f50\u5230\u771f\u5b9e\u56fe\u50cf\u6d41\u5f62\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4ee5\u5f80\u4f2a\u8ff9\u68c0\u6d4b\u66f4\u5f3a\u6cdb\u5316\u6027\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u9ad8\u96be\u5ea6\u573a\u666f\u4e0b\u68c0\u51faAIGI\uff0c\u5177\u5907\u53d6\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.02223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02223", "abs": "https://arxiv.org/abs/2602.02223", "authors": ["Junchi Feng", "Nikhil Ballem", "Mahya Beheshti", "Giles Hamilton-Fletcher", "Todd Hudson", "Maurizio Porfiri", "William H. Seiple", "John-Ross Rizzo"], "title": "Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type", "comment": null, "summary": "Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u5728\u79fb\u52a8\u548c\u9759\u6001\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u8bbe\u5907\u548cOCR\u5f15\u64ce\u3002\u7ed3\u679c\u663e\u793a\u8bc6\u522b\u51c6\u786e\u7387\u968f\u6b65\u884c\u901f\u5ea6\u3001\u62cd\u6444\u89d2\u5ea6\u7684\u53d8\u5316\u800c\u4e0b\u964d\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u8bc4\u6d4b\u4e86\u79fb\u52a8\u573a\u666f\u4e0b\u7684OCR\u8868\u73b0\u3002Google Vision\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff0cPaddleOCR\u8868\u73b0\u4e5f\u5f88\u63a5\u8fd1\u3002\u624b\u673a\u4e3b\u6444\u50cf\u5934\u548c\u80a9\u90e8\u4f69\u6234\u7684\u6444\u50cf\u5934\u51c6\u786e\u7387\u6700\u9ad8\u3002", "motivation": "\u73b0\u6709OCR\u6280\u672f\u5728\u8f85\u52a9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u5e94\u7528\u4e2d\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u771f\u5b9e\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u6311\u6218\u3002\u4e3a\u66f4\u597d\u4e86\u89e3OCR\u5728\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u8feb\u5207\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "method": "\u5206\u522b\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e0b\u8bc4\u4f30OCR\u6027\u80fd\u3002\u9759\u6001\u6d4b\u8bd5\u6db5\u76d6\u4e0d\u540c\u8ddd\u79bb\uff081-7\u7c73\uff09\u548c\u6c34\u5e73\u89c6\u89d2\uff080-75\u5ea6\uff09\uff1b\u52a8\u6001\u6d4b\u8bd5\u901a\u8fc7\u8c03\u8282\u6b65\u884c\u901f\u5ea6\uff080.8-1.8 m/s\uff09\u548c\u6444\u50cf\u5934\u4f4d\u7f6e\uff08\u5934\u6234\u3001\u80a9\u6234\u3001\u624b\u6301\uff09\uff0c\u5bf9\u624b\u673a\u4e3b\u6444\u3001\u8d85\u5e7f\u89d2\u548c\u667a\u80fd\u773c\u955c\u7684\u6027\u80fd\u8fdb\u884c\u5bf9\u6bd4\u3002\u56db\u6b3e\u4e3b\u6d41OCR\u5f15\u64ce\uff08Google Vision\uff0cPaddleOCR 3.0\uff0cEasyOCR\uff0cTesseract\uff09\u5728\u4e0d\u540c\u8ddd\u79bb\u548c\u89d2\u5ea6\u4e0b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b57\u7b26\u7ea7\u51c6\u786e\u7387\u7528Levenshtein\u8ba1\u7b97\u3002", "result": "\u8bc6\u522b\u51c6\u786e\u7387\u968f\u6b65\u884c\u901f\u5ea6\u548c\u62cd\u6444\u89d2\u5ea6\u589e\u5927\u800c\u4e0b\u964d\u3002Google Vision\u8bc6\u522b\u51c6\u786e\u7387\u6700\u9ad8\uff0cPaddleOCR 3.0\u662f\u6700\u5f3a\u5f00\u6e90\u9009\u624b\u3002\u624b\u673a\u4e3b\u6444\u5728\u6240\u6709\u8bbe\u5907\u4e2d\u8868\u73b0\u6700\u4f73\uff1b\u80a9\u90e8\u6444\u50cf\u5934\u4f69\u6234\u4f4d\u7f6e\u6574\u4f53\u51c6\u786e\u7387\u7565\u9ad8\uff0c\u4f46\u4e0e\u5934\u6234\u3001\u624b\u6301\u5dee\u5f02\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "conclusion": "\u5728\u771f\u5b9e\u79fb\u52a8\u573a\u666f\u4e0b\uff0cOCR\u8bc6\u522b\u6027\u80fd\u4f1a\u53d7\u901f\u5ea6\u548c\u89d2\u5ea6\u5f71\u54cd\u3002Google Vision\u8868\u73b0\u6700\u4f73\uff0cPaddleOCR 3.0\u7d27\u968f\u5176\u540e\u3002\u624b\u673a\u4e3b\u6444\u4f18\u4e8e\u5176\u4ed6\u6444\u50cf\u5934\uff0c\u80a9\u6234\u6444\u50cf\u5934\u7565\u4f18\u4f46\u5dee\u5f02\u4e0d\u5927\u3002\u5b9e\u9a8c\u4e3a\u8f85\u52a9\u6280\u672f\u7684\u5b9e\u9645\u90e8\u7f72\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u9760\u4f9d\u636e\u3002"}}
{"id": "2602.02227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02227", "abs": "https://arxiv.org/abs/2602.02227", "authors": ["Harold Haodong Chen", "Xinxiang Yin", "Wen-Jie Shu", "Hongfei Zhang", "Zixin Zhang", "Chenfei Liao", "Litao Guo", "Qifeng Chen", "Ying-Cong Chen"], "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation", "comment": "Code: https://github.com/EnVision-Research/LatentMorph", "summary": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\\%$ on GenEval and $25\\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\\%$ and $11\\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\\%$ and token consumption by $51\\%$; and (IV) exhibits $71\\%$ cognitive alignment with human intuition on reasoning invocation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6LatentMorph\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u9690\u7a7a\u95f4\u4e2d\u5d4c\u5165\u9690\u6027\u63a8\u7406\u6d41\u7a0b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u7684\u751f\u6210\u4e0e\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u63a8\u7406\u548c\u751f\u6210\u8fc7\u7a0b\u4e0a\u7f3a\u4e4f\u7075\u6d3b\u7684\u3001\u7c7b\u4eba\u5f0f\u7684\u52a8\u6001\u81ea\u6211\u4f18\u5316\u80fd\u529b\u3002\u663e\u5f0f\u63a8\u7406\u65b9\u6cd5\u6d89\u53ca\u591a\u6b21\u6587\u672c\u548c\u56fe\u50cf\u7684\u89e3\u7801\u3001\u91cd\u7f16\u7801\uff0c\u5e26\u6765\u4fe1\u606f\u635f\u5931\u4e0e\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u9690\u5f0f\u63a8\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u667a\u80fd\u3002", "method": "LatentMorph\u5f15\u5165\u4e86\u56db\u4e2a\u8f7b\u91cf\u7ec4\u4ef6\uff1a\uff081\uff09condenser\u7528\u4e8e\u5c06\u4e2d\u95f4\u751f\u6210\u72b6\u6001\u538b\u7f29\u4e3a\u77ed\u671f\u89c6\u89c9\u8bb0\u5fc6\uff1b\uff082\uff09translator\u5c06\u9690\u5f0f\u7684\u63a8\u7406\u7ed3\u679c\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u4fe1\u606f\uff1b\uff083\uff09shaper\u52a8\u6001\u4f18\u5316\u4e0b\u4e00\u4e2a\u56fe\u7247token\u9884\u6d4b\uff1b\uff084\uff09RL-trained invoker\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u4e9b\u63a8\u7406\u90fd\u5728\u8fde\u7eed\u7684\u9690\u7a7a\u95f4\u5b8c\u6210\uff0c\u89c4\u907f\u4e86\u4f20\u7edf\u663e\u5f0f\u63a8\u7406\u7684\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLatentMorph\u5728GenEval\u548cT2I-CompBench\u4e0a\u5206\u522b\u5c06Janus-Pro\u6a21\u578b\u63d0\u534716%\u548c25%\uff1b\u5728WISE\u548cIPV-Txt\u7b49\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u663e\u5f0f\u63a8\u7406\u65b9\u6cd5TwiG 15%\u548c11%\uff1b\u63a8\u7406\u65f6\u95f4\u7f29\u77ed44%\uff0ctoken\u6d88\u8017\u964d\u4f4e51%\uff1b\u53ec\u56de\u63a8\u7406\u4e0e\u4eba\u7c7b\u76f4\u89c9\u7684\u8ba4\u77e5\u4e00\u81f4\u6027\u8fbe71%\u3002", "conclusion": "LatentMorph\u6846\u67b6\u901a\u8fc7\u9690\u6027\u3001\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7075\u6d3b\u6027\u3001\u6548\u7387\u548c\u7c7b\u4eba\u8ba4\u77e5\u80fd\u529b\uff0c\u4e3a\u63a8\u7406\u589e\u5f3a\u578b\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02232", "abs": "https://arxiv.org/abs/2602.02232", "authors": ["Andrea Matteazzi", "Dietmar Tutsch"], "title": "LiFlow: Flow Matching for 3D LiDAR Scene Completion", "comment": null, "summary": "In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e3D\u6fc0\u5149\u96f7\u8fbe\u573a\u666f\u8865\u5168\u7684\u6d41\u5339\u914d\u6846\u67b6LiFlow\uff0c\u6709\u6548\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u7684\u8868\u73b0\uff0c\u5e76\u8fbe\u5230\u4e86\u591a\u9879\u6307\u6807\u4e0a\u7684\u6700\u65b0\u6700\u4f18\u6210\u679c\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\u7ecf\u5e38\u56e0\u906e\u6321\u548c\u8ddd\u79bb\u8fc7\u8fdc\u800c\u7a00\u758f\u4e0d\u5168\uff0c\u5f71\u54cd\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u5b58\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u521d\u59cb\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5206\u5e03\u4e0d\u4e00\u81f4\u5e26\u6765\u7684\u6cdb\u5316\u6027\u548c\u8865\u5168\u8d28\u91cf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\uff08flow matching\uff09\u7684\u70b9\u4e91\u751f\u6210\u8865\u5168\u8fc7\u7a0b\u6a21\u578b\uff0c\u7edf\u4e00\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u521d\u59cb\u6570\u636e\u5206\u5e03\u3002\u5177\u4f53\u5730\uff0c\u6a21\u578b\u4f7f\u7528\u6700\u8fd1\u90bb\u6d41\u5339\u914d\u635f\u5931\u548cChamfer\u8ddd\u79bb\u635f\u5931\uff0c\u5206\u522b\u5f3a\u5316\u70b9\u4e91\u8865\u5168\u7684\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u8986\u76d6\u6548\u679c\u3002", "result": "\u6240\u63d0LiFlow\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u8868\u660e\u5176\u5728\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6d41\u5339\u914d\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u70b9\u4e91\u8865\u5168\u4e2d\u7684\u8bad\u7ec3-\u63a8\u7406\u5206\u5e03\u4e0d\u4e00\u81f4\u96be\u9898\uff0c\u6574\u4f53\u63d0\u5347\u4e86\u573a\u666f\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u3002LiFlow\u4f5c\u4e3a\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u4e3a\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a763D\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.02318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02318", "abs": "https://arxiv.org/abs/2602.02318", "authors": ["Xiang Li", "Yupeng Zheng", "Pengfei Li", "Yilun Chen", "Ya-Qin Zhang", "Wenchao Ding"], "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation", "comment": "Accepted by RA-L", "summary": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS\u2020. With depth integration, DiScene\u2020 attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiScene\uff0c\u4e00\u79cd\u878d\u5408\u591a\u5c42\u6b21\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u578b\u7a00\u758f\u67e5\u8be2\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5360\u7528\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u6743\u8861\u3002\u5bc6\u96c6\u65b9\u6cd5\u5bf9\u7a7a\u4f53\u7d20\u8ba1\u7b97\u6d6a\u8d39\u5927\uff0c\u5e38\u89c4\u7a00\u758f\u67e5\u8be2\u6cd5\u53c8\u5728\u590d\u6742\u591a\u6837\u73af\u5883\u4e0b\u6613\u5931\u6548\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u517c\u5177\u9ad8\u6548\u4e0e\u9c81\u68d2\u6027\u7684\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "DiScene\u63d0\u51fa\u4e24\u5927\u521b\u65b0\uff1a1\uff09\u591a\u5c42\u6b21\u4e00\u81f4\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u4ece\u5927\u6559\u5e08\u6a21\u578b\u5206\u56db\u5c42\uff08\u7f16\u7801\u5668\u3001\u67e5\u8be2\u3001\u7a7a\u95f4\u3001\u951a\u70b9\uff09\u5411\u8f7b\u91cf\u5b66\u751f\u6a21\u578b\u4f20\u9012\u5c42\u6b21\u8868\u793a\uff1b2\uff09\u6559\u5e08\u5f15\u5bfc\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u51b7\u542f\u52a8\u8fc7\u7a0b\uff0c\u52a0\u901f\u6a21\u578b\u6536\u655b\u3002\u540c\u65f6\u5728\u4e0d\u4f9d\u8d56\u6df1\u5ea6\u5148\u9a8c\u548c\u878d\u5408\u6df1\u5ea6\u4fe1\u606f\u4e24\u79cd\u60c5\u51b5\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728Occ-Scannet\u57fa\u51c6\u4e0a\uff0cDiScene\u5728\u65e0\u6df1\u5ea6\u5148\u9a8c\u4e0b\u8fbe\u523023.2 FPS\uff0c\u6027\u80fd\u8d85OPUS\u57fa\u7ebf36.1%\uff0c\u751a\u81f3\u4f18\u4e8e\u6df1\u5ea6\u589e\u5f3a\u7248OPUS\u2020\u3002\u878d\u5408\u6df1\u5ea6\u540e\uff0cDiScene\u2020\u8d85\u8d8aEmbodiedOcc 3.7%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.62\u500d\uff0c\u5e76\u5728Occ3D-nuScenes\u548c\u771f\u5b9e\u73af\u5883\u5747\u6709\u4f18\u8868\u73b0\u3002", "conclusion": "DiScene\u517c\u5177\u9ad8\u6548\u4e0e\u9c81\u68d2\uff0c\u63a8\u8fdb\u4e863D\u5360\u7528\u9884\u6d4b\u9886\u57df\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5bf9\u5b9e\u9645\u673a\u5668\u4eba\u611f\u77e5\u573a\u666f\u5e94\u7528\u6709\u5f88\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.02334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02334", "abs": "https://arxiv.org/abs/2602.02334", "authors": ["Fatemeh Zargarbashi", "Dhruv Agrawal", "Jakob Buhmann", "Martin Guay", "Stelian Coros", "Robert W. Sumner"], "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations", "comment": null, "summary": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u4eba\u4f53\u52a8\u4f5c\u6570\u636e\u4e2d\u7684\u5185\u5bb9\u548c\u98ce\u683c\u8fdb\u884c\u6709\u6548\u89e3\u8026\uff0c\u5b9e\u73b0\u52a8\u4f5c\u98ce\u683c\u8fc1\u79fb\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u9002\u7528\u4e8e\u65b0\u98ce\u683c\u3002", "motivation": "\u4eba\u4f53\u52a8\u4f5c\u6570\u636e\u4e2d\u65e2\u5305\u542b\u8bed\u4e49\u5185\u5bb9\uff0c\u4e5f\u6709\u7ec6\u817b\u7684\u98ce\u683c\u4fe1\u606f\uff0c\u800c\u5982\u4f55\u5c06\u4e24\u8005\u5206\u79bb\u4ee5\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u4e00\u76f4\u5177\u6709\u6311\u6218\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u98ce\u683c\u4e0e\u5185\u5bb9\u65e0\u6cd5\u6709\u6548\u89e3\u8026\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5229\u7528\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08RVQ-VAE\uff09\u6765\u4ece\u7c97\u5230\u7ec6\u5730\u5b66\u4e60\u52a8\u4f5c\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u65b0\u9896\u7684\u4fe1\u606f\u6cc4\u9732\u635f\u5931\uff0c\u4ee5\u4ee3\u7801\u672c\u65b9\u5f0f\u5f3a\u5316\u5185\u5bb9\u4e0e\u98ce\u683c\u7684\u5206\u79bb\u3002\u901a\u8fc7\u91cf\u5316\u4ee3\u7801\u4ea4\u6362\uff08Quantized Code Swapping\uff09\u5b9e\u73b0\u65e0\u9700\u9488\u5bf9\u65b0\u98ce\u683c\u5fae\u8c03\u7684\u52a8\u4f5c\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u4f5c\u98ce\u683c\u8fc1\u79fb\u3001\u98ce\u683c\u79fb\u9664\u3001\u52a8\u4f5c\u878d\u5408\u7b49\u5e94\u7528\u4e0a\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u8026\u4eba\u4f53\u52a8\u4f5c\u7684\u5185\u5bb9\u4e0e\u98ce\u683c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u53ca\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u5177\u5907\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5904\u7406\u65b0\u98ce\u683c\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.02341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02341", "abs": "https://arxiv.org/abs/2602.02341", "authors": ["Zhenpeng Huang", "Jiaqi Li", "Zihan Jia", "Xinhao Li", "Desen Meng", "Lingxue Song", "Xi Chen", "Liang Li", "Limin Wang"], "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization", "comment": "NeurIPS 2025", "summary": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u957f\u89c6\u9891\u6807\u6ce8\u7684\u65b0\u65b9\u6cd5LongVPO\uff0c\u5b9e\u73b0\u4e86\u77ed\u4e0a\u4e0b\u6587\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u8d85\u957f\u89c6\u9891\u7684\u9ad8\u6548\u3001\u7a33\u5065\u7406\u89e3\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u8d85\u957f\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u8ba1\u7b97\u5f00\u9500\u9ad8\u4ee5\u53ca\u7f3a\u4e4f\u957f\u89c6\u9891\u6807\u6ce8\u3002\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u4e14\u80fd\u6269\u5c55\u77ed\u4e0a\u4e0b\u6587\u6a21\u578b\u5230\u957f\u89c6\u9891\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u65b9\u6cd5\u5206\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u5408\u6210\u951a\u5b9a\u77ed\u89c6\u9891\u7247\u6bb5\u7684\u95ee\u9898-\u504f\u597d\u4e09\u5143\u7ec4\uff0c\u7ed3\u5408\u89c6\u89c9\u76f8\u4f3c\u6027\u53ca\u95ee\u9898\u7279\u5f02\u6027\u8fc7\u6ee4\uff0c\u5f31\u5316\u4f4d\u7f6e\u504f\u5dee\uff0c\u4ec5\u8bc4\u4f30\u77ed\u7247\u7247\u6bb5\u964d\u4f4e\u8ba1\u7b97\u91cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u9012\u5f52\u751f\u6210\u957f\u89c6\u9891\u5206\u6bb5\u63cf\u8ff0\uff0c\u518d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u591a\u6bb5\u63a8\u7406\u95ee\u9898\u53ca\u975e\u4f18\u89e3\u7b54\u6848\uff0c\u901a\u8fc7\u591a\u6bb5\u63a8\u7406\u4efb\u52a1\u8c03\u6574\u6a21\u578b\u504f\u597d\u3002\u8bad\u7ec3\u53ea\u970016K\u5408\u6210\u6837\u672c\uff0c\u65e0\u987b\u4eba\u5de5\u6807\u6ce8\u3002", "result": "LongVPO\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u5f00\u6e90\u6a21\u578b\uff0c\u540c\u65f6\u5728\u77ed\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\uff08\u5982MVBench\uff09\uff0c\u65e0\u9700\u589e\u52a0\u5927\u91cf\u7b97\u529b\u6216\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "LongVPO\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u8def\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e14\u65e0\u9700\u9ad8\u989d\u7684\u4eba\u529b\u6210\u672c\u3002"}}
{"id": "2602.02354", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02354", "abs": "https://arxiv.org/abs/2602.02354", "authors": ["Albert Kwok", "Zheyuan Hu", "Dounia Hammou"], "title": "Implicit neural representation of textures", "comment": "Albert Kwok and Zheyuan Hu contributed equally to this work", "summary": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u7d22\u4e86\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5982\u4f55\u4f5c\u4e3a\u65b0\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7528\u4e8e\u7eb9\u7406\u8868\u793a\uff0c\u5e76\u5728UV\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u8fde\u7eed\u64cd\u4f5c\uff0c\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u6d88\u8017\u548c\u6e32\u67d3\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u5b9e\u65f6\u6e32\u67d3\u3001mipmap\u5339\u914d\u548cINR\u7a7a\u95f4\u751f\u6210\u7b49\u5e94\u7528\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5df2\u88ab\u8bc1\u5b9e\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u4f46\u4f20\u7edf\u7eb9\u7406\u8868\u8fbe\u901a\u5e38\u4f9d\u8d56\u79bb\u6563\u65b9\u5f0f\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u8fde\u7eed\u578b\u7eb9\u7406INR\uff0c\u5e76\u7814\u7a76\u5176\u5728\u4e0d\u540c\u6307\u6807\u4e4b\u95f4\u7684\u5e73\u8861\u53ca\u5176\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4f5c\u4e3a\u7eb9\u7406INR\uff0c\u4ee4\u5176\u5728\u8fde\u7eed\u7684UV\u5750\u6807\u7a7a\u95f4\u5185\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u5bf9\u6bd4\u3001\u5206\u6790\u5176\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u6d88\u8017\u548c\u6e32\u67d3\u65f6\u95f4\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\uff0c\u5c06\u6240\u63d0\u51fa\u65b9\u6cd5\u5e94\u7528\u4e8e\u5b9e\u65f6\u6e32\u67d3\u3001mipmap\u62df\u5408\u53caINR\u7a7a\u95f4\u751f\u6210\u7b49\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6240\u8bbe\u8ba1\u7684INR\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u6d88\u8017\u53ca\u6e32\u67d3\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u5b9e\u73b0\u4e86\u826f\u597d\u8868\u73b0\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6307\u6807\u4e4b\u95f4\u8fbe\u5230\u5408\u7406\u6743\u8861\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8fde\u7eed\u9690\u5f0f\u795e\u7ecf\u7eb9\u7406\u8868\u8fbe\u7684\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u5177\u5907\u4f18\u52bf\uff0c\u8fd8\u80fd\u670d\u52a1\u4e8e\u4f17\u591a\u8ba1\u7b97\u673a\u56fe\u5f62\u76f8\u5173\u4efb\u52a1\uff0c\u672a\u6765\u6709\u671b\u5728\u5b9e\u65f6\u6e32\u67d3\u7b49\u9886\u57df\u5f97\u5230\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.02356", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02356", "abs": "https://arxiv.org/abs/2602.02356", "authors": ["Wangduo Xie", "Matthew B. Blaschko"], "title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction", "comment": null, "summary": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u81ea\u9002\u5e94\u5206\u7bb1\uff08NAB\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u77e9\u5f62\u5f62\u72b6\u5148\u9a8c\u878d\u5165\u7a00\u758f\u89c6\u89d2CT\u91cd\u5efa\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9488\u5bf9\u5de5\u4e1aCT\u7a00\u758f\u91c7\u6837\u91cd\u5efa\u7cbe\u5ea6\u6709\u9650\u3001\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u6709\u6548\u5229\u7528\u7269\u4f53\u5f62\u72b6\u5148\u9a8c\uff08\u5982\u5e38\u89c1\u7684\u77e9\u5f62\u7ed3\u6784\uff09\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u7a00\u758f\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u964d\u4f4e\u751f\u4ea7\u6210\u672c\u3002", "method": "\u63d0\u51faNAB\u65b9\u6cd5\uff0c\u5c06\u7a7a\u95f4\u5750\u6807\u6620\u5c04\u4e3a\u201c\u5206\u7bb1\u201d\u5411\u91cf\u7a7a\u95f4\u3002\u636e\u70b9\u91c7\u7528\u57fa\u4e8e\u5e73\u79fb\u53cc\u66f2\u6b63\u5207\u51fd\u6570\u5dee\u503c\u7684\u65b0\u578b\u5206\u7bb1\u673a\u5236\uff0c\u8be5\u673a\u5236\u53ef\u7ed5\u8f93\u5165\u5e73\u9762\u6cd5\u5411\u65cb\u8f6c\uff0c\u5b9e\u73b0\u5bf9\u77e9\u5f62\u7ed3\u6784\u7684\u9ad8\u6548\u8868\u8fbe\u3002\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u81ea\u52a8\u8c03\u6574\u5206\u7bb1\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5927\u5c0f\u3001\u9661\u5ced\u5ea6\u548c\u89d2\u5ea6\uff09\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8870\u51cf\u7cfb\u6570\u3002\u5e73\u6ed1\u5ea6\u53ef\u8c03\uff0c\u9002\u5e94\u66f4\u590d\u6742\u5f62\u72b6\u3002", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cNAB\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u5c06\u5206\u7bb1\u51fd\u6570\u63a8\u5e7f\u5230\u66f4\u901a\u7528\u8868\u8fbe\u540e\uff0c\u5bf9\u533b\u5b66\u6570\u636e\u96c6\u4e5f\u8868\u73b0\u7a33\u5065\u3002\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "NAB\u521b\u65b0\u6027\u5730\u5c06\u77e9\u5f62\u7ed3\u6784\u5f62\u72b6\u5148\u9a8c\u96c6\u6210\u8fdb\u795e\u7ecf\u7f51\u7edcCT\u91cd\u5efa\u6846\u67b6\uff0c\u4e3a\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5f15\u5165\u5148\u9a8c\u4fe1\u606f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5177\u5907\u826f\u597d\u5b9e\u9a8c\u8868\u73b0\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.02370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02370", "abs": "https://arxiv.org/abs/2602.02370", "authors": ["Uma Meleti", "Jeffrey J. Nirschl"], "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes", "comment": "Accepted for publication at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578bSNGP\uff0c\u7528\u4e8e\u63d0\u5347\u6570\u5b57\u75c5\u7406\u9886\u57df\u4e2d\u5bf9\u5f02\u5e38\u6837\u672c\u8bc6\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6570\u5b57\u75c5\u7406\u9886\u57df\u4e2d\u5bf9\u5f02\u5e38\u5206\u5e03\u7684\u6837\u672c\uff08OOD\uff09\u8bc6\u522b\u80fd\u529b\u8f83\u5f31\uff0c\u4e14\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u964d\u4f4e\u4e86\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u5e76\u5f71\u54cd\u91c7\u7eb3\u3002\u89e3\u51b3\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u53ca\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u5b89\u5168\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u5b9e\u73b0\u5e76\u6d4b\u8bd5\u4e86Spectral-normalized Neural Gaussian Process\uff08SNGP\uff09\uff0c\u901a\u8fc7\u5bf9\u795e\u7ecf\u7f51\u7edc\u6dfb\u52a0\u8c31\u5f52\u4e00\u5316\uff0c\u5e76\u5c06\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u66ff\u6362\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u5c42\u6765\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\uff0c\u5e76\u4e0e\u786e\u5b9a\u6027\u6a21\u578b\u548cMonte Carlo Dropout\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "SNGP\u5728\u516d\u4e2a\u6d89\u53ca\u4e09\u7c7b\u751f\u7269\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u5206\u5e03\u5185\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8d28\u91cf\u548c\u5f02\u5e38\u5206\u5e03(OOD)\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "SNGP\u53ca\u7c7b\u4f3c\u6a21\u578b\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u90e8\u7f72\u5b89\u5168\u6027\u5e76\u589e\u5f3a\u75c5\u7406\u5b66\u5bb6\u7684\u4fe1\u4efb\u3002"}}
{"id": "2602.02380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02380", "abs": "https://arxiv.org/abs/2602.02380", "authors": ["Yibin Wang", "Yuhang Zang", "Feng Han", "Jiazi Bu", "Yujie Zhou", "Cheng Jin", "Jiaqi Wang"], "title": "Unified Personalized Reward Model for Vision Generation", "comment": "Website: https://codegoat24.github.io/UnifiedReward/flex", "summary": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u89c6\u89c9\u751f\u6210\u7684\u4e2a\u6027\u5316\u5956\u52b1\u6a21\u578bUnifiedReward-Flex\uff0c\u5728\u63a8\u7406\u4e0a\u66f4\u52a0\u7075\u6d3b\u3001\u5177\u5907\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5728\u4e3b\u89c2\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u9762\u524d\u8868\u73b0\u66f4\u4f18\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5bf9\u9f50\u548c\u5224\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u4ee5\u7edf\u4e00\u6807\u51c6\u8bc4\u5224\u751f\u6210\u5185\u5bb9\uff0c\u5ffd\u7565\u4e86\u5177\u4f53\u5185\u5bb9\u548c\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u4e3b\u89c2\u504f\u597d\u7684\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\u3002\u4f5c\u8005\u53d7\u4eba\u7c7b\u8bc4\u5224\u8fc7\u7a0b\u542f\u53d1\uff0c\u5c1d\u8bd5\u5f15\u5165\u4e2a\u6027\u5316\u4e0e\u7075\u6d3b\u5224\u522b\u673a\u5236\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u4e0d\u8db3\u3002", "method": "UnifiedReward-Flex\u878d\u5408\u4e86\u5956\u52b1\u5efa\u6a21\u4e0e\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5177\u4f53\u505a\u6cd5\u5305\u62ec\uff1a\u5148\u7ed3\u5408\u751f\u6210\u5185\u5bb9\u53ca\u5176\u8bed\u4e49\u610f\u56fe\u52a8\u6001\u6784\u5efa\u5206\u5c42\u8bc4\u4f30\u6807\u51c6\uff0c\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5148\u8fdbVLM\u84b8\u998f\u63a8\u7406\u3001\u504f\u597d\u5bf9\u76f4\u63a5\u4f18\u5316\uff09\u63d0\u5347\u6a21\u578b\u5224\u522b\u6027\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "result": "UnifiedReward-Flex\u96c6\u6210\u8fdb\u56fe\u50cf\u3001\u89c6\u9891\u751f\u6210\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u5f3a\u5316\u63a8\u7406\u80fd\u529b\u548c\u4e3b\u89c2\u504f\u597d\u7684\u533a\u5206\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UnifiedReward-Flex\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u590d\u6742\u3001\u591a\u5143\u7684\u89c6\u89c9\u751f\u6210\u573a\u666f\uff0c\u4e3a\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.02388", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02388", "abs": "https://arxiv.org/abs/2602.02388", "authors": ["Rajalaxmi Rajagopalan", "Debottam Dutta", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization", "comment": null, "summary": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u62c9\u8fd1\u751f\u6210\u7ed3\u679c\u4e0e\u7528\u6237\u5fc3\u4e2d\u76ee\u6807\u56fe\u50cf\u7684\u8ddd\u79bb\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bed\u8a00\u63cf\u8ff0\u7684\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u5728\u7528\u6237\u96be\u4ee5\u7528\u8bed\u8a00\u7cbe\u51c6\u8868\u8fbe\u76ee\u6807\u56fe\u50cf\u7ec6\u8282\u65f6\uff0c\u751f\u6210\u56fe\u50cf\u4e0e\u7528\u6237\u7406\u60f3\u56fe\u50cf\u5b58\u5728\u5dee\u8ddd\u3002\u4f5c\u8005\u5e0c\u671b\u7814\u7a76\u5982\u4f55\u5728\u8bed\u8a00\u65e0\u6cd5\u7ee7\u7eed\u7f29\u5c0f\u5dee\u8ddd\u65f6\uff0c\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u8fdb\u4e00\u6b65\u4f18\u5316\u751f\u6210\u7ed3\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51faMultiBO\uff08Multi-Choice Preferential Bayesian Optimization\uff09\u7b97\u6cd5\u3002\u65b9\u6cd5\u6d41\u7a0b\u662f\uff1a1\uff09\u57fa\u4e8e\u5df2\u6709\u751f\u6210\u56fe\u50cf$x^{p*}$\uff0c\u751f\u6210$K$\u4e2a\u65b0\u56fe\u50cf\uff1b2\uff09\u7528\u6237\u5728\u8fd9\u4e9b\u56fe\u50cf\u4e2d\u7ed9\u51fa\u76f8\u5bf9\u504f\u597d\u53cd\u9988\uff1b3\uff09\u65b9\u6cd5\u5229\u7528\u53cd\u9988\u8c03\u4f18\u6269\u6563\u6a21\u578b\uff0c\u518d\u751f\u6210\u4e0b\u4e00\u8f6e$K$\u5f20\u56fe\u7247\u3002\u591a\u8f6e\u53cd\u9988\u8fed\u4ee3\u540e\uff0c\u5c06\u751f\u6210\u66f4\u52a0\u63a5\u8fd1\u7528\u6237\u5185\u5fc3\u7406\u60f3\u7684\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u901a\u8fc7$30$\u4f4d\u7528\u6237\u7684\u4e3b\u89c2\u8bc4\u5206\u4ee5\u53ca\u4e0e$5$\u4e2a\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u7684\u5b9a\u91cf\u5bf9\u6bd4\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u4e0e\u7528\u6237\u5fc3\u4e2d\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff1a\u7528\u591a\u9009\u5f0f\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u7f3a\u4e4f\u660e\u786e\u76ee\u6807\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u9010\u6b65\u5f97\u5230\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u4e2a\u6027\u5316\u751f\u6210\u56fe\u50cf\u3002\u8fd9\u4e00\u65b9\u6cd5\u5f88\u9002\u5408\u4e8e\u65e0\u6cd5\u76f4\u63a5\u63cf\u8ff0\u76ee\u6807\u7684\u751f\u6210\u573a\u666f\u3002"}}
{"id": "2602.02393", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02393", "abs": "https://arxiv.org/abs/2602.02393", "authors": ["Ruiqi Wu", "Xuanhua He", "Meng Cheng", "Tianyu Yang", "Yong Zhang", "Zhuoliang Kang", "Xunliang Cai", "Xiaoming Wei", "Chunle Guo", "Chongyi Li", "Ming-Ming Cheng"], "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory", "comment": "14 pages, 8 figures", "summary": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Infinite-World\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e0b\u8d85\u5343\u5e27\u56fe\u50cf\u7684\u8fde\u8d2f\u89c6\u89c9\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e16\u754c\u5efa\u6a21\u957f\u671f\u7a7a\u95f4\u4e0e\u52a8\u4f5c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u867d\u7136\u5728\u5408\u6210\u6570\u636e\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u7531\u4e8e\u59ff\u6001\u4f30\u7b97\u566a\u58f0\u548c\u89c2\u6d4b\u70b9\u7a00\u5c11\uff0c\u5efa\u6a21\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u63d0\u5347\u5bf9\u771f\u5b9e\u4e16\u754c\u3001\u957f\u5e8f\u5217\u89c6\u9891\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "1. \u5f15\u5165\u5206\u5c42\u65e0\u59ff\u6001\u8bb0\u5fc6\u538b\u7f29\u5668\uff08HPMC\uff09\uff0c\u7528\u9012\u5f52\u65b9\u6cd5\u5c06\u5386\u53f2\u4fe1\u606f\u538b\u7f29\u4e3a\u5b9a\u957f\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\u3002\n2. \u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u4f5c\u79bb\u6563\u6a21\u5757\uff0c\u5c06\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u79bb\u6563\u4e3a\u4e09\u6001\u903b\u8f91\uff0c\u589e\u5f3a\u5bf9\u566a\u58f0\u8f68\u8ff9\u7684\u9c81\u68d2\u6027\u3002\n3. \u91c7\u7528\u91cd\u8bbf\u4e30\u5bcc\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u7d27\u51d1\u6570\u636e\u96c6\u6fc0\u53d1\u6a21\u578b\u957f\u7a0b\u56de\u73af\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5ba2\u89c2\u6307\u6807\u548c\u7528\u6237\u5b9e\u9a8c\u4e2d\uff0cInfinite-World\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u52a8\u4f5c\u53ef\u63a7\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Infinite-World\u6a21\u578b\u80fd\u6709\u6548\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u8bb0\u5fc6\u548c\u751f\u6210\u957f\u5e8f\u5217\uff0c\u63d0\u5347\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.02401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02401", "abs": "https://arxiv.org/abs/2602.02401", "authors": ["Xinshun Wang", "Peiming Li", "Ziyi Wang", "Zhongbin Fang", "Zhichao Deng", "Songtao Wu", "Jason Li", "Mengyuan Liu"], "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation", "comment": null, "summary": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6Superman\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u4e0e\u57fa\u4e8e\u9aa8\u67b6\u7684\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u7ed3\u5408\uff0c\u7a81\u7834\u73b0\u6709\u8fd0\u52a8\u5206\u6790\u65b9\u6cd5\u5272\u88c2\u95ee\u9898\uff0c\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8fd0\u52a8\u5206\u6790\u9886\u57df\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u611f\u77e5\u6a21\u578b\u53ea\u80fd\u8f93\u51fa\u6587\u672c\uff0c\u65e0\u6cd5\u751f\u6210\u52a8\u4f5c\uff1b2\uff09\u751f\u6210\u6a21\u578b\u96be\u4ee5\u4ece\u89c6\u89c9\u8f93\u5165\u7406\u89e3\u52a8\u4f5c\u4e14\u591a\u53ea\u5904\u7406\u9759\u6001\u59ff\u6001\uff1b3\uff09\u73b0\u6709\u52a8\u4f5c\u8bcd\u5178\u4ec5\u57fa\u4e8e\u9aa8\u67b6\uff0c\u7f3a\u4e4f\u89c6\u89c9\u8054\u7cfb\u3002\u8fd9\u9020\u6210\u4e86\u611f\u77e5\u4e0e\u751f\u6210\u4efb\u52a1\u5272\u88c2\uff0c\u6a21\u578b\u9002\u7528\u8303\u56f4\u6709\u9650\u3002", "method": "Superman\u6846\u67b6\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u4e00\u662f\u63d0\u51fa\u4e86\u89c6\u89c9\u5f15\u5bfc\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\uff08Vision-Guided Motion Tokenizer\uff09\uff0c\u5229\u75283D\u9aa8\u67b6\u548c\u89c6\u89c9\u6570\u636e\u7684\u51e0\u4f55\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u8054\u5408\u5b66\u4e60\uff0c\u5f62\u6210\u7edf\u4e00\u52a8\u4f5c\u8bcd\u5178\uff1b\u4e8c\u662f\u5728\u8fd9\u4e00\u52a8\u4f5c\u8bed\u8a00\u57fa\u7840\u4e0a\uff0c\u8bad\u7ec3\u4e86\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\uff0c\u540c\u65f6\u5904\u7406\u89c6\u9891\u611f\u77e5\uff083D\u9aa8\u67b6\u59ff\u6001\u4f30\u8ba1\uff09\u548c\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u751f\u6210\uff08\u9884\u6d4b\u3001\u63d2\u503c\uff09\u7b49\u4efb\u52a1\u3002", "result": "\u5728Human3.6M\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\uff0cSuperman\u65b9\u6cd5\u5728\u6240\u6709\u8fd0\u52a8\u5206\u6790\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230SOTA\u6216\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u5de5\u4f5c\u8bc1\u660e\u4e86\u5c06\u89c6\u89c9\u4e0e\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4eba\u4f53\u52a8\u4f5c\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.02408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02408", "abs": "https://arxiv.org/abs/2602.02408", "authors": ["Jiaxing Qiu", "Kaihua Hou", "Roxana Daneshjou", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning", "comment": null, "summary": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReasonEdit\uff0c\u8fd9\u662f\u9996\u4e2a\u5141\u8bb8\u7528\u6237\u5728\u7f16\u8f91\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u65f6\u89e3\u91ca\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u7f16\u8f91\u65b9\u6cd5\u3002ReasonEdit\u5728\u96be\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4e3b\u6d41VLM\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u5668\u4e3b\u8981\u53ea\u9488\u5bf9\u7b80\u5355\u4efb\u52a1\uff0c\u5f88\u5c11\u5904\u7406\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u573a\u666f\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u6a21\u578b\u548c\u4eba\u8fdb\u884c\u6df1\u5165\u63a8\u7406\u3002\u7f3a\u4e4f\u9762\u5411\u63a8\u7406\u4efb\u52a1\u7684\u9ad8\u6548\u7f16\u8f91\u65b9\u6cd5\u6210\u4e3a\u4e86\u5236\u7ea6VLM\u5b9e\u9645\u5e94\u7528\u7684\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51faReasonEdit\u65b9\u6cd5\uff0c\u5728\u6a21\u578b\u7f16\u8f91\u65f6\u8ba9\u7528\u6237\u540c\u6b65\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5b58\u50a8\u5230codebook\u4e2d\u3002\u63a8\u7406\u68c0\u7d22\u57fa\u4e8e\u4e00\u79cd\u53d7\u7f51\u7edc\u79d1\u5b66\u542f\u53d1\u7684\u62d3\u6251\u5747\u8861\u591a\u6a21\u6001\u5d4c\u5165\u65b0\u65b9\u6cd5\uff0c\u4ece\u800c\u5728\u63a8\u7406/\u7f16\u8f91\u65f6\u53ea\u68c0\u7d22\u4e0e\u5f53\u524d\u95ee\u9898\u6700\u76f8\u5173\u7684\u63a8\u7406\u4e8b\u5b9e\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41VLM\u548c\u591a\u4e2a\u4ee5\u63a8\u7406\u4e3a\u6838\u5fc3\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cReasonEdit\u5728\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7f16\u8f91\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c06\u4eba\u7c7b\u63a8\u7406\u878d\u5165\u6a21\u578b\u7f16\u8f91\u4e0d\u4ec5\u53ef\u4ee5\u7ea0\u9519\uff0c\u8fd8\u80fd\u6781\u5927\u63d0\u5347\u7f16\u8f91\u7684\u6cdb\u5316\u6548\u679c\uff0c\u4e3a\u63a8\u7406\u7c7b\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u77e5\u8bc6\u6ce8\u5165\u548c\u4fee\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02409", "abs": "https://arxiv.org/abs/2602.02409", "authors": ["Abid Hassan", "Tuan Ngo", "Saad Shafiq", "Nenad Medvidovic"], "title": "Catalyst: Out-of-Distribution Detection via Elastic Scaling", "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($\u03b3$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $\u03b3$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.", "AI": {"tldr": "Catalyst\u662f\u4e00\u79cd\u7528\u4e8e\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u7684\u540e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5377\u79ef\u5c42\u6c60\u5316\u524d\u7684\u901a\u9053\u7edf\u8ba1\u91cf\u6765\u589e\u5f3a\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002", "motivation": "\u76ee\u524d\u4f18\u79c0\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u5206\u7c7b\u5668\u8f93\u51fa\u7684logits\u6216\u5168\u5c40\u5e73\u5747\u6c60\u5316\u540e\u7684\u7279\u5f81\u5411\u91cf\uff0c\u4f46\u5ffd\u7565\u4e86\u6c60\u5316\u524d\u7279\u5f81\u56fe\u7684\u901a\u9053\u7edf\u8ba1\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u53ef\u80fd\u643a\u5e26\u4e30\u5bcc\u4e14\u4e92\u8865\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u52a0\u4ee5\u5229\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faCatalyst\u6846\u67b6\uff0c\u5b9e\u65f6\u4ece\u6c60\u5316\u524d\u7279\u5f81\u56fe\u7684\u539f\u59cb\u901a\u9053\u7edf\u8ba1\u91cf\uff08\u5982\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u6700\u5927\u6fc0\u6d3b\u503c\uff09\u8ba1\u7b97\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u56e0\u5b50\u03b3\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u5206\u6570\u8fdb\u884c\u5f39\u6027\u878d\u5408\uff08\u4e58\u6027\u8c03\u5236\uff09\uff0c\u8fdb\u4e00\u6b65\u533a\u5206\u5206\u5e03\u5185\u4e0e\u5206\u5e03\u5916\u6837\u672c\u3002Catalyst\u53ef\u4ee5\u7ed3\u5408\u57fa\u4e8elogit\u3001\u7279\u5f81\u6216\u8ddd\u79bb\u7684\u591a\u79cdOOD\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "Catalyst\u5927\u5e45\u63d0\u5347\u5404\u7c7bOOD\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u5728CIFAR-10\uff08ResNet-18\uff09\u4e0a\u5e73\u5747\u8bef\u62a5\u7387\u4e0b\u964d32.87%\uff0cCIFAR-100\uff08ResNet-18\uff09\u4e0b\u964d27.94%\uff0cImageNet\uff08ResNet-50\uff09\u4e0b\u964d22.25%\u3002", "conclusion": "\u6c60\u5316\u524d\u901a\u9053\u7edf\u8ba1\u5305\u542b\u91cd\u8981\u4fe1\u606f\uff0cCatalyst\u80fd\u5145\u5206\u6316\u6398\u5176\u6f5c\u529b\uff0c\u5bf9\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u826f\u597d\u517c\u5bb9\u6027\u548c\u63d0\u5347\u6548\u679c\u3002"}}
{"id": "2602.02426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02426", "abs": "https://arxiv.org/abs/2602.02426", "authors": ["Simon-Olivier Duguay", "Hugo Baudchon", "Etienne Lalibert\u00e9", "Helene Muller-Landau", "Gonzalo Rivas-Torres", "Arthur Ouaknine"], "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond", "comment": "22 pages, 8 figures", "summary": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u70ed\u5e26\u68ee\u6797\u6811\u51a0\u8bc6\u522b\u96be\u9898\uff0c\u63d0\u51fa\u4e86SelvaMask\u6570\u636e\u96c6\u548c\u57fa\u4e8eVFM\u7684\u5206\u5272\u65b0\u65b9\u6cd5\uff0c\u5728\u5bc6\u96c6\u68ee\u6797\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u6811\u51a0\u5206\u5272\u65b9\u6cd5\uff08\u5982transformer\u6a21\u578b\uff09\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5728\u70ed\u5e26\u68ee\u6797\u7b49\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4f9d\u7136\u8f83\u5dee\uff0c\u4e9f\u9700\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u53d1\u5e03\u4e86SelvaMask\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u5904\u65b0\u70ed\u5e26\u68ee\u6797\u76848,800\u591a\u4e2a\u4eba\u5de5\u7cbe\u7ec6\u6807\u6ce8\u7684\u6811\u51a0\uff0c\u5e76\u8fdb\u884c\u6ce8\u91ca\u4e00\u81f4\u6027\u5206\u6790\u3002\u65b9\u6cd5\u4e0a\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u4e0e\u9886\u57df\u4e13\u7528\u68c0\u6d4b\u63d0\u793a\uff08detection-prompter\uff09\u7684\u6a21\u5757\u5316\u68c0\u6d4b-\u5206\u5272\u6d41\u6c34\u7ebf\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5bc6\u96c6\u70ed\u5e26\u68ee\u6797\u6570\u636e\u548c\u5916\u90e8\u6e29\u5e26\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u9886\u5148\u7684\u5206\u5272\u8868\u73b0\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u901a\u7528\u6a21\u578b\u548c\u7aef\u5230\u7aef\u6709\u76d1\u7763\u65b9\u6848\u3002", "conclusion": "SelvaMask\u4e0d\u4ec5\u6210\u4e3a\u6811\u51a0\u5206\u5272\u7684\u91cd\u8981\u57fa\u51c6\uff0c\u4e5f\u63a8\u52a8\u4e86\u70ed\u5e26\u4e43\u81f3\u5168\u7403\u68ee\u6797\u666e\u9002\u76d1\u6d4b\u7684\u53d1\u5c55\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u5c06\u516c\u5f00\u91ca\u653e\u3002"}}
{"id": "2602.02437", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02437", "abs": "https://arxiv.org/abs/2602.02437", "authors": ["Dianyi Wang", "Chaofan Ma", "Feng Han", "Size Wu", "Wei Song", "Yibin Wang", "Zhixiong Zhang", "Tianhang Wang", "Siyuan Wang", "Zhongyu Wei", "Jiaqi Wang"], "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing", "comment": null, "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniReason\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4ee5\u534f\u8c03\u6587\u672c\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u6df1\u5ea6\u63a8\u7406\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\uff0c\u4e14\u5e38\u5c06\u6587\u672c\u751f\u6210\u4e0e\u56fe\u50cf\u7f16\u8f91\u5206\u5f00\uff0c\u7f3a\u4e4f\u63a8\u7406\u534f\u540c\u3002\u4f5c\u8005\u8bd5\u56fe\u6253\u7834\u4e24\u8005\u5272\u88c2\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u751f\u6210\u4efb\u52a1\u89c6\u4e3a\u6709\u77e5\u8bc6\u589e\u5f3a\u7684\u89c4\u5212\u73af\u8282\uff0c\u5f15\u5165\u9690\u5f0f\u7ea6\u675f\uff1b\u7f16\u8f91\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8c03\u6574\uff0c\u901a\u8fc7\u81ea\u53cd\u7ea0\u9519\u3002\u4e24\u8005\u5728\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u878d\u5408\uff0c\u5e76\u7c7b\u6bd4\u4eba\u7c7b\u5148\u89c4\u5212\u518d\u5b8c\u5584\u7684\u8ba4\u77e5\u6d41\u7a0b\u3002\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u5305\u542b\u4e94\u5927\u77e5\u8bc6\u9886\u57df\u3001\u7ea630\u4e07\u6837\u672c\u7684\u63a8\u7406\u6570\u636e\u96c6\u4ee5\u53ca\u89c6\u89c9\u81ea\u7ea0\u6b63\u6570\u636e\u96c6\u3002", "result": "UniReason\u5728WISE\u3001KrisBench\u4e0eUniREditBench\u7b49\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e00\u822c\u5408\u6210\u4efb\u52a1\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "UniReason\u5b9e\u73b0\u4e86\u751f\u6210\u4e0e\u7f16\u8f91\u80fd\u529b\u7684\u63a8\u7406\u7edf\u4e00\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u62d3\u5c55\u6f5c\u529b\u3002"}}
{"id": "2602.02471", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.02471", "abs": "https://arxiv.org/abs/2602.02471", "authors": ["Edwin Kys", "Febian Febian"], "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network", "comment": "8 pages, 3 figures, 1 table", "summary": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u4e8eSwin U-Net\u548cTransformer\u7684\u81ea\u52a8\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u95e8\u63a7\u68c0\u6d4b\u673a\u5236\u663e\u8457\u51cf\u5c11\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u865a\u5047\u6b63\u4f8b\uff08\u5e7b\u89c9\uff09\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5206\u5272\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u6d88\u9664\u65e0\u7ed3\u6784\u5207\u7247\u4e2d\u7684\u9519\u8bef\u5206\u5272\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5206\u5272\u6a21\u578b\u5728\u653e\u5c04\u6cbb\u7597\u4e2d\u5e38\u51fa\u73b0\u5931\u771f\u5206\u5272\uff08\u5982\u5e7b\u89c9\uff09\u95ee\u9898\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u76ee\u6807\u7ed3\u6784\u7684\u5207\u7247\u4e2d\u66f4\u4e3a\u4e25\u91cd\u3002\u4e3a\u63d0\u5347\u5206\u5272\u7ed3\u679c\u7684\u89e3\u5256\u5b66\u5408\u7406\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e9f\u9700\u5f00\u53d1\u80fd\u6709\u6548\u6291\u5236\u65e0\u6548\u5206\u5272\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eSwin U-Net\u7ed3\u6784\uff0c\u878d\u5408\u591a\u5934Transformer\u673a\u5236\uff0c\u5f15\u5165\u8de8\u5207\u7247\u4e0a\u4e0b\u6587\u4fe1\u606f\u6574\u5408\uff0c\u5e76\u65b0\u589e\u5e76\u884c\u68c0\u6d4b\u5206\u652f\uff0c\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u673a\u8fdb\u884c\u5207\u7247\u7ea7\u68c0\u6d4b\uff0c\u4e0e\u50cf\u7d20\u7ea7\u5206\u5272\u6d41\u5171\u540c\u4f5c\u7528\u3002\u68c0\u6d4b\u8f93\u51fa\u7528\u4e8e\u95e8\u63a7\u5206\u5272\u9884\u6d4b\uff0c\u6291\u5236\u89e3\u5256\u65e0\u6548\u5207\u7247\u4e2d\u7684\u865a\u5047\u6b63\u4f8b\uff0c\u5e76\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u5207\u7247\u7ea7Tversky\u635f\u5931\u51fd\u6570\u4ee5\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728The Cancer Imaging Archive\u7684Prostate-Anatomical-Edge-Cases\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u95e8\u63a7\u6a21\u578b\u5728Dice\u635f\u5931\u4e0a\uff080.013\u00b10.036\uff09\u663e\u8457\u4f18\u4e8e\u65e0\u95e8\u63a7\u57fa\u7ebf\u6a21\u578b\uff080.732\u00b10.314\uff09\uff0c\u68c0\u6d4b\u6982\u7387\u4e0e\u89e3\u5256\u7ed3\u6784\u5b9e\u9645\u5b58\u5728\u5f3a\u76f8\u5173\uff0c\u51e0\u4e4e\u5b8c\u5168\u53bb\u9664\u4e86\u865a\u5047\u5206\u5272\u3002\u65e0\u95e8\u63a7\u6a21\u578b\u5728\u6240\u6709\u5207\u7247\u4e0a\u5747\u8868\u73b0\u51fa\u66f4\u5927\u6ce2\u52a8\u548c\u6301\u7eed\u865a\u5047\u6b63\u4f8b\u3002", "conclusion": "\u57fa\u4e8e\u68c0\u6d4b\u95e8\u63a7\u7684\u5206\u5272\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5206\u5272\u7684\u89e3\u5256\u5408\u7406\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u4e34\u5e8a\u653e\u5c04\u6cbb\u7597\u81ea\u52a8\u52fe\u753b\u5de5\u4f5c\u6d41\u4e2d\u5177\u5907\u6781\u5927\u5e94\u7528\u524d\u666f\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u9884\u6d4b\u4e14\u4e0d\u5f71\u54cd\u6709\u6548\u533a\u57df\u7684\u5206\u5272\u8d28\u91cf\u3002"}}
{"id": "2602.02493", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02493", "abs": "https://arxiv.org/abs/2602.02493", "authors": ["Zehong Ma", "Ruihan Xu", "Shiliang Zhang"], "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "comment": "Project Pages: https://zehong-ma.github.io/PixelGen/", "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.", "AI": {"tldr": "PixelGen\u662f\u4e00\u79cd\u5728\u50cf\u7d20\u7a7a\u95f4\u76f4\u63a5\u751f\u6210\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u6269\u6563\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u611f\u77e5\u635f\u5931\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u8d85\u8d8a\u4e86\u4e3b\u6d41\u9690\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u50cf\u7d20\u7a7a\u95f4\u6269\u6563\uff08Pixel Diffusion\uff09\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u6709\u6f5c\u529b\u907f\u514dVAE\u7b49\u9690\u7a7a\u95f4\u65b9\u6cd5\u4e2d\u7684\u74f6\u9888\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u7ef4\u50cf\u7d20\u7a7a\u95f4\u5b58\u5728\u5927\u91cf\u611f\u77e5\u65e0\u5173\u4fe1\u53f7\uff0c\u4f18\u5316\u96be\u5ea6\u5927\uff0c\u751f\u6210\u8d28\u91cf\u843d\u540e\u4e8e\u9690\u7a7a\u95f4\uff08Latent Space\uff09\u6269\u6563\u6a21\u578b\u3002\u8be5\u5de5\u4f5c\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u50cf\u7d20\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u4e0d\u5f15\u5165\u9690\u53d8\u91cf\u548c\u989d\u5916\u9636\u6bb5\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u7b80\u5316\u751f\u6210\u8303\u5f0f\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86PixelGen\u6846\u67b6\uff0c\u5728\u50cf\u7d20\u7a7a\u95f4\u76f4\u63a5\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u5f15\u5165\u4e24\u79cd\u611f\u77e5\u635f\u5931\uff1a\u4e00\u79cd\u662fLPIPS\u635f\u5931\uff0c\u7528\u4e8e\u63d0\u5347\u5c40\u90e8\u7eb9\u7406\u4e0e\u7ed3\u6784\u7684\u611f\u77e5\u8d28\u91cf\uff1b\u53e6\u4e00\u79cd\u662f\u57fa\u4e8eDINO\u7279\u5f81\u7684\u611f\u77e5\u635f\u5931\uff0c\u7528\u4e8e\u589e\u5f3a\u5168\u5c40\u8bed\u4e49\u8868\u8fbe\u3002\u611f\u77e5\u76d1\u7763\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u5bf9\u4eba\u7c7b\u66f4\u6709\u610f\u4e49\u7684\u611f\u77e5\u6d41\u5f62\u3002", "result": "PixelGen\u5728ImageNet-256\u6570\u636e\u96c6\u4e0a\u65e0\u9700classifier-free guidance\uff0c\u4ec5\u752880\u8f6e\u8bad\u7ec3\u5c31\u5b9e\u73b0\u4e865.11\u7684FID\u5206\u6570\uff0c\u4f18\u4e8e\u5f53\u524d\u5f3a\u5927\u7684\u9690\u7a7a\u95f4\u6269\u6563\u57fa\u7ebf\u3002\u5728\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\uff0c\u83b7\u5f970.79\u7684GenEval\u5206\u6570\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "PixelGen\u7ed3\u5408\u4e86\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u7684\u7aef\u5230\u7aef\u7b80\u6d01\u6027\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u80fd\u529b\uff0c\u907f\u514d\u4e86VAE\u3001\u9690\u7a7a\u95f4\u7b49\u590d\u6742\u673a\u5236\uff0c\u662f\u4e00\u79cd\u66f4\u7b80\u5355\u800c\u9ad8\u6548\u7684\u751f\u6210\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u590d\u73b0\u548c\u63a8\u5e7f\u3002"}}
