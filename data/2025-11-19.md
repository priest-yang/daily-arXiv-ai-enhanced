<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 136]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation](https://arxiv.org/abs/2511.13744)
*Zhijie Qiao,Zhong Cao,Henry X. Liu*

Main category: cs.CV

TL;DR: 论文提出了nuCarla数据集，一个大型、标准化、适用于闭环仿真的BEV感知数据集，旨在促进端到端自动驾驶的研究和发展。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要基于真实世界、非交互式场景，支持开环学习，对闭环仿真的帮助有限，且缺乏有利于学习中间表示的数据集，导致端到端闭环模型效果远逊色于规则方法。

Method: 作者构建了nuCarla数据集，基于CARLA模拟器，完全兼容nuScenes格式，规模与nuScenes相当但类别分布更加均衡，并可直接应用于闭环仿真，内置高性能BEV骨干网络作为基准。

Result: nuCarla实现了与最先进检测性能相媲美的高性能BEV基线，为闭环E2E研究提供了可用数据和模型基准。

Conclusion: nuCarla的开放数据和模型大大加速了闭环端到端自动驾驶模型的发展，为安全可靠的自动驾驶研究奠定基础。

Abstract: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving.

</details>


### [2] [Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition](https://arxiv.org/abs/2511.13775)
*Dongdong Zhao,Ranxin Fang,Changtian Song,Zhihui Liu,Jianwen Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过估计模型不确定性和采用两阶段检测方法，显著提升了开放集识别(OSR)中对未知类别样本的判别能力。


<details>
  <summary>Details</summary>
Motivation: 开放集识别要求模型既要识别已知类别，又能拒绝未知类别，但当未知样本与已知类语义相似时，特征空间重叠容易导致模型过度自信，把未知类错分为已知类，损害决策边界的清晰，需解决该过度自信问题。

Method: 框架包含：（1）扰动式不确定性估计模块，通过对参数可控扰动，生成多样化预测以量化不确定性；（2）未知检测模块，采用基于学习的不同分类器，构成两阶段流程，将前一步的不确定性用于区分已知与未知样本。

Result: 在三个公开数据集上的实验结果表明，该方法在OSR任务中性能优于现有方法。

Conclusion: 显式缓解类别特征空间重叠带来的过度自信，有效提升了开放集识别对未知类别的检测与区分能力。

Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.

</details>


### [3] [Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection](https://arxiv.org/abs/2511.13784)
*Yogesh Kumar,Anand Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的面向新类别的时序建模方法，解决了仅凭极少标注样本对视频新物体进行检测的问题，通过过滤高置信度特征实现高效和鲁棒的特征传播，无需复杂的区域提议或特定任务训练。


<details>
  <summary>Details</summary>
Motivation: 传统的视频目标检测依赖大量标注数据和复杂的区域提议方法，在面对新类别和标注稀缺时表现较差。FSVOD任务旨在用极少的样本检测视频中的新目标，但面临时序一致性、遮挡和外观变化等难题，因此亟需高效泛化的检测方法。

Method: 作者提出基于新物体感知的时序特征传播方法，通过特征过滤选出高置信度物体特征在帧间传播，结合少样本条件下训练的检测和分类网络，在减少噪声积累的同时提高检测准确性，且摆脱了对显式目标管道提议的依赖。

Result: 在FSVOD-500、FSYTV-40、VidOR和VidVRD数据集的5-shot设定下，都实现了3.7%到5.3%的AP提升。1-shot、3-shot、10-shot等更极端设定下同样获得了明显性能改进。

Conclusion: 所提方法能有效提升少样本场景下视频新类别物体检测的准确性和时序鲁棒性，同时大幅简化了时空建模流程，实用性强。代码已开源。

Abstract: Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit

</details>


### [4] [FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching](https://arxiv.org/abs/2511.13794)
*Huayi Zhu,Xiu Shu,Youqiang Xiong,Qiao Liu,Rui Chen,Di Yuan,Xiaojun Chang,Zhenyu He*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的多模态图像融合方法，提升了推理速度和结构一致性，并能适应多任务场景。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图像融合方法往往依赖于特定任务模型，导致训练成本高、可扩展性差，而生成式方法则因采样路径复杂推理速度慢。因此，需要一种高效通用且推理高效的新方法。

Method: 作者将图像融合建模为从源模态到融合图像分布的概率转运，采用Flow Matching范式提升采样效率。通过收集多模型融合结果并用任务感知选择函数选出最优伪标签，解决无高质量监督的问题。提出Fusion Refiner模块，分治增强伪标签退化成分。多任务场景下，结合弹性权重固化和经验回放机制，保持跨任务性能与持续学习能力。

Result: 方法在多个图像融合任务上取得了具有竞争力的表现，同时加快了推理速度，保持了模型轻量化。

Conclusion: 该方法兼具高效性、结构一致性和跨任务泛化能力，适用于多种图像融合任务，是轻量高效的图像融合新方案。

Abstract: Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.

</details>


### [5] [A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion](https://arxiv.org/abs/2511.13795)
*Weiying Shen,Hao Yu,Yu Dong,Pan Liu,Yu Han,Xin Wen*

Main category: cs.CV

TL;DR: 该论文提出了一种无需轨迹的实时交通事故检测框架Mapfusion，利用扩散模型直接对道路分段动态地图进行建模和比对，实现高效准确的事故识别。


<details>
  <summary>Details</summary>
Motivation: 当前事故检测方法往往依赖于车辆轨迹获取和追踪，这在实际应用中受限于传感器精度、遮挡等问题。为了克服这些局限，作者尝试从道路分段层面直接处理交通动态数据，简化数据需求并提升检测效率。

Method: 提出了一个两阶段的事故检测框架。第一阶段采用基于扩散模型的Mapfusion，通过对道路分段动态图进行加噪和去噪过程（纯噪声到真实地图的转换），结合时间嵌入和ControlNet引入背景上下文，生成合理的未来道路分段动态图。第二阶段通过比较监测到的分段地图与扩散模型生成的正常演化地图，实现事故检测。

Result: 在无事故数据上训练后，Mapfusion能够准确生成逼真的道路分段动态图，并且在不同采样间隔下表现稳健。真实事故数据实验表明，所提两阶段方法能够准确检测实际交通事故。

Conclusion: 该研究证实，无需传统轨迹追踪，仅利用道路分段动态地图结合扩散模型，可以实现高精度、实时的交通事故检测，为主动安全管理和交通效率提升提供了新技术路径。

Abstract: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.

</details>


### [6] [Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model](https://arxiv.org/abs/2511.13800)
*Huiwen Wu,Shuo Zhang,Yi Liu,Hongbin Ye*

Main category: cs.CV

TL;DR: 本文提出了一种针对地震数据的自适应双网格基础模型训练策略（ADATG），结合了Hilbert编码，能有效处理地震数据的高低频特征，提升视觉地震基础模型的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的视觉基础模型（如ViT）在处理地震数据时，往往采用顺序化分词，难以同时捕捉地震数据中的高频和低频信息，影响模型在地震场景下的表现。因此需要设计面向地震数据特点的专用预训练策略和编码方法。

Method: 1. 首先利用频谱分解将地震数据拆分为高频和低频成分；2. 采用分层Hilbert编码有效地表示和保留空间结构信息；3. 借鉴ViT中的频率原则，采取自适应训练策略，先关注粗粒度（低频）信息，再逐步聚焦细粒度（高频）特征。

Result: 通过大量实验验证，提出的方法在地震图像预训练任务中取得了有效且高效的表现，优于传统的ViT基础模型。

Conclusion: 本研究证明了针对地震数据特性设计数据编码和训练策略的重要性，提高了地震视觉基础模型的预训练质量，为相关AI模型在地震领域的应用奠定了基础。

Abstract: Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.

</details>


### [7] [Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video](https://arxiv.org/abs/2511.13802)
*Filippo Cenacchi. Longbing Cao,Mitchell McEwan,Deborah Richards*

Main category: cs.CV

TL;DR: 本文提出了一种利用短时对脸视频进行非语言化、被动性痴呆筛查的方法，并构建了公开新数据集YT DemTalk，取得了优异筛查准确率。


<details>
  <summary>Details</summary>
Motivation: 目前大多数痴呆筛查方法依赖语音或有脚本的访谈，需要语言转录和临床参与，局限于受控环境，难以大规模、跨文化推广。作者希望开发一种无需依赖语言与文本，更自然、可扩展的视频分析方法。

Method: 通过分析面对摄像头的视频中的面部时序微动态，包括眨眼、小幅口颌运动、视线变化和细微头部调整，建立可解释的面部微动态时序数据，并将其转化为统计特征用于筛查。每个视频窗口以各动作的活动占比编码，最终训练轻量级分类器。作者还构建了300个视频片段组成的YT DemTalk数据集（150例自述痴呆、150对照）用于测试。

Result: 在YT DemTalk数据集上，消融实验表明视线不稳定性和口颌动态信息量最大。轻量级模型在痴呆预测上达到了AUROC 0.953、AP 0.961、F1分数0.851、准确率0.857的优异表现。

Conclusion: 只需分析自然、非语言化视频中的面部微动态即可实现高效的痴呆筛查，无需语音或主动干预，为大规模、跨语种、跨文化的痴呆筛查提供了新的方向。

Abstract: We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.

</details>


### [8] [Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark](https://arxiv.org/abs/2511.13853)
*Xinxin Liu,Zhaopan Xu,Kai Wang,Yong Jae Lee,Yuzhang Shang*

Main category: cs.CV

TL;DR: 该论文提出了Gen-ViRe，一个用于评估视频生成模型推理能力的新基准，系统地测量视频模型的多步规划和抽象推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型通过Chain-of-Thought (CoT) 能进行符号推理，但它们难以实现贴近真实世界的、物理连续动态的推理。现有视频生成模型虽能生成连续画面，却缺乏对其推理能力的系统评估，尤其是在多步规划、算法逻辑和抽象推理等方面。缺乏针对CoF推理能力的评估，阻碍了模型能力的深入理解与改进。

Method: 作者提出Gen-ViRe基准，基于认知科学和AI实际应用，将视频生成模型的推理能力分解为六大认知维度和24个子任务。通过多源数据整理、极简提示和基于视觉大模型辅助的评测标准，实现了对视频生成模型推理能力的量化评估。

Result: 在SOTA（最先进）的视频生成模型上的实验显示，这些模型虽有较好视觉效果，但在推理深度方面存在明显不足。Gen-ViRe为评估模型推理能力建立了基准和诊断工具。

Conclusion: Gen-ViRe作为首个系统性评估视频模型推理能力的框架，为模型能力深入剖析和后续改进提供了方法和参考，有助于推进真正具备世界模拟能力的AI系统的发展。

Abstract: While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.

</details>


### [9] [RSPose: Ranking Based Losses for Human Pose Estimation](https://arxiv.org/abs/2511.13857)
*Muhammed Can Keles,Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

TL;DR: 本文针对基于热图的人体姿态估计算法存在的损失函数与评价指标不一致、热图不平衡等问题，提出了基于排序（ranking-based）的新损失函数RSPose，有效提升了关键点定位精度和mAP评分，在多个主流数据集（COCO、CrowdPose、MPII）上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主流热图（heatmap）方法常用的均方误差（MSE）损失未能充分关注关节点的精度定位，并且损失和最终评估指标（如mAP）不一致，影响模型表现，因此亟需能更好对齐损失与评价，对高置信和精准定位进行优化的新方法。

Method: 作者提出了采用基于排序（ranking-based）的损失函数，从理论与实验角度提升热图峰值对位置的精准定位能力，并增强置信分与定位质量的相关性。该损失适用于一维和二维热图，能和mAP评价指标良好对齐，改进了Non-Maximum Suppression（NMS）过程中的准确实例选择。

Result: 新提出的RSPose方法结合ViTPose-H模型在COCO-val集合上取得了79.9 mAP，超越以往SOTA（state-of-the-art）；在SimCC Resnet-50骨干网上，COCO-val集mAP提升1.5点至73.6。多个数据集（COCO、CrowdPose、MPII）实验均验证了其优越性。

Conclusion: RSPose基于排序的损失函数首次有效对齐了pose估计中的损失项和评估指标，显著提升了热图法人体姿态估计的精度与鲁棒性，为后续研究提供了新的思路和方向。

Abstract: While heatmap-based human pose estimation methods have shown strong performance, they suffer from three main problems: (P1) "Commonly used Mean Squared Error (MSE)" Loss may not always improve joint localization because it penalizes all pixel deviations equally, without focusing explicitly on sharpening and correctly localizing the peak corresponding to the joint; (P2) heatmaps are spatially and class-wise imbalanced; and, (P3) there is a discrepancy between the evaluation metric (i.e., mAP) and the loss functions.
  We propose ranking-based losses to address these issues.
  Both theoretically and empirically, we show that our proposed losses are superior to commonly used heatmap losses (MSE, KL-Divergence). Our losses considerably increase the correlation between confidence scores and localization qualities, which is desirable because higher correlation leads to more accurate instance selection during Non-Maximum Suppression (NMS) and better Average Precision (mAP) performance. We refer to the models trained with our losses as RSPose.
  We show the effectiveness of RSPose across two different modes: one-dimensional and two-dimensional heatmaps, on three different datasets (COCO, CrowdPose, MPII).
  To the best of our knowledge, we are the first to propose losses that align with the evaluation metric (mAP) for human pose estimation.
  RSPose outperforms the previous state of the art on the COCO-val set and achieves an mAP score of 79.9 with ViTPose-H, a vision transformer model for human pose estimation.
  We also improve SimCC Resnet-50, a coordinate classification-based pose estimation method, by 1.5 AP on the COCO-val set, achieving 73.6 AP.

</details>


### [10] [Segmenting Collision Sound Sources in Egocentric Videos](https://arxiv.org/abs/2511.13863)
*Kranti Kumar Parida,Omar Emara,Hazel Doughty,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了全新的碰撞声音源分割（CS3）任务，旨在从视频帧中分割出产生碰撞声音的物体，并提出了弱监督的音频条件分割方法，在EPIC-CS3和Ego4D-CS3两个基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类能通过声音识别物体属性，受到这个能力启发，作者关注于如何自动分割出引起碰撞声的物体，尤其是在视频环境复杂、物体小、交互短暂的第一视角视频中，现有方法难以有效利用音频和视觉信息联合识别碰撞物体。

Method: 提出了基于弱监督的音频条件分割方法，结合了大模型CLIP和SAM2，并利用第一视角中的手部物体（即手里拿的物品）作为关键线索，辅助锁定潜在碰撞声音源。

Result: 在EPIC-CS3和Ego4D-CS3两个新的基准上，所提方法在mIoU指标上分别超过竞争方法3倍和4.7倍。

Conclusion: 文中方法有效提升了碰撞声声音源的视觉分割性能，显示出多模态结合和弱监督学习在实际复杂场景中的巨大潜力。

Abstract: Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.
  To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.

</details>


### [11] [GRLoc: Geometric Representation Regression for Visual Localization](https://arxiv.org/abs/2511.13864)
*Changyang Li,Xuejian Ma,Lixiang Liu,Zhan Li,Qingan Yan,Yi Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉定位方法，将绝对位姿回归（APR）的问题转化为从图像中回归3D几何表示，并由此推断相机位姿，显著提升了定位的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 传统的APR模型像黑盒一样，直接从图像回归6自由度位姿，容易过拟合训练视角，缺乏对真实3D几何的理解，因此需要一种更几何驱动的方法以提升泛化能力。

Method: 受新视图合成技术启发，作者提出GRR（Geometric Representation Regression）范式：先预测两个解耦的几何量——射线束方向（用于估算相机旋转）和点云地图（用于估算相机平移），最终利用微分的确定性求解器将几何量转换为6-DoF位姿。该方法在视觉-几何映射和最终位姿计算间做了解耦，使神经网络更好地嵌入几何先验。

Result: 在7-Scenes和Cambridge Landmarks数据集上均获得了当前最优的定位精度，且实验表明旋转与平移的显式解耦能显著提升效果。

Conclusion: 将APR反向为几何表示回归，并解耦旋转与平移，不仅提升了位姿估计性能，也验证了引入几何先验有助于建构更健壮、可泛化的视觉定位系统。

Abstract: Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.

</details>


### [12] [H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2511.13869)
*Xueyang Li,Zongren Wang,Yuliang Zhang,Zixuan Pan,Yu-Jen Chen,Nishchal Sapkota,Gelei Xu,Danny Z. Chen,Yiyu Shi*

Main category: cs.CV

TL;DR: 本研究开发了专用于膀胱癌复发预测的多序列MRI数据集，并提出了新的H-CNN-ViT模型，显著提升了复发检测性能。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌术后复发率极高（最高可达78%），需要精确的影像随访。然而，多序列增强MRI扫描的解读受术后多种改变影响，即便经验丰富的放射科医生也存在较大挑战。AI辅助诊断工具在复发预测方面已展现潜力，但缺少专门的数据集制约了进一步创新。

Method: 作者首先推出了专门针对膀胱癌复发预测的多序列、多模态MRI数据集，为领域提供了可用的基准数据。随后提出了H-CNN-ViT模型，该模型采用分支架构对每种成像模态独立处理，并通过层次化门控注意力机制，实现CNN（本地特征）与ViT（全局特征）有效融合。

Result: 在自建数据集上，所提H-CNN-ViT模型AUC达到78.6%，优于现有最优方法。

Conclusion: 该研究不仅为膀胱癌复发预测提供了急需的数据资源，还展示了新模型在特征融合和复发检测任务上的领先性能，有望助力AI在临床膀胱癌随访中的实际应用。

Abstract: Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT}.

</details>


### [13] [QwenCLIP: Boosting Medical Vision-Language Pretraining via LLM Embeddings and Prompt tuning](https://arxiv.org/abs/2511.13876)
*Xiaoyang Wei,Camille Kurtz,Florence Cloppet*

Main category: cs.CV

TL;DR: 本文提出了QwenCLIP，一种将大语言模型（LLM）嵌入作为文本编码模块的新型视觉-语言框架，有效提升医学领域长文本与医学图像的对齐和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在医学和计算机视觉任务中表现优越，但其文本编码器最大仅支持77个token，无法很好处理内容丰富的长篇医学报告。虽然医学领域的专用BERT模型延长了输入长度至512 tokens，但仍然有限，且语义理解能力较浅。需求是设计一种能高效处理长文本医学报告并精准与医学影像对齐的模型。

Method: 作者将CLIP的文本编码器替换为LLM嵌入（如Qwen3-Embedding），利用LLM更大的上下文窗口和更丰富的文本表示能力。同时，引入可学习prompt提升跨模态对齐能力。该方法使模型能够捕捉长文本医学报告中的复杂语义信息。

Result: QwenCLIP能更全面地理解和表征长形式的临床文本，在医学影像-文本对齐及相关辐射学基准任务上表现显著优于现有方法。

Conclusion: 用LLM级别的文本嵌入和可学习的prompt，QwenCLIP解决了医疗场景下长文本理解与跨模态匹配的问题，为医学影像与报告的结合提供更强大、通用的基础设施。

Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong generalization for vision-language tasks in computer vision and medical domains, yet its text encoder accepts only up to 77 tokens, which limits its ability to represent long and information-rich radiology reports. Recent adaptations using domain-specific encoders, such as PubMedBERT or ClinicalBERT, mitigate this issue by leveraging medical corpora, but remain constrained by their limited input length (typically 512 tokens) and relatively shallow semantic understanding. To address these limitations, we propose QwenCLIP, a vision-language framework that replaces CLIP's text encoder with a large language model (LLM)-based embedding module (e.g., Qwen3-Embedding) and introduces learnable prompts to enhance cross-modal alignment. By leveraging the extended context window and richer representations of LLMs, QwenCLIP captures comprehensive medical semantics from long-form clinical text, substantially improving medical image-text alignment and downstream performance on radiology benchmarks. Our code is publicly available at https://github.com/Wxy-24/QwenCLIP.

</details>


### [14] [Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection](https://arxiv.org/abs/2511.13877)
*Pandiyaraju V,Abishek Karthik,Jaspin K,Kannan A,Jaime Lloret*

Main category: cs.CV

TL;DR: 提出一种结合EfficientNet和VGG19的新型混合模型，用于椎间盘退变的医学影像分类，创新性引入力伪牛顿提升层和稀疏特征约减层，大幅提升性能，显著优于传统迁移学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法在高维医学影像分析中存在特征表达有限和冗余问题，尤其难以捕捉精细解剖特征，因此有必要设计更精细的结构来提升模型对医学影像的诊断能力。

Method: 采用EfficientNet和VGG19混合架构，并设计“伪牛顿提升层”对特征权重进行细致优化，并引入“稀疏特征约减层”去除多余特征，从而强化特征表达。

Result: 提出的模型在椎间盘退变DICOM图像分类中取得precision 0.9、recall 0.861、F1 0.88、loss 0.18、准确率88.1%的成绩，全面优于EfficientNet基线模型。

Conclusion: 该模型有效解决了传统迁移学习在医学影像高维特征处理上的不足，提升了分类性能，为自动化医学影像诊断提供了有力工具。

Abstract: This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.

</details>


### [15] [VLMs Guided Interpretable Decision Making for Autonomous Driving](https://arxiv.org/abs/2511.13881)
*Xin Hu,Taotao Jing,Renran Tian,Zhengming Ding*

Main category: cs.CV

TL;DR: 本文发现当前视觉-语言模型（VLMs）用于自动驾驶中的决策存在表现不一致问题，提出将VLMs从决策生成者转变为语义增强器，并提出多模态交互架构和后处理模块，有效提升决策准确性和可解释性，在两个自动驾驶基准上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用VLMs做自动驾驶决策的方法依赖手工提示词，且表现不稳定，缺乏鲁棒性和泛化能力，难以满足真实世界需求。本文正是关注VLMs在复杂场景下的局限性，期望提升其在决策可靠性和可解释性方面的能力。

Method: 作者首先评估了现有主流开源VLMs在自动驾驶高层决策任务上的表现并发现其缺陷。进而提出将VLMs用作场景语义增强器，通过其强大的场景理解能力为已有视觉基准注入结构化、语义丰富的场景描述，并在此基础上设计多模态交互架构，将视觉与语言特征融合，提升决策和解释能力。此外，设计后处理模块，用于进一步提升预测的可靠性。

Result: 在两个自动驾驶公开基准上进行大量实验，结果显示该方法在决策准确性和解释性上均达到当前最佳（state-of-the-art）水平。

Conclusion: 通过创新性地将VLMs作用由决策者转为语义增强器，提出的多模态架构和后处理机制，为VLMs在自动驾驶等安全关键场景中可靠、可解释的应用提供了新思路和有效方案。

Abstract: Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.

</details>


### [16] [Revisiting Data Scaling Law for Medical Segmentation](https://arxiv.org/abs/2511.13883)
*Yuetan Chu,Zhongyi Han,Gongning Luo,Xin Gao*

Main category: cs.CV

TL;DR: 本论文探讨了医学解剖分割任务中，训练数据集规模与分割性能之间的幂律关系，并通过新的图像增强方法，提升数据利用效率和模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习中数据规模与性能的关系已较为明确，但医学解剖分割领域对此研究较少，且该领域获取标注数据成本高，因此探索其扩展性规律和提升数据利用效率的方法具有重要意义。

Method: 作者分析了15个分割任务、4种成像模态中的数据规模扩展规律，并引入两种形变引导增强策略：随机弹性形变和基于配准的形变。同时，提出了一种新的可扩展图像增强方法，通过基于配准的测地子空间生成真实形变，以丰富训练样本。

Result: 实验显示，无论是基于已注册形变还是生成的新形变，均大幅提升了数据利用效率。所提生成形变方法不仅性能最佳，还能加快收敛速度，而且在不增加额外数据的情况下超越幂律扩展趋势。

Conclusion: 本文工作加深了对医学图像分割扩展性和拓扑变化影响的理解，为该领域更低标注和计算成本的高效模型开发提供了新思路。

Abstract: The population loss of trained deep neural networks often exhibits power law scaling with the size of the training dataset, guiding significant performance advancements in deep learning applications. In this study, we focus on the scaling relationship with data size in the context of medical anatomical segmentation, a domain that remains underexplored. We analyze scaling laws for anatomical segmentation across 15 semantic tasks and 4 imaging modalities, demonstrating that larger datasets significantly improve segmentation performance, following similar scaling trends. Motivated by the topological isomorphism in images sharing anatomical structures, we evaluate the impact of deformation-guided augmentation strategies on data scaling laws, specifically random elastic deformation and registration-guided deformation. We also propose a novel, scalable image augmentation approach that generates diffeomorphic mappings from geodesic subspace based on image registration to introduce realistic deformation. Our experimental results demonstrate that both registered and generated deformation-based augmentation considerably enhance data utilization efficiency. The proposed generated deformation method notably achieves superior performance and accelerated convergence, surpassing standard power law scaling trends without requiring additional data. Overall, this work provides insights into the understanding of segmentation scalability and topological variation impact in medical imaging, thereby leading to more efficient model development with reduced annotation and computational costs.

</details>


### [17] [Uni-Hema: Unified Model for Digital Hematopathology](https://arxiv.org/abs/2511.13889)
*Abdul Rehman,Iqra Rasool,Ayesha Imran,Mohsen Ali,Waqas Sultani*

Main category: cs.CV

TL;DR: 本论文提出了Uni-Hema，一个整合检测、分类、分割、形态预测和推理等多任务能力的统一模型，用于数字化血液病理学的多病种分析，能够在多个影像和文本任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的血液病理影像模型往往仅针对单一任务或单一模态，无法在疾病多样性和复杂性的实际临床场景下提供统一、高效的多任务推理。作者希望通过统一模型解决多疾病、多任务、多模态分析需求，提升数字化血液病理学的应用价值。

Method: 作者提出Uni-Hema模型，融合视觉与文本（多模态）输入，并支持检测、分类、分割、形态预测和视觉问答等多种任务。Uni-Hema构建于Hema-Former多模态模块之上，整合了46个公开数据集（包括70万+图像、2.1万问答对），实现任务层级的多粒度表示学习。

Result: 在大规模实验中，Uni-Hema在多种血液学任务上表现出与单任务/单数据集模型相当或更优的性能。同时，模型在细胞级别提供了具备解释性的形态学推理结果。

Conclusion: Uni-Hema树立了多任务、多模态数字化血液病理学的新标准，能够为单细胞级别的疾病分析与解释带来更高效、更全面的支持。相关代码将开放共享。

Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.

</details>


### [18] [Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models](https://arxiv.org/abs/2511.13891)
*Seyed Mohamad Ali Tousi,John A. Lory,G. N. DeSouza*

Main category: cs.CV

TL;DR: 本论文提出了首个用于侵蚀沟（Ephemeral Gullies）检测的弱监督方法，并公开了相关数据集和代码，在遥感高分辨率图像上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 侵蚀沟极易在农田中形成，对农业生态环境造成威胁，但由于其生命周期短、很难获得精准标注，传统计算机视觉和机器学习方法在自动检测上效果有限。

Method: 采用基于远程遥感图像和视觉-语言模型（VLM）的弱监督学习管道，通过利用VLM内置知识和教师-学生模型框架，教师利用VLM生成的噪声标签学习，学生则结合教师标签和噪声感知损失函数进行弱监督学习。

Result: 公开了第一个专用于半监督侵蚀沟检测的数据集（覆盖13年，18000余张高分辨率遥感图）。实验数据表明该方法在弱监督条件下明显优于VLM和标注模型本身。

Conclusion: 提出的弱监督方法和数据集为基于遥感影像的侵蚀沟自动检测带来了新途径，为相关研究和应用奠定了基础。

Abstract: Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.

</details>


### [19] [Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors](https://arxiv.org/abs/2511.13897)
*Mert Onur Cakiroglu,Idil Bilge Altun,Zhihe Lu,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: 本文提出了一种基于压缩视频中的运动矢量（MV），可拓展且与模型无关的生成视频时序真实性评价框架。MV直接从如H.264或HEVC编码视频中提取，用来分析生成视频与真实视频在运动层面的差异，并用于下游任务增强。


<details>
  <summary>Details</summary>
Motivation: 当前生成视频模型普遍在时序真实性上存在不足，主流评价指标更侧重空间外观，对运动捕捉不敏感。研究需要一个专注于时序真实性，且高效、易于扩展的评测方法。

Method: 作者利用压缩视频流中的编解码器生成的MV，提取其统计特征，通过KL散度、JS散度和Wasserstein距离来量化生成视频与真实视频的运动分布差异。还通过运动可视化与MV桶热力图揭示细节缺陷。此外，将MV信息以多种方式与RGB特征融合，用于下游判别分类任务。

Result: 实验表明，不同生成模型在运动真实性上表现不同，Pika和SVD在部分指标上最接近真实视频，CogVideo偏差最大。MV与RGB的融合能极大提升真假视频判别准确率，部分模型准确率可达99%。

Conclusion: 压缩域MV为评价和诊断生成视频运动缺陷、增强判别模型时序推理能力提供了高效且有效的方法。开源实现进一步便于该方法的实际应用。

Abstract: Temporal realism remains a central weakness of current generative video models, as most evaluation metrics prioritize spatial appearance and offer limited sensitivity to motion. We introduce a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted directly from compressed video streams. Codec-generated MVs from standards such as H.264 and HEVC provide lightweight, resolution-consistent descriptors of motion dynamics. We quantify realism by computing Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between MV statistics of real and generated videos. Experiments on the GenVidBench dataset containing videos from eight state-of-the-art generators reveal systematic discrepancies from real motion: entropy-based divergences rank Pika and SVD as closest to real videos, MV-sum statistics favor VC2 and Text2Video-Zero, and CogVideo shows the largest deviations across both measures. Visualizations of MV fields and class-conditional motion heatmaps further reveal center bias, sparse and piecewise constant flows, and grid-like artifacts that frame-level metrics do not capture. Beyond evaluation, we investigate MV-RGB fusion through channel concatenation, cross-attention, joint embedding, and a motion-aware fusion module. Incorporating MVs improves downstream classification across ResNet, I3D, and TSN backbones, with ResNet-18 and ResNet-34 reaching up to 97.4% accuracy and I3D achieving 99.0% accuracy on real-versus-generated discrimination. These findings demonstrate that compressed-domain MVs provide an effective temporal signal for diagnosing motion defects in generative videos and for strengthening temporal reasoning in discriminative models. The implementation is available at: https://github.com/KurbanIntelligenceLab/Motion-Vector-Learning

</details>


### [20] [SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing](https://arxiv.org/abs/2511.13904)
*Yuqiang Lin,Sam Lockyer,Florian Stanek,Markus Zarbock,Adrian Evans,Wenbin Li,Nic Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的实时多摄像头车辆跟踪（MCVT）系统SAE-MCVT，实现了在城市级别场景下的实时和高效车辆跟踪。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统中，车辆跟踪对于异常检测、交通密度估算等应用很重要，然而当前研究多关注精度，忽视了实际部署所需的实时性与可扩展性，特别是在摄像头数量众多的城市场景下，该需求尤为迫切。

Method: 提出SAE-MCVT框架，采用边缘与中心协同架构。边缘设备负责摄像头视频流的对象检测、跟踪、地理映射和特征提取，只传输精简的元数据（车辆位置和深度特征）到中心节点。中心节点利用自监督学习得到的相邻摄像头时空联系，执行跨摄像头关联。

Result: 在RoundaboutHD数据集上，系统可实时处理2K 15FPS视频流，并获得61.2的IDF1得分，证明了其实时性和可扩展性。

Conclusion: SAE-MCVT是首个适用于城市级部署的可扩展实时多摄像头车辆跟踪系统，兼顾了准确率、实时性与大规模应用场景下的实用性。

Abstract: In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.

</details>


### [21] [Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles](https://arxiv.org/abs/2511.13909)
*Chalamalasetti Kranti*

Main category: cs.CV

TL;DR: 本文评估了多模态大语言模型在理解道路安全规范方面的能力，发现其在安全推理方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，AI系统需严格遵守道路安全规范。因此，了解多模态大语言模型是否能准确理解道路安全知识具有重要意义。

Method: 作者从学校教科书中收集交通标志和道路安全规范的图像，构建了一个试点数据集，并在零样本设定下评估了多模态大语言模型的理解能力。

Result: 实验结果表明，现有模型在道路安全推理方面有明显难度，表现出与人类学习之间的差距。

Conclusion: 多模态大语言模型目前尚难胜任类似道路安全场景的推理任务，揭示了现有模型与实际需求之间的差距，为后续相关研究提供了参考。

Abstract: Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.

</details>


### [22] [Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding](https://arxiv.org/abs/2511.13924)
*Qingyang Yan,Guangyao Chen,Yixiong Zou*

Main category: cs.CV

TL;DR: 本文发现传统基于强化学习的Chain-of-Thought（CoT）推理在视觉定位任务中，尤其是CoT输出变长或变复杂时，可能反而降低性能。为此，提出了一种基于课程学习的优化方法CuRPO，在多个数据集上效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT推理在多个领域表现良好，但在视觉定位任务里强化学习细调CoT反而会使表现恶化，且数据集越大未必总是带来更好效果。作者希望解决这一困境，提高模型在复杂视觉理解任务中的性能和泛化能力。

Method: 提出CuRPO方法，基于课程学习思想，结合CoT输出长度和gIoU奖励，将训练过程从简单到复杂样本逐步优化，有针对性地提升模型推理能力。

Result: 在RefCOCO、RefCOCO+、RefCOCOg和LISA等基准数据集上，CuRPO均超越主流方法，对比Visual-RFT在RefCOCO数据集上最高提升12.52 mAP，且在小样本学习与复杂文本描述下表现尤为突出。

Conclusion: CuRPO不仅在各类视觉定位任务中展现出优越的准确性与泛化力，还证明了课程学习结合复杂性指标在提升CoT模型表现中的有效性和稳定性。

Abstract: Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.

</details>


### [23] [Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets](https://arxiv.org/abs/2511.13944)
*Noam Glazner,Noam Tsfaty,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 本文提出了一种基于聚类的视频帧选择策略，通过将视觉上相似的帧进行分组后再划分数据集，从而缓解信息泄露问题。


<details>
  <summary>Details</summary>
Motivation: 在视频数据集中，常见的划分方法可能导致相似帧被分布到不同的数据子集中，引发信息泄露，降低模型评估的可靠性。为解决这一问题，作者提出新的策略。

Method: 先对视频帧进行视觉特征提取与聚类，将相似帧聚为一组，然后以组为单位进行训练集、验证集和测试集的划分。

Result: 该方法能生成更加具代表性、均衡且可靠的数据集分割，有效减少信息泄露。

Conclusion: 基于聚类的帧选择策略能够提升视频数据集分割的科学性和实验评估的可信度，建议在相关任务中采用。

Abstract: We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.

</details>


### [24] [Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers](https://arxiv.org/abs/2511.13945)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Damien Teney,Anton van den Hengel*

Main category: cs.CV

TL;DR: 本文提出通过在预训练阶段使用过程生成的非视觉、非语义数据，对视觉Transformer（ViT）注入通用归纳偏置，从而提升其在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在不同领域表现优异，但其底层归纳偏置较弱。研究动机在于利用非视觉、非语义的程序化数据，探索能否强化ViT模型的泛用能力和训练效率。

Method: 首先使用形式文法等简单算法生成内容与实际视觉任务无关的程序化数据，用该数据预训练ViT，且跳过通用的视觉patch嵌入机制。随后，再用真实图像数据进行常规训练。对比纯图像数据训练的方法，量化两阶段训练带来的效率和效果提升。

Result: 在ImageNet-1k上，仅分配1%的训练预算用于程序化数据，即可带来1.7%以上的准确率提升。换言之，1%的过程生成数据带来的效果相当于28%的图像数据。预训练使模型的数据利用率和收敛速度大幅提升。

Conclusion: 使用程序化、无语义的数据预训练可以显著提升ViT在后续真实图像任务上的表现，表明这一策略有潜力成为高效且通用的预训练方法。

Abstract: Transformers show remarkable versatility across domains, suggesting the existence of inductive biases beneficial across modalities. In this work, we explore a new way to instil such generic biases in vision transformers (ViTs) by pretraining on procedurally-generated data devoid of visual or semantic content. We generate this data with simple algorithms such as formal grammars, so the results bear no relationship to either natural or synthetic images. We use this procedurally-generated data to pretrain ViTs in a warm-up phase that bypasses their visual patch embedding mechanisms, thus encouraging the models to internalise abstract computational priors. When followed by standard image-based training, this warm-up significantly improves data efficiency, convergence speed, and downstream performance. On ImageNet-1k for example, allocating just 1% of the training budget to procedural data improves final accuracy by over 1.7%. In terms of its effect on performance, 1% procedurally generated data is thus equivalent to 28% of the ImageNet-1k data. These findings suggest a promising path toward new data-efficient and domain-agnostic pretraining strategies.

</details>


### [25] [Single Tensor Cell Segmentation using Scalar Field Representations](https://arxiv.org/abs/2511.13947)
*Kevin I. Ruiz Vargas,Gabriel G. Galdino,Tsang Ing Ren,Alexandre L. Cunha*

Main category: cs.CV

TL;DR: 本论文提出了一种基于标量场的细胞图像分割方法，通过训练网络学习连续标量场，再利用分水岭算法实现细胞分割，在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 细胞图像分割对于生物医学分析至关重要，但现有方法通常能耗高、对异常值敏感且计算复杂。作者旨在提出一种更鲁棒、计算高效且实现简单的方法，特别适合边缘计算环境。

Method: 作者将细胞分割建模为图像域中的连续标量场学习，该标量场由深度神经网络（如U-Net）参数化，通过求解泊松方程与稳态热扩散方程的残差，并用分水岭算法对标量场分割。方法不需要正则化，仅通过最小化残差即可，这为模型实现在鲁棒性和边界清晰度上带来好处。

Result: 方法在公开的细胞图像分割数据集上取得了有竞争力的结果。实验显示，该方法不仅可以实现精确的细胞实例分割，还在内存占用、训练/推理速度和能耗上表现突出。

Conclusion: 提出的方法以简单、几何直观的方式实现了优秀的细胞分割，且易于实现和部署，非常适合资源受限的边缘计算场景。

Abstract: We investigate image segmentation of cells under the lens of scalar fields. Our goal is to learn a continuous scalar field on image domains such that its segmentation produces robust instances for cells present in images. This field is a function parameterized by the trained network, and its segmentation is realized by the watershed method. The fields we experiment with are solutions to the Poisson partial differential equation and a diffusion mimicking the steady-state solution of the heat equation. These solutions are obtained by minimizing just the field residuals, no regularization is needed, providing a robust regression capable of diminishing the adverse impacts of outliers in the training data and allowing for sharp cell boundaries. A single tensor is all that is needed to train a \unet\ thus simplifying implementation, lowering training and inference times, hence reducing energy consumption, and requiring a small memory footprint, all attractive features in edge computing. We present competitive results on public datasets from the literature and show that our novel, simple yet geometrically insightful approach can achieve excellent cell segmentation results.

</details>


### [26] [Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM](https://arxiv.org/abs/2511.14499)
*Jack Qin,Zhitao Wang,Yinan Zheng,Keyu Chen,Yang Zhou,Yuanxin Zhong,Siyuan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Risk Semantic Distillation (RSD)的新框架，通过结合视觉-语言模型（VLM）和鸟瞰图（BEV）特征，提升自动驾驶系统对复杂和未知场景的泛化能力，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在复杂场景中表现优异，但在遇到新场景或传感器配置时的泛化能力仍有限。现有融合VLM的方法会引入系统不一致或算力过高等新问题，有必要探索兼顾泛化与实际可用性的解决方案。

Method: 作者提出RSD框架，将VLM中的因果风险估计通过RSD模块（RiskHead）蒸馏至BEV特征中，生成可解释的风险关注图，让BEV特征学会更丰富的风险感知能力，从而优化下游的路径规划和风险规避。

Result: 在Bench2Drive基准测试中，RSD显著提升了自动驾驶系统的感知与路径规划能力，特别是在处理复杂和不可预知驾驶场景时表现优异。

Conclusion: RSD框架能够有效结合VLM的强大表征能力和BEV的空间特性，通过强化风险关注提升了自动驾驶模型的泛化和安全能力，使系统更接近人类驾驶者的表现。

Abstract: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.

</details>


### [27] [EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation](https://arxiv.org/abs/2511.13948)
*Matin Daghyani,Lyuyang Wang,Nima Hashemi,Bassant Medhat,Baraa Abdelsamad,Eros Rojas Velez,XiaoXiao Li,Michael Y. C. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 本文提出了一种名为EchoAgent的自动化超声心动图分析框架，能够实现结构化、可解释的视频级别诊断分析，并通过新的模型提升测量的可靠性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在超声心动图分析中无法很好地支持视频级别的时空推理及基于指南的测量解释，临床需求是提升自动化工具的可信度和实用性。

Method: EchoAgent框架整合了多个专用视觉工具，由大语言模型（LLM）调度，实现时序定位、空间测量及临床解释。其创新点包括设计了测量可行性预测模型，自动判别心脏解剖结构在每一帧中的可测量性，并建立了包含丰富视频-查询对的基准数据集进行评估。

Result: EchoAgent即使面对更复杂的时空视频分析任务，依然能得到准确、可解释的输出，结果紧扣可视证据与临床指南，提升了结果的透明度与可追溯性。

Conclusion: 本研究证明了基于代理、符合指南的自动化超声心动图视频分析的可行性，EchoAgent为心脏超声领域构建值得信赖的AI工具奠定了基础，并指明了新的研究方向。

Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.

</details>


### [28] [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](https://arxiv.org/abs/2511.14751)
*Yutian Chen,Yuheng Qiu,Ruogu Li,Ali Agha,Shayegan Omidshafiei,Jay Patrikar,Sebastian Scherer*

Main category: cs.CV

TL;DR: 提出了一种无需重训练即可加速视觉几何Transformer的新方法Co-Me，通过置信度引导合并低置信度token，大幅提升推理速度且几乎无性能损失。


<details>
  <summary>Details</summary>
Motivation: 当前视觉几何Transformer计算量大，难以实时应用，现有加速方法如相似性合并或剪枝存在信息遗漏、性能下降等问题。需要一种兼顾计算效率和模型表现的新机制。

Method: Co-Me方法通过蒸馏一个轻量级置信度预测器，基于token的不确定性排序，将低置信度token进行合并，优先保留模型关注区域，无需对主模型重训练或微调。该策略适用于多视角和流式视觉Transformer，效率提升与序列长度正相关。

Result: 在VGGT和MapAnything模型上，Co-Me分别实现了11.3倍和7.2倍的速度提升，同时保持了性能不下降。与基于相似性的合并或剪枝方法相比，Co-Me能更可靠地维持空间覆盖和Transformer的关注区域。

Conclusion: Co-Me大幅提升了视觉几何Transformer的推断效率，为实时3D感知和重建等实际场景提供了可行的加速方案，是无需改动或重训练基础模型的有效优化手段。

Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.

</details>


### [29] [Learning Skill-Attributes for Transferable Assessment in Video](https://arxiv.org/abs/2511.13993)
*Kumar Ashutosh,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文提出了一种可迁移的视频表示方法用于跨领域和领域内技能评估，显著提升了现有模型的泛化能力和反馈解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估模型通常专注于单一运动项目，且依赖昂贵且稀缺的专家标注，难以适应广泛的“长尾”运动项目需求。为解决这一难题，作者探索开发通用性强、可迁移的视频技能表征方法，以扩展至更多运动领域。

Method: 作者提出CrossTrainer方法，自动发现如平衡、控制、手部姿势等跨运动项目通用的技能属性，并训练多模态大语言模型，能够针对新视频生成可操作性反馈建议并评估技能水平。

Result: 在多个数据集上，CrossTrainer方法在跨运动（迁移）和同运动（领域内）场景下均取得了最多60%的性能提升，相比现有技术有明显优势。

Conclusion: 通过抽象和利用表征人类技能的共享行为特征，本文方法大幅提升了多模态大模型技能评估与反馈能力，为更广泛的视频技能分析及应用场景提供了有效方案。

Abstract: Skill assessment from video entails rating the quality of a person's physical performance and explaining what could be done better. Today's models specialize for an individual sport, and suffer from the high cost and scarcity of expert-level supervision across the long tail of sports. Towards closing that gap, we explore transferable video representations for skill assessment. Our CrossTrainer approach discovers skill-attributes, such as balance, control, and hand positioning -- whose meaning transcends the boundaries of any given sport, then trains a multimodal language model to generate actionable feedback for a novel video, e.g., "lift hands more to generate more power" as well as its proficiency level, e.g., early expert. We validate the new model on multiple datasets for both cross-sport (transfer) and intra-sport (in-domain) settings, where it achieves gains up to 60% relative to the state of the art. By abstracting out the shared behaviors indicative of human skill, the proposed video representation generalizes substantially better than an array of existing techniques, enriching today's multimodal large language models.

</details>


### [30] [CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2511.14014)
*Xianming Gu,Lihui Wang,Ying Cao,Zeyu Deng,Yingfeng Ou,Guodong Hu,Yi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积字典特征解耦（CD-DPE）策略的双提示专家网络，实现多对比度MRI超分辨率重建，在同类方法中表现优异且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多对比度MRI超分辨率重建可以提升图像细节和组织分辨率，对早期诊断和临床决策至关重要。但不同成像对比度间的信息差异使得有效利用高分辨率参考图像成为一大挑战，传统方法存在特征整合效果不佳和信息冗余等问题。

Method: 作者提出基于卷积字典特征解耦（CD-DPE）的双提示专家网络。首先采用迭代卷积字典特征解耦模块（CD-FDM）将特征分解为跨对比度和内对比度成分，降低干扰和冗余；再利用新设计的双提示特征融合专家模块（DP-FFEM），通过频域提示和自适应路由，分别指导参考特征选择和特征融合方式，有效提升重建质量。

Result: 在公开的多对比度MRI数据集上，所提模型在细节重建方面优于目前主流方法。进一步在未见过的数据集上测试，验证了该方法的优良泛化能力。

Conclusion: 提出的CD-DPE方法能有效解决多对比度MRI超分辨率重建中的特征冗余和融合难题，提升诊断价值，在实际医学图像处理中具备广泛应用潜力。

Abstract: Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.

</details>


### [31] [RISE: Single Static Radar-based Indoor Scene Understanding](https://arxiv.org/abs/2511.14019)
*Kaichen Zhou,Laura Dodds,Sayed Saad Afzal,Fadel Adib*

Main category: cs.CV

TL;DR: 本论文提出了一种基于单一静态毫米波雷达的室内场景理解系统RISE，实现了高效的布局重构与目标检测，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 传统的RGB或LiDAR传感器虽具有高空间分辨率，但易受遮挡影响且存在隐私风险；毫米波雷达虽然能穿透障碍且更隐私安全，但空间分辨率低，难以进行几何推理。作者希望解决如何在保障隐私的前提下，实现精确的室内场景重建与目标识别。

Method: 提出了RISE系统：1）通过双角度多路径增强模型分析雷达的到达角与发射角，利用原本被认为是噪声的多路径反射信号获取额外的几何信息。2）基于模拟-现实分层扩散框架，将破碎凌乱的雷达响应转化为完整的室内布局重建和目标检测结果。同时构建了大规模数据集（5万帧，100条真实轨迹）。

Result: 实验表明，RISE在布局重构中将Chamfer Distance降至16厘米，比当前最优方法降低了60%；同时首次实现mmWave雷达的目标检测，IoU达58%。

Conclusion: RISE为单一静态毫米波雷达实现几何感知与隐私保护的室内场景理解奠定了新基础，推动了该领域的发展。

Abstract: Robust and privacy-preserving indoor scene understanding remains a fundamental open problem. While optical sensors such as RGB and LiDAR offer high spatial fidelity, they suffer from severe occlusions and introduce privacy risks in indoor environments. In contrast, millimeter-wave (mmWave) radar preserves privacy and penetrates obstacles, but its inherently low spatial resolution makes reliable geometric reasoning difficult.
  We introduce RISE, the first benchmark and system for single-static-radar indoor scene understanding, jointly targeting layout reconstruction and object detection. RISE is built upon the key insight that multipath reflections, traditionally treated as noise, encode rich geometric cues. To exploit this, we propose a Bi-Angular Multipath Enhancement that explicitly models Angle-of-Arrival and Angle-of-Departure to recover secondary (ghost) reflections and reveal invisible structures. On top of these enhanced observations, a simulation-to-reality Hierarchical Diffusion framework transforms fragmented radar responses into complete layout reconstruction and object detection.
  Our benchmark contains 50,000 frames collected across 100 real indoor trajectories, forming the first large-scale dataset dedicated to radar-based indoor scene understanding. Extensive experiments show that RISE reduces the Chamfer Distance by 60% (down to 16 cm) compared to the state of the art in layout reconstruction, and delivers the first mmWave-based object detection, achieving 58% IoU. These results establish RISE as a new foundation for geometry-aware and privacy-preserving indoor scene understanding using a single static radar.

</details>


### [32] [MRI Plane Orientation Detection using a Context-Aware 2.5D Model](https://arxiv.org/abs/2511.14021)
*SangHyuk Kim,Daniel Haehn,Sumientra Rampersad*

Main category: cs.CV

TL;DR: 本研究提出了一种可自动识别MRI切片解剖平面（轴向、冠状、矢状）的分类器，并展示其在提高诊断和数据处理准确性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 缺少平面方向元数据会导致分析复杂、异质数据融合时域偏移增加，以及影响诊断分类器的准确性，因此需开发自动化方法生成可靠的平面方向元数据。

Method: 提出了一种2.5D上下文感知模型，利用多切片信息学习鲁棒特征，分别在3D切片序列和静态2D图像上进行训练，并与传统2D模型进行对比。同时在脑肿瘤检测任务中验证了自动生成元数据的实用性。

Result: 2D基线模型准确率为98.74%，而2.5D模型提升至99.49%，错误减少60%。基于元数据增强的分类策略，将肿瘤检测的准确率从97.0%提升至98.0%，误诊率下降33.3%。

Conclusion: 2.5D模型显著提升了平面方向判别准确率，并能实质性提高后续医疗诊断任务的效果。代码和工具以开源方式集成到交互式Web应用中，推动实际应用落地。

Abstract: Humans can easily identify anatomical planes (axial, coronal, and sagittal) on a 2D MRI slice, but automated systems struggle with this task. Missing plane orientation metadata can complicate analysis, increase domain shift when merging heterogeneous datasets, and reduce accuracy of diagnostic classifiers. This study develops a classifier that accurately generates plane orientation metadata. We adopt a 2.5D context-aware model that leverages multi-slice information to avoid ambiguity from isolated slices and enable robust feature learning. We train the 2.5D model on both 3D slice sequences and static 2D images. While our 2D reference model achieves 98.74% accuracy, our 2.5D method raises this to 99.49%, reducing errors by 60%, highlighting the importance of 2.5D context. We validate the utility of our generated metadata in a brain tumor detection task. A gated strategy selectively uses metadata-enhanced predictions based on uncertainty scores, boosting accuracy from 97.0% with an image-only model to 98.0%, reducing misdiagnoses by 33.3%. We integrate our plane orientation model into an interactive web application and provide it open-source.

</details>


### [33] [LINGUAL: Language-INtegrated GUidance in Active Learning for Medical Image Segmentation](https://arxiv.org/abs/2511.14028)
*Md Shazid Islam,Shreyangshu Bera,Sudipta Paul,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合自然语言指导的主动学习新框架（LINGUAL），可大大减少医学图像分割中的专家标注时间并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中主动学习虽然可以减少标注量，但仍因边界模糊和复杂导致专家操作费时费力。现有方法中标注范围和准确度存在困难，而自然语言描述能极大减少专家负担。

Method: 提出LINGUAL框架，允许专家用自然语言对图像中的感兴趣区域（ROI）进行描述，系统通过in-context learning将语义指令转化为可执行程序，自动完成标注分割等任务，无需专家人工描绘边界。

Result: LINGUAL在主动领域自适应（ADA）任务上取得与或优于传统主动学习基线的性能，并将标注时间平均减少约80%。

Conclusion: LINGUAL实现了降低专家标注成本和难度，同时保持分割效果，是医学图像分割主动学习的有前景方法。

Abstract: Although active learning (AL) in segmentation tasks enables experts to annotate selected regions of interest (ROIs) instead of entire images, it remains highly challenging, labor-intensive, and cognitively demanding due to the blurry and ambiguous boundaries commonly observed in medical images. Also, in conventional AL, annotation effort is a function of the ROI- larger regions make the task cognitively easier but incur higher annotation costs, whereas smaller regions demand finer precision and more attention from the expert. In this context, language guidance provides an effective alternative, requiring minimal expert effort while bypassing the cognitively demanding task of precise boundary delineation in segmentation. Towards this goal, we introduce LINGUAL: a framework that receives natural language instructions from an expert, translates them into executable programs through in-context learning, and automatically performs the corresponding sequence of sub-tasks without any human intervention. We demonstrate the effectiveness of LINGUAL in active domain adaptation (ADA) achieving comparable or superior performance to AL baselines while reducing estimated annotation time by approximately 80%.

</details>


### [34] [Training-free Detection of AI-generated images via Cropping Robustness](https://arxiv.org/abs/2511.14030)
*Sungik Choi,Hankook Lee,Moontae Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的AI生成图像检测方法（WaRPAD），利用自监督模型对图像高频扰动的敏感性，通过分割和波形分解实现鲁棒检测，在多个数据集和生成模型上表现优异，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI视觉生成模型的发展，AI生成图像检测变得十分重要。现有方法多依赖特定数据集或模型训练，适应性和泛化能力有限。因此，作者希望设计一种与具体数据集无关、无需额外训练的数据泛化检测方法。

Method: 作者利用自监督模型在训练时采用的RandomResizedCrop增强，使得模型学到分辨率不变的特征。基于此，提出WaRPAD方法：
1. 通过Haar小波分解提取图像高频信息方向。
2. 计算扰动下的特征表示变化，定义检测分数。
3. 将图像缩放至模型输入的倍数、切分小块，对每块单独评分。
4. 平均每块分数得到最终检测分数。

Result: 在包含不同分辨率和领域的真实数据集以及23种生成模型的AI图片上，WaRPAD方法表现出色，检测精度高且对测试时的扰动鲁棒性强。并且，该方法适用于多种自监督模型。

Conclusion: WaRPAD实现了免训练、跨领域的AI生成图像检测，不仅在多个数据集和模型下性能优越，还展现了极强的泛化和鲁棒能力，能广泛应用于不同的自监督模型。

Abstract: AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.

</details>


### [35] [FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization](https://arxiv.org/abs/2511.14031)
*Rong Zhang,Jinxiao Li,Jingnan Wang,Zhiwen Zuo,Jianfeng Dong,Wei Li,Chi Wang,Weiwei Xu,Xun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的服装图像生成方法，能够高质量、可控性强地生成穿着指定服装的人体图片，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在电商等场景中，自动生成穿着特定服装的真人图片具有实际需求，但现有方法在细节保真和外观可控性上存在不足。特别是传统的衣物变形步骤容易导致纹理失真，且难以精细控制生成效果。

Method: 提出了FashionMAC，这是一个无衣物变形的扩散生成框架。其核心思想是去除服装变形环节，直接对分割出的服装区域进行扩展(outpaint)，从而保持服装细节。同时提出了区域自适应解耦注意力（RADA）机制和链式掩模注入策略，用以实现对人物外观精细控制，即根据文本属性有针对性地调整生成区域。

Result: 大量实验表明，FashionMAC在生成质量和可控性方面均优于现有主流方法。

Conclusion: FashionMAC能够高保真实性且可控地生成穿着特定服装的人体图片，有效解决了传统方法纹理失真和细粒度控制不足的问题，具备广阔的实际应用前景。

Abstract: Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.

</details>


### [36] [Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping](https://arxiv.org/abs/2511.14033)
*Sun Han Neo,Sachith Seneviratne,Herath Mudiyanselage Viraj Vidura Herath,Abhishek Saha,Sanka Rasnayaka,Lucy Amanda Marshall*

Main category: cs.CV

TL;DR: 本论文提出采用潜在扩散模型对粗网格洪水图进行超分辨，从而在大幅提升推理速度的同时保证精细网格的精度，提高洪水预测的实时性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统流体动力学洪水模型计算高分辨率结果计算量大，不适用于大范围实时预测；而现有基于神经网络的方法虽然速度快但泛化能力有限。因此，迫切需要一种既高效又可泛化的新方法。

Method: 方法采用潜在扩散模型，在粗网格洪水模拟图基础上生成高分辨率结果。模型训练时引入物理信息，提高可解释性。还探索了迁移学习，加快模型适应新地区能力。

Result: 实验表明，潜在扩散模型生成高保真洪水图的速度大幅提升，精度不亚于传统精细网格模型。同时模型具备良好的跨区域泛化能力，并借助迁移学习进一步加快新地区适应。

Conclusion: 该方法实现低计算成本、高精度、高泛化性的洪水风险实时管理，结合物理知识提升了模型可解释性，有潜力广泛应用于实际防洪决策。

Abstract: Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at https://github.com/neosunhan/flood-diff.

</details>


### [37] [Saliency-Guided Deep Learning for Bridge Defect Detection in Drone Imagery](https://arxiv.org/abs/2511.14040)
*Loucif Hebbache,Dariush Amirkhani,Mohand Saïd Allili,Jean-François Lapointe*

Main category: cs.CV

TL;DR: 本文提出了一种基于无人机图像的混凝土桥梁缺陷自动检测与分类新方法，结合显著性分析和改进的YOLOX深度学习模型，实验证明其在准确率和计算效率上均表现优异，适用于自动化检测系统。


<details>
  <summary>Details</summary>
Motivation: 传统桥梁缺陷检测方法效率低、易受主观影响，自动化手段尤其基于无人机和深度学习能够显著提升检测的准确性和实用性。因此，作者希望提出一种高效、自动化的桥梁缺陷检测和分类方法，以满足实际工程需求。

Method: 方法包含两个阶段：第一阶段利用显著性分析提出缺陷候选区域；第二阶段采用基于YOLOX的深度学习检测器，在通过增强显著性区域亮度的图像上进行目标检测和分类。该方法综合利用了表面纹理变化和深度学习强大的特征提取能力。

Result: 在标准数据集上进行了实验，结果表明该方法在缺陷检测与分类的准确率和计算效率方面均有良好表现。

Conclusion: 提出的基于显著性和YOLOX的混凝土桥梁缺陷检测与分类框架准确高效，适合在自供能检测系统中实际应用，具有较大应用潜力。

Abstract: Anomaly object detection and classification are one of the main challenging tasks in computer vision and pattern recognition. In this paper, we propose a new method to automatically detect, localize and classify defects in concrete bridge structures using drone imagery. This framework is constituted of two main stages. The first stage uses saliency for defect region proposals where defects often exhibit local discontinuities in the normal surface patterns with regard to their surrounding. The second stage employs a YOLOX-based deep learning detector that operates on saliency-enhanced images obtained by applying bounding-box level brightness augmentation to salient defect regions. Experimental results on standard datasets confirm the performance of our framework and its suitability in terms of accuracy and computational efficiency, which give a huge potential to be implemented in a self-powered inspection system.

</details>


### [38] [Semantic Context Matters: Improving Conditioning for Autoregressive Models](https://arxiv.org/abs/2511.14063)
*Dongyang Jin,Ryan Xu,Jianhao Zeng,Rui Lan,Yancheng Bai,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了SCAR方法，提升了自回归图像生成模型在图像编辑任务中的表现，实现了更强的指令遵循和视觉质量，并在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型虽然在图像生成方面表现出色，但在图像编辑与指令遵循上存在条件控制弱、低效等问题，导致生成内容偏差和视觉伪影。为提升编辑能力和表现力，需要新的方法增强条件处理。

Method: SCAR包含两个核心：压缩语义预填充，将高层语义压缩为高效前缀实现更好条件输入；语义对齐引导，使解码最后的视觉表征与目标语义对齐，提升指令遵循度。这些设计可以在无需大幅结构改动下适配多种自回归范式。

Result: SCAR在图像指令编辑和可控生成任务上，表现出更优的视觉保真度和语义对齐能力，超越先前基于自回归的编辑方法，并保持良好的可控性。

Conclusion: SCAR提升了自回归生成模型在编辑与指令控制上的性能，优化了效率与通用性，为相关领域提供了更有效的解决方案。

Abstract: Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.

</details>


### [39] [CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs](https://arxiv.org/abs/2511.14072)
*Jingyu Lei,Gaoang Wang,Der-Horng Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉标记压缩方法CORE，大幅提升了视觉-语言大模型（LVLM）的效率，并在高压缩率下依然保持了接近原始性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在处理高分辨率图像时，视觉标记数量随分辨率增加呈二次增长，导致算力和内存消耗极高。现有压缩方法缺乏高层语义理解，容易造成信息冗余或上下文丢失。

Method: 提出了CORE框架，通过高效的分割解码器生成物体掩码，提供高层次语义引导视觉标记的合并形成以物体为中心的紧凑表示。此外，提出了质心引导的排序机制，保持合并后标记的空间顺序，保留重要的位置信息。

Result: 在六个权威基准中，CORE在固定比率压缩任务上达到SOTA。在自适应压缩率场景下实现了显著的效率提升。在极端压缩（仅保留2.2%可视标记）下仍能保持97.4%的基线性能。

Conclusion: CORE证明了物体为中心的视觉标记压缩在提升LVLM效率和效果方面的优越性，有助于场景下的大模型部署和应用。

Abstract: Large Vision-Language Models (LVLMs) usually suffer from prohibitive computational and memory costs due to the quadratic growth of visual tokens with image resolution. Existing token compression methods, while varied, often lack a high-level semantic understanding, leading to suboptimal merges, information redundancy, or context loss. To address these limitations, we introduce CORE (Compact Object-centric REpresentations), a new paradigm for visual token compression. CORE leverages an efficient segmentation decoder to generate object masks, which serve as a high-level semantic prior to guide the merging of visual tokens into a compact set of object-centric representations. Furthermore, a novel centroid-guided sorting mechanism restores a coherent spatial order to the merged tokens, preserving vital positional information. Extensive experiments show that CORE not only establishes a new state-of-the-art on six authoritative benchmarks for fixed-rate compression, but also achieves dramatic efficiency gains in adaptive-rate settings. Even under extreme compression, after aggressively retaining with only 2.2% of all visual tokens, CORE still maintains 97.4% of baseline performance. Our work demonstrates the superiority of object-centric representations for efficient and effective LVLM processing.

</details>


### [40] [Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification](https://arxiv.org/abs/2511.14082)
*Yao Qin,Yangyang Yan,YuanChao Yang,Jinhua Pang,Huanyong Bi,Yuan Liu,HaiHua Wang*

Main category: cs.CV

TL;DR: 提出了一种无需训练即可根据具体任务自动生成分类器参数的新方法（ZS-TMS），并显著提升了医学图像小样本、零样本分类的表现，尤其适用罕见病。


<details>
  <summary>Details</summary>
Motivation: 深度学习医学图像分析受到对大量人工标注数据的依赖，尤其罕见病数据难以获得，专家标注又昂贵，因此亟需摆脱对“大数据”的依赖。

Method: 作者提出Semantic-Guided Parameter Synthesizer（SGPS），利用大规模预训练生成模型，结合极少量多模态任务信息（如一张图片和配套临床描述），直接生成特定分类任务的模型参数，无需迁移学习或微调。

Result: 在ISIC 2018皮肤病变集和自建罕见病数据集上的极少样本分类（1-5 shot）大幅超过现有先进的小样本和零样本方法，刷新了相关SOTA。

Conclusion: 该方法为快速开发部署AI诊断工具、尤其解决罕见病数据稀缺问题提供了新路径，极大推动了医学AI无大数据条件下的落地。

Abstract: Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.

</details>


### [41] [Automated glenoid bone loss measurement and segmentation in CT scans for pre-operative planning in shoulder instability](https://arxiv.org/abs/2511.14083)
*Zhonghao Liu,Hanxue Gu,Qihang Li,Michael Fox,Jay M. Levin,Maciej A. Mazurowski,Brian C. Lau*

Main category: cs.CV

TL;DR: 本研究开发并验证了一套全自动深度学习流程，通过三维CT自动测量肩胛盂骨缺损，结果与人工结果高度一致，优于医生间一致性。


<details>
  <summary>Details</summary>
Motivation: 目前肩关节不稳治疗中，准确测量肩胛盂骨缺损对手术方案制定至关重要，但现有人工或半自动方法耗时且一致性不足。

Method: 本研究收集了91例患者CT并配有人工标注，提出多阶段算法：1）以U-Net自动分割肩胛盂和肱骨；2）第二个网络检测解剖学地标；3）通过PCA等几何拟合计算骨缺损百分比。

Result: 自动测量的数值与多专家共识判读高度一致（ICC 0.84优于医生间0.78），在低、高骨缺损亚组表现同样出色。分类低、中、高骨缺损召回率良好，无明显误分。

Conclusion: 该自动流程准确高效，可作为肩关节不稳术前规划及严重骨缺损患者筛查的可靠工具。代码与数据集已开源。

Abstract: Reliable measurement of glenoid bone loss is essential for operative planning in shoulder instability, but current manual and semi-automated methods are time-consuming and often subject to interreader variability. We developed and validated a fully automated deep learning pipeline for measuring glenoid bone loss on three-dimensional computed tomography (CT) scans using a linear-based, en-face view, best-circle method. Shoulder CT images of 91 patients (average age, 40 years; range, 14-89 years; 65 men) were retrospectively collected along with manual labels including glenoid segmentation, landmarks, and bone loss measurements. The multi-stage algorithm has three main stages: (1) segmentation, where we developed a U-Net to automatically segment the glenoid and humerus; (2) anatomical landmark detection, where a second network predicts glenoid rim points; and (3) geometric fitting, where we applied principal component analysis (PCA), projection, and circle fitting to compute the percentage of bone loss. The automated measurements showed strong agreement with consensus readings and exceeded surgeon-to-surgeon consistency (intraclass correlation coefficient (ICC) 0.84 vs 0.78), including in low- and high-bone-loss subgroups (ICC 0.71 vs 0.63 and 0.83 vs 0.21, respectively; P < 0.001). For classifying patients into low, medium, and high bone-loss categories, the pipeline achieved a recall of 0.714 for low and 0.857 for high severity, with no low cases misclassified as high or vice versa. These results suggest that our method is a time-efficient and clinically reliable tool for preoperative planning in shoulder instability and for screening patients with substantial glenoid bone loss. Code and dataset are available at https://github.com/Edenliu1/Auto-Glenoid-Measurement-DL-Pipeline.

</details>


### [42] [Error-Driven Scene Editing for 3D Grounding in Large Language Models](https://arxiv.org/abs/2511.14086)
*Yue Zhang,Zun Wang,Han Lin,Jialu Li,Jianing Yang,Yonatan Bitton,Idan Szpektor,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出了一种基于3D场景编辑的新方法，结合错误驱动机制，有效提升3D大模型的语言与视觉空间要素的对齐和理解精度。


<details>
  <summary>Details</summary>
Motivation: 尽管3D大模型（3D-LLMs）取得进展，但在3D环境中将语言准确地与视觉和空间元素对齐仍有限，原因之一是缺乏针对空间理解的大规模、高质量3D训练数据，导致模型存在固有偏差。

Method: 作者提出通过精细的3D场景编辑来生成“反事实”样本，针对模型错误进行最小、相关的场景修改，无需昂贵的数据采集或场景重构。提出DEER-3D框架，采用分解、诊断、编辑与再训练四步，定位并针对模型的具体弱点（如属性或空间关系识别错误）生成定向监督，辅助模型迭代优化。

Result: 在多个3D基准任务和数据集上验证了所提方法，发现通过该编辑管线可不断显著提升模型的空间对齐与场景理解能力。

Conclusion: DEER-3D验证了以模型错误为导向的定向3D场景编辑，是提升3D大模型空间对齐与语言推理能力的有效工具。

Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.

</details>


### [43] [GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention](https://arxiv.org/abs/2511.14087)
*Jun Ding,Shang Gao*

Main category: cs.CV

TL;DR: 提出了GCA-ResUNet，一种通过引入分组坐标注意力（GCA）机制提升分割精度且高效的医学图像分割网络，兼顾了全局建模与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net结构虽分割效果良好，但因难以建模长距离依赖受限；而Transformer虽可增强全局信息获取却消耗大量算力，且需大规模数据集。作者希望设计一种既能高效提取全局特征又计算量小的分割算法。

Method: 将分组坐标注意力机制（GCA）嵌入ResNet-50残差块中，通过分组式方式在通道和空间维度建模全局依赖，显著增强特征表示能力和边界捕捉能力，同时仅引入极少参数量和FLOP开销。

Result: 在Synapse数据集上GCA-ResUNet取得了86.11%的Dice分数，在ACDC数据集上获得92.64%，不但超过多种SOTA基线且推理速度快、计算效率高。

Conclusion: GCA机制可高效赋能卷积网络以全局建模能力，实现高精度、低资源消耗的医学图像分割，在实际医学分割中具备应用前景。

Abstract: Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.

</details>


### [44] [SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts](https://arxiv.org/abs/2511.14093)
*Fan Zhang,Haoyuan Ren,Fei Ma,Qiang Yin,Yongsheng Zhou*

Main category: cs.CV

TL;DR: 本文提出SMGeo，一种可提示的端到端Transformer模型，实现无人机图像与卫星图像跨视角目标定位，显著提升准确率并支持实时交互。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段“检索-匹配”方案容易在大视角、尺度差异和复杂背景下产生累积误差，难以高效精准地完成跨视角目标地理定位。

Method: SMGeo采用纯Transformer架构：利用Swin-Transformer对无人机与卫星图像进行联合特征编码，引入网格级稀疏专家混合（GMoE）机制自适应处理不同来源、内容、尺度的特征，并通过无锚框检测头直接进行坐标回归，避免锚框带来的尺度偏差。模型还支持基于点击的实时提示和交互。

Result: 在无人机到卫星定位任务中，SMGeo在IoU=0.25准确率及mIoU等指标上大幅领先于DetGeo等主流方法（如准确率分别为87.51%、62.50%、61.45% vs DetGeo的61.97%、57.66%、54.05%），消融实验也验证了多项设计对性能的提升。

Conclusion: SMGeo通过Transformer统一编码与专家机制，有效克服跨视角差异与定位难题，在目标地理定位任务中取得了显著领先，为交互式地理定位提供了新思路。

Abstract: Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images. Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage "retrieval-matching" pipelines are prone to cumulative errors. To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization. This model supports click prompting and can output object Geo-localization in real time when prompted to allow for interactive use. The model employs a fully transformer-based architecture, utilizing a Swin-Transformer for joint feature encoding of both drone and satellite imagery and an anchor-free transformer detection head for coordinate regression. In order to better capture both inter-modal and intra-view dependencies, we introduce a grid-level sparse Mixture-of-Experts (GMoE) into the cross-view encoder, allowing it to adaptively activate specialized experts according to the content, scale and source of each grid. We also employ an anchor-free detection head for coordinate regression, directly predicting object locations via heat-map supervision in the reference images. This approach avoids scale bias and matching complexity introduced by predefined anchor boxes. On the drone-to-satellite task, SMGeo achieves leading performance in accuracy at IoU=0.25 and mIoU metrics (e.g., 87.51%, 62.50%, and 61.45% in the test set, respectively), significantly outperforming representative methods such as DetGeo (61.97%, 57.66%, and 54.05%, respectively). Ablation studies demonstrate complementary gains from shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts.

</details>


### [45] [BCE3S: Binary Cross-Entropy Based Tripartite Synergistic Learning for Long-tailed Recognition](https://arxiv.org/abs/2511.14097)
*Weijia Fan,Qiufu Li,Jiajun Wen,Xiaoyang Peng*

Main category: cs.CV

TL;DR: 本文提出了一种基于二元交叉熵(BCE)的新型三协同学习方法（BCE3S），显著提升了长尾识别任务中的类别特征分布紧凑性、可分离性和平衡性，达到了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于交叉熵(CE)的长尾识别方法难以同时保证特征分布紧凑性与类别区分度，且Softmax带来的分类器权重不平衡会加剧长尾分布的不利影响。因此有必要设计新的损失函数与优化策略。

Method: 作者提出BCE3S方法，包括：(1) BCE联合学习：用BCE替代CE，同时优化分类器权重与特征，Sigmoid解耦了特征与分类器的权重分布不均；(2) BCE对比学习：进一步提升类内特征紧凑性；(3) BCE均匀学习：平衡各分类器向量的区分度，并结合联合学习提升特征表现。

Result: 在CIFAR10-LT、CIFAR100-LT、ImageNet-LT和iNaturalist2018四个主流长尾数据集上，BCE3S显著提升了样本特征的紧凑性与可分离性，实现了所有类别分类器向量的分布平衡，取得了当前最优（SOTA）的识别性能。

Conclusion: 基于BCE的三协同学习框架可以有效缓解长尾识别中的类别不平衡问题，提升识别准确率，具有较强应用推广价值。

Abstract: For long-tailed recognition (LTR) tasks, high intra-class compactness and inter-class separability in both head and tail classes, as well as balanced separability among all the classifier vectors, are preferred. The existing LTR methods based on cross-entropy (CE) loss not only struggle to learn features with desirable properties but also couple imbalanced classifier vectors in the denominator of its Softmax, amplifying the imbalance effects in LTR. In this paper, for the LTR, we propose a binary cross-entropy (BCE)-based tripartite synergistic learning, termed BCE3S, which consists of three components: (1) BCE-based joint learning optimizes both the classifier and sample features, which achieves better compactness and separability among features than the CE-based joint learning, by decoupling the metrics between feature and the imbalanced classifier vectors in multiple Sigmoid; (2) BCE-based contrastive learning further improves the intra-class compactness of features; (3) BCE-based uniform learning balances the separability among classifier vectors and interactively enhances the feature properties by combining with the joint learning. The extensive experiments show that the LTR model trained by BCE3S not only achieves higher compactness and separability among sample features, but also balances the classifier's separability, achieving SOTA performance on various long-tailed datasets such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and iNaturalist2018.

</details>


### [46] [FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration](https://arxiv.org/abs/2511.14099)
*Jingren Liu,Shuning Xu,Qirui Yang,Yun Wang,Xiangyu Chen,Zhong Ji*

Main category: cs.CV

TL;DR: 本文提出FAPE-IR框架，通过引入频率感知与多模态大语言模型规划，实现多种图像退化的统一恢复，性能优越且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 目前AIO-IR方法多依赖特定任务设计或隐式路由，难以适应真实复杂、多样化的图像退化场景。迫切需要通用、高效、可解释的图像恢复模型。

Method: 方法引入冻权重的多模态大语言模型进行图像分析与恢复步骤规划，并结合基于LoRA的专家混合模块，实现高、低频任务动态分派。利用扩散模型作为执行器，同时在训练中加入对抗训练及频率正则项增强恢复质量。

Result: 在7项主流恢复任务上达到/超过现有SOTA水平，且在多种混合退化下展现强大的零样本泛化能力。

Conclusion: FAPE-IR成功结合语义规划和频率恢复机制，实现了统一且可解释的多退化图像恢复框架，有效提升图像质量和方法适用广度。

Abstract: All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.

</details>


### [47] [Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations](https://arxiv.org/abs/2511.14100)
*Yiqing Shen,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了一种针对隐式查询的视频编辑新任务和模型RIVER，可依据复杂的语义推理进行视频编辑，并在多个基准数据集上取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动视频编辑方法主要依赖于用户提供的视频内容的明确描述和精确的编辑目标定位（空间位置和时间边界），但实际情境下用户通常只能通过隐式、语义化的查询来表达编辑需求，因此需要能够进行多步推理的视频编辑系统。

Method: 提出RIVER（Reasoning-based Implicit Video Editor）模型。RIVER将推理与生成过程解耦：先生成包含空间关系、时间轨迹和语义属性的“数字孪生”视频表示，再结合隐式文本查询交给大型语言模型进行多步推理，输出结构化编辑指令，引导基于扩散模型的编辑器实现像素级编辑。训练过程中引入基于推理准确度和生成质量的强化学习奖励。此外，作者还构建了一个包含100个视频与519条隐式查询的推理视频编辑评测集RVEBenchmark。

Result: RIVER在RVEBenchmark数据集上获得最佳表现。同时在VegGIE与FiVE两个现有视频编辑基准上也超过了六种主流方法，达到了最新最优的性能。

Conclusion: RIVER模型能有效处理从隐式、语义化查询到具体像素级编辑的整个推理与生成流程，推动了智能、高级视频编辑交互方式的发展。

Abstract: Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.

</details>


### [48] [RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment](https://arxiv.org/abs/2511.14107)
*Zeyu Cheng,Tongfei Liu,Tao Lei,Xiang Hua,Yi Zhang,Chengkai Tang*

Main category: cs.CV

TL;DR: 本文提出了一种高效、轻量级、可实时运行的自监督单目深度估计方法RTS-Mono，兼顾模型精度与推理速度，支持真实环境部署。


<details>
  <summary>Details</summary>
Motivation: 当前自监督单目深度估计算法应用于自动驾驶和机器人导航场景广泛，但主流模型体量大、资源消耗高，而轻量化尝试通常牺牲了精度，限制了其真实世界的实际部署。

Method: 提出RTS-Mono方法，采用基于Lite-Encoder的轻量级编码器，并设计多尺度稀疏融合的解码器结构以减少信息冗余、提升速度与保持性能。

Result: 在KITTI数据集实验中，RTS-Mono在低参数量（3M）下，于高低分辨率条件均达到最新SOTA水平，对比主流轻量方案显著提升了各项误差指标。在Nvidia Jetson Orin上实现49 FPS实时推理。

Conclusion: 本文提出的RTS-Mono兼具高性能、高效率和轻量级特性，可以在现实世界嵌入式设备上高效应用，有望推进自监督单目深度估计算法的实际部署与应用。

Abstract: Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.

</details>


### [49] [$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors](https://arxiv.org/abs/2511.14109)
*Zhenyu Li,Tianyi Shang*

Main category: cs.CV

TL;DR: 提出了一种带有几何约束的VPR新方法，通过不对称归一化和几何信息提升视觉定位表现，实验结果领先其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输聚合方法在特征分布差异大时受限，具体表现在Sinkhorn算法对源和目标分布一视同仁，导致聚合效果不足。为解决特征与聚类中心分布不匹配的问题，提出改进方法。

Method: 作者提出$A^2$GC-VPR，在特征-聚类分配时采用行列归一化平均，区别对待源和目标分布，实现不对称匹配。同时引入可学习的坐标嵌入，将空间几何信息与特征相结合，利用兼容度分数促使空间相近特征归于同一类。

Result: 在MSLS、NordLand和Pittsburgh数据集上进行实验，方法表现优于当前主流方法，在匹配准确性与鲁棒性上都有提升。

Conclusion: $A^2$GC-VPR有效克服了传统对称聚合在分布不一致情况下的不足，并且结合空间信息，能够提升视觉定位准确率和鲁棒性。

Abstract: Visual Place Recognition (VPR) aims to match query images against a database using visual cues. State-of-the-art methods aggregate features from deep backbones to form global descriptors. Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions. We propose an asymmetric aggregation VPR method with geometric constraints for locally aggregated descriptors, called $A^2$GC-VPR. Our method employs row-column normalization averaging with separate marginal calibration, enabling asymmetric matching that adapts to distributional discrepancies in visual place recognition. Geometric constraints are incorporated through learnable coordinate embeddings, computing compatibility scores fused with feature similarities, thereby promoting spatially proximal features to the same cluster and enhancing spatial awareness. Experimental results on MSLS, NordLand, and Pittsburgh datasets demonstrate superior performance, validating the effectiveness of our approach in improving matching accuracy and robustness.

</details>


### [50] [CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer](https://arxiv.org/abs/2511.14111)
*Srivathsan Sivakumar,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 本文提出了一种新型高效的视觉Transformer架构Cascaded-ViT（CViT），可在不降低精度的前提下大幅减少计算量与能耗，非常适合在移动端与无人机等资源受限设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer虽然表现优异，但资源消耗大，难以在低功耗平台部署。因此，迫切需要设计轻量、高效且能兼顾精度的ViT架构。

Method: 作者提出了一种Cascaded-Chunk Feed Forward Network（CCFFN）的新型前馈网络模块，将输入特征划分后分别处理，提高参数和计算效率，并整合到总体Cascaded-ViT架构中。

Result: 在ImageNet-1K数据集上，CViT-XL模型Top-1精度达到75.5%，同时FLOPs降低了15%，能耗降低3.3%，优于EfficientViT-M5。CViT多个型号均展现出当前最低的计算能耗优势。此外，在新的APF（Accuracy-Per-FLOP）指标上，多型号CViT均排名前列。

Conclusion: CViT架构在保持高准确率的同时，兼具高计算效率及低能耗，特别适用于对算力和电量有严格限制的设备如手机与无人机。

Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.

</details>


### [51] [Coffee: Controllable Diffusion Fine-tuning](https://arxiv.org/abs/2511.14113)
*Ziyao Zeng,Jingcheng Ni,Ruyi Liu,Alex Wong*

Main category: cs.CV

TL;DR: 该论文提出了Coffee方法，通过语言描述灵活指定“非期望概念”，在不需要额外训练的情况下实现对文本到图像扩散模型的可控微调，防止模型在微调过程中学习到或混合进不希望出现的概念。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然易于通过微调实现个性化，但在微调过程中很容易学到数据中包含的不希望出现的概念，并且这些概念容易与用户的提示词纠缠，导致后续包括偏见消除、防止恶意适应、属性解耦和更通用的策略微调等应用受到阻碍。因此，该问题亟需解决。

Method: 提出Coffee方法，通过文本描述指定“非期望概念”，在微调过程中用正则化方式约束用户提示词的嵌入向量不与这些概念对齐。整个过程无需额外训练，仅通过修改文本描述即可灵活调整需要排除的概念。

Result: 在和“非期望概念”配对的用户提示图像上进行微调实验，结果显示Coffee方法可以有效防止扩散模型在微调过程中学习到指定的不希望出现的概念，且效果优于现有方法。

Conclusion: Coffee无需额外训练，就能通过语言指定需排除的概念，实现了高效、灵活、可控的微调，对于防止有害概念注入、降低偏见和实现更细粒度的模型控制具有重要意义。

Abstract: Text-to-image diffusion models can generate diverse content with flexible prompts, which makes them well-suited for customization through fine-tuning with a small amount of user-provided data. However, controllable fine-tuning that prevents models from learning undesired concepts present in the fine-tuning data, and from entangling those concepts with user prompts, remains an open challenge. It is crucial for downstream tasks like bias mitigation, preventing malicious adaptation, attribute disentanglement, and generalizable fine-tuning of diffusion policy. We propose Coffee that allows using language to specify undesired concepts to regularize the adaptation process. The crux of our method lies in keeping the embeddings of the user prompt from aligning with undesired concepts. Crucially, Coffee requires no additional training and enables flexible modification of undesired concepts by modifying textual descriptions. We evaluate Coffee by fine-tuning on images associated with user prompts paired with undesired concepts. Experimental results demonstrate that Coffee can prevent text-to-image models from learning specified undesired concepts during fine-tuning and outperforms existing methods. Code will be released upon acceptance.

</details>


### [52] [O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model](https://arxiv.org/abs/2511.14368)
*Rishi Gupta,Mukilan Karuppasamy,Shyam Marjit,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了一个包含手绘素描、真实图片及自然语言指令的大规模三元组数据集，并基于该数据集训练了新型大型视觉语言模型O3SLM，在素描理解相关多项任务上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 目前的大型视觉语言模型在实际应用中难以理解手绘素描等抽象视觉输入，主要瓶颈在于缺乏同时包含素描、实拍图像及文本指令的大规模多模态数据集。

Method: 作者构建了一个新颖的大规模素描-图片-指令三元组数据集，用于模型的预训练与指令微调，并据此训练了O3SLM模型。

Result: O3SLM在对象定位、计数、基于素描的图像检索（包括细粒度检索）及视觉问答等多项素描相关任务上，结合现有主流素描数据集和新生成的数据集，在素描理解和推理方面显著超越已有LVLM。

Conclusion: 通过提出新数据集和O3SLM模型，提升了LVLM在手绘素描理解任务中的表现，为视觉语言模型应用于更丰富抽象视觉场景提供了基础。

Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

</details>


### [53] [Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models](https://arxiv.org/abs/2511.14120)
*Hao Zhen,Yunxiang Yang,Jidong J. Yang*

Main category: cs.CV

TL;DR: 本文提出了一个多视角、分阶段的行人-车辆事件分析系统MP-PVIR，能将多路视频数据自动处理为结构化诊断报告，实现对交通事故发生机制的深入理解和预防建议。


<details>
  <summary>Details</summary>
Motivation: 现有视频系统只能检测交通事件的发生，难以详细解析行人行为阶段及其因果链，对事件的理解和预防指导有限。作者希望通过引入分阶段、多视角的智能分析体系，提升交通安全系统对行车事故的解读与干预能力。

Method: 设计了MP-PVIR统一框架，包括四步流程：1）基于事件触发采集多视角视频；2）行人行为分阶段自动切分；3）针对每个阶段进行多视角融合推理；4）分层次合成推理结果，诊断和溯源事故原因。系统依赖两个专门训练的视觉-语言模型（TG-VLM用于行为阶段切分，PhaVR-VLM进行多视角语义分析），并结合大型语言模型输出详细诊断报告。

Result: MP-PVIR在行为阶段切分（mIoU=0.4881）、多视角分析的字幕生成（得分33.063）及问题回答准确率（最高64.7%）上表现优秀。在Woven Traffic Safety数据集测试中，该框架可有效将多路视频转化为具指导意义的分析报告。

Conclusion: MP-PVIR拓展了基于AI的交通安全分析能力，能系统化地解析多视角下行人-车辆事件的发生过程及预防措施，有助于推动车路协同交通安全系统的智能化升级。

Abstract: Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.

</details>


### [54] [Attention Via Convolutional Nearest Neighbors](https://arxiv.org/abs/2511.14137)
*Mingi Kang,Jeová Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: 本文提出了一种统一卷积与自注意力机制的新框架——ConvNN（Convolutional Nearest Neighbors），通过k近邻聚合视角，将二者看作邻域选择和聚合的特例，实现两者的无缝结合，在CIFAR-10和CIFAR-100数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然卷积神经网络（CNN）和Transformer自注意力在计算机视觉领域各自取得了巨大进展，但二者架构通常视为根本不同。作者试图寻找两者之间的内在联系，以便系统性地探索二者之间的连续谱，为视觉架构设计提供统一原理。

Method: 作者提出ConvNN框架，将卷积的邻居选择（基于空间）和自注意力的邻居选择（基于特征相似性）统一为k近邻聚合，能够作为卷积层和注意力层的替代，探索中间形态。框架在VGG和ViT架构上实验，并对k值和结构变体做消融分析。

Result: 1）VGG中混合空间与特征选择能够提升CIFAR-10和CIFAR-100准确率；2）ConvNN替换ViT中的标准注意力可获得更好性能；3）插值于卷积与注意力机制之间能带来正则化和感受野平衡的益处。

Conclusion: 卷积和自注意力其实是k近邻聚合的两种极端，本文提出的ConvNN框架打破了二者的界限，为更有原则、更易解释的视觉架构设计提供了方法基础。

Abstract: The shift from Convolutional Neural Networks to Transformers has reshaped computer vision, yet these two architectural families are typically viewed as fundamentally distinct. We argue that convolution and self-attention, despite their apparent differences, can be unified within a single k-nearest neighbor aggregation framework. The critical insight is that both operations are special cases of neighbor selection and aggregation; convolution selects neighbors by spatial proximity, while attention selects by feature similarity, revealing they exist on a continuous spectrum. We introduce Convolutional Nearest Neighbors (ConvNN), a unified framework that formalizes this connection. Crucially, ConvNN serves as a drop-in replacement for convolutional and attention layers, enabling systematic exploration of the intermediate spectrum between these two extremes. We validate the framework's coherence on CIFAR-10 and CIFAR-100 classification tasks across two complementary architectures: (1) Hybrid branching in VGG improves accuracy on both CIFAR datasets by combining spatial-proximity and feature-similarity selection; and (2) ConvNN in ViT outperforms standard attention and other attention variants on both datasets. Extensive ablations on $k$ values and architectural variants reveal that interpolating along this spectrum provides regularization benefits by balancing local and global receptive fields. Our work provides a unifying framework that dissolves the apparent distinction between convolution and attention, with implications for designing more principled and interpretable vision architectures.

</details>


### [55] [SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM](https://arxiv.org/abs/2511.14143)
*An Yu,Weiheng Lu,Jian Li,Zhenfei Zhang,Yunhang Shen,Felix X. -F. Ye,Ming-Ching Chang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态框架SMART，通过结合音频与视觉特征，并引入基于镜头的时序结构，有效提升了视频时间片段定位（Video Moment Retrieval）的精度。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视频片段检索方法仅基于单一视觉模态及较为粗糙的时序理解，难以应对复杂视频内容，限制了性能提升。

Method: 提出SMART框架：1）融合音频与视觉特征，丰富模态表征；2）采用Shot-aware Token Compression机制，在每个镜头内保留高信息量的token，既减小冗余又保留细粒度时序信息；3）优化prompt设计，更好地利用多模态特征。

Result: 在Charades-STA和QVHighlights两个主流评测数据集上，SMART优于现有方法。例如在Charades-STA上，R1@0.5提升了1.61%，R1@0.7提升了2.59%。

Conclusion: SMART框架通过音视频信息融合、细粒度镜头分析与高效token压缩，显著提升了复杂视频片段检索的效果，具有较强的实际应用潜力。

Abstract: Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.

</details>


### [56] [iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion](https://arxiv.org/abs/2511.14149)
*Hao Wang,Linqing Zhao,Xiuwei Xu,Jiwen Lu,Haibin Yan*

Main category: cs.CV

TL;DR: 该论文提出iGaussian，一种基于3D高斯的单帧相机位姿实时估计框架，通过端到端两阶段架构，实现对比现有方法更快、更准的视觉定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯场景的单张图片相机位姿估计方法需多轮渲染-对比-优化，计算开销大，难以实时应用于机器人等场景。

Method: iGaussian提出两阶段前馈网络：首先通过基于高斯场景先验的姿态回归网络进行6自由度粗略位姿估计，引入空间均匀采样与注意力机制；随后利用特征匹配与多模型融合精细化估计，核心创新为无需可微渲染的跨相关模块将图像与3D高斯特征对齐，并结合加权多视角预测器融合视角特征。

Result: 在NeRF Synthetic、Mip-NeRF 360和T&T+DB等数据集上，iGaussian在姿态估计精度（中位旋转误差降至0.2°）和实时性（2.87 FPS，实现10倍于优化式方法的速度）方面均显著超越现有技术。

Conclusion: iGaussian显著提升了单帧相机位姿估计的效率和准确性，具备移动机器人等实时场景中的应用潜力。

Abstract: Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian

</details>


### [57] [Wave-Former: Through-Occlusion 3D Reconstruction via Wireless Shape Completion](https://arxiv.org/abs/2511.14152)
*Laura Dodds,Maisy Lam,Waleed Akbar,Yibo Cheng,Fadel Adib*

Main category: cs.CV

TL;DR: 本文提出了Wave-Former，一种利用毫米波信号实现完全遮挡下日常物体高精度3D重建的新方法。通过三阶段流程和结合物理特性的形状补全过程，模型在遮挡环境下也能推理完整3D几何信息。实验显示其重建能力优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前利用毫米波(mmWave)信号的3D重建方法在处理被遮挡物体时存在覆盖范围有限、噪声高等不足，难以满足机器人、增强现实和物流等领域的应用需求。作者希望提升在完全遮挡情况下的3D重建精度与泛化能力。

Method: Wave-Former采用物理感知的三阶段流程：1) 从毫米波原始信号中提出候选几何表面；2) 使用为毫米波设计的Transformer模型对物体形状进行补全；3) 通过熵引导筛选最终表面。训练用合成点云，推理时能良好泛化到真实场景。

Result: Wave-Former在与最新的基线方法对比测试中，将重建召回率从54%提高到72%，同时精度维持在85%的高水平，表明其对被遮挡物体的3D重建性能显著提升。

Conclusion: Wave-Former为毫米波辅助的3D重建提供了更高效、精准的方案，显著拓展了毫米波技术在视觉遮挡场景中的实际应用，为多个行业的智能系统提供了技术基础。

Abstract: We present Wave-Former, a novel method capable of high-accuracy 3D shape reconstruction for completely occluded, diverse, everyday objects. This capability can open new applications spanning robotics, augmented reality, and logistics. Our approach leverages millimeter-wave (mmWave) wireless signals, which can penetrate common occlusions and reflect off hidden objects. In contrast to past mmWave reconstruction methods, which suffer from limited coverage and high noise, Wave-Former introduces a physics-aware shape completion model capable of inferring full 3D geometry. At the heart of Wave-Former's design is a novel three-stage pipeline which bridges raw wireless signals with recent advancements in vision-based shape completion by incorporating physical properties of mmWave signals. The pipeline proposes candidate geometric surfaces, employs a transformer-based shape completion model designed specifically for mmWave signals, and finally performs entropy-guided surface selection. This enables Wave-Former to be trained using entirely synthetic point-clouds, while demonstrating impressive generalization to real-world data.In head-to-head comparisons with state-of-the-art baselines, Wave-Former raises recall from 54% to 72% while maintaining a high precision of 85%.

</details>


### [58] [Learning Representation and Synergy Invariances: A Povable Framework for Generalized Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2511.14157)
*Xun Lin,Shuai Wang,Yi Yu,Zitong Yu,Jiale Zhou,Yizhong Liu,Xiaochun Cao,Alex Kot,Yefeng Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态人脸反欺诈（FAS）方法——RiSe，旨在解决多模态FAS在跨领域应用中表现显著下降的问题。通过理论分析和实验验证，该方法取得了最新的跨领域性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，多模态FAS虽然利用多种视觉模态，但在面对新域（未见过的数据环境）时，性能下降比单模态方法更严重。原因在于两大被忽略的泛化风险，亟需应对。

Method: 作者提出RiSe方法：针对表示不变性风险，引入AsyIRM，通过在径向空间学习不变的球面决策边界适应数据分布的不对称性，同时在角度空间保留领域信息；针对协同不变性风险，引入MMSD，通过跨样本混合和解耦自监督任务，增强内在、可泛化的特征表现。

Result: 理论和实验均证明，RiSe显著提升了跨领域泛化能力，达到了该领域的最新性能水平。

Conclusion: 论文表明，针对多模态FAS中泛化风险的专门建模、分析和解决，对提升跨域应用效果十分有效，提出的RiSe方法为今后FAS系统的跨域应用提供了理论支持和实践范例。

Abstract: Multimodal Face Anti-Spoofing (FAS) methods, which integrate multiple visual modalities, often suffer even more severe performance degradation than unimodal FAS when deployed in unseen domains. This is mainly due to two overlooked risks that affect cross-domain multimodal generalization. The first is the modal representation invariant risk, i.e., whether representations remain generalizable under domain shift. We theoretically show that the inherent class asymmetry in FAS (diverse spoofs vs. compact reals) enlarges the upper bound of generalization error, and this effect is further amplified in multimodal settings. The second is the modal synergy invariant risk, where models overfit to domain-specific inter-modal correlations. Such spurious synergy cannot generalize to unseen attacks in target domains, leading to performance drops. To solve these issues, we propose a provable framework, namely Multimodal Representation and Synergy Invariance Learning (RiSe). For representation risk, RiSe introduces Asymmetric Invariant Risk Minimization (AsyIRM), which learns an invariant spherical decision boundary in radial space to fit asymmetric distributions, while preserving domain cues in angular space. For synergy risk, RiSe employs Multimodal Synergy Disentanglement (MMSD), a self-supervised task enhancing intrinsic, generalizable modal features via cross-sample mixing and disentanglement. Theoretical analysis and experiments verify RiSe, which achieves state-of-the-art cross-domain performance.

</details>


### [59] [MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs](https://arxiv.org/abs/2511.14159)
*Huiyi Chen,Jiawei Peng,Dehai Min,Changchang Sun,Kaijie Chen,Yan Yan,Xu Yang,Lu Cheng*

Main category: cs.CV

TL;DR: 该论文提出了MVI-Bench，这是第一个专门用于评估误导性视觉输入对大规模视觉-语言模型（LVLMs）鲁棒性影响的综合基准。研究发现现有LVLMs在面对误导性视觉输入时存在显著脆弱性，并提供了新的评测指标和数据集。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs的鲁棒性评估大多只关注幻觉或者误导性文本输入，忽视了现实应用中误导性视觉输入带来的挑战。因此需要一个系统化工具来专门评估这类鲁棒性。

Method: 作者基于三层次误导性视觉输入分类——视觉概念、视觉属性、视觉关系，构建了六大代表类、共1,248条专家标注的问题实例，并提出了细粒度的MVI-Sensitivity指标，评估18个主流LVLM模型的表现。

Result: 大量实验揭示出现有LVLMs在误导性视觉输入下存在显著的鲁棒性短板。新提出的MVI-Sensitivity指标能够有效区分模型的细粒度鲁棒性。

Conclusion: MVI-Bench为评估和改善LVLMs的视觉输入鲁棒性提供了重要工具并提出了实际改进建议，有助于推动更可靠、更健壮的视觉-语言模型的发展。

Abstract: Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.

</details>


### [60] [AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs](https://arxiv.org/abs/2511.14169)
*Xinliang Zhang,Lei Zhu,Hangzhou He,Shuang Zeng,Ourui Fu,Jiakui Hu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种面向对象级别的自适应token压缩策略，用于改进多模态大语言模型（MLLMs）的图文理解，显著减少token数量，提高效率，并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs通过将图像分割为patch级别的tokens来实现图文统一理解和推理。但这种做法导致token数量呈指数级增长，带来高计算和存储成本，并与人类视觉认知系统不一致，进而引发幻觉和冗余问题。

Method: 论文提出了一种对象级别的token合并（token merging）方法，实现自适应token压缩。该策略模拟人类视觉系统在感知对象时的分层和聚合方式，有效降低了token数量。

Result: 在多个全面的基准测试中，所提方法只使用了原模型10%的token数量，仍能达到原始模型96%的性能。与其他相关工作对比，提出的方法在压缩率与性能间达到更优平衡。

Conclusion: 对象级别的token合并显著提升了MLLMs的计算效率，并有效缓解传统patch级token化带来的计算和认知问题，为图文理解提供了更符合人类认知的新范式。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.

</details>


### [61] [DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2511.14179)
*Yanshan Li,Ke Ma,Miaomiao Wei,Linhui Dai*

Main category: cs.CV

TL;DR: 本文提出了一种基于博弈论的自监督对比学习框架DoGCLR，有效提升骨架动作识别的表现，特别是在关键运动区域的建模和负样本选择上实现突破。


<details>
  <summary>Details</summary>
Motivation: 当前自监督对比学习在骨架动作识别领域主要存在（1）对所有骨架区域一视同仁，导致运动信息损失和（2）负样本选择不优的问题。本研究旨在突破这些限制，提升特征表达和识别性能。

Method: 作者将正负样本的建模过程转化为一种动态的优势博弈，平衡语义保持与判别能力。方法包括时空双重加权定位机制定位关键运动区域并进行区域增强，以及基于熵的优势策略动态管理负样本，优先保留信息量丰富的负样本。

Result: 在NTU RGB+D和PKU-MMD数据集上，DoGCLR取得了SOTA水平的表现，对比提升最高达2.7%，并在更具挑战性的测试集上表现出卓越鲁棒性。

Conclusion: DoGCLR在动作识别表现和鲁棒性上均优于现有方法，区域关注与硬负样本策略能够有效提升自监督骨架动作识别的效果。

Abstract: Existing self-supervised contrastive learning methods for skeleton-based action recognition often process all skeleton regions uniformly, and adopt a first-in-first-out (FIFO) queue to store negative samples, which leads to motion information loss and non-optimal negative sample selection. To address these challenges, this paper proposes Dominance-Game Contrastive Learning network for skeleton-based action Recognition (DoGCLR), a self-supervised framework based on game theory. DoGCLR models the construction of positive and negative samples as a dynamic Dominance Game, where both sample types interact to reach an equilibrium that balances semantic preservation and discriminative strength. Specifically, a spatio-temporal dual weight localization mechanism identifies key motion regions and guides region-wise augmentations to enhance motion diversity while maintaining semantics. In parallel, an entropy-driven dominance strategy manages the memory bank by retaining high entropy (hard) negatives and replacing low-entropy (weak) ones, ensuring consistent exposure to informative contrastive signals. Extensive experiments are conducted on NTU RGB+D and PKU-MMD datasets. On NTU RGB+D 60 X-Sub/X-View, DoGCLR achieves 81.1%/89.4% accuracy, and on NTU RGB+D 120 X-Sub/X-Set, DoGCLR achieves 71.2%/75.5% accuracy, surpassing state-of-the-art methods by 0.1%, 2.7%, 1.1%, and 2.3%, respectively. On PKU-MMD Part I/Part II, DoGCLR performs comparably to the state-of-the-art methods and achieves a 1.9% higher accuracy on Part II, highlighting its strong robustness on more challenging scenarios.

</details>


### [62] [UniSER: A Foundation Model for Unified Soft Effects Removal](https://arxiv.org/abs/2511.14183)
*Jingdong Zhang,Lingzhi Zhang,Qing Liu,Mang Tik Chiu,Connelly Barnes,Yizhou Wang,Haoran You,Xiaoyang Liu,Yuqian Zhou,Zhe Lin,Eli Shechtman,Sohrab Amirghodsi,Xin Li,Wenping Wang,Xiaohang Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种统一的影像恢复模型UniSER，能在单一框架下修复多种由镜头炫光、雾霾、阴影及反射等软效应带来的退化问题，并在实际应用中优于现有专业或通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多专注于单一类型的影像退化，模型专业性强但缺乏通用性，且未能捕捉这些软效应退化间的共性。新一代大型通用模型虽具备编辑能力，但需复杂提示、在细粒度任务上效果不佳。此外，目前公开数据集在物理合理性及多样性上也存在不足。

Method: 作者构建了包含380万对样本的大型数据集，涵盖更多物理意义明确的样本，并在此基础上设计了专门的训练流程，对Diffusion Transformer进行微调，学习通用且鲁棒的影像恢复先验，同时集成细致的遮罩与强度控制模块。

Result: UniSER模型在针对软效应的多类别恢复任务中，显著优于以往专业模型和通用大模型，在各类真实场景下表现出高鲁棒性和高还原度。

Conclusion: 通过统一建模和大规模数据驱动，UniSER展现出恢复软效应退化的优异表现，证明了不同影像退化问题间存在可共享的本质，进一步拓宽了图像恢复的实际应用能力。

Abstract: Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.

</details>


### [63] [GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation](https://arxiv.org/abs/2511.14184)
*Xuan Zhao,Zhongyu Zhang,Yuge Huang,Yuxi Mi,Guodong Mu,Shouhong Ding,Jun Wang,Rizen Guo,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出一种新的图像分词器GloTok，通过利用全局关系信息，使分词特征的语义分布更均匀，从而提升了图像重建和生成的质量，在ImageNet-1k测试中达到了当前领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分词方法依赖已有视觉模型的局部语义特征进行监督，但这种局部方法使得语义分布不够均匀，影响生成质量。VA-VAE表明更均匀的分布有助于提升生成效果，因此需要新的方法提升语义分布的均匀性。

Method: 提出了Global Perspective Tokenizer (GloTok)，通过全局关系信息建模语义分布。具体包括：1）全局码本直方图关系学习，将预训练模型在整个数据集上的语义迁移到分词码本；2）引入残差学习模块，恢复被量化损失的细节，减少重建误差。

Result: GloTok实现了更均匀的语义潜在分布，有效辅助自回归模型训练，使生成高质量图像时无需访问预训练模型。在ImageNet-1k数据集上，GloTok在图像重建和生成质量上均优于已有方法。

Conclusion: 通过引入全局关系建模和残差模块，GloTok明显提升了图像生成和重建的表现，为语义分布均匀化和高质量生成提供了新思路。

Abstract: Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.

</details>


### [64] [PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation](https://arxiv.org/abs/2511.14185)
*Xiangyu Li,Chen Wang,Yumao Liu,Dengbo He,Jiahao Zhang,Ke Ma*

Main category: cs.CV

TL;DR: 本文提出了首个完全由自动驾驶模式下收集的端到端基准数据集，用于真实环境下自动驾驶车辆的行为安全评估。数据集包含超过100小时的自然数据和丰富注释，可持续更新扩展。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集多为人工或未知驾驶模式采集，仅适用于感知和预测的初步训练，无法充分评估黑盒控制下自动驾驶车辆的实际行为安全。

Method: 作者采集了市场上多款量产自动驾驶车辆在实际道路场景下，自动驾驶模式下的驾驶数据。原始数据被切分为32727个关键帧，每帧包含4路同步摄像头图像和高精度GNSS/IMU数据（定位精度0.8厘米），并针对周围车辆、行人、交通灯和标志提供详细2D注释。此外，提供了每帧过去6s和未来5s的车辆轨迹以及多种场景级属性。

Result: 利用所构建的数据集，作者采用端到端轨迹预测模型，自动驾驶关键帧上车辆运动的平均位移误差（ADE）为1.4米。数据集每周仍持续扩充约10小时新数据。

Conclusion: 该数据集为自动驾驶行为分析和安全评估提供了真实、持续的研究基础，有助于推动自动驾驶系统在实际环境下的可靠性与安全性提升。

Abstract: Most existing autonomous-driving datasets (e.g., KITTI, nuScenes, and the Waymo Perception Dataset), collected by human-driving mode or unidentified driving mode, can only serve as early training for the perception and prediction of autonomous vehicles (AVs). To evaluate the real behavioral safety of AVs controlled in the black box, we present the first end-to-end benchmark dataset collected entirely by autonomous-driving mode in the real world. This dataset contains over 100 hours of naturalistic data from multiple production autonomous-driving vehicle models in the market. We segment the original data into 32,727 key frames, each consisting of four synchronized camera images and high-precision GNSS/IMU data (0.8 cm localization accuracy). For each key frame, 20 Hz vehicle trajectories spanning the past 6 s and future 5 s are provided, along with detailed 2D annotations of surrounding vehicles, pedestrians, traffic lights, and traffic signs. These key frames have rich scenario-level attributes, including driver intent, area type (covering highways, urban roads, and residential areas), lighting (day, night, or dusk), weather (clear or rain), road surface (paved or unpaved), traffic and vulnerable road users (VRU) density, traffic lights, and traffic signs (warning, prohibition, and indication). To evaluate the safety of AVs, we employ an end-to-end motion planning model that predicts vehicle trajectories with an Average Displacement Error (ADE) of 1.4 m on autonomous-driving frames. The dataset continues to expand by over 10 hours of new data weekly, thereby providing a sustainable foundation for research on AV driving behavior analysis and safety evaluation.

</details>


### [65] [Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation](https://arxiv.org/abs/2511.14186)
*Zhaoyu Liu,Kan Jiang,Murong Ma,Zhe Hou,Yun Lin,Jin Song Dong*

Main category: cs.CV

TL;DR: 本文提出了UMEG-Net模型，用于在仅有少量标注数据的条件下，实现体育视频中关键事件的精准识别，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精确事件检测对体育分析至关重要，但受限于事件发生频率高、视觉细微差异大等因素，现有方法对大数据集依赖严重，且在少样本场景下表现不佳。获取大量标注数据在实际中又十分困难，因此亟需一种能够在少样本条件下实现精确事件检测的方法。

Method: 提出了统一多实体图网络（UMEG-Net），将人体骨架与运动特定目标关键点统一建模为一个图结构。方法结合了先进的时空图卷积网络和多尺度时序特征提取，同时利用多模态蒸馏，将关键点图结构的知识转移到视觉特征中以提升效果。

Result: UMEG-Net在仅有有限标注数据的少样本场景下，取得了稳健出色的表现，并显著优于主流基线方法。

Conclusion: 本文提出的UMEG-Net方法为体育类精确事件检测提供了高效、可扩展的少样本解决方案，在实际数据稀缺情况下也能保持卓越性能。

Abstract: Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.

</details>


### [66] [Hierarchical Semantic Learning for Multi-Class Aorta Segmentation](https://arxiv.org/abs/2511.14187)
*Pengcheng Shi*

Main category: cs.CV

TL;DR: 该论文提出了一种结合课程学习和新型分形softmax方法，处理主动脉及分支血管3D解剖分割任务，显著提升了分割准确性和效率，适用于临床实时应用。


<details>
  <summary>Details</summary>
Motivation: 主动脉疾病如夹层、动脉瘤等需及时干预，3D解剖分析精度要求高，现有分割方法忽视了解剖结构的层次关系且易受类别不平衡影响，急需更智能、有效的分割策略。

Method: 引入课程学习框架和分形softmax损失函数，以模仿人类认知方式分阶段学习解剖结构，从简单到复杂逐步建模层次关系。先聚焦占比大的主干类别，再学习难度更高的罕见关键结构，有效缓解类别不平衡。采用两阶段推理策略加速推理过程。

Result: 在验证集上，提出的分层语义损失使nnU-Net ResEnc M的Dice分数提升11.65%，在测试集上整体平均Dice分数较基准提升5.6%。模型推理速度最快可提升5倍。实验结果显示分割精度和效率均有明显提升。

Conclusion: 该方法显著改进了血管层次结构分割的准确率和推理速度，具备实时临床应用潜力。相关代码已开源，便于后续研究复现和推广。

Abstract: The aorta, the body's largest artery, is prone to pathologies such as dissection, aneurysm, and atherosclerosis, which often require timely intervention. Minimally invasive repairs involving branch vessels necessitate detailed 3D anatomical analysis. Existing methods often overlook hierarchical anatomical relationships while struggling with severe class imbalance inherent in vascular structures. We address these challenges with a curriculum learning strategy that leverages a novel fractal softmax for hierarchical semantic learning. Inspired by human cognition, our approach progressively learns anatomical constraints by decomposing complex structures from simple to complex components. The curriculum learning framework naturally addresses class imbalance by first establishing robust feature representations for dominant classes before tackling rare but anatomically critical structures, significantly accelerating model convergence in multi-class scenarios. Our two-stage inference strategy achieves up to fivefold acceleration, enhancing clinical practicality. On the validation set at epoch 50, our hierarchical semantic loss improves the Dice score of nnU-Net ResEnc M by 11.65%. The proposed model demonstrates a 5.6% higher Dice score than baselines on the test set. Experimental results show significant improvements in segmentation accuracy and efficiency, making the framework suitable for real-time clinical applications. The implementation code for this challenge entry is publicly available at: https://github.com/PengchengShi1220/AortaSeg24. The code for fractal softmax will be available at https://github.com/PengchengShi1220/fractal-softmax.

</details>


### [67] [Online Data Curation for Object Detection via Marginal Contributions to Dataset-level Average Precision](https://arxiv.org/abs/2511.14197)
*Zitang Sun,Masakazu Yoshimura,Junji Otsuka,Atsushi Irie,Takeshi Ohashi*

Main category: cs.CV

TL;DR: DetGain是一种专门为目标检测设计的在线数据筛选方法，通过动态选择对模型性能提升最有价值的训练样本，有效提升检测精度，且易于集成到不同检测框架中。


<details>
  <summary>Details</summary>
Motivation: 高质量、经过挑选的数据集在提升模型性能方面优于大量未过滤数据，但目标检测领域由于结构复杂性和领域差异，在线数据筛选方法很少被推广应用。作者希望解决目标检测任务中数据利用效率的问题。

Method: DetGain通过估算每张图片对数据集平均精度（AP）的边际影响，结合全局分数分布和教师-学生模型的贡献差距，动态筛选出对模型提升最有价值的训练样本。该方法对检测架构无依赖，可直接集成到多种检测器中。

Result: 在COCO数据集和多种主流目标检测器上的实验结果表明，DetGain能够稳定提升模型准确率，且在低质量数据和知识蒸馏结合时表现出强健性和可扩展性。

Conclusion: DetGain为目标检测领域提供了一种通用且高效的数据筛选策略，不仅提升了数据利用效率，还能进一步与其他技术（如知识蒸馏）结合增强性能，具有广泛的应用前景。

Abstract: High-quality data has become a primary driver of progress under scale laws, with curated datasets often outperforming much larger unfiltered ones at lower cost. Online data curation extends this idea by dynamically selecting training samples based on the model's evolving state. While effective in classification and multimodal learning, existing online sampling strategies rarely extend to object detection because of its structural complexity and domain gaps. We introduce DetGain, an online data curation method specifically for object detection that estimates the marginal perturbation of each image to dataset-level Average Precision (AP) based on its prediction quality. By modeling global score distributions, DetGain efficiently estimates the global AP change and computes teacher-student contribution gaps to select informative samples at each iteration. The method is architecture-agnostic and minimally intrusive, enabling straightforward integration into diverse object detection architectures. Experiments on the COCO dataset with multiple representative detectors show consistent improvements in accuracy. DetGain also demonstrates strong robustness under low-quality data and can be effectively combined with knowledge distillation techniques to further enhance performance, highlighting its potential as a general and complementary strategy for data-efficient object detection.

</details>


### [68] [Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification](https://arxiv.org/abs/2511.14203)
*Yunhe Liu*

Main category: cs.CV

TL;DR: 本论文提出了MCFormer，一种多尺度相关感知Transformer网络，用于提升船舶重识别效果，解决现有方法无法有效应对船舶图像中的高身份内变化和局部部件缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有船舶重识别方法多直接借鉴行人Re-ID算法，未能考虑船舶图像内高变化及局部缺失所带来的同一身份内的异常样本问题，影响实际应用表现。

Method: MCFormer包含两个创新模块：一是全局相关模块（GCM），通过在所有输入图像间构建相似度亲和矩阵，实现特征聚合，建模全局相关性；二是局部相关模块（LCM），利用动态内存库对正样本的局部特征进行挖掘和对齐，补偿局部丢失或遮挡区域。整体模型融合多尺度的全局与局部特征相关性，加强特征表达鲁棒性。

Result: 在三个公开船舶重识别基准上，MCFormer取得了当前最优的识别性能，优于现有方法。

Conclusion: MCFormer有效缓解了船舶图像中高身份内变化和局部缺失带来的负面影响，提升了模型在实际场景中的重识别可靠性和精度。

Abstract: Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.

</details>


### [69] [InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior](https://arxiv.org/abs/2511.14208)
*Weimin Bai,Suzhe Xu,Yiwei Ren,Jinhua Hao,Ming Sun,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为InstantViR的新方法，实现了超快的视频逆问题恢复，在保持高感知质量的同时极大提升了速度，使扩散模型的视频重建可以应用到实时场景。


<details>
  <summary>Details</summary>
Motivation: 视频逆问题对流媒体、远程通信和AR/VR等应用非常重要，需要同时兼顾重建质量和低延迟。但现有的扩散模型方法要么带来时间一致性差的问题，要么推理速度太慢，无法实时应用。

Method: InstantViR使用了蒸馏方法，将高质量的双向视频扩散模型（Teacher）蒸馏为一个基于因果自回归结构的学生模型（Student），可实现单次前向推理即可恢复视频，并完全避免了测试时的迭代优化。同时引入高效的LeanVAE结构，配合创新的教师空间正则化蒸馏方案，有效提升处理速度。该方法不需要成对的清洁/噪声视频数据，只需扩散教师模型和已知退化算子。

Result: 在视频修复（如随机补全、高斯去模糊、超分辨率）任务上，InstantViR的重建质量媲美甚至超过传统扩散方法，同时在NVIDIA A100 GPU上速度可达35FPS以上，速度提升高达100倍。

Conclusion: InstantViR证明了基于扩散模型的视频重建能够兼容实时、交互、可编辑、流式等应用场景，为高质量视频恢复的实际应用奠定了基础。

Abstract: Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.

</details>


### [70] [Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution](https://arxiv.org/abs/2511.14210)
*N Dinesh Reddy,Sudeep Pillai*

Main category: cs.CV

TL;DR: Orion 是一个能够处理任意模态输入与输出的视觉智能代理框架，通过集成多种视觉工具，实现自主、主动的视觉推理任务，并在多个基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型多侧重描述性输出，难以高效应对复杂、多步骤的视觉任务。论文旨在通过代理化思路，整合专业视觉工具，实现面向实际应用的生产级视觉智能，从而克服现有模型的局限。

Method: Orion 采用代理式架构，具备多工具调用能力。该系统能根据任务需要，有序调用目标检测、关键点定位、全景分割、OCR、几何分析等工具，完成包含多个复杂步骤的视觉流程，并结合神经感知与符号推理机制，实现主动视觉智能。

Result: Orion 在MMMU、MMBench、DocVQA和MMLongBench等多个公开基准任务上取得了有竞争力甚至领先的表现，展现了其跨模态理解及复杂视觉任务执行能力。

Conclusion: Orion 实现了由被动视觉理解向主动工具驱动的视觉智能转变，为视觉AI agent系统提供了新范式，具有产业落地与实际部署价值。

Abstract: We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.

</details>


### [71] [Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration](https://arxiv.org/abs/2511.14213)
*Wenjie Li,Yulun Zhang,Guangwei Gao,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出了Measurement-Constrained Sampling（MCS）方法，让极低质量人脸图像可以根据不同文本提示生成多样化的高质量复原。方法通过约束生成过程，兼顾原输入结构和多元语义，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的盲脸复原方法往往只能产生唯一的高质量结果，无法反映出极低质量输入下可能的多种不同还原样貌，限制了实际应用中的多样化需求。

Method: 作者将盲脸复原任务建模为一个受测量约束的生成任务。具体做法是在粗糙复原基础上，通过可控的退化过程构建逆问题，并在文本到图像扩散模型内部引入双重测量约束：前向测量保证生成结果与输入结构对齐，反向测量则提供投影空间，使生成能被不同文本提示灵活引导。

Result: 实验表明，新的MCS方法能够生成与文本提示相符、具有多样性的复原结果，并在多个指标上优于现有的盲脸复原方法。

Conclusion: MCS提升了盲脸复原的多样性和可控性，实现了输入结构保真与多种语义指导的兼得，为实际应用中的多解盲复原提供了有效工具。

Abstract: Blind face restoration (BFR) may correspond to multiple plausible high-quality (HQ) reconstructions under extremely low-quality (LQ) inputs. However, existing methods typically produce deterministic results, struggling to capture this one-to-many nature. In this paper, we propose a Measurement-Constrained Sampling (MCS) approach that enables diverse LQ face reconstructions conditioned on different textual prompts. Specifically, we formulate BFR as a measurement-constrained generative task by constructing an inverse problem through controlled degradations of coarse restorations, which allows posterior-guided sampling within text-to-image diffusion. Measurement constraints include both Forward Measurement, which ensures results align with input structures, and Reverse Measurement, which produces projection spaces, ensuring that the solution can align with various prompts. Experiments show that our MCS can generate prompt-aligned results and outperforms existing BFR methods. Codes will be released after acceptance.

</details>


### [72] [StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model](https://arxiv.org/abs/2511.14223)
*Yifan Yang,Zhi Cen,Sida Peng,Xiangwei Chen,Yifu Deng,Xinyu Zhu,Fan Jia,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 本文提出了一种自回归扩散模型，实现了实时、低延迟的语音驱动三维人脸动画，解决了现有方法在长语音输入下的延迟和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的3D人脸动画方法，处理超过训练长度的音频表现差、且计算延迟高，严重限制了其实时应用能力。

Method: 该文提出自回归扩散模型，以流式方式处理音频，并结合有限历史帧和音频片段形成动态条件，逐步生成高质量的人脸动画序列。

Result: 实验和实时交互演示表明，该方法能灵活处理不同长度音频，生成高质量、同步性好的人脸动画，并显著降低延迟。

Conclusion: 所提方法实现了实时、高效、泛化能力强的语音驱动三维人脸动画，推动了技术在实际应用中的落地。

Abstract: This paper focuses on the task of speech-driven 3D facial animation, which aims to generate realistic and synchronized facial motions driven by speech inputs.Recent methods have employed audio-conditioned diffusion models for 3D facial animation, achieving impressive results in generating expressive and natural animations.However, these methods process the whole audio sequences in a single pass, which poses two major challenges: they tend to perform poorly when handling audio sequences that exceed the training horizon and will suffer from significant latency when processing long audio inputs. To address these limitations, we propose a novel autoregressive diffusion model that processes input audio in a streaming manner. This design ensures flexibility with varying audio lengths and achieves low latency independent of audio duration. Specifically, we select a limited number of past frames as historical motion context and combine them with the audio input to create a dynamic condition. This condition guides the diffusion process to iteratively generate facial motion frames, enabling real-time synthesis with high-quality results. Additionally, we implemented a real-time interactive demo, highlighting the effectiveness and efficiency of our approach. We will release the code at https://zju3dv.github.io/StreamingTalker/.

</details>


### [73] [Breaking the Passive Learning Trap: An Active Perception Strategy for Human Motion Prediction](https://arxiv.org/abs/2511.14237)
*Juncheng Hu,Zijian Zhang,Zeyu Wang,Guoyu Wang,Yingji Li,Kedi Lyu*

Main category: cs.CV

TL;DR: 该论文提出了一种主动感知策略（APS）用于3D人体动作预测，通过商空间表示和辅助学习目标显式地优化时空建模，有效提升预测精度，在多个数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作预测方法过分依赖神经网络对时空关系的隐式建模，导致学习效率低、冗余信息多、缺乏主动引导的显式机制。因此，亟需一种方式提升模型对运动特性的有意义建模。

Method: 本文提出Active Perceptual Strategy（APS）：1）设计数据感知模块，通过将姿态投影到商空间，联合切向向量与Grassmann投影，有效减少冗余、加强语义解耦并施加运动约束；2）设计网络感知模块，通过主动遮蔽或加入噪声，构造辅助监督信号，由辅助学习网络主动适应扰动信息，强化时空依赖关系的学习。APS可无缝集成到不同的预测模型中。

Result: APS方法在多个公开数据集上取得了显著领先：H3.6M提升16.3%、CMU Mocap提升13.9%、3DPW提升10.1%。

Conclusion: 主动感知策略通过显式运动属性建模和辅助监督结构，有效提升了3D人体动作预测的表现，并且具备模型无关性和广泛适用性。

Abstract: Forecasting 3D human motion is an important embodiment of fine-grained understanding and cognition of human behavior by artificial agents. Current approaches excessively rely on implicit network modeling of spatiotemporal relationships and motion characteristics, falling into the passive learning trap that results in redundant and monotonous 3D coordinate information acquisition while lacking actively guided explicit learning mechanisms. To overcome these issues, we propose an Active Perceptual Strategy (APS) for human motion prediction, leveraging quotient space representations to explicitly encode motion properties while introducing auxiliary learning objectives to strengthen spatio-temporal modeling. Specifically, we first design a data perception module that projects poses into the quotient space, decoupling motion geometry from coordinate redundancy. By jointly encoding tangent vectors and Grassmann projections, this module simultaneously achieves geometric dimension reduction, semantic decoupling, and dynamic constraint enforcement for effective motion pose characterization. Furthermore, we introduce a network perception module that actively learns spatio-temporal dependencies through restorative learning. This module deliberately masks specific joints or injects noise to construct auxiliary supervision signals. A dedicated auxiliary learning network is designed to actively adapt and learn from perturbed information. Notably, APS is model agnostic and can be integrated with different prediction models to enhance active perceptual. The experimental results demonstrate that our method achieves the new state-of-the-art, outperforming existing methods by large margins: 16.3% on H3.6M, 13.9% on CMU Mocap, and 10.1% on 3DPW.

</details>


### [74] [Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization](https://arxiv.org/abs/2511.14238)
*Yan Huang,Yongyi Su,Xin Lin,Le Zhang,Xun Xu*

Main category: cs.CV

TL;DR: 本文提出了WeSTAR框架，用于提升单目深度估计基础模型在新领域的鲁棒性和泛化能力。通过结合自监督、分层归一化及弱监督信号，有效增强模型表现。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本泛化上已取得进展，但面向特定下游任务若可使用部分数据，其性能能否进一步提升还未被充分探讨。本文旨在回答基础模型在有弱标注/适应数据下，能否更好泛化的问题。

Method: 提出WeSTAR框架，使用稠密自训练结构化自监督作为核心目标；引入语义感知分层归一化，通过实例分割图提升归一化稳定性和多尺度能力；采用高效的弱监督，利用成对序关系进行适应，增强结构性约束；并通过权重正则稳定LoRA训练，防止遗忘基础知识。

Result: 在各类真实与干扰分布外数据集上进行了广泛实验，表明WeSTAR能持续提升模型泛化能力，在多个基准测试中达到最新最优性能。

Conclusion: WeSTAR有效提升了单目深度估计基础模型对各种新领域和异常场景的适应和泛化能力，是当前最先进的方法之一。

Abstract: The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

</details>


### [75] [V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization](https://arxiv.org/abs/2511.14247)
*Wenkai Lin,Qiming Xia,Wen Li,Xun Huang,Chenglu Wen*

Main category: cs.CV

TL;DR: 本文提出了一种基于激光雷达定位的多智能体协同感知框架，无需GNSS，能在GNSS失效环境下实现稳定、精确的协同感知。


<details>
  <summary>Details</summary>
Motivation: 传统基于GNSS的定位方法在GNSS信号被屏蔽或干扰的环境中容易失效，导致多智能体间的数据无法准确对齐，从而影响协同感知的效果。

Method: 提出了一个轻量级姿态生成器（PGC）用于估算姿态与置信度，并开发了姿态感知时空对齐Transformer（PASTAT），实现基于置信度的空间对齐与时序上下文建模。同时构建了专门的数据集V2VLoc用于训练与评测。

Result: 在V2VLoc仿真数据集上，该方法在GNSS失效条件下取得了最先进的性能，并在真实V2V4Real数据集上进一步验证了方法的有效性与泛化能力。

Conclusion: 本文提出的GNSS-free激光雷达协同感知方案解决了GNSS失效情况下的多智能体协同感知难题，具有较强的实际应用潜力和推广价值。

Abstract: Multi-agents rely on accurate poses to share and align observations, enabling a collaborative perception of the environment. However, traditional GNSS-based localization often fails in GNSS-denied environments, making consistent feature alignment difficult in collaboration. To tackle this challenge, we propose a robust GNSS-free collaborative perception framework based on LiDAR localization. Specifically, we propose a lightweight Pose Generator with Confidence (PGC) to estimate compact pose and confidence representations. To alleviate the effects of localization errors, we further develop the Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT), which performs confidence-aware spatial alignment while capturing essential temporal context. Additionally, we present a new simulation dataset, V2VLoc, which can be adapted for both LiDAR localization and collaborative detection tasks. V2VLoc comprises three subsets: Town1Loc, Town4Loc, and V2VDet. Town1Loc and Town4Loc offer multi-traversal sequences for training in localization tasks, whereas V2VDet is specifically intended for the collaborative detection task. Extensive experiments conducted on the V2VLoc dataset demonstrate that our approach achieves state-of-the-art performance under GNSS-denied conditions. We further conduct extended experiments on the real-world V2V4Real dataset to validate the effectiveness and generalizability of PASTAT.

</details>


### [76] [ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation](https://arxiv.org/abs/2511.14259)
*Zitong Xu,Huiyu Duan,Xiaoyu Wang,Zhaolin Cai,Kaiwei Zhang,Qiang Hu,Jing Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了ManipBench，是一个大规模AI图像篡改检测和定位的基准数据集，涵盖多种编辑模型和操作类型，并配有解释性信息。作者基于此开发了ManipShield模型，实现了统一检测、定位及解释，在多个数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，当前图像编辑方法远超传统deepfake技术，给篡改检测带来巨大挑战。然而，现有检测基准测试集内容单一、模型覆盖有限且解释性不足，限制了检测方法的泛化与可解释性能力。因此，作者希望通过构建更全面、丰富且具有解释性的基准来推动该领域发展。

Method: 1) 构建ManipBench：收集并制作了含超过45万张由25个SOTA模型编辑、12类操作生成的篡改图像数据集，其中部分有边界框、判断线索和文本解释等多层次注释。2) 提出ManipShield：基于多模态大语言模型，结合对比LoRA微调与任务特定解码器，实现对图像篡改的统一检测、定位与解释。

Result: ManipShield模型在ManipBench和多个公开数据集上进行了大量实验，结果表明其不仅达到当前最优水平，还能有效泛化到未见过的新型篡改模型。

Conclusion: ManipBench为AI生成图像篡改检测与解释性研究提供了全面基准，ManipShield则验证了基于多模态大语言模型的统一检测方法的有效性与强泛化能力，为后续相关研究提供了新思路。

Abstract: With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.

</details>


### [77] [Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery](https://arxiv.org/abs/2511.14270)
*Yiming Zeng,Xi-Le Zhao,Wei-Hao Wu,Teng-Yu Ji,Chao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯溅射（Gaussian Splatting）的低秩张量表示（GSLR）框架，显著提升多维图像的表征能力，尤其在捕捉局部高频信息方面优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统张量奇异值分解（t-SVD）在多维图像表示中有限制，包括：1）潜在张量的近似不够精细，无法充分捕捉空间局部高频信息；2）变换矩阵通常为固定基（如DFT和DCT），难以精准表示mode-3方向的局部高频信息。为解决这两个痛点，亟需更强表达力的张量分解方法。

Method: 提出GSLR框架，通过2D高斯溅射生成潜在张量，通过1D高斯溅射生成变换矩阵，两者互补，能够高效、连续且紧凑地表征多维图像的结构。论文还以无监督方式将该框架应用于多维图像恢复任务。

Result: 在多个多维图像恢复实验中，GSLR框架在局部高频信息捕捉和整体复原质量上，均优于当前最先进的方法。

Conclusion: GSLR为多维图像提供了更强的低秩张量表达能力，尤其在局部高频特征的捕捉与恢复方面，明显提升了表现，有望为相关领域带来效能突破。

Abstract: Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.

</details>


### [78] [Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation](https://arxiv.org/abs/2511.14271)
*Weimin Bai,Yubo Li,Weijian Luo,Zeqiang Lai,Yequan Wang,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 本文提出VLM3D框架，利用大型视觉-语言模型作为可微分的语义和空间评审者，有效提升Text-to-3D任务中的细节语义对齐和3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管当前Text-to-3D生成技术进步迅速，但主流模型在细致语义捕捉和空间一致性方面存在显著缺陷，导致生成结果在精细层面和结构上的失败。

Method: 作者提出VLM3D框架，将大型视觉-语言模型（VLM）用于同时评估语义与几何一致性。具体通过VLM的Yes/No对数几率，提取双重查询评审信号，将其分别作为奖励目标应用于基于优化的生成流程，以及作为推理时引导用于优化基于前馈的生成流程。

Result: VLM3D框架在常规评测基准上，作为优化目标时显著超越了现有方法。作为推理引导模块也能有效纠正主流3D模型生成过程中的严重空间错误。

Conclusion: VLM3D为在多种3D生成流程中注入VLM强大的语义与空间理解能力提供了通用、可解释的方案，显著提升了Text-to-3D生成的细致性和空间合理性。

Abstract: Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.

</details>


### [79] [Free Lunch to Meet the Gap: Intermediate Domain Reconstruction for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.14279)
*Tong Zhang,Yifan Zhao,Liangyu Wang,Jia Li*

Main category: cs.CV

TL;DR: 该论文提出了一种利用中间域代理进行跨域小样本学习的新方法，显著提升跨域迁移能力。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习(CDFSL)要求在源领域和目标领域差异较大、数据稀缺的情况下实现知识迁移。传统方法主要关注泛化特征表示，难以解决语义不一致、域差异大和数据稀少等挑战。

Method: 作者提出构建中间域代理(IDP)：利用源领域特征嵌入构建码本，再用该码本重构目标域特征，并从视觉风格和语义内容分析中间域属性。此外，提出基于IDP的快速域对齐方法，用IDP引导目标特征变换，通过中间域重建和目标变换协同学习。

Result: 该方法在8个跨域小样本学习基准上均超过了当前最先进的模型。

Conclusion: 通过创新的中间域代理和高效的协同学习策略，有效提升了模型在跨域小样本学习场景下的表现，表明中间域代理对于解决领域差异和数据稀缺问题具有实际价值。

Abstract: Cross-Domain Few-Shot Learning (CDFSL) endeavors to transfer generalized knowledge from the source domain to target domains using only a minimal amount of training data, which faces a triplet of learning challenges in the meantime, i.e., semantic disjoint, large domain discrepancy, and data scarcity. Different from predominant CDFSL works focused on generalized representations, we make novel attempts to construct Intermediate Domain Proxies (IDP) with source feature embeddings as the codebook and reconstruct the target domain feature with this learned codebook. We then conduct an empirical study to explore the intrinsic attributes from perspectives of visual styles and semantic contents in intermediate domain proxies. Reaping benefits from these attributes of intermediate domains, we develop a fast domain alignment method to use these proxies as learning guidance for target domain feature transformation. With the collaborative learning of intermediate domain reconstruction and target feature transformation, our proposed model is able to surpass the state-of-the-art models by a margin on 8 cross-domain few-shot learning benchmarks.

</details>


### [80] [NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction](https://arxiv.org/abs/2511.14283)
*Zi-Chen Xi,Jiahui Huang,Hao-Xiang Chen,Francis Williams,Qun-Ce Xu,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D隐式表面重建方法NeuralSSD，能够从点云数据中高质量地重建表面，并在多个数据集取得了领先的重建精度及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 隐式方法能高效处理表面形状和拓扑的变化，但现有方法难以保证重建表面和输入点云紧密拟合，因此需要设计更有效的模型与拟合机制。

Method: 提出了基于神经Galerkin方法的求解器NeuralSSD，引入了新的能量方程以平衡点云信息，并设计了新型三维卷积网络用于更好地学习点云结构，实现了对重建表面的优化拟合。

Result: 在ShapeNet和Matterport等多个挑战性数据集上，NeuralSSD在表面重建的准确性和泛化性方面均取得了当前最优的结果。

Conclusion: NeuralSSD能够有效从点云中重建高质量的3D隐式表面，且兼具准确性、稳定性与泛化能力，推动了相关领域的发展。

Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.

</details>


### [81] [NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration](https://arxiv.org/abs/2511.14286)
*Luohong Wu,Matthias Seibold,Nicola A. Cavalcanti,Yunke Ao,Roman Flepp,Aidana Massalimova,Lilian Calvet,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: 本文提出了一种名为NeuralBoneReg的自监督骨骼表面配准方法，实现了对骨骼表面的鲁棒、自动、模态无关的三维点云配准，能有效促进计算机及机器人辅助手术中的跨模态配准任务。


<details>
  <summary>Details</summary>
Motivation: 在计算机及机器人辅助手术（CAOS）中，术前影像的个体化手术方案需要精确地在手术过程中进行映射，而由于影像模态的多样性，配准过程复杂且容易出错，因此亟需一种鲁棒且自动化的骨表面配准方法，实现不同模态间的高准确匹配。

Method: 提出NeuralBoneReg框架，包括一个用于学习术前骨骼模型的隐式神经无符号距离场（UDF）模块，以及一个通过生成变换假设完成全局初始化和局部细化的MLP配准模块；该方法采用自监督方式，无需跨受试者的训练数据，通过3D点云进行模态无关的配准。

Result: 在UltraBones100k（CT-超声）、SpineDepth（CT-RGB-D）以及新引入的UltraBones-Hip（CT-超声，包含髋部）三个公开数据集上，NeuralBoneReg与当前先进方法相比，配准精度匹敌或优于现有方法，其中UltraBones100k上平均RRE/RTE为1.68°/1.86 mm，UltraBones-Hip为1.88°/1.89 mm，SpineDepth为3.79°/2.45 mm。

Conclusion: NeuralBoneReg展现出优异的解剖与模态泛化能力，可为CAOS中的跨模态骨表面高精度配准提供强有力支持。

Abstract: In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories. During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data. However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone. Robust, automatic, and modality-agnostic bone surface registration is therefore clinically important. We propose NeuralBoneReg, a self-supervised, surface-based framework that registers bone surfaces using 3D point clouds as a modality-agnostic representation. NeuralBoneReg includes two modules: an implicit neural unsigned distance field (UDF) that learns the preoperative bone model, and an MLP-based registration module that performs global initialization and local refinement by generating transformation hypotheses to align the intraoperative point cloud with the neural UDF. Unlike SOTA supervised methods, NeuralBoneReg operates in a self-supervised manner, without requiring inter-subject training data. We evaluated NeuralBoneReg against baseline methods on two publicly available multi-modal datasets: a CT-ultrasound dataset of the fibula and tibia (UltraBones100k) and a CT-RGB-D dataset of spinal vertebrae (SpineDepth). The evaluation also includes a newly introduced CT--ultrasound dataset of cadaveric subjects containing femur and pelvis (UltraBones-Hip), which will be made publicly available. NeuralBoneReg matches or surpasses existing methods across all datasets, achieving mean RRE/RTE of 1.68°/1.86 mm on UltraBones100k, 1.88°/1.89 mm on UltraBones-Hip, and 3.79°/2.45 mm on SpineDepth. These results demonstrate strong generalizability across anatomies and modalities, providing robust and accurate cross-modal alignment for CAOS.

</details>


### [82] [GEN3D: Generating Domain-Free 3D Scenes from a Single Image](https://arxiv.org/abs/2511.14291)
*Yuxin Zhang,Ziyu Lu,Hongbo Duan,Keyu Fan,Pengting Luo,Peiyu Zhuang,Mengyu Yang,Houde Liu*

Main category: cs.CV

TL;DR: 提出了一种能从单张图片生成高质量、广域通用3D场景的新方法Gen3d，实验显示其在多数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经3D重建方法需要大量视角采集，限制了适用范围，而生成高质量3D场景对具身智能和世界模型发展至关重要。

Method: Gen3d方法先将单张RGBD图像提升为初始点云，然后通过维护和扩展世界模型，最后采用高斯点渲染进行3D场景优化。

Result: 在多个数据集上的实验表明Gen3d在生成世界模型、合成高保真和一致性新视角方面表现出很强的泛化能力和优越性能。

Conclusion: Gen3d有效解决了只需单图即可生成高质量、多样性3D场景的问题，推动了3D场景生成和相关AI领域的发展。

Abstract: Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.

</details>


### [83] [SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation](https://arxiv.org/abs/2511.14302)
*Sahar Nasirihaghighi,Negin Ghamsarian,Yiping Li,Marcel Breeuwer,Raphael Sznitman,Klaus Schoeffmann*

Main category: cs.CV

TL;DR: 本文提出了SAM-Fed框架，通过高能力分割基础模型指导轻量级客户端，实现更高效的联邦半监督医学图像分割，有效提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床应用很重要，但数据隐私保护和高昂的专家标注成本导致标注数据稀缺。虽然联邦半监督学习(FSSL)能缓解数据共享问题，但伪标签可靠性和客户端设备算力受限等现实条件限制了现有方法的性能。

Method: 提出SAM-Fed框架：利用高能力的分割基础模型为轻量级客户端提供训练指导，结合双重知识蒸馏和自适应一致性机制，优化像素级伪标签监督。

Result: 在皮肤病变和肠息肉分割任务以及同质/异质客户端设置下，SAM-Fed方法的分割性能均优于当前主流的FSSL方法。

Conclusion: SAM-Fed能有效提升低算力设备在联邦半监督医学图像分割中的精度和稳定性，具有实际推广价值。

Abstract: Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.

</details>


### [84] [Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model](https://arxiv.org/abs/2511.14310)
*Jiancheng Fang,Shaoyu Wang,Junlin Wang,Weiwen Wu,Yikun Zhang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diffusion-Refined Neural Attenuation Fields (Diff-NAF)的新方法，以解决多源静态CT系统在超稀疏采样下重建质量显著下降的问题。该方法通过迭代优化显著提升CT重建质量，在模拟和真实数据上都取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 多源静态CT因其能够快速重建图像，适合临床和工业中对时效性要求高的场景。然而，实际应用中常受限于超稀疏采样，导致图像重建质量严重受损，传统方法难以有效恢复高质量图像，因此亟需创新算法解决该难题。

Method: 提出Diff-NAF框架，将神经衰减场（NAF）和双分支条件扩散模型结合。首先用超稀疏采样训练初始NAF，通过角度先验指导的投影合成策略生成新投影，再用基于扩散模型的模块进行精化。这些精化后的投影作为伪标签用于下一轮训练，实现多轮迭代优化。

Result: 在多个模拟3D CT体积数据和真实投影数据上，Diff-NAF相比传统方法在超稀疏条件下取得了更高质量的重建结果。

Conclusion: Diff-NAF能够持续增强超稀疏条件下的投影完整性和重建质量，是解决多源静态CT超稀疏重建的有效方法。

Abstract: Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.

</details>


### [85] [Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs](https://arxiv.org/abs/2511.14315)
*Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dental3R的新方法，解决了远程正畸中利用稀疏口腔照片进行高保真3D重建的问题，相较于传统方法在数据稀疏和光照不一致等条件下有明显优势。


<details>
  <summary>Details</summary>
Motivation: 远程正畸依赖智能手机照片，而传统口内三维重建技术如三维扫描仪昂贵且对远程不可及。现有3DGS技术难以应对标准临床三张无姿态照片在视角、光照及表面反光等问题。稀疏视图还会导致重建结果细节丢失，影响诊断。

Method: 提出Dental3R方法，有两大关键：一是几何感知配对策略（GAPS），智能筛选高价值照片对，优化几何初始化和降低内存消耗；二是结合点云和波形正则化目标对3DGS模型进行训练，通过小波变换保持细致牙釉质边缘和牙缝细节，并去除高频噪声伪影。

Result: 在950个临床及195个视频评测数据上，Dental3R对稀疏未定姿态口内照片可实现稳定、细节保留且高质量的新视角合成，显著优于现有最先进的方法。

Conclusion: Dental3R为远程正畸领域提供了一套高效、抗干扰、高保真的稀疏口腔照片三维重建方案，有望推动远程牙科数字化的发展。

Abstract: Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.

</details>


### [86] [LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices](https://arxiv.org/abs/2511.14322)
*Nanjun Li,Ziyue Hao,Quanqiang Wang,Xuanyin Wang*

Main category: cs.CV

TL;DR: 提出了一种高效、轻量级且适用于边缘设备的单阶段坐姿识别网络LSP-YOLO，解决了现有方法在计算和实时性能上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有依赖多阶段流程的坐姿识别方法在嵌入式边缘设备上侵入性高、计算量大、实时性能差，因此亟需一种兼顾效率和精度的轻量级识别方案。

Method: 受YOLOv11-Pose启发，设计了单阶段网络LSP-YOLO，引入部分卷积（PConv）和Similarity-Aware Activation Module（SimAM）以提升特征提取效率，提出轻量模块Light-C3k2，并在输出端通过逐点卷积直接将关键点映射为姿态类别；同时引入中间监督机制以优化估计与分类的融合。此外，构建了包含六类姿态共5000张图片的数据集用于训练与测试。

Result: 最轻量版LSP-YOLO-n模型在PC上精度达到94.2%，帧率251Fps，模型大小仅1.9MB。在资源受限的平台（SV830C+GC030A）下也可实现高精度实时推理。

Conclusion: 该方法具有效率高、模型轻量、易部署等优点，适合智能教室、康复以及人机交互场景应用。

Abstract: With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.

</details>


### [87] [Step by Step Network](https://arxiv.org/abs/2511.14329)
*Dongchen Han,Tianzhu Ye,Zhuofan Xia,Kaiyi Chen,Yulin Wang,Hanting Chen,Gao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Step by Step Network（StepsNet）的新的残差结构，以突破深层神经网络在理论与实际表现之间的瓶颈，显著提升神经网络深度可扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上更深的网络具有更强的能力，现有残差网络在实际深度扩展中遭遇性能瓶颈。论文分析发现存在两大障碍：shortcut降级与通道宽度受限。

Method: 作者提出了StepsNet，通过在通道维度分割特征并堆叠宽度递增的模块，逐步学习特征，从而既解决shortcut降级，也突破宽度限制。该结构能够灵活应用于各种模型。

Result: 在图像分类、目标检测、语义分割和语言建模等多项任务上的大量实验表明，StepsNet在各类场景下均显著优于传统残差网络。

Conclusion: StepsNet作为广泛采用的残差结构的优良推广形式，不仅充分释放了深度模型的潜力，也为更深网络的设计提供了新的思路和实践路径。

Abstract: Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.

</details>


### [88] [ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding](https://arxiv.org/abs/2511.14336)
*Bohan Zhang,Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhuoxiao Li,Zhe Tang,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 本文提出了无需训练的新方法ArchMap，通过几何归一化和平稳的知识库约束，实现了更精确、稳定、泛化性强的口腔3D扫描语义理解，适用于临床正畸数字化流程。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法对特定扫描设备、庞大标注数据和扫描条件要求高，影响不同设备间泛化能力和临床应用。原始口内3D扫描数据存在姿态不一致、几何不完整、无纹理等问题，导致统一语义解释困难。

Method: 提出ArchMap方法，通过几何感知的弓形展开模块，将3D网格标准化为对齐、连续的多视图投影；同时构建牙科知识库，将分层牙齿本体、牙列阶段、临床语义编码纳入推理约束，实现无监督且基于知识引导的多模态符号推理。

Result: 在共1060例正畸前后病例上验证，ArchMap在牙齿计数、解剖分区、牙列阶段分类及拥挤、缺牙、修复、龋坏等条件识别任务中均表现出更好的准确性、语义一致性和在稀疏/噪声场景下的鲁棒性，优于现有监督/视觉大模型基线。

Conclusion: ArchMap无需训练，通过几何归一与本体知识指导的符号推理，为数字正畸3D扫描数据的结构化语义分析提供了强泛化能力和高实用性的解决方案。

Abstract: A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.

</details>


### [89] [Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs](https://arxiv.org/abs/2511.14343)
*Yiyi Miao,Taoyu Wu,Ji Jiang,Tong Chen,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 作者提出了一种新的3D牙模和头影测量X光片配准方法（DentalSCR），可在临床条件下实现更准确、鲁棒的配准。


<details>
  <summary>Details</summary>
Motivation: 传统基于强度的配准方法在临床中往往受图像失真、低对比度、投影变形等因素影响，导致配准不准或失败。因此亟需更稳健并且符合解剖实际的方法。

Method: 方法包括建立U-Midline Dental Axis（UMDA）统一坐标系，标准化投影几何，然后使用表面投影（表面DRR+高斯点）合成类似X光的投影图像，利用对称双向Chamfer距离进行2D轮廓配准，采用分层粗到细优化策略以兼顾大范围和高精度。

Result: 在34个专业标记的病例上测试，DentalSCR在后牙点、下颌、整体轮廓误差和曲线距离等指标均明显优于传统方法，表现出高准确率和低误差。

Conclusion: DentalSCR有效克服了实际头影测量X光片中存在的各种干扰因素，实现了高保真的3D-2D配准，优于传统方法，临床价值较高。

Abstract: Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.

</details>


### [90] [ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries](https://arxiv.org/abs/2511.14349)
*Junfu Pu,Teng Wang,Yixiao Ge,Yuying Ge,Chen Li,Ying Shan*

Main category: cs.CV

TL;DR: 本文提出了ARC-Chapter大规模视频分章节模型，并构建了包含百万级章节标注的中英双语数据集，实现了在长视频内容结构化上的新突破。模型在实际和下游任务上都表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前长视频内容如讲座、播客、纪录片激增，用户对高效内容结构化需求增强。然而以往方法因标注粒度粗、规模小，难以处理长视频中细致、复杂的章节划分。

Method: 作者构建了一个中英双语的长视频章节标注数据集，融合ASR文本、场景文字、视觉描述，多层级注释包括短标题至长摘要。基于大规模数据训练ARC-Chapter模型，并提出新评价指标GRACE，更精准衡量章节划分的灵活性和语义相似性。

Result: ARC-Chapter在多项实验中大幅提升了性能，F1得分提升14.0%，SODA得分提升11.3%。模型还在YouCook2等下游密集视频描述任务上带来最优转移性能。

Conclusion: ARC-Chapter树立了长视频分章节的新标杆，配置大规模多语言、层次化数据，适应现实视频结构拆解需求，并能显著提升相关下游任务的表现。

Abstract: The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.

</details>


### [91] [IBGS: Image-Based Gaussian Splatting](https://arxiv.org/abs/2511.14357)
*Hoang Chuong Nguyen,Wei Mao,Jose M. Alvarez,Miaomiao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像的高斯投影方法，能高效提升三维新视角合成任务的图像细节和视角相关效果，同时保持存储成本低。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯投影方法在视角合成时，难以处理空间变色和高频细节，如高光等，仅用低阶球谐函数表达色彩带来真实性不足。此前的改进方法要么用全球纹理图，在复杂场景下效果差，要么对每个高斯用纹理图，导致存储量剧增。

Method: 作者提出基于图像的高斯投影（Image-Based Gaussian Splatting）：用3DGS正常流程得到基础颜色，再通过从邻近训练图像学习得到的残差进行修正，从而更好地拟合高频细节和视角变化。该方法直接利用高分辨率图像信息，无需复杂存储结构或大规模参数化。

Result: 在标准的新视角合成（NVS）基准测试上，该方法大幅提升了成像质量，尤其是在细节还原和视角相关特效方面优于以往高斯投影方法，同时没有增加存储开销。

Conclusion: 提出的基于图像的高斯投影方法兼顾高质量渲染和存储高效性，为三维新视角合成任务提供了更好的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.

</details>


### [92] [Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements](https://arxiv.org/abs/2511.14361)
*Gustavo Adolpho Bonesso,Carlos Marcelo Gurjão de Godoy,Tammy Hentona Osaki,Midori Hentona Osaki,Bárbara Moreira Ribeiro Trindade dos Santos,Regina Célia Coelho*

Main category: cs.CV

TL;DR: 本研究开发并验证了一款名为Bapp的手机应用，该应用可高效、准确地实时分析眼睑运动，为眨眼评估提供了便携、客观的新工具。


<details>
  <summary>Details</summary>
Motivation: 目前眨眼评估工具操作复杂、成本高、临床适用性有限，亟需更便捷、经济的新方法来对眼睑运动进行客观评估和临床监测。

Method: 研究团队基于Flutter框架开发了Bapp应用，并集成Google ML Kit，在移动端实现实时眨眼检测。通过Pauli​​sta医学院专家手工标注的45份临床视频数据，与Bapp检测结果进行对比和定量评估，采用准确率、召回率和F1-Score等评价指标。

Result: Bapp在真实患者视频测试中表现出优异的检测效果，准确率达98.4%，召回率96.9%，整体准确度98.3%。

Conclusion: Bapp作为移动端眨眼分析工具，具备高度可靠性和便携性，为日常临床眨眼监测及术后评估提供了有效的替代方案，有望在临床应用中取代传统人工计数，提升眼部健康管理效率。

Abstract: Blinking is a vital physiological process that protects and maintains the health of the ocular surface. Objective assessment of eyelid movements remains challenging due to the complexity, cost, and limited clinical applicability of existing tools. This study presents the clinical validation of Bapp (Blink Application), a mobile application developed using the Flutter framework and integrated with Google ML Kit for on-device, real-time analysis of eyelid movements. The validation occurred using 45 videos from real patients, whose blinks were manually annotated by ophthalmology specialists from the Paulista School of Medicine of the Federal University of Sao Paulo (EPM-UNIFESP) to serve as the ground truth. Bapp's performance was evaluated using standard metrics, including Precision, Recall, and F1-Score, with results demonstrating 98.4% precision, 96.9% recall, and an overall accuracy of 98.3%. These outcomes confirm the reliability of Bapp as a portable, accessible, and objective tool for monitoring both normal and abnormal eyelid movements. The application offers a promising alternative to traditional manual blink counting, supporting continuous ocular health monitoring and postoperative evaluation in clinical environments.

</details>


### [93] [Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection](https://arxiv.org/abs/2511.14371)
*Xiaolin Wang,Houzhang Fang,Qingshan Li,Lu Wang,Yi Chang,Luxin Yan*

Main category: cs.CV

TL;DR: 该论文针对红外无人机目标图像在快速运动下容易出现运动模糊，导致难以检测的问题，提出了一个端到端的联合特征域去模糊与检测框架（JFD3），并验证了其在构建的数据集上性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的去模糊方法主要关注视觉质量，并未针对检测任务中目标与背景的可区分性进行优化。红外无人机场景下，运动模糊严重影响了检测性能，因此亟须提升模糊条件下的特征表征能力，增强检测效果。

Method: 提出了JFD3框架，包括共享权重的双分支结构：清晰分支对模糊分支进行特征监督，通过特征恢复网络引导模糊分支强化检测特征，另设计频域结构引导模块，将目标结构信息融合进检测网络浅层，并用自监督损失约束两个分支的特征一致性。同时，构建了红外模糊无人机图像基准数据集（IRBlurUAV）。

Result: 在IRBlurUAV数据集上进行了大量实验，结果显示JFD3框架不仅能有效提升检测准确率，同时保持了实时检测效率。

Conclusion: JFD3框架通过端到端特征级联合去模糊和检测，有效提升了红外运动模糊无人机场景下的检测性能，为相关任务提供了可行的新方法和数据基准。

Abstract: Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background. Generally, detection performance heavily depends on the discriminative feature representation between target and background. Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection. Improving feature representation for detection under blur conditions remains challenging. In this paper, we propose a novel Joint Feature-Domain Deblurring and Detection end-to-end framework, dubbed JFD3. We design a dual-branch architecture with shared weights, where the clear branch guides the blurred branch to enhance discriminative feature representation. Specifically, we first introduce a lightweight feature restoration network, where features from the clear branch serve as feature-level supervision to guide the blurred branch, thereby enhancing its distinctive capability for detection. We then propose a frequency structure guidance module that refines the structure prior from the restoration network and integrates it into shallow detection layers to enrich target structural information. Finally, a feature consistency self-supervised loss is imposed between the dual-branch detection backbones, driving the blurred branch to approximate the feature representations of the clear one. Wealso construct a benchmark, named IRBlurUAV, containing 30,000 simulated and 4,118 real infrared UAV target images with diverse motion blur. Extensive experiments on IRBlurUAV demonstrate that JFD3 achieves superior detection performance while maintaining real-time efficiency.

</details>


### [94] [A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents](https://arxiv.org/abs/2511.14376)
*Alfonso Pedro Ridao*

Main category: cs.CV

TL;DR: 本文提出了一种名为肩部展示评估（SPE）的算法，用于自动评估生物特征证件照片中肩部摆正与否，并在小型数据集上验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 国际生物特征证件标准要求受试者肩部正对镜头，但现有自动化质量评估方法很少对肩部姿态进行量化评估。为弥补这一空缺，研究者提出了新算法。

Method: 该方法只利用常见姿态估计算法可提供的两侧肩膀3D坐标，量化肩部的偏航和横滚角度，以判定肩部是否摆正。

Result: 算法在121张肖像图片数据集上测试，与人工标签呈现较高的皮尔逊相关系数（约0.80），并通过Error-versus-Discard方法验证了对不合格样本的识别能力。

Conclusion: 该算法是一种轻量化、有效的工具，可用于自动化身份信息采集系统中的肩部姿态合规性检测。

Abstract: International standards for biometric identity documents mandate strict compliance with pose requirements, including the square presentation of a subject's shoulders. However, the literature on automated quality assessment offers few quantitative methods for evaluating this specific attribute. This paper proposes a Shoulder Presentation Evaluation (SPE) algorithm to address this gap. The method quantifies shoulder yaw and roll using only the 3D coordinates of two shoulder landmarks provided by common pose estimation frameworks. The algorithm was evaluated on a dataset of 121 portrait images. The resulting SPE scores demonstrated a strong Pearson correlation (r approx. 0.80) with human-assigned labels. An analysis of the metric's filtering performance, using an adapted Error-versus-Discard methodology, confirmed its utility in identifying non-compliant samples. The proposed algorithm is a viable lightweight tool for automated compliance checking in enrolment systems.

</details>


### [95] [Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving](https://arxiv.org/abs/2511.14386)
*Kangqiao Zhao,Shuo Huai,Xurui Song,Jun Luo*

Main category: cs.CV

TL;DR: 本论文提出了一种针对自动驾驶中的立体视觉深度估计算法的物理对抗样本攻击方法，通过3D全局伪装纹理进行攻击，能够有效欺骗深度估计模型。


<details>
  <summary>Details</summary>
Motivation: 当前大多数对抗样本攻击集中于2D贴片且主要针对单目感知，对双目立体视觉下的物理对抗样本（PAEs）攻击研究极少。实际应用中，自动驾驶更常用双目感知，因此有必要探索并攻破该领域的安全性。

Method: 作者设计了一种3D的全局伪装纹理物理对抗样本（PAE），替代局部2D贴片攻击方式，使攻击对不同立体摄像头视角保持一致。为适应双目摄像头的视差，提出新的3D渲染模块，对抗样本可与实景位置、朝向精准匹配。同时，提出一种新型融合攻击，将目标精细优化地融入背景环境，提高隐蔽性和攻击成功率。

Result: 在对多种主流立体视觉模型的广泛实验评估中，所提出PAE可有效误导模型输出错误的深度估计信息，攻击效果优于现有的隐藏攻击方法。

Conclusion: 该工作首次实现了针对立体视觉深度估计的高隐蔽性、高有效性物理对抗样本，显示了自动驾驶安全领域新的潜在安全隐患和亟需关注的研究方向。

Abstract: Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.

</details>


### [96] [Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition](https://arxiv.org/abs/2511.14391)
*Fabian Schmidt,Noushiq Mohammed Kayilan Abdul Nazar,Markus Enzweiler,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出了一种称为TLS-Assist的模块化冗余层，提升大语言模型（LLM）在自动驾驶决策中的安全和守规能力，主要聚焦于对交通信号灯和标志的识别。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动驾驶中的推理、泛化能力强，但现有方法难以确保严格遵守交通规则，且对识别重要的小型交通元素（如红绿灯、交通标志）不够可靠。为提升安全性与合规性，亟需一种辅助机制。

Method: 提出TLS-Assist模块，将交通信号灯和标志的检测结果转为结构化自然语言消息，注入大语言模型输入中，显式强化模型对关键安全线索的关注。该方案兼容多种模型，并支持单视角与多视角摄像头。

Result: 在CARLA仿真环境的LangAuto基准上对TLS-Assist进行评测，结果显示驾驶性能相较基线（LMDrive、BEVDriver）分别有最高14%和7%的提升，且明显减少了信号灯和标志违规率。

Conclusion: TLS-Assist有效弥补了LLM自动驾驶系统在交通信号理解上的短板，提高了安全性和交通规则遵循度，在不同模型和摄像头配置下均有推广性和实用价值。

Abstract: Large Language Models (LLMs) are increasingly used for decision-making and planning in autonomous driving, showing promising reasoning capabilities and potential to generalize across diverse traffic situations. However, current LLM-based driving agents lack explicit mechanisms to enforce traffic rules and often struggle to reliably detect small, safety-critical objects such as traffic lights and signs. To address this limitation, we introduce TLS-Assist, a modular redundancy layer that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition. TLS-Assist converts detections into structured natural language messages that are injected into the LLM input, enforcing explicit attention to safety-critical cues. The framework is plug-and-play, model-agnostic, and supports both single-view and multi-view camera setups. We evaluate TLS-Assist in a closed-loop setup on the LangAuto benchmark in CARLA. The results demonstrate relative driving performance improvements of up to 14% over LMDrive and 7% over BEVDriver, while consistently reducing traffic light and sign infractions. We publicly release the code and models on https://github.com/iis-esslingen/TLS-Assist.

</details>


### [97] [BEDLAM2.0: Synthetic Humans and Cameras in Motion](https://arxiv.org/abs/2511.14394)
*Joachim Tesch,Giorgio Becherini,Prerana Achar,Anastasios Yiannakidis,Muhammed Kocabas,Priyanka Patel,Michael J. Black*

Main category: cs.CV

TL;DR: 作者提出了BEDLAM2.0数据集，显著扩展了现有的BEDLAM数据集，通过增加多样的摄像机、人体外形、动作、服饰、发型、3D环境和鞋子，提升了数据的多样性和真实性，从而推动了3D人体运动在世界坐标系下的估计研究。


<details>
  <summary>Details</summary>
Motivation: 大多数现有方法只在图像坐标系下估计人体运动，而实际应用常常需要在世界坐标系下估计，且人体和摄像机可能同时运动。受限于现有数据集丰富性，相关研究进展缓慢。因此，亟需高质量、丰富多样、带有真实摄像机和人体运动真值的新数据集。

Method: 作者构建了BEDLAM2.0数据集，较前作BEDLAM，扩展了摄像机类型及运动，丰富了人体外形、动作、服装、发型、3D场景及鞋子设置。并公开了渲染视频、人体参数、摄像机运动等真值信息，以及合法使用的3D资源。

Result: 在多个先进方法上比较训练效果，结果表明BEDLAM2.0在世界坐标系3D人体运动估计任务上显著优于BEDLAM数据集，有效提升了模型准确率。

Conclusion: BEDLAM2.0为3D人体运动估计提供了更加真实多样的训练数据，对相关方向研究具有重要推动作用，在世界坐标系下的3D人体估计任务表现卓越。

Abstract: Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.

</details>


### [98] [Stage Aware Diagnosis of Diabetic Retinopathy via Ordinal Regression](https://arxiv.org/abs/2511.14398)
*Saksham Kumar,D Sridhar Aditya,T Likhil Kumar,Thulasi Bikku,Srinivasarao Thota,Chandan Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于有序回归（Ordinal Regression）的糖尿病视网膜病变（DR）检测新方法，在APTOS-2019眼底图像数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是可预防失明的主要原因，早期筛查和干预至关重要。现有方法在分级检测准确性和与临床评价一致性上仍有提升空间。

Method: 采用了绿色通道提取、噪声掩膜和CLAHE多种预处理方法，结合有序回归算法，对APTOS-2019眼底图像数据集进行DR分级检测。

Result: 所提方法在APTOS数据集上使用QWK指标（Quadratic Weighted Kappa）评估，得到0.8992的高分，优于以往方法。

Conclusion: 基于有序回归的DR检测方法有效提升了分级准确性，并与临床分级高度一致，为自动化筛查提供了新基准。

Abstract: Diabetic Retinopathy (DR) has emerged as a major cause of preventable blindness in recent times. With timely screening and intervention, the condition can be prevented from causing irreversible damage. The work introduces a state-of-the-art Ordinal Regression-based DR Detection framework that uses the APTOS-2019 fundus image dataset. A widely accepted combination of preprocessing methods: Green Channel (GC) Extraction, Noise Masking, and CLAHE, was used to isolate the most relevant features for DR classification. Model performance was evaluated using the Quadratic Weighted Kappa, with a focus on agreement between results and clinical grading. Our Ordinal Regression approach attained a QWK score of 0.8992, setting a new benchmark on the APTOS dataset.

</details>


### [99] [Language as an Anchor: Preserving Relative Visual Geometry for Domain Incremental Learning](https://arxiv.org/abs/2511.14401)
*Shuyi Geng,Tao Zhou,Yi Zhou*

Main category: cs.CV

TL;DR: 本文针对领域增量学习（DIL）中的知识保持与迁移难题，提出了通过文本语义锚点来统一跨领域视觉表达的新方法LAVA，有效提升了在标准基准上的表现。


<details>
  <summary>Details</summary>
Motivation: DIL面临分布不断变化，需保持过去知识的挑战。现有方法或将不同领域强行对齐，导致语义混淆和干扰；或将每个领域参数隔离，形成知识孤岛，难以迁移和复用，从而加剧遗忘。因此，需要新方法兼顾知识的整合与保持。

Method: 提出LAVA框架，通过文本为锚点，用类别名称之间的语义相似度定义的相对几何结构，间接对齐不同领域的视觉特征，而非直接特征对齐。这样维持了各领域间语义一致性，并促进领域知识的桥接与重用。

Result: 在标准DIL基准数据集上，LAVA实现了较现有主流方法更优的表现，显著提升了模型的鲁棒性和知识保持能力。

Conclusion: LAVA通过语言锚定跨域视觉表达，在保持知识和推动特征聚合之间达到了新的平衡，缓解了知识孤岛和干扰问题，为领域增量学习提供了有效方案。

Abstract: A key challenge in Domain Incremental Learning (DIL) is to continually learn under shifting distributions while preserving knowledge from previous domains. Existing methods face a fundamental dilemma. On one hand, projecting all domains into a single unified visual space leads to inter-domain interference and semantic distortion, as large shifts may vary with not only visual appearance but also underlying semantics. On the other hand, isolating domain-specific parameters causes knowledge fragmentation, creating "knowledge islands" that hamper knowledge reuse and exacerbate forgetting. To address this issue, we propose LAVA (Language-Anchored Visual Alignment), a novel DIL framework that replaces direct feature alignment with relative alignment driven by a text-based reference anchor. LAVA guides the visual representations of each incoming domain to preserve a consistent relative geometry, which is defined by mirroring the pairwise semantic similarities between the class names. This anchored geometric structure acts as a bridge across domains, enabling the retrieval of class-aware prior knowledge and facilitating robust feature aggregation. Extensive experiments on standard DIL benchmarks demonstrate that LAVA achieves significant performance improvements over state-of-the-arts. Code is available at https://github.com/ShuyiGeng/LAVA.

</details>


### [100] [Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays](https://arxiv.org/abs/2511.14411)
*Ravi Shankar Prasad,Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 本文提出Cranio-ID框架，使用YOLO-pose模型自动在2D颅骨X光片和面部光学图像间标注标志点，并通过图结构和跨模态匹配实现语义对应，在公开数据集上验证取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 颅面鉴定以及生物医学领域对颅骨关键点的定位有重要需求，但传统方法耗时且需专家经验，现有自动化方法缺乏大规模验证，可靠性不足。

Method: 1. 在2D颅骨X光片和面部图像上用训练好的YOLO-pose模型自动标注关键点；2. 将关键点转换为图结构，利用跨注意力和最优传输算法实现两种模态间的语义对应和匹配。

Result: 在S2F和CUHK公开数据集上，Cranio-ID在可靠性和准确率方面均显著优于现有方法，能有效应用于不同域（颅骨-面部、素描-面部）的比对。

Conclusion: 提出的方法为法医学及相关领域的跨模态颅面匹配提供了更准确可靠的新工具，具有较强的实用性和泛化能力。

Abstract: In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important. Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise. Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks. However, these methods are not reliable due to insufficient large-scale validation studies. In this paper, we proposed a novel framework Cranio-ID: First, an automatic annotation of landmarks on 2D skulls (which are X-ray scans of faces) with their respective optical images using our trained YOLO-pose models. Second, cross-modal matching by formulating these landmarks into graph representations and then finding semantic correspondence between graphs of these two modalities using cross-attention and optimal transport framework. Our proposed framework is validated on the S2F and CUHK datasets (CUHK dataset resembles with S2F dataset). Extensive experiments have been conducted to evaluate the performance of our proposed framework, which demonstrates significant improvements in both reliability and accuracy, as well as its effectiveness in cross-domain skull-to-face and sketch-to-face matching in forensic science.

</details>


### [101] [Learning to See Through a Baby's Eyes: Early Visual Diets Enable Robust Visual Intelligence in Humans and Machines](https://arxiv.org/abs/2511.14440)
*Yusen Cai,Bhargava Satya Nunna,Qing Lin,Mengmi Zhang*

Main category: cs.CV

TL;DR: 本研究模拟婴儿视觉发育阶段，通过对自监督学习模型施加与婴儿视觉类似的限制，显著提升了模型对物体识别的鲁棒性及对视觉认知发育过程的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 婴儿视觉起初低清晰度、无彩色且时序连续，随后逐步发育。研究婴儿“阶段性视觉输入”能否为机器视觉提供进化启发与稳健特性。

Method: 提出CATDiet训练范式：在自监督学习模型的训练数据中逐步从灰度到彩色，从模糊到清晰，并保持视频的时间连续性，从而模拟婴儿视觉。使用十个数据集评测不同视觉任务表现。同时，提出CombDiet训练策略，将CATDiet与标准SSL结合并保留时间连续性。

Result: 所有CATDiet变体在物体识别鲁棒性上均有提升，并出现了与生物发育（如大脑V1区突触密度变化、视觉悬崖反应）一致的发展模式。CombDiet在对象识别和深度感知任务中优于常规自监督学习，且推广性强。

Conclusion: 模拟婴儿视觉发展阶段能够有效增强机器视觉的鲁棒性和泛化性，为理解和设计更智能的人工视觉系统提供新的理论基础和实践路径。

Abstract: Newborns perceive the world with low-acuity, color-degraded, and temporally continuous vision, which gradually sharpens as infants develop. To explore the ecological advantages of such staged "visual diets", we train self-supervised learning (SSL) models on object-centric videos under constraints that simulate infant vision: grayscale-to-color (C), blur-to-sharp (A), and preserved temporal continuity (T)-collectively termed CATDiet. For evaluation, we establish a comprehensive benchmark across ten datasets, covering clean and corrupted image recognition, texture-shape cue conflict tests, silhouette recognition, depth-order classification, and the visual cliff paradigm. All CATDiet variants demonstrate enhanced robustness in object recognition, despite being trained solely on object-centric videos. Remarkably, models also exhibit biologically aligned developmental patterns, including neural plasticity changes mirroring synaptic density in macaque V1 and behaviors resembling infants' visual cliff responses. Building on these insights, CombDiet initializes SSL with CATDiet before standard training while preserving temporal continuity. Trained on object-centric or head-mounted infant videos, CombDiet outperforms standard SSL on both in-domain and out-of-domain object recognition and depth perception. Together, these results suggest that the developmental progression of early infant visual experience offers a powerful reverse-engineering framework for understanding the emergence of robust visual intelligence in machines. All code, data, and models will be publicly released.

</details>


### [102] [Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding](https://arxiv.org/abs/2511.14446)
*Hong Gao,Yiming Bao,Xuezhen Tu,Yutong Xu,Yue Jin,Yiyang Mu,Bin Zhong,Linan Yue,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的无训练的Agentic Video Intelligence (AVI) 框架，实现高效、可解释的视频理解与推理，无需昂贵模型或强化学习训练，并在多个基准数据集上显示出优秀性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解方法（包括流行的视觉—语言模型和基于Agent的方法）普遍存在对视频整体单遍处理、缺乏反复推理能力、受限于封闭专有模型或高昂训练成本等问题。作者希望设计一个更贴近人类理解流程、自由度更高且无需冗长训练的新框架，提升视频理解的推理能力与可解释性。

Method: 提出Agentic Video Intelligence (AVI)框架，核心创新包含：（1）人类启发的三阶段推理流程（检索-感知-复盘），结合全局探索和局部深入分析；（2）基于实体图的结构化视频知识库，并配置多粒度集成工具，作为agent的互动环境；（3）开源模型集成方案，融合推理大语言模型、基础计算机视觉模型和视觉-语言模型，无依赖专有API和强化学习训练。

Result: 在LVBench、VideoMME-Long、LongVideoBench和Charades-STA等多个视频理解和长时推理基准数据集上，AVI框架取得了有竞争力的性能，并显著提升了可解释性。

Conclusion: AVI是一个灵活、无训练需求、可扩展且可解释的视频理解新框架，能在无需专有模型或训练的前提下，支持复杂推理并达到行业领先水平。

Abstract: Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.

</details>


### [103] [DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval](https://arxiv.org/abs/2511.14449)
*Zongwei Zhen,Biqing Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种对话式文本到图像检索框架DIR-TIR，通过多轮交互逐步逼近目标图像，极大提高了检索准确性和交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像检索方法多依赖于一次性描述，难以捕捉用户细致意图，缺乏交互性与纠错能力，因此亟需构建能持续交流、动态理解用户需求的检索系统。

Method: DIR-TIR框架包括对话细化模块和图像细化模块。对话细化模块主动向用户提问，完善目标图像描述；图像细化模块则分析生成图像与用户意图的差异，不断缩小语义差距。二者协同，利用多轮对话迭代提升检索效果。

Result: 在多个不同的图像数据集上，DIR-TIR显著超越了只用初始描述的基线方法，检索精度和用户交互体验均有显著提升。

Conclusion: DIR-TIR框架通过多轮交互和协同细化，有效增强了文本到图像检索的可控性和容错性，为对话式检索领域带来新的技术进展。

Abstract: This paper addresses the task of interactive, conversational text-to-image retrieval.
  Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.
  The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.
  Complementarily, the Image Refiner identifies perceptual gaps between generated images and user intentions, strategically reducing the visual-semantic discrepancy. By leveraging multi-turn dialogues, DIR-TIR provides superior controllability and fault tolerance compared to conventional single-query methods, significantly improving target image hit accuracy.
  Comprehensive experiments across diverse image datasets demonstrate our dialogue-based approach substantially outperforms initial-description-only baselines, while the synergistic module integration achieves both higher retrieval precision and enhanced interactive experience.

</details>


### [104] [CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring](https://arxiv.org/abs/2511.14469)
*Mingchen Zhong,Xin Lu,Dong Li,Senyan Xu,Ruixuan Jiang,Xueyang Fu,Baocai Yin*

Main category: cs.CV

TL;DR: 本文提出了CompEvent框架，将事件相机数据和RGB帧在复杂域实现全流程融合，显著提升低光照视频去模糊效果，并超过了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有在低光照和运动模糊视频去模糊任务中，事件相机与传统相机数据结合的方法大多采用分阶段策略，无法同时有效应对低照度和运动模糊的双重挑战。该文提出新方法以充分融合两类数据，提升极端场景下的去模糊质量。

Method: 提出了CompEvent复杂神经网络框架，包含复杂时序对齐GRU（用复杂值卷积和GRU递归对事件与视频流进行时序对齐和融合）和复杂空频学习模块（在空间与频率域对信号统一复杂值处理，实现深度融合）。通过复杂域神经网络实现对两种模态的全时空融合学习。

Result: 实验结果表明，CompEvent在低光照视频去模糊任务上，整体性能优于当前最先进（SOTA）的方法。

Conclusion: 复杂值神经网络能最大化地融合事件与RGB两类数据优势，实现低照度环境下的出色视频去模糊效果。CompEvent作为统一全流程架构，推动极端感知场景下的视觉增强发展。

Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.

</details>


### [105] [Learning Subglacial Bed Topography from Sparse Radar with Physics-Guided Residuals](https://arxiv.org/abs/2511.14473)
*Bayu Adhi Tama,Jianwu Wang,Vandana Janeja,Mostafa Cham*

Main category: cs.CV

TL;DR: 该论文提出了一种物理引导的残差学习框架，用于精准预测冰盖下床面地形，集成了深度学习和物理约束，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的冰下床面地形对冰盖建模至关重要，但雷达观测稀疏、不均，现有方法难以精确重建冰床地形，尤其在未观测区域。

Method: 将观测到的冰盖表面信息和基于BedMachine的先验结合，利用物理引导的深度网络（以ResNet-50为编码器，DeepLabV3+为解码器），训练时融入多尺度质量守恒、总变差、拉普拉斯阻尼、厚度非负、与先验一致等多种物理和数据项，通过残差学习预测床面厚度残差。采用特殊的分块留出法，防止信息泄漏，评估其泛化能力。

Result: 在格陵兰两个子区域测试，所提方法在核心留出区域测试中准确度高、结构真实感强，优于U-Net、Attention U-Net、FPN及普通CNN等基准方法。

Conclusion: 所提基于物理的残差框架，能够在数据稀疏、存在域转移情况下，生成空间连续、物理合理的冰床地形，可用于实际操作性的冰盖地形图绘制。

Abstract: Accurate subglacial bed topography is essential for ice sheet modeling, yet radar observations are sparse and uneven. We propose a physics-guided residual learning framework that predicts bed thickness residuals over a BedMachine prior and reconstructs bed from the observed surface. A DeepLabV3+ decoder over a standard encoder (e.g.,ResNet-50) is trained with lightweight physics and data terms: multi-scale mass conservation, flow-aligned total variation, Laplacian damping, non-negativity of thickness, a ramped prior-consistency term, and a masked Huber fit to radar picks modulated by a confidence map. To measure real-world generalization, we adopt leakage-safe blockwise hold-outs (vertical/horizontal) with safety buffers and report metrics only on held-out cores. Across two Greenland sub-regions, our approach achieves strong test-core accuracy and high structural fidelity, outperforming U-Net, Attention U-Net, FPN, and a plain CNN. The residual-over-prior design, combined with physics, yields spatially coherent, physically plausible beds suitable for operational mapping under domain shift.

</details>


### [106] [2D Gaussians Spatial Transport for Point-supervised Density Regression](https://arxiv.org/abs/2511.14477)
*Miao Shang,Xiaopeng Hong*

Main category: cs.CV

TL;DR: 本文提出了一种新的高斯空间迁移（GST）框架，通过高斯点分布方法将图像坐标空间的概率分布与标注图之间实现高效映射，并在众多计算机视觉任务中大幅提升了训练效率和表现。


<details>
  <summary>Details</summary>
Motivation: 传统的最优传输方案在计算像素与标注之间的对应关系时，需要在训练过程中反复迭代计算传输方案，效率较低。为此，作者希望设计一种效率更高、无需迭代的传输方法，以提升在实际视觉任务中的应用效率。

Method: 作者提出基于高斯点分布（Gaussian Splatting）的方法来估算像素和标注的对应关系，然后基于贝叶斯概率推导传输方案。将得到的传输方案嵌入到通用网络优化中，并设计了衡量映射后差异的损失函数。

Result: 在拥挤人群计数和关键点检测等代表性视觉任务上进行了大量实验，验证了方法的有效性。与传统最优传输方法相比，GST在训练过程中无需反复迭代，显著提升了效率。

Conclusion: 高斯空间迁移（GST）方法为像素与标注的高效、准确对应提供了新思路，在主流视觉任务中表现优异，并且极大改善了训练时的运算效率，具有广泛应用前景。

Abstract: This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.

</details>


### [107] [Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation](https://arxiv.org/abs/2511.14481)
*Aditi Agarwal,Anjali Jain,Nikita Saxena,Ishan Deshpande,Michal Kazmierski,Abigail Annkah,Nadav Sherman,Karthikeyan Shanmugam,Alok Talekar,Vaibhav Rajan*

Main category: cs.CV

TL;DR: 本文提出了一种通过分割感知的超分辨方法（SEED-SR）以提升农田边界分割的精度，显著优于主流超分辨结合分割方法。


<details>
  <summary>Details</summary>
Motivation: 小农场的精准边界划分依赖高分辨率卫星图像，而高分辨图像获取频率低，无法进行频繁监测。现有方法无法兼顾高放大倍数与下游分割任务需求。

Method: 提出SEED-SR方法，结合条件潜空间扩散模型与多源多光谱的地理基础模型，绕过像素空间超分辨，直接在分割感知潜空间进行超分辨与分割融合。

Result: 在两大真实数据集上，SEED-SR方法实现高达20×放大倍数，并在实例分割和语义分割指标上分别比当前先进超分辨方法提升了25.5%和12.9%。

Conclusion: SEED-SR有效融合多源低、高分辨卫星影像，优化了小农田分割的频率与精度，为农业遥感下游任务奠定了基础。

Abstract: Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.

</details>


### [108] [Parameter Aware Mamba Model for Multi-task Dense Prediction](https://arxiv.org/abs/2511.14503)
*Xinzhuo Yu,Yunzhi Zhuge,Sitong Gong,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种新颖的多任务密集预测解码器框架PAMM，基于状态空间模型参数增强任务间交互，通过双参数专家和多方向Hilbert扫描提升多任务学习能力，在公开数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多任务密集预测对任务间的相互关系理解不足，现有方法多聚焦于卷积和注意力机制，难以充分挖掘复杂任务间交互。

Method: 提出参数感知Mamba模型(PAMM)，引入状态空间模型以丰富参数表达，设计双状态空间参数专家融入任务先验，利用S4结构实现全局任务先验融合，并用多方向Hilbert扫描提升2D数据特征感知。

Result: 在NYUD-v2和PASCAL-Context两个多任务密集预测基准上的实验显示，PAMM方法在准确性和任务交互等方面优于现有方法。

Conclusion: PAMM能够高效建模多任务间关联，提升多任务学习密集预测的性能，具有实际推广与应用价值。

Abstract: Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.

</details>


### [109] [D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images](https://arxiv.org/abs/2511.14518)
*Taifour Yousra Nabila,Azeddine Beghdadi,Marie Luong,Zuheng Ming,Habib Zaidi,Faouzi Alaya Cheikh*

Main category: cs.CV

TL;DR: 本论文提出了一种新型的低剂量CT图像增强架构D-PerceptCT，有效提升了图像的结构和细节，同时避免了过度平滑，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT虽然能减少辐射危害，但导致图像质量下降，尤其是细节和结构信息丢失。现有提升方法容易导致过度平滑、噪声估计过高，损失关键医学细节，因此有必要开发能在保障安全的前提下，提升图像感知质量的新方法。

Method: 提出D-PerceptCT模型，包括ViDex视觉双路径提取器（结合DINOv2预训练模型的语义先验和局部空间特征）与全局-局部状态空间块（捕捉全局信息和多尺度特征），同时设计了受人类视觉系统对比敏感度启发的深度感知相关损失函数（DPRLF），突出结构和感知相关细节。

Result: 在Mayo2016数据集上，通过大量实验表明D-PerceptCT在低剂量CT增强方面显著优于SOTA方法，能更好地保留结构和纹理信息，提升临床诊断需求的关键细节表现。

Conclusion: D-PerceptCT结合人类视觉感知机制与深度学习语义特征，有效改善了低剂量CT图像质量，为临床应用提供了更清晰、可靠的影像支撑，具有推广和进一步研究价值。

Abstract: Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.

</details>


### [110] [A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement](https://arxiv.org/abs/2511.14521)
*Yufeng Tian,Yifan Chen,Zhe Sun,Libang Chen,Mingyu Dou,Jijun Lu,Ye Zheng,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的合成数据集生成方法，通过将空气中的自然图像转化为水下退化版本，解决了水下图像增强领域因高质量数据不足而导致模型表现有限的问题，使模型具备更好的颜色还原与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前水下图像增强和复原方法受限于高质量成对数据集的缺乏。真实水下场景很难获得干净的参考标签，现有数据集参考图像经常由算法人工选取，存在真实性与一致性不足，难以为模型提供可靠的监督信号。这一问题限制了深度学习方法在水下图像颜色还原、细节复原等方面的性能提升和泛化。

Method: 论文提出用空气中的自然图像作为无歧义的参考目标，通过生成式数据框架，将其转换为6种典型水下退化类型，生成大规模合成数据集。具体地，研究采用无配对的图像到图像转换技术，生成具有准确“真实标签”的数据样本，从而支持水下图像到干净场景的映射学习。

Result: 在6种主流网络结构及3个独立测试集上进行广泛评测，实验结果显示基于所提合成数据集训练的模型在颜色恢复和泛化性能上与现有数据集持平甚至更优。

Conclusion: 本研究为水下图像增强任务提供了一种可靠、可扩展的基于数据驱动的解决方案，显著提升了水下图像复原和增强的基础支持。公开的合成数据集将有助于进一步研究和落地应用。

Abstract: Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.

</details>


### [111] [DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation](https://arxiv.org/abs/2511.14530)
*Xiangchen Yin,Jiahui Yuan,Zhangchi Hu,Wenzhang Sun,Jie Chen,Xiaozhen Qiao,Hao Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种新的视频变分自编码器（VAE）方法——DeCo-VAE，通过对视频内容进行显式解耦，有效压缩潜在表示并提升重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频VAE未充分利用帧间内容相似性，造成潜变量冗余，限制了表示的紧凑性和生成效率。为此，作者旨在减小冗余、提高建模效率。

Method: 将视频内容分解为关键帧、运动信息和残差三个部分，对每个部分分别设计专用编码器，采用共享的3D解码器以保持时空一致性。引入解耦自适应训练策略，部分编码器冻结、顺序训练，增强了静态和动态特征的建模稳定性与准确性。

Result: 大量定量与定性实验表明，DeCo-VAE在视频重建任务上取得了更优的性能及更紧凑的潜在表达。

Conclusion: 解耦后的潜变量建模能够有效提升视频VAE的表达能力和重建效果，为视频生成等任务带来新的进展。

Abstract: Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.

</details>


### [112] [Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details](https://arxiv.org/abs/2511.14539)
*Qiang Bai,Bojian Wu,Xi Yang,Zhizhong Han*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的方法，将多个神经有符号距离函数(SDFs)表示在一个公共空间，以获得更高保真度的几何细节和更紧凑的潜编码，并在公开基准上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的神经SDF通常只能很好地表示单一形状或场景，在分析多个SDF时，因为潜空间的信息有限，导致几何细节损失，难以兼具高保真和紧凑表示，因此亟需新的表示方法。

Method: 作者融合了基于泛化和基于过拟合的学习策略，同时提出一种新的采样策略，用于训练期间的查询点选择，这样能够提升训练效率，并消除多个SDF间带来的伪影影响。

Result: 实验在主流数据集和评测指标上，提供了数值和视觉评估结果，显示该方法在表达能力和编码紧凑性方面优于最新SDF表征方法。

Conclusion: 论文验证了新方法能够在不牺牲编码紧凑性的前提下，大幅提升高保真几何细节的恢复能力，为多SDF高质量表征提供了一种有效方案。

Abstract: Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.

</details>


### [113] [Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction](https://arxiv.org/abs/2511.14540)
*Hao Tian,Chenyangguang Zhang,Rui Liu,Wen Shen,Xiaolin Qin*

Main category: cs.CV

TL;DR: 本文提出了一种无物体先验的动态3D高斯斑点（Gaussian Splatting）方法，有效实现复杂手物交互场景的三维重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手物交互的3D重建方法普遍依赖物体先验，难以处理无先验的相互遮挡和交互变化场景，且在结构与外观建模上存在局限。因此，亟需一种能够更高效、更真实建模复杂手物交互的新方法。

Method: 作者基于动态3D高斯斑点方法，提出交互感知的手物高斯建模，通过新增可优化参数采用分段线性假设以增强结构表达力。同时，将手的形状信息嵌入物体变形场，建立交互感知动态场，刻画两者紧密且互补的运动。针对优化过程中的困难，采用渐进策略与显式正则化，分步处理动态区域和静态背景，保障重建的流畅性和现实性。

Result: 实验结果显示，所提方法在重建动态手物交互场景时，优于当前所有基于动态3D高斯斑点的相关方法，在结构清晰度、运动连贯和物理现实性等评价指标上均达到了新的最优。

Conclusion: 本文方法无须物体先验，能高效、真实地重建复杂的动态手物交互场景，在相关领域具有显著应用前景和推广价值。

Abstract: This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.

</details>


### [114] [ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection](https://arxiv.org/abs/2511.14554)
*Mohammad Romani*

Main category: cs.CV

TL;DR: 提出了一种多模态伪造检测框架ForensicFlow，通过融合RGB、纹理和频域证据，有效提升了Deepfake视频检测的准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单流CNN方法无法完整捕捉Deepfake伪造在空间、纹理和频域的多尺度特征，导致检测鲁棒性和泛化能力受限。

Method: 提出了ForensicFlow三通道框架，分别以ConvNeXt提取全局视觉特征，Swin Transformer提取细粒度纹理，CNN用于频域特征。采用注意力机制进行时序与分支融合，动态聚焦伪造证据。

Result: 在Celeb-DF(v2)上，ForensicFlow取得了AUC 0.9752、F1 0.9408和准确率0.9208，均优于单模态基线。消融实验与Grad-CAM进一步验证模型有效性。

Conclusion: 多模态特征融合方法显著提升了Deepfake视频检测的鲁棒性与准确性，对抗细微伪造表现优异，具备实际应用潜力。

Abstract: Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.

</details>


### [115] [Explaining Digital Pathology Models via Clustering Activations](https://arxiv.org/abs/2511.14558)
*Adam Bajger,Jan Obdržálek,Vojtěch Kůr,Rudolf Nenutil,Petr Holub,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: 本文提出一种基于聚类的可解释性技术，用以分析卷积神经网络在数字病理学中的表现，区别于以突出单张图像预测区域为主的显著图方法。该方法不仅提供全局性的模型行为解释，还能细致展示模型内部机制，经可视化后提升理解和信任，促进临床应用，并通过前列腺癌检测案例验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学深度学习模型的可解释性限制了其在临床的广泛应用。已有的显著图方法（如GradCAM等）只能展示模型对单一图像的局部决策依据，难以反映整体行为，临床人员难以完全信任模型。因此，作者希望开发一种能提供全局性模型解释、便于理解且提升信任度的方法。

Method: 作者提出一种基于聚类的解释方法，将模型处理的病理切片数据及其内部特征分布进行聚类，通过聚类结果的可视化，为模型决策提供全局和细粒度的解释。同时，该方法对比传统显著性方法，更注重模型整体行为。

Result: 所提方法在前列腺癌检测的现有卷积神经网络模型上进行了实验。通过聚类和可视化，能够辨识模型在不同特征分布下作出决策的模式，并有助于用户理解及核查模型表现。实验结果显示，该方法可有效用于模型解释。

Conclusion: 提出的基于聚类的解释方法能为数字病理学模型带来更深入的全局理解，提升临床医生对AI模型的信心，有助于促进其在实际医疗流程中的采用。

Abstract: We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.

</details>


### [116] [OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models](https://arxiv.org/abs/2511.14582)
*Keda Tao,Kele Shao,Bohan Yu,Weiqiang Wang,Jian liu,Huan Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种无需训练的音频引导音视频联合token压缩方法OmniZip，在加速推理和节省内存的同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 当前OmniLLMs在处理音频和视频token序列时，计算量大成为瓶颈。而现有的token压缩方法无法有效地对多模态token进行联合压缩。为了解决这一问题，需要开发新的多模态token压缩框架。

Method: 提出了OmniZip框架，首先识别出关键音频token，对每个时间组计算音频保留分数，动态指导视频token的裁剪，并利用跨模态相似性保留音频锚点信息。对每个时间窗口，采用交错的时空方式对视频token进行压缩。整个过程是训练无关的。

Result: OmniZip在无训练情况下，相较于其它高性能方法，实现了3.42倍的推理速度提升和1.4倍的内存消耗减少，同时保持了模型的性能。

Conclusion: OmniZip为多模态token的高效压缩提供了新的思路，实现了推理加速和资源节省，具备实际应用价值。

Abstract: Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.

</details>


### [117] [Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease](https://arxiv.org/abs/2511.14588)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习框架，实现了白质高信号（WMH）的自动分割与定位，并通过不同数据集及ADNI队列验证了其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数WMH自动分割方法仅能提供整体病灶体积，忽略了病灶在不同白质区域的空间分布信息，而区域性分布对AD及相关痴呆的诊断和研究具有重要意义。

Method: 设计并实现了基于深度学习的WMH分割和定位模型，不仅分割整体WMH，还能定量评估不同解剖区域的WMH负荷，并结合脑结构体积评估其诊断价值。方法在公开数据集及ADNI独立队列上进行了测试和比较。

Result: 模型能够准确预测各解剖区域的WMH负荷，与人工参考标准高度一致。区域性WMH体积在疾病分类上优于整体WMH负荷，与脑萎缩指标结合后，AUC最高达0.97。特定的白质区域（如前部通路）与诊断状态高度相关，表现出AD的局部易损性。

Conclusion: 区域化的WMH定量信息相比于整体负荷更有助于疾病早期诊断和分型，将局部病灶指标与脑萎缩标志物结合有助于提升神经退行性疾病的临床评估与筛查能力。

Abstract: White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.

</details>


### [118] [CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2511.14599)
*Dongqing Xie,Yonghuang Wu,Zisheng Ai,Jun Min,Zhencun Jiang,Shaojin Geng,Lei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态MRI脑肿瘤分割的自蒸馏方法CCSD，能应对实际临床中模态缺失的问题，并在公开数据集上验证了其领先性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实际临床MRI数据中常有某些模态缺失，严重影响基于深度学习的脑肿瘤分割模型性能和泛化能力。为解决这一挑战，需要能灵活处理多样模态组合的分割方法。

Method: 提出一种新的跨模态组成自蒸馏（CCSD）框架。采用共享-特定的编码器-解码器架构，并包含两种自蒸馏机制：（1）层级模态自蒸馏，转移不同模态层级的知识，减少语义差异；（2）渐进模态组合蒸馏，通过训练阶段模拟逐步丢弃模态，提升对于模态缺失问题的鲁棒性。

Result: 在公开脑肿瘤分割基准上，CCSD在多种模态缺失场景下展现了最先进的分割性能，模型的泛化性和稳定性也得到充分验证。

Conclusion: CCSD框架能有效缓解多模态MRI分割中模态缺失造成的性能问题，提升了医学图像分割的适应性和实用价值。

Abstract: The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.

</details>


### [119] [MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts](https://arxiv.org/abs/2511.14601)
*Nathaniel Putera,Daniel Vilet Rodríguez,Noah Videcrantz,Julia Machnio,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 本文聚焦于阿尔茨海默症（AD）认知能力下降的建模，比较了基于表格数据和MRI影像特征的预测能力，提出了新的标签策略和特征提取方法，并分析了不同特征的优缺点。


<details>
  <summary>Details</summary>
Motivation: 早期识别和个性化管理阿尔茨海默症患者的认知退化轨迹具有重要临床意义。现有基于表格的预测方法虽表现稳定，但难以捕捉大脑内部的细微变化，鉴于此，有必要评估并结合影像特征以提升整体预测效果。

Method: 采用基于Dynamic Time Warping聚类的方法，对AD患者的认知变化轨迹进行分组标注。通过无监督的3D Vision Transformer（ViT）模型，在标准化并增广的MRI影像上预训练，提取保持解剖学信息的嵌入特征。之后，在机器学习和深度学习架构中评估这些影像嵌入，并与传统表格特征、卷积网络等方法进行对比。

Result: 表格与体积特征（如年龄、性别、脑区体积等）能够较好地预测认知能力高度退化或极稳定风险群体（AUC约0.70）；ViT提取的MRI嵌入则更善于识别认知稳定个体（AUC 0.71）。但对于中等程度异质性变化的患者，各方法均较困难。

Conclusion: 表格特征适合极端高风险的识别，ViT MRI嵌入则对稳定个体更加敏感。应采用多模态融合手段以提升AD进展建模的全面性和精度。

Abstract: Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.

</details>


### [120] [XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation](https://arxiv.org/abs/2511.14604)
*Yilin Zhang,Leo D. Westbury,Elaine M. Dennison,Nicholas C. Harvey,Nicholas R. Fuggle,Rahman Attar*

Main category: cs.CV

TL;DR: 该论文提出了一种结合髋部X光影像和临床结构化数据的多模态深度学习框架XAttn-BMD，有效提升了股骨颈骨密度预测的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 骨密度低会显著增加骨折风险，是骨质疏松的关键表现。如何利用现有影像和临床数据提升骨密度预测效果，是改善公共健康的重要问题。

Method: 提出XAttn-BMD框架，通过双向交叉注意力机制动态融合X光影像特征和结构化临床元数据特征。同时，设计加权Smooth L1损失函数，针对骨密度分布不均并优先关注临床显著病例。通过Hertfordshire Cohort Study数据集开展大量实验和消融分析。

Result: XAttn-BMD在回归泛化和鲁棒性方面优于传统基线模型。采用交叉注意力融合显著超过简单特征拼接，MSE降低16.7%，MAE降低6.03%，R2提升16.4%。二分类分析结果表明该方法在实际筛查场景中具有应用前景。

Conclusion: 多模态数据的有效交互融合、定制的损失函数能大幅提升股骨颈BMD预测的准确性和临床效用，为骨健康管理提供了新的技术方案。

Abstract: Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.

</details>


### [121] [3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology](https://arxiv.org/abs/2511.14613)
*Mohammad Vali Sanian,Arshia Hemmat,Amirhossein Vahidi,Jonas Maaskola,Jimmy Tsz Hang Lee,Stanislaw Makarchuk,Yeliz Demirci,Nana-Jane Chipampe,Omer Bayraktar,Lassi Paavolainen,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: 本文提出了一种新型3D空间转录组预测方法HoloTea，能够高效、精准地重建组织的3D表达谱，对比现有方法在精度和扩展性上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有大部分基于组学和病理切片的预测方法仅处理切片级（2D）信息，忽略了组织的3D结构，影响对组织空间组织与疾病机制的整体理解。部分现有3D方法也难以规模化和泛化。

Method: HoloTea提出了一种3D感知流匹配框架，通过在共享特征空间中找到相邻切片的形态学对应区域，并融合这些信息到轻量级的ControlNet中，实现跨切片的空间连续建模。其在数据建模时引入了与3D一致的ZINB（零膨胀负二项）先验，并结合空间经验先验优化表达计数预测。整体模型通过全局注意力块实现更大规模3D数据的处理。

Result: HoloTea在三个不同类型与分辨率的空间转录组数据集上进行评估，对比2D/3D基线模型，均有显著的3D表达重建准确性和泛化能力提升。

Conclusion: HoloTea能更精准地再现组织3D表达分布，有望推动高精度3D虚拟组织构建，加速生物标志物发现与疾病机制研究。

Abstract: A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.

</details>


### [122] [Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap](https://arxiv.org/abs/2511.14620)
*Md Fokhrul Islam,Sajeda Al-Hammouri,Christopher J. Arellano,Kavan Hazeli,Heman Shakeri*

Main category: cs.CV

TL;DR: 本研究提出了BioST-GCN模型，通过融合位姿与生物力学信息提升了跌倒预测准确率，但模型在模拟数据和真实世界之间存在显著性能差距，凸显了现实数据验证的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 老年人因跌倒导致受伤乃至失去独立性，亟需非侵入式、有效的跌倒预测系统。现有视觉方法因缺乏真实跌倒数据而难以优化。

Method: 提出了Biost-GCN（生物力学时空图卷积网络），采用双流结构，将人体姿态与生物力学特征通过交叉注意力机制进行融合，提升模型对跌倒预测的时空建模与解读能力。

Result: 在MCF-UA和MUVIM两个模拟数据集上，BioST-GCN在F1分数上分别较基础ST-GCN提升5.32%和2.91%。但在无监督迁移至真实未见主体时，F1分数从89.0%骤降至35.9%，说明存在明显的“模拟-现实”鸿沟。

Conclusion: BioST-GCN提升了模拟环境下跌倒预测的准确性，但因模拟数据偏差，模型在现实场景的泛化能力有限，尤其在老年慢病或虚弱群体中更突出。亟需通过个性化建模与隐私保护的数据收集，增强模型现实适应性，实现有效的老年人跌倒风险预警。

Abstract: Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as `intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.

</details>


### [123] [SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2511.14633)
*Meiying Gu,Jiawei Zhang,Jiahe Li,Xiaohan Yu,Haonan Luo,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: 提出\net{}方法，通过引入立体几何-纹理对齐和伪特征增强的几何一致性，有效解决了在稀疏视角下高斯分布表面重建易过拟合的问题，提升了重建精度和新视角渲染质量，达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点方案在场景表面重建上获得了很大进展，但在输入视角稀疏时容易过拟合，导致重建质量下降。为解决这一不足，需找到兼顾几何和渲染性能的方案。

Method: 1. 提出Stereo Geometry-Texture Alignment（立体几何-纹理对齐），联合优化表面重建与新视角渲染；2. 提出Pseudo-Feature Enhanced Geometry Consistency（伪特征增强的几何一致性）机制，通过结合训练和未见视角，实现多视角的几何一致性，缓解稀疏视角下的过拟合。

Result: 在DTU、BlendedMVS、Mip-NeRF360等数据集上的实验显示，方法综合表现优于当前主流方法，实现了更准确、更精细的表面重建和更高质量的新视角合成。

Conclusion: 提出方法能在稀疏视角重建任务中有效缓解过拟合，兼顾几何与渲染质量，提升了综合性能，具有良好的应用前景。

Abstract: Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.

</details>


### [124] [SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology](https://arxiv.org/abs/2511.14639)
*Marco Acerbis,Swarnadip Chatterjee,Christophe Avenel,Joakim Lindblad*

Main category: cs.CV

TL;DR: 本文提出了SLAM-AGS预训练框架，通过联合弱监督和自监督目标，并结合梯度调整，有效提升了低目击率下病理切片分析的性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理细胞学由于实例级标签成本高且难以获得，且阳性样本极少（低目击率），导致下游任务表现受限。当前方法难以充分利用弱标签和低目击率数据。

Method: 提出SLAM-AGS框架，包含两个学习目标：对切片阴性样本用弱监督相似性目标，对切片阳性样本用自监督对比目标。为防止多任务冲突导致学习崩溃，引入自适应梯度调整（Adaptive Gradient Surgery）。最终采用注意力聚合机制进行袋级预测和异常实例检索。

Result: 在公开骨髓细胞学数据集上（目击率10%至0.5%），SLAM-AGS在袋级F1分数和细胞阳性检索能力上均优于其他方法，低目击率下提升最为显著。

Conclusion: 解决任务梯度冲突后，SLAM-AGS框架能在极低目击率条件下稳定预训练并提升下游任务性能，对资源稀缺场景具有实际价值。此外，代码和评测框架已开源，有助于复现和进一步推广。

Abstract: Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.

</details>


### [125] [RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT](https://arxiv.org/abs/2511.14649)
*John M. Oyer,Ali Namvar,Benjamin A. Hoff,Wassim W. Labaki,Ella A. Kazerooni,Charles R. Hatt,Fernando J. Martinez,MeiLan K. Han,Craig J. Galbán,Sundaresh Ram*

Main category: cs.CV

TL;DR: RepAir是一种新型三阶段框架，用于增强胸部CT中气道分割的准确性和完整性，在各项指标上超越现有U-Net方法。


<details>
  <summary>Details</summary>
Motivation: 人工标注气道结构耗时且难以实现，现有基于U-Net的自动方法常常导致气道分割不连贯，影响下游生物标志物提取和定量分析的可靠性。

Method: 提出了RepAir三阶段框架：首先由基于nnU-Net的分割网络生成初始气道掩膜；其次，通过基于骨架的算法识别潜在断点并提出重连建议；最后，1D卷积分类器判别哪些连接为真实的解剖分支，哪些为假分支或堵塞路径。

Result: 在健康与病理样本构成的ATM'22和AeroPath两个数据集上，RepAir在体素级和拓扑结构度量上均优于Bronchinet、NaviAirway等现有方法，分割得到的气道树更完整、更符合解剖学。

Conclusion: RepAir方法不仅保证了分割精度，还极大提升了气道连贯性与解剖一致性，有望用于高质量的肺部定量分析和临床研究。

Abstract: Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATM'22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.

</details>


### [126] [Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms](https://arxiv.org/abs/2511.14654)
*Marius Dubosc,Yann Fischer,Zacharie Auray,Nicolas Boutry,Edwin Carlinet,Michael Atlan,Thierry Geraud*

Main category: cs.CV

TL;DR: 本文提出了一种结合时间信息的视网膜动静脉分割方法，通过引入专门的脉冲分析特征，使标准的深度学习分割网络能有效利用Doppler全息的时序数据。该方法在保留模型简单性的同时，性能与更复杂的模型媲美，为量化视网膜血流动力学带来了新可能。


<details>
  <summary>Details</summary>
Motivation: Doppler全息能高时序分辨获取视网膜血流动力学信息，但现有分割方法仅依赖空间信息，未能利用时序数据，限制了对血流动态的量化分析。研究者希望开发同时利用时序和空间信息的动静脉分割方法，以提升分割精度和血流分析能力。

Method: 方法上，作者在标准的U-Net分割架构前增加一道脉冲分析流程，从Doppler全息时序数据中提取关键的脉冲特征。这些特征与原始空间图像一同输入U-Net，从而让网络既利用空间结构又利用时间动态，训练过程与传统深度学习分割类似。

Result: 实验表明，经脉冲特征预处理后的U-Net分割结果，与依赖注意力机制或迭代推理等更复杂架构的模型性能相当。同时，模型结构保持简洁，易于实现和部署。

Conclusion: 时间分辨的预处理能让标准分割网络充分利用Doppler全息时序特征，大幅提升动静脉分割质量。这种方法为利用深度学习深入分析视网膜血流动力学提供了新途径，并推动相关研究与应用发展。

Abstract: Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/

</details>


### [127] [Impact of Image Resolution on Age Estimation with DeepFace and InsightFace](https://arxiv.org/abs/2511.14689)
*Shiyar Jamo*

Main category: cs.CV

TL;DR: 本研究评估了不同图像分辨率对基于深度学习的人脸年龄估计准确率的影响，发现图像分辨率对模型表现有显著影响，224x224为最佳分辨率。


<details>
  <summary>Details</summary>
Motivation: 许多年龄估计应用要求输入图像分辨率各异，目前还不清楚分辨率变化对于主流深度学习人脸年龄估计模型的影响。该研究旨在填补该领域的空白。

Method: 作者选用IMDB-Clean数据集，挑选1000张图片并分别按7种分辨率（共7000样本）处理，分别用DeepFace和InsightFace模型进行年龄估计，并用MAE、标准差和MedAE进行评估。

Result: 两种模型在224x224分辨率时表现最好，DeepFace的MAE为10.83，InsightFace为7.46，较低或较高分辨率都显著降低准确性。此外，InsightFace推理速度始终快于DeepFace。

Conclusion: 人脸年龄估计模型对输入分辨率敏感，最佳分辨率为224x224，过低或过高都会带来误差，InsightFace不仅更准也更快。

Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.

</details>


### [128] [HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring](https://arxiv.org/abs/2511.14698)
*Sriram Srinivasan,Srinivasan Aruchamy,Siva Ram Krisha Vadali*

Main category: cs.CV

TL;DR: 本论文提出了一种用于边境监控的地震多活动识别新方法（HyMAD），能够更准确地区分同时发生的人类、动物与车辆活动。通过先进深度学习结构提升了多活动识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统地震传感器隐藏性强但难以区分复杂、多源、重叠的活动信号，容易导致误报、漏报，降低监控系统可信度。因此需要一种更精确的多活动信号识别方法。

Method: 提出一种基于时空特征融合的深度神经网络HyMAD模型。该模型结合了SincNet提取的频谱特征、RNN建模的时序关系、自注意力机制强化特征表达，以及跨模态融合模块以实现地震信号高效多标签分类。

Result: 在真实边防数据集上，HyMAD能泛化到复杂的多活动场景，对人、动物和车辆的重叠活动实现了鲁棒的多标签分类，实验中取得了有竞争力的识别性能。

Conclusion: HyMAD方法为地震传感在实际安全监控中的多活动识别提供了强有力、可扩展的技术手段，有助于提高边境安防监控系统的准确性和可靠性。

Abstract: Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.

</details>


### [129] [Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images](https://arxiv.org/abs/2511.14702)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合心电图（ECG）和心脏MRI的多模态方法，有效提升了心肌瘢痕分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 心肌瘢痕的准确分割对于心脏组织活性评估至关重要，但由于MRI成像对比度变化和伪影，分割任务充满挑战。心电图可提供补充生理信息，例如传导异常透露瘢痕位置。因此，作者希望融合ECG和MRI信息，提升分割的稳健性和生理一致性。

Method: 方法包括开发一种新型多模态框架，将从ECG推断的电生理信息与结合AHA-17心脏解剖图谱的解剖先验相融合。为应对不同时间采集的ECG与LGE-MRI，作者提出了一种时间感知特征融合（TAFF）机制，动态加权并融合来自不同模态的特征。

Result: 在临床数据集上，方法显著优于当前主流的图像单模态（如nnU-Net）基线：瘢痕分割Dice系数由0.6149提升至0.8463，精度和召回率也达到>0.9的高水平。

Conclusion: 将生理信息（ECG）与解剖知识融入MRI分割算法能够极大增强心肌瘢痕分割的准确性与稳健性，为心脏疾病评估开辟了新方向。

Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.

</details>


### [130] [FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation](https://arxiv.org/abs/2511.14712)
*Yunfeng Wu,Jiayi Song,Zhenxiong Tan,Zihao He,Songhua Liu*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练即可用预训练Diffusion Transformer模型生成超高分辨率视频的新方法，大幅提升效率和成像质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的视频生成网络由于attention机制的计算和内存复杂度随分辨率二次增长，因此难以端到端处理超高分辨率视频，面临高昂的算力与内存消耗。作者希望突破该限制，在无需重新训练的前提下提升分辨率。

Method: 作者设计了一个基于滑动窗口的本地注意力机制，保持query token在训练阶段的感受野。然而单独采用局部窗口会导致重复内容和全局一致性缺失。为此提出一种双路径管线：主干为本地窗口注意力，辅以新颖的跨窗口全局注意力引导分支，并创新性地缓存跨注意力分支减少三维全局注意力频繁计算，显著提升推理效率。整体框架完全无需再训练。

Result: 实验显示，该方法能够在不经过再训练的情况下，生成具有细致视觉细节且一致性强的超高分辨率视频。在VBench标准上表现优异，优于多种需要训练的对比方法，且推理效率更高。

Conclusion: 该方案实现了训练门槛低、推理成本低且生成质量高的高分辨率视频生成，为实际部署Transformer视频生成模型提供了重要技术路线，推动了无训练范式在高效视频生成领域的发展。

Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

</details>


### [131] [Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model](https://arxiv.org/abs/2511.14716)
*Xiyuan Wang,Muhan Zhang*

Main category: cs.CV

TL;DR: 本文提出了DSD（Diffusion as Self-Distillation）框架，实现了扩散模型编码、解码、扩散三部分的端到端单网络训练，显著提升了效率和性能，避免了以往多阶段、多模块训练的缺点。


<details>
  <summary>Details</summary>
Motivation: 传统潜在扩散模型结构复杂，需要单独的编码器、解码器和扩散网络，分阶段训练，效率低下且性能受限，不能像其他视觉基础模型一样简化为单网络端到端训练。

Method: 作者首次分析了端到端联合训练失败的根源——"潜变量坍塌"，通过类比扩散与自蒸馏的无监督学习，提出DSD方法，从训练目标出发稳固潜空间。其核心是对训练目标关键部分的改进，使同一个网络能够稳定地同时学习编码、解码和扩散。

Result: 在ImageNet $256\times 256$ 条件生成任务上，DSD方法分别以仅42M、118M、205M参数量，在50个epoch下实现了FID=13.44/6.38/4.25的优异表现，而且无需额外使用classifier-free-guidance等技术辅助。

Conclusion: DSD为扩散模型提供了首个稳定且高效的端到端单网络训练方案，显著简化了架构，提升了生成性能，为视觉领域大模型的扩散一体化发展奠定了基础。

Abstract: Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.

</details>


### [132] [Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising](https://arxiv.org/abs/2511.14719)
*Yifan Wang,Liya Ji,Zhanghan Ke,Harry Yang,Ser-Nam Lim,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种零样本框架，无需额外微调，即可将合成视频转化为高仿真度的真实感视频，方法基于扩散视频模型并引入结构感知信息作为条件，提升了空间和时间域的结构一致性和真实感。


<details>
  <summary>Details</summary>
Motivation: 合成视频常用于模拟等应用，但往往缺乏真实感，阻碍现实世界应用。现有方法难以同时兼顾结构一致性和高真实感，且常需模型微调或依赖模拟器额外输出。

Method: 提出基于扩散视频基础模型的零样本增强框架，通过辅助模型从合成视频中估算深度、语义、边缘等结构感知信息，并将其作为生成/去噪过程条件，引导输出与原视频在结构和语义层面对齐，无需改变底层模型或进一步训练。

Result: 在增强合成视频真实感的实验中，该方法在保持结构一致性的同时，展现了领先的真实感效果，优于现有基线方法。

Conclusion: 本方法无需微调且高效通用，能够兼顾结构一致性和高真实感，为合成视频的真实性增强提供了有效解决方案。

Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.

</details>


### [133] [A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments](https://arxiv.org/abs/2511.14742)
*Stefan Cobeli,Kazi Shahrukh Omar,Rodrigo Valença,Nivan Ferreira,Fabio Miranda*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视图的3D城市数据探索方法，利用神经隐式场高效表达环境，提升了大规模城市数据的探索效率和体验。


<details>
  <summary>Details</summary>
Motivation: 虽然3D城市数据日益丰富，但高遮挡、视角调整繁琐等问题导致大规模探索效率低下。作者希望简化3D数据交互、提高探索效率。

Method: 作者提出一种视图场方法，用神经隐式场高效构建3D环境表示。该表示既支持快速查询（计算可视性等视图指标），也支持逆向查询（寻找满足特定需求的最佳视角），减少了遮挡影响。

Result: 方法在定量实验、实际案例和专家反馈中表现优异。可以高效找到理想视角，实现建筑立面可见性分析和户外空间视角评估。

Conclusion: 该神经隐式场方法能有效提升3D城市数据探索体验，助力实际城市分析任务，如可视性评估、日照分析和城市开发影响评价。

Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.

</details>


### [134] [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](https://arxiv.org/abs/2511.14749)
*Alexander Vedernikov,Puneet Kumar,Haoyu Chen,Tapio Seppänen,Xiaobai Li*

Main category: cs.CV

TL;DR: 本论文提出利用视觉大语言模型（VLMs）和课程学习的标注优化框架，以提升视频数据中用户参与度识别的准确性，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频数据中的互动参与度识别受限于主观、噪声较大的标签，这极大影响了模型表现，亟需新的解决方案提升标签可靠性与模型鲁棒性。

Method: 提出基于VLMs的标注改进框架，通过问卷抽取行为指征，将数据划分为高/低可靠性子集，并结合课程学习与软标签细化训练策略，有效引导模型逐步学习含糊样本并动态调整监督强度。

Result: 在多个基准测试上显著改善了性能，如EngageNet部分设置提升1.21%，在DREAMS/PAFE数据集上的F1得分分别提升0.22和0.06。

Conclusion: 通过解决标签主观性与噪声问题，结合大模型能力与课程性质的训练方式，可大幅提升视频互动识别表现，优于以往方法。

Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.

</details>


### [135] [UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning](https://arxiv.org/abs/2511.14760)
*Rui Tian,Mingfei Gao,Haiming Gang,Jiasen Lu,Zhe Gan,Yinfei Yang,Zuxuan Wu,Afshin Dehghan*

Main category: cs.CV

TL;DR: 本文提出了UniGen-1.5，多模态大语言模型，兼具图片理解、生成与编辑能力，在多项评测中表现优异，超过了当前最先进的公开模型。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI技术的发展，现有模型大多只能专注于图片理解或生成本身，较难同时在图片编辑等多任务上表现突出。因此，作者希望设计一个统一模型，兼顾多种图片相关能力。

Method: 在UniGen的基础上，作者提升了模型结构与训练流程，并引入统一的强化学习策略，利用共享奖励模型提升图像生成与编辑能力。同时，通过轻量的编辑指令对齐阶段，增强模型对编辑指令的理解能力，优化强化学习效果。

Result: 实验显示，UniGen-1.5在GenEval上得分0.89，在ImgEdit上得分4.31，均超越了BAGEL等现有模型，部分指标达到GPT-Image-1等私有模型水平。

Conclusion: UniGen-1.5实现了统一、多能力的多模态大语言模型，在图片理解、生成及编辑任务上均取得了先进成果，证明了所提方法的有效性。

Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.

</details>


### [136] [ARC Is a Vision Problem!](https://arxiv.org/abs/2511.14761)
*Keya Hu,Ali Cy,Linlu Qiu,Xiaoman Delores Ding,Runqian Wang,Yeyin Eva Zhu,Jacob Andreas,Kaiming He*

Main category: cs.CV

TL;DR: 这篇论文提出用纯视觉的方法（VARC）来解决抽象与推理语料库（ARC）任务，将其视为图像到图像的转换问题，利用视觉Transformer实现，从零训练取得了当前最佳效果。


<details>
  <summary>Details</summary>
Motivation: ARC任务传统上被视为语言推理或符号推理问题，然而其本质是视觉拼图式的任务，现有方法对视觉信息利用不足。作者希望用更契合任务本质的视觉范式来提升表现。

Method: 作者将ARC任务中的输入输出以像素形式嵌入到“画布”中，适配为图像对映射。采用标准的视觉Transformer（如ViT）架构，仅用ARC训练数据从零训练，并在测试时进一步优化（test-time training）。

Result: 提出的VARC方法在ARC-1基准上达到60.4%的准确率，显著超过既有从零训练的方法，并和主流LLM结果相当，接近人类平均表现。

Conclusion: 将ARC任务视觉化处理后，用视觉模型可以显著提升性能，这为人类通用推理任务的AI研究提供了新方向，也表明视觉方法并不弱于语言方法甚至有潜在优势。

Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [137] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 本文探讨了对大模型生成文本进行水印，发现现有方法会损害文本质量，且容易被对抗性攻击移除水印，尤其是回译攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本的广泛应用，如何准确检测其来源成为亟需解决的问题。水印方法作为一种检测手段备受关注，但存在实际应用阻力。

Method: 作者评估了几种主流的文本水印方案，通过对比同义改写和回译两种对抗性攻击，分析水印鲁棒性，并用语言学指标评判水印对文本质量和风格的影响。

Result: 结果显示，当前水印技术虽能较好保留语义，但偏离原文写作风格，且对回译等对抗性攻击十分脆弱。

Conclusion: 目前的文本水印方法尚难兼顾检测效能、文本质量与抗攻击性，推广应用面临挑战。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [138] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为RT（Refine Thought）的方法，通过多次前向传播增强文本嵌入模型的语义推理能力，并在多项基准任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在语义推理相关任务上的表现仍有提升空间，作者希望找到一种方法进一步激发模型已具备但未完全释放的语义推理能力。

Method: RT方法是在推理阶段对同一文本进行多次前向传播，取这些嵌入的整合结果作为最终语义表示，不需重新训练或修改模型结构，相当于一种测试时增强。

Result: RT方法在BRIGHT和PJBenchmark1语义推理基准任务上显著提升了性能，并且在C-MTEB等通用语义理解任务上保持表现稳定。

Conclusion: RT能够进一步激活解码式文本嵌入模型预训练过程中学到的语义推理能力，是一种有效的测试时提升方法。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [139] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 本文提出了两种基于质控（QE）信息的片段级错误纠正方法，并赢得了WMT 2025相关子任务榜首。最大亮点是无需训练直接选择多模型生成结果中的最佳译文。


<details>
  <summary>Details</summary>
Motivation: 自动译后编辑（APE）虽常与机器翻译质量估测（QE）联合训练以提高效果，但APE容易过度修正，反而损害译文质量。因此作者探索无需训练、基于QE信息的纠错新方法。

Method: 提出两种训练无关的新方法：第一种（主方法）通过QE系统评估不同LLM生成的多个译文，选择分数最高的作为最终译文；第二种方法让LLM根据QE对翻译中检测到的错误片段进行有条件替换，同时应用启发式规则控制编辑次数，提高Gain-to-Edit比。

Result: 第一种方法在WMT 2025对应子任务中以Delta COMET 0.0201分数获得第一名，第二种方法为-0.0108。

Conclusion: 基于QE结果选择多译文中最佳者，不仅简单而且有效地提升了翻译质量，是训练无关方法的有力证明。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [140] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本文提出GM-Extract数据集和一套评测方法，系统性评估了大语言模型在长距离信息检索中的表现，总结了现有缓解方案，分析它们在实际应用中的效果和局限。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在“迷失中间”问题，即在长上下文下难以有效检索远距离内容，这对基于检索的应用造成挑战。作者希望通过真实应用场景下的基准和分析，更好理解及优化该现象。

Method: 1. 构建了GM-Extract基准数据集，专注变量（控制参数）检索任务。2. 提出两大评估指标：文档级（空间检索能力）和变量抽取级（语义检索能力）。3. 系统分析了7-8B参数规模模型在多文档任务（键值抽取、问答）中的表现，探索数据组织方式对检索性能的影响。4. 总结文献中的缓解手段（分黑盒与白盒两类），并在该基准上实证评测。

Result: 不同数据组织策略对检索性能影响显著，但并未持续观察到典型的U型表现；通过困惑度分析，揭示了不同模型表现规律。对于现有缓解策略，其提升效果受具体情景影响较大，有时反而会带来负面作用。

Conclusion: 当前缓解“迷失中间”现象的手段在真实检索应用中的有效性具有高度情境相关性，不能保证一概提升，需结合具体应用仔细设计。为后续相关研究及模型改进提供了参考依据。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [141] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: 本文提出了一种方法，利用大语言模型（LLM）理解电商搜索中带有最高级修饰词的查询意图，并将其结构化，显著提升了搜索排序效果。


<details>
  <summary>Details</summary>
Motivation: 传统的搜索引擎面对如“最受欢迎”、“最好的”等最高级词语时，难以正确理解背后复杂的用户意图；准确理解这些表达背后的含义对于提升电商搜索体验非常重要。

Method: 设计了一套框架，利用LLM解析出查询中潜在的最高级意图，通过自动将查询分解为属性-值提示，并与检索阶段并行生成，提升系统效率。为解决LLM推理速度慢的问题，将解析到的最高级语义迁移到轻量级模型上，实现高效推理。

Result: 与传统方法相比，所提方法在平均准确率（MAP）提升10.9分，在平均倒数排名（MRR）提升5.9分。

Conclusion: 该方法不仅能有效理解和利用最高级语义，同时实现了高效部署，为检索系统中的语言解释和最高级表达迁移提供了新思路，有助于实际系统落地。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [142] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: 本研究提出MoRA-RAG框架，将灾后调查报告转化为结构化的多灾种推理基础，并提升知识抽取的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 灾后调查报告包含多灾种相互作用的重要证据，但因其叙述无结构，系统性知识迁移困难。传统LLM在缺少领域知识时易输出不可靠或臆造内容，需要新的方法提升对领域知识的处理能力。

Method: 提出MoRA-RAG框架：1）通过动态Mixture-of-Retrieval机制，将查询分发至不同灾种数据库；2）采用agentic chunking保持上下文连贯性；3）引入验证循环，评估证据充分性、优化查询、不足时启动针对性检索。同时构建HazardRecQA数据集用于评估。

Result: MoRA-RAG在HazardRecQA上准确率达94.5%，比零样本LLM高30%，比最新RAG系统高10%，并显著降低幻觉现象。开源LLM通过本方法可达到与闭源模型相当的性能。

Conclusion: MoRA-RAG为从灾后文档中提取可信知识、用于灾害弹性研究开辟了新范式，极大提升了可靠性和实用性。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [143] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 本文提出了一种名为HiEAG的分层证据信息增强框架，显著提升了多模态OOC假信息检测任务中外部一致性检查及检测准确率。


<details>
  <summary>Details</summary>
Motivation: 以往多模态OOC假信息检测主要关注图文内部一致性，却忽视了与外部证据的一致性，因此难以精准判断信息真伪。

Method: 提出HiEAG框架，基于多模态大语言模型，将外部一致性检查分解为检索、重排序（AESP自动证据选择提示）、重写（AEGP自动证据生成提示）等模块，提升与外部证据对比的相关性和适应性。同时支持推理解释和指令微调。

Result: 在多个基准数据集上，HiEAG的整体准确率超过现有SOTA方法，显示出卓越的检测效果和解释能力。

Conclusion: HiEAG有效弥补了现有方法中外部一致性不足的问题，为多模态假信息检测提供了更强大、可解释的检测工具。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [144] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: 本文提出均衡多标签情感分类数据集，并结合多种模型提升分类效果，显著优于原有不均衡数据。


<details>
  <summary>Details</summary>
Motivation: 多标签情感分类可检测文本多种情感，但现有数据如GoEmotions类别极度不均衡，导致模型难以准确识别罕见情感。

Method: 1. 构建数据集：整合GoEmotions、用RoBERTa模型在Sentiment140上预测情感、GPT-4 mini人工标注文本，确保28类情感均衡。2. 模型设计：使用预训练FastText词嵌入，卷积层提取局部特征，BiLSTM捕获上下文，注意力机制关注情感相关词，Sigmoid输出实现多标签，混合精度训练提升效率。

Result: 基于均衡数据集训练的模型，在准确率、精确率、召回率、F1分数和AUC等多项指标上，都大幅超越了使用原始不均衡数据训练的模型。

Conclusion: 平衡情感类别与改进模型结构能明显提升多标签情感分类效果，尤其对于罕见情感的检测尤为有效。

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [145] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击方法Stealth Fine-Tuning，能够轻松突破推理增强视觉-语言模型（RVLMs）的安全对齐机制，生成有害推理内容。


<details>
  <summary>Details</summary>
Motivation: 虽然RVLMs通过安全对齐机制限制有害行为，但其暴露的推理链（CoT）增加了攻击面，需要检验其安全性的薄弱环节及提出新的攻击手法。

Method: 提出Stealth Fine-Tuning攻击方法，利用段级干扰引导模型生成有害推理内容，并将这些内容用于自监督微调，通过轮次加权损失，实现轻量且分布一致的微调。此外，采用少量（499条）样本和有限计算资源完成攻击验证。

Result: Stealth Fine-Tuning仅用499个样本和3小时显著提升攻击成功率（ASR），相比IDEATOR方法提升38.52%，并且模型原有的一般推理能力基本保持不变。多项基准实验（如AdvBench）证明其高效且低成本。

Conclusion: Stealth Fine-Tuning为突破RVLMs安全对齐带来严峻挑战，展示了模型安全防御存在较大漏洞，对未来安全机制设计提出新的要求。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [146] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 该论文提出利用生成高质量合成出院小结的方式，缓解ICD编码长尾分布带来的数据不均衡，提升稀有及零样本ICD编码的预测性能。


<details>
  <summary>Details</summary>
Motivation: ICD编码任务中存在大量稀有和零样本代码，由于这些代码在数据集如MIMIC-III中的样本极少，导致模型在这些类别上的表现较差，整体macro-F1分数低。

Method: 作者提出了一种数据驱动的框架，首先基于真实的共现关系、ICD描述、同义词、分类体系及相似临床笔记，构建锚定于稀有编码的多标签组合，然后用这些结构化提示生成合成出院小结，共计生成90,000条、覆盖7,902个ICD代码的合成数据，并用这些数据对主流Transformer模型（PLM-ICD和GKI-ICD）进行再训练。

Result: 在原始数据与扩展数据（含合成数据）上微调模型，实验结果表明：引入合成数据后，macro-F1得分有小幅提升，micro-F1得分保持强势，整体超越现有最好水平。

Conclusion: 尽管合成数据带来的提升相对计算成本较小，但结果表明精心设计的合成数据有助于提升长尾ICD编码预测的公平性。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [147] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: 本文提出了HyperABSA——一种动态超图方法，通过分层聚类自动构建样本特异的超边结构，提高了短文本场景下的方面级情感分析（ABSA）表现。


<details>
  <summary>Details</summary>
Motivation: 方面级情感分析中，不同方面存在情感冲突，且短文本语境信息稀疏。已有基于图的方法只能建模点对点关系，需要为不同关系视角构建多张图，导致模型冗余、参数开销大、信息融合过程中容易误差传播，尤其不适合短文本低资源场景。

Method: 作者提出HyperABSA，利用动态超图结构，通过样本级分层聚类确定超边，把相关的方面和情感词灵活地聚合到一起。特别引入一种创新性的聚类终止策略（acceleration-fallback cutoff），自适应确定超边粒度，大幅提高建模效率和鲁棒性。

Result: 在Lap14、Rest14、MAMS三个基准数据集上，HyperABSA比先进的图神经网络基线有一致提升，结合RoBERTa预训练模型获得显著效果，显示该方法在短文本ABSA任务中优势突出。

Conclusion: 动态超图构建为ABSA带来了高效、强大的新选择，有望推广至其它短文本NLP任务。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [148] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 本研究结合了基于Transformer的关系抽取与知识图谱匹配，用于多项选择题的解答，并保证输出过程的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱因构建成本高而静态，难以动态表现语句含义。Transformer关系抽取可动态生成知识图谱，为理解文本内容提供新途径。

Method: 提出方法通过关系抽取将选择题转为关系图，并与真实知识图谱比对，判断其真实性，在填空题解答中实现过程追溯。

Result: 实验表明，该方法能正确解答约70%的问题，并保证推理可追踪。

Conclusion: 该方法具有效率和可追溯性，不同题目类别对准确率影响大，有潜力用于真实场景。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [149] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出了一种选择性由弱到强泛化（W2SG）框架，通过智能判断何时采用弱监督，有效提升大模型对齐效果并增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前超人类模型的表现已经超过人类，但高质量对齐数据稀缺，依赖“弱监督”方法带来鲁棒性问题，有部分弱标签可能对模型有害，因此亟需更优的对齐办法。

Method: 提出了一种选择性W2SG方法。首先训练二分类器P(IK)识别模型能自信回答的问题，对这些问题采用模型自标标签对齐，其它问题采用弱标签。同时，利用图平滑算法进一步优化弱标签质量。

Result: 在三个基准任务上进行了大量实验，结果表明本文方法在所有基线算法中均取得更优表现。

Conclusion: 选择性W2SG方法可有效提升弱到强对齐的效果，并且二分类器P(IK)具有良好的任务泛化能力，为未来超对齐（Superalignment）开辟了新途径。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [150] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 本文分析了大模型（LLMs）在遇到具有象征性意义的语言触发器（如修饰词、否定、数字、例外、命名实体）时出现幻觉（hallucination）的问题，并提出用符号语言知识追踪幻觉发生的分层定位方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在处理带有符号特性的语言元素时易产生幻觉，但幻觉的具体起因尚不清楚，缺乏基于语言知识、能分层追踪幻觉形成的系统性方法。

Method: 提出了一种基于符号语言与语义知识的层级追踪框架，用于分析模型在各层对符号性触发器的处理过程。通过对五个主流模型在HaluEval和TruthfulQA上的行为分析，通过关注度（attention）方差等指标，对比不同模型与层级的表现。

Result: 发现否定等符号性元素在前2-4层导致模型关注度方差急剧不稳定，幻觉现象在大模型和不同变体中发生率依然很高（78.3%-83.7%），并贯穿模型更深层。

Conclusion: LLMs幻觉本质上是符号语言处理失败，而非一般性生成问题。符号性语义知识是理解与定位幻觉机制的核心。

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [151] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: 本文提出了DeepEL框架，将大语言模型（LLMs）全面集成到实体链接（Entity Linking，EL）的各个阶段，并引入自我验证机制，显著提升EL任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能提升实体链接中的某些环节表现，但通常只用于任务的某一阶段，未能充分发挥其整体能力，且孤立消歧难以获得最优结果。

Method: 作者设计了DeepEL框架，贯穿实体识别、候选生成与消歧等阶段，深度嵌入LLMs。此外，提出基于全局上下文的自我验证机制，使模型在句内实体间建立更紧密联系并修正自身预测。

Result: 在十个基准数据集上实验，DeepEL在整体F1分数上比最优方法平均提升2.6%，在领域外数据上提升达4%。

Conclusion: 实验表明，LLMs深度集成和自我验证机制能大幅推进实体链接精度，推动该领域新进展。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [152] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 本文提出了一种针对阿拉伯语语法纠错的多系统集成方法ArbESC+，显著优于单模型方法。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语复杂的形态和句法结构使语法纠错比其他语言更具挑战性，现有方法多为单模型，未充分利用多系统融合的潜力。

Method: 提出ArbESC+框架，集成多种模型（AraT5、ByT5、mT5、AraBART、AraBART+Morph+GEC等）产生候选纠错，提取数值特征后，通过分类器选择最佳纠错，采用进一步支持技术优化输出。

Result: 该系统在三个数据集（QALB-14测试集、QALB-15 L1和L2）上表现优异，F0.5分别达82.63%、84.64%、65.55%，整体优于单一模型。

Conclusion: ArbESC+是首个阿拉伯语多系统集成纠错方法，有助于推动高质量的阿拉伯语文本处理工具开发，具有实际应用和研究意义。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [153] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 本论文提出了一种专为音乐领域定制的大型语言模型训练框架，通过高质量大规模音乐相关语料、专业数据处理流程和创新训练目标，提升了模型在音乐内容处理上的能力，并设计了新的评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在通用任务表现优异，但在如音乐等专业领域，受限于语料规模、纯净度及数据与任务匹配等因素，性能较弱。特别是在音乐娱乐领域，缺乏高质量定制语料和合适的训练目标。因此，作者希望提升音乐领域LLM的专用能力。

Method: 1) 构建包含开源和自有数据的、规模达400亿token的音乐相关自然语言语料库。2) 采用轻量级分类器对领域文本进行筛选与加权，通过多阶段清洗、去重及隐私保护增强数据质量。3) 整合多源音乐文本与元数据，构建结构化的领域知识基础。4) 在训练中引入基于参考模型的token级软评分和统一损失比准则，用于数据选择和动态权重调整，提升训练效果。5) 设计MusicSimpleQA基准，采用自动化评分检验模型事实能力。

Result: 论文构建了高质量、结构化的音乐语料库，并提出了创新的数据处理和训练方法，有效提升了在音乐领域的训练和评测能力。MusicSimpleQA基准工具也实现了对音乐领域模型事实准确性的自动评测。

Conclusion: 该工作在高质量语料和优化训练目标上取得进展，提出了一套可扩展、通用的音乐领域大模型数据训练框架和评测工具，为音乐方向领域模型建设奠定了基础。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [154] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: 本文提出了一种新的电影自动配音模型Authentic-Dubber，通过引入真实导演-演员交互流程，实现更具情感表达力的自动配音。


<details>
  <summary>Details</summary>
Motivation: 现有自动配音方法简单模拟演员即兴配音，忽略了实际流程中导演与演员之间针对情绪等信息的充分互动和指导，导致配音缺乏真实感与情感表现力。

Method: 1) 构建多模态参考片段库，并利用大语言模型深入理解情感表征；2) 提出基于情感相似度的检索增强策略，有效检索与目标场景情感最相关的多模态信息；3) 设计渐进式图结构语音生成方法，逐步融合检索到的情感知识来生成配音。

Result: 在V2C Animation基准数据集上，主观和客观实验均验证了该方法在情感表达、配音贴合性等方面优于现有方法。

Conclusion: Authentic-Dubber有效复现了真实的导演-演员配音互动流程，显著提升了电影自动配音的情感丰富性和真实感。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [155] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: 本论文提出AfriSpeech-MultiBench，是首个覆盖10多个国家、7大领域、100多种非洲英语口音的领域特定语音识别评估套件，并对多种主流ASR和多模态模型做了系统性benchmark。


<details>
  <summary>Details</summary>
Motivation: 当前国际主流语音识别与语音接口技术发展迅速，但缺少适用于非洲多样性英语口音和应用场景的模型评测工具，阻碍了包容性语音技术的开发和部署。

Method: 作者构建了AfriSpeech-MultiBench评测套件，涵盖多国非洲英语口音及金融、法律、医疗等七大领域，采用自发/非自发语音并评测了开源/封闭ASR模型及多模态LLM，并对其在各种口音和语境下的性能进行规范化benchmark分析。

Result: 实验证明：开源ASR模型在自发语音下表现出色，但对非母语嘈杂环境适应性差；多模态LLM对口音鲁棒但对领域命名实体表现不佳；专有模型在干净语音表现最好但对国家和领域差异大。经过非洲英语微调的模型既能保持高精度又能实现低延迟。

Conclusion: AfriSpeech-MultiBench的发布为非洲多样性场景下语音模型的公正评估提供了基准，促进了面向弱势群体的包容性语音技术发展和实际应用。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [156] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 本文提出一种熵引导训练框架，实现对大型推理模型输出链式推理过程的有效压缩，在减少输出长度的同时保持甚至提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然表现出色，但链式推理过程过长导致高计算成本和难以部署。现有压缩方法存在熵冲突，既想让推理更短又想保持高准确率，影响了效果。

Method: 作者分析发现熵冲突源自逻辑连接词在目标函数下被鼓励但在压缩下被惩罚，提出根据熵动态调整训练方式：低熵时鼓励简洁思路，高熵时加强紧凑条件下的探索，从而兼顾准确率和推理简短。

Result: 在六个数学基准测试集上，该方法将推理链的长度压缩至原来的20%，同时准确率与基线持平或更高。

Conclusion: 熵引导训练框架有效解决了推理压缩与准确率提升的矛盾，推动了大模型短链高效推理的实际应用。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [157] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本论文探讨了大语言模型（LLM）在置信度输出中，推理策略对置信度估计结果的影响，并提出通过预测概率分布来提升推理深度和置信度质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注利用LLM生成可解释的置信度输出，但推理策略如何影响置信度估计仍缺乏探讨。有效和可靠的置信度输出对于下游应用尤为关键。

Method: 作者提出让LLM输出对所有候选答案的概率分布（而非单一答案），促使模型更全面地考量解答空间，为每个候选分配合理的置信分数。这种策略结合了chain-of-thought推理，使置信度表达更具深入性和透明度。

Result: 实验表明，概率分布预测方法对不同模型和多种任务均有优势，且不依赖于预先确定的答案空间。即使模型经过强化学习微调，优势依然明显。进一步分析显示该方法的推理模式也与人的期望相契合。

Conclusion: 要求LLM输出概率分布能有效提升其置信度估计的深度和准确性，有助于实现更可靠的应用，并能输出符合人类理解的推理过程。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [158] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: 论文介绍了AraLingBench，这是首个专为评估大型语言模型在阿拉伯语语言能力方面设计的全人工标注基准。评估显示模型刷分容易，但实际语言理解不足。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语大模型主要依赖知识型评测，难以反映其真实语言理解能力。作者希望通过精细化、多方面考察语言结构能力，揭示模型实际短板。

Method: 制作用于测试阿拉伯语语法、形态、拼写、阅读理解和句法等五大核心领域的150道专家设计的选择题。用该基准评测了35个阿拉伯语及双语语言模型。

Result: 模型在表层任务（如记忆、模式识别）表现良好，但在深入语法与句法推理方面普遍有短板，反映出“高分但理解力不足”的现象。

Conclusion: AraLingBench揭示目前阿拉伯语及多语种LLM存在理解力短板，强调未来应加强基础语言能力训练。该基准为后续模型诊断与改进提供了工具。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [159] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: 本文提出了ConInstruct基准，专门评估大型语言模型（LLM）在面对包含冲突约束的用户指令时的检测和解决能力。实验发现，大部分专有LLM具备很强的冲突检测能力，而开源模型中只有DeepSeek-R1表现出接近的水平，但当前LLM很少主动向用户汇报冲突或请求澄清，暴露出现有模型的重要不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM如何遵循指令，但很少研究指令中含有冲突约束这一常见但复杂的实际情况。作者认为理解和解决冲突对于提升模型实用性和安全性至关重要，因此提出针对该问题的评测基准。

Method: 作者构建了名为ConInstruct的数据集，包含各种带有冲突约束的指令。通过该数据集对多种LLM进行冲突检测和冲突解决能力的测试与分析。主要评测了专有模型和开源模型的表现，并分析其处理冲突时的响应行为。

Result: 专有的LLM普遍有较强的冲突检测能力，但开源模型中仅DeepSeek-R1表现较好（F1-score 91.5%），Claude-4.5-Sonnet次之（87.3%）。但不论专有还是开源模型，在发现冲突时都鲜有主动向用户反馈或请求进一步说明。

Conclusion: 虽然当下主流LLM冲突检测能力逐步提升，并能在复杂指令场景中表现良好，但在冲突显性通知和用户交互方面存在明显不足。未来应加强LLM在这一能力上的设计和训练，以提升其实际交互和应用表现。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [160] [The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models](https://arxiv.org/abs/2511.14365)
*Prathamesh Kalamkar,Ned Letcher,Meissane Chami,Sahger Lad,Shayan Mohanty,Prasanna Pendse*

Main category: cs.CL

TL;DR: 本文提出通过扩充预训练大语言模型的词表，将自然语言与分子结构进行统一表示，从而解决化学表达中的“分词瓶颈”问题。实验表明，该方法提升了化学任务的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型处理化学任务时，通用文本分词器会将SMILES等化学表达割裂为无语义的子词，导致模型理解和生成化学信息受限。为此，需开发针对化学领域的分词和建模方法。

Method: 作者通过向已有大语言模型的词表中添加具有化学意义的新词（targeted vocabulary extension），再继续利用化学领域文本对模型进行预训练，以融合这些新知识，实现对化学信息的更好处理。

Result: 采用该策略后，在多种下游化学任务上，模型性能明显优于只用通用词表和无化学知识增强的基线模型。

Conclusion: 词表扩充和专门预训练能有效缓解化学领域的分词瓶颈，使大语言模型在化学应用中表现更优。

Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

</details>


### [161] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: LLM在现有基准上表现趋于饱和，难以区分前沿模型。为此，作者提出了ATLAS——一个跨学科、高难度且高原创性的科学评测套件，用于更好地评估与区分前沿大模型的科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有高难度基准测试存在学科局限、答案格式单一、易数据污染等问题，与真实科学问题存在差距，不能有效区分当前最强的LLM。

Method: 作者构建了ATLAS套件，涵盖数学、物理、化学、生物、计算机、地球科学、材料科学七大领域，由领域专家原创或改编约800道高难度问题。题目设计注重跨学科融合，多步推理及开放式表达，并通过专家审查与对抗性测试确保题目质量。评测采用LLM评委自动评分体系，实现更细腻的答案评判。

Result: 初步实验表明，ATLAS能够有效地区分现有主流大模型在复杂科学推理方面的能力，并展现出高度区分性和现实科学问题的高拟真度。

Conclusion: ATLAS为AGI研究提供了真实可信、长期开放的评测标准，有望成为衡量大模型向通用人工智能进展的新标尺。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [162] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了一种新的归一化上下文校准方法（NCC），针对大语言模型在多标签预测任务中的标签长度偏置问题进行校正，显著提升了模型在多个数据集和任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多选任务中存在标签长度偏置，即不同长度的标签即便进行过常规归一化处理后也会受到不一致对待，影响预测结果的公平性和准确性。现有校准方法忽略了多token标签的偏置。

Method: 作者提出了NCC（Normalized Contextual Calibration）方法，对全标签级别上的概率进行归一化和校准，以修正长度偏置，不仅适用于常规标签预测，还能推广到多选题等任务。方法包括全标签归一化和上下文校准，并在与in-context learning结合时综合测试。

Result: NCC在多个数据集和多种大模型上实现了统计显著的效果提升，F1最高提升10%；在与上下文学习结合时，NCC对样例选择不敏感，且所需标注例子更少、置信度评估更可靠。

Conclusion: NCC有效缓解了多token标签偏置问题，提升了大模型方法的鲁棒性与性能，尤其适合现实应用中标签为多token的场景。

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [163] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 本文针对大语言模型在教育场景下的安全性问题，提出了EduHarm基准和三阶段防护框架（TSSF），以系统性提升模型对恶意攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育领域应用广泛，但易受到“越狱”和微调攻击，既有评测方法未能针对教育特有的安全需求。因此，亟需新的评测基准和防护机制。

Method: 本文首先构建EduHarm基准，涵盖五大教育场景的安全-不安全指令对，用于系统评测教育用大模型的安全性。以此为基础，作者提出三阶段防护框架（TSSF）：1）安全感知注意力重定向，聚焦不安全token判别特征；2）逐层安全判断，多层聚合识别不安全指令；3）防御驱动双通路，将安全与不安全请求分路处理。

Result: 在8类越狱攻击和3个微调攻击数据集上的大量实验证明，TSSF能有效提升模型安全性，防止对良性查询的过度拒绝，同时保持正常的有用性。

Conclusion: TSSF为教育大模型提供了切实可行的系统化安全防护，同时构建的EduHarm基准丰富了教育场景下安全性研究，为后续研究提供新工具与思路。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [164] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4是一个大规模、基于云的医疗大模型评测平台，涵盖超过70万个任务，支持对LLMs、多模态模型和智能体进行全面评测。结果显示，基础模型在安全和伦理上表现不佳，多模态模型跨模态推理能力有限，而基于智能体的架构显著提升了整体性能，尤其是在安全性上。该平台紧贴中国临床规范，为医疗AI的落地和监管提供了实用参考。


<details>
  <summary>Details</summary>
Motivation: 随着医疗大语言模型、多模态模型和智能体的发展，迫切需要能反映实际临床流程和安全要求的评测体系。以往评测不足以覆盖医疗应用的复杂场景，并且缺乏对安全性、伦理等关键问题的系统评估。作者希望建立一个覆盖广泛、标准严格，并能与中国具体医疗规范对齐的评测平台，推动医疗AI的真实可用性。

Method: 作者构建了MedBench v4，包含70多万个由专家精心设计的任务，覆盖24个主专业和91个次专业，并设有面向LLMs、多模态模型、智能体的专门评测赛道。所有题目经过多轮临床专家复审，主观题由人类标定的大模型判分。平台对15个前沿模型包含基础模型、多模态模型和智能体进行了系统测试，指标全面涵盖准确性与安全伦理等。

Result: 基线LLMs平均得分54.1/100，最佳为Claude Sonnet 4.5，得分62.5/100，但安全及伦理维度表现较差仅18.4/100。多模态模型整体表现更弱，平均得分47.5/100（最佳GPT-5为54.9/100），主要因跨模态推理不足。智能体架构显著提升性能，平均分达79.8/100，安全任务上Claude Sonnet 4.5智能体可达88.9/100。

Conclusion: MedBench v4揭示了现有基础和多模态模型在临床多模态推理及安全性上的不足，同时验证了面向治理的智能体调度方案能大幅提升医疗AI的综合和安全能力。平台与中国临床和监管需求对齐，为医疗AI落地、评估和监管提供了权威工具，为相关从业者、开发商和政策制定者提供了重要参考依据。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [165] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: 本文介绍了Tell Me系统，这是一种结合大型语言模型的心理健康辅助工具，旨在为用户和研究人员提供可获得、具情境感知的心理支持。该系统集成了RAG助理、合成对话生成器和Well-being AI团队，具有个性化对话、数据生成和自我关怀等多项功能。


<details>
  <summary>Details</summary>
Motivation: 心理健康领域专业辅导资源有限，数据隐私和可用数据匮乏，导致研究与应用受阻。作者希望通过AI手段扩展心理健康支持的可及性，丰富相关研究与数据。

Method: 整合三大功能：1）采用RAG模型实现个性化、知识支撑的对话助理；2）基于用户档案的合成客户-治疗师对话生成，用于研究和扩充数据集；3）利用CrewAI为用户生成每周自我关怀计划和冥想音频。系统不仅提供自我反思空间，还评估了RAG助理效果，包括自动和人工用户评测。

Result: 系统实现了多种创新功能，包括生成个性化对话、合成心理健康对话数据以及动态自我关怀计划。评估结果显示，RAG助理在精选的心理健康场景下表现良好，获得自动和人工正面反馈。

Conclusion: Tell Me系统拓展了心理健康支持的边界，为用户和研究者提供了低门槛、可扩展的工具，有助于补足专业资源不足，带来NLP与心理健康跨学科合作新契机，推动以人为本的AI应用创新。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [166] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 本文探讨了将强化学习（RL）应用于大语言模型（LLM）Agent的有效方法，并提出了一个新的训练框架Agent-R1，以提升LLM Agent在复杂任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM Agent结合强化学习的应用处于起步阶段，存在方法探索不足、缺乏灵活与易扩展的训练框架等问题。为推动该领域发展，需要更适用于LLM Agent的RL方法以及相应的工具。

Method: 本文首先系统性地将马尔可夫决策过程（MDP）框架扩展用于LLM Agent，详尽定义其关键组成部分。其次，提出Agent-R1训练框架，具备模块化、灵活、易用等特点，支持多种任务和交互环境的适配。

Result: 在多跳问答基准任务上进行了实验，初步验证了提出的方法和框架的有效性。

Conclusion: 研究为LLM Agent中的RL方法提供了理论和工具支持，所提框架有助于该领域进一步发展。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [167] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: 该论文提出并发布了LiveRAG基准数据集，用于系统性评估基于RAG（检索增强生成）技术的问答系统。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术在生成式AI方案中的应用日益广泛，缺乏系统性评估这些系统效果的标准和工具。因此，作者有动机建立一个全面的、可公开获取的评测基准，促进RAG系统的科学研究和实用能力提升。

Method: 作者开发了LiveRAG基准，包含895个合成问答，来源于SIGIR'2025 LiveRAG挑战赛的实际评测问题，并补充了答案、支撑性论据、难度与判别性评分等信息。难度与判别性基于项反应理论（IRT）通过赛题参与者的实际答题表现计算得出。

Result: 分析显示，LiveRAG基准具备问题多样性、难度区间广、判别力强，能有效区分不同RAG系统的性能。该基准公开可用，有助于标准化RAG系统的评测流程。

Conclusion: LiveRAG基准的发布为RAG问答系统的科学研究和性能评估提供了重要工具，有助于推动领域发展和系统的鲁棒性提升。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [168] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估从同一文档中提取声明的方法，通过比对模型自动提取和人工标注的声明集，计算其对齐得分，进而评价模型提取效果，强调了文档级声明抽取评估的难点和当前方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着自动事实核查需求增加，从文档级别自动抽取声明变得重要，但如何评估这些自动抽取的声明的质量却缺乏有效方法。尤其是在涉及非标准语言和本地化语境时，现有评估方法更显不足。

Method: 作者研究了两组针对同一文档的声明集的对齐与相似性计算，提出一种声明集对齐和评估方法，能够比较模型提取和人工标注声明，并作为评价模型提取表现和人工一致性的新度量标准。

Result: 在新采集的捷克语和斯洛伐克语新闻评论声明数据集上实验，结果表明，当前主流评估方法在文档级声明抽取情境中存在局限，难以捕捉语义相似性以及声明的原子性、核查价值和去背景等重要属性。

Conclusion: 本文强调了需要发展更先进的声明集对齐与评估方法，以更准确地反映声明抽取系统在真实、复杂语境下的性能。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [169] [Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages](https://arxiv.org/abs/2511.14598)
*Noam Dahan,Omer Kidron,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本文提出利用报纸头版摘要（Front-Page Teasers）自动收集多语言高质量摘要数据，并首次构建了希伯来语多文档摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 许多语言缺乏高质量摘要数据，影响多语言自动摘要研究进展。

Method: 作者观察到历史报纸的头版导语经常是由编辑对全文新闻进行的自然摘要。该方法通过自动识别并提取这些摘要-正文对，结合适用于多种语言资源状况的自动化收集流程，实现大规模数据采集。

Result: 发现头版摘要普遍存在于七种不同语言的报纸中，且适用于多文档摘要任务。成功构建了希伯来语的首个多文档摘要数据集HEBTEASESUM。

Conclusion: 论文为低资源语言自动摘要研究提供了新的大规模自然标签数据来源和自动化采集方法，显著拓展了摘要研究的语言广度和应用前景。

Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.

</details>


### [170] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 本研究利用电子健康记录（EHR）数据，通过聚类和多状态建模，动态追踪急性肾损伤（AKI）患者，揭示其向慢性肾病（CKD）进展的轨迹及风险因素。


<details>
  <summary>Details</summary>
Motivation: AKI患者进展为CKD的风险高，但如何早期识别高风险患者仍是临床难题。

Method: 利用EHR数据，整合纵向医疗代码和肌酐测量，聚类识别AKI后的临床状态轨迹，并用多状态模型估算状态转移和CKD进展概率，以及不同亚组的风险因素。

Result: 在20,699例AKI患者中，17%进展为CKD。研究识别了15种不同的AKI后临床状态，且这些状态与CKD进展概率显著不同。大部分患者在研究期间状态较为稳定。既有（如AKI严重度、糖尿病等）和新发现的CKD风险因素在不同状态下影响不同。

Conclusion: 基于数据驱动的方法可有效识别高风险AKI患者，有助于开发早期CKD风险预警和干预的决策支持工具。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [171] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文通过构建人工标注的新闻数据集，比较了多种大语言模型和人类在新闻政治偏见检测上的表现，发现模型与人的判断存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 虽然NLP模型已能自动检测新闻偏见，但模型与人类判断的一致性尚不明朗，因此有必要系统地评估它们的差异。

Method: 作者手动标注新闻样本，使用多种LLM（GPT、BERT、RoBERTa、FLAN）与人类标注进行对比，分析模型与人类在偏见极性和一致性等方面的表现，通过一致性指标量化差异。

Result: 在传统Transformer模型中，RoBERTa与人的标签一致性最高；在零样本设定下，生成式模型GPT与人类注释的总体一致性最强。微调后的RoBERTa模型在所有基线Transformer中取得最佳准确率和一致性。

Conclusion: 研究表明，人类与LLM在感知媒体政治倾向方面存在系统性不同，自动化检测需要结合人类可解释性与模型可扩展性，推动混合评估框架的发展。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [172] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 该论文提出用视觉-语言模型（VLM）指导多智能体系统，从而提升端到端的自动化科学发现能力。VLM作为判定者实时检验数据分析的图表，帮助系统自我纠错并适应新数据，在多个领域实验中展现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动化科学发现过程中，端到端多智能体系统容易在数据分析和推理过程中出现错误，且缺乏有效自我纠正和可解释性机制。为提高系统自动纠错、适应性与结果可解释性，作者探索引入VLM辅助评判和引导。

Method: 作者将VLM作为裁判，依据动态生成的领域特定标准对科学可视化结果（如图表）进行审核。智能体在获得反馈后能即时修正自身分析路径，并适应新数据集。论文在宇宙学和天体化学领域进行了案例分析，并设立10项任务基准进行评估。

Result: 实验显示，VLM增强系统在10项数据驱动科学发现任务中的pass@1评分为0.7-0.8，显著高于仅用代码（0.2-0.3）或代码加文本（0.4-0.5）的基线系统。此外，VLM也提升了决策过程的可审核性和解释性。

Conclusion: 视觉-语言模型有助于多智能体系统在科学探索中实现自我纠错、适应新数据、提升任务表现及结果可解释性。这一方案可在多个科学领域为自主发现提供有力支持。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [173] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: 这篇论文提出了一种专为罕见病诊断设计的新模型RareSeek R1，将大型领域专用临床语料库与临床医生验证的推理数据集相结合，通过多阶段指令微调、链式思维学习及图检索提升诊断准确率，实现了当前最优表现。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响全球数亿人，传统诊断流程慢且依赖有限、嘈杂的证据。现有的大型语言模型缺乏真实世界EHR数据、最新医学知识，且容易产生幻觉，亟需更高效、透明和可靠的自动化罕见病辅助诊断方法。

Method: 作者构建了大量专科临床语料与医生标注数据，采用阶段性指令微调、链式思维推理和图结构检索相结合，开发了RareSeek R1模型，并在多中心EHR叙事和公开基准上进行评测。

Result: RareSeek R1在多中心EHR数据和公开基准上达到最优准确率，具备强泛化和对嘈杂/重叠表型鲁棒性，增强检索带来最大提升。模型在辅助医生任务中表现出与资深医生相当的水平，并可审计其推理过程。

Conclusion: RareSeek R1推动了以叙述为先、知识整合的推理范式，有助于缩短罕见病诊断流程，并为临床可审计与可迁移的决策支持提供新方案。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [174] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 本论文探讨了语言处理中的比较错觉现象，提出并实证支持基于噪声通道贝叶斯推断的理论。研究通过结合统计语言模型与行为数据，构建量化模型，成功预测和解释了比较错觉的强度变化，并扩展了解释范围。


<details>
  <summary>Details</summary>
Motivation: 尽管人类在语言理解中很聪明，却经常在特定结构上产生系统性错觉。理解和解释这种“比较错觉”现象有助于揭示人类语言处理的认知机制，推动统一理论的发展。

Method: 研究者基于噪声通道模型，将统计语言模型和人类行为数据结合，量化各类合理解释的后验概率，并据此直接预测比较错觉的强度。同时关注了不同句法成分（如代词与名词短语）的影响。

Result: 模型不仅能定量解释比较错觉强度的细微差异，还首次解释了“than”从句主语为代词与为名词短语所造成的效果差异。

Conclusion: 结果强化了噪声通道推断作为语言处理统一理论的地位，说明该理论对错觉与非错觉现象均有良好解释力，并能产生新颖且经实验证实的预测。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


### [175] [Bias in, Bias out: Annotation Bias in Multilingual Large Language Models](https://arxiv.org/abs/2511.14662)
*Xia Cui,Ziyi Huang,Naeemeh Adel*

Main category: cs.CL

TL;DR: 本文提出了一个针对多语言大模型(LLM)注释偏见的分析与缓解框架，并提出相应的识别与缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型往往受到注释偏见的影响，尤其是在文化多样性环境中，导致模型输出偏差甚至引发社会危害。为此，亟需系统识别和缓解注释过程中的各种偏见。

Method: 作者梳理了注释偏见的类型，包括任务说明偏见、注释者自身偏见，以及上下文和文化偏见；综述了现有的检测方法（如注释者一致性、模型分歧和元数据分析），并介绍了多语言模型分歧和文化推断等新方法；提出了主动与被动的偏见缓解措施，比如多样化招募、准则迭代优化和后处理修正等；提出了一套适用于多语言场景的集成消偏方法，并进行了伦理分析。

Result: (1) 系统归纳了注释偏见类型；(2) 总结和对比了多种偏见检测指标；(3) 提出了面向多语言场景的集成偏见缓解方法；(4) 深入分析了注释过程中的伦理问题。

Conclusion: 本文的框架和方法为更公平、更具文化适应性的LLM数据注释流程提供了理论与实践基础，有助于推动多语言大模型的健康发展。

Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.

</details>


### [176] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 本文提出了一种模块化RAG（检索增强生成）框架，用于合同管理，能够自动识别和优化有问题的合同条款修订，实测准确率超80%。


<details>
  <summary>Details</summary>
Motivation: 合同管理过程中，审查与协商条款时经常遇到问题条款，修订流程繁琐且缺乏足够有标签数据，自动化处理遇到挑战。

Method: 提出了结合合成数据生成、语义条款检索、可接受性分类及基于奖励的优化模块的RAG流程，能自动检测和优化合同条款的修订。

Result: 与行业合作伙伴共同开发与评估，该系统在实际低资源场景下，可在识别及优化问题修订上达到80%以上的准确率。

Conclusion: 该框架能显著提升合同修订流程的效率和准确性，为实际合同管理工作提供了可行的自动化加速工具。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [177] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: 该论文研究了Heaps’定律（或Herdan定律）描述的词型-词次关系，并发现该关系在对数-对数尺度下仍然存在轻微的曲率，不完全符合幂律。作者提出用二次函数可以更好地拟合词型-词次曲线。


<details>
  <summary>Details</summary>
Motivation: 传统的Heaps’定律认为词型-词次关系应为幂律，即在log-log图上为直线。但实际数据在log-log图上呈现出轻微的凹曲，提示幂律关系并不完美，需要更准确的数学描述。

Method: 作者采集了20本英文小说（部分为翻译作品）的词型-词次统计数据，将其在log-log尺度下分别用一次与二次多项式进行回归分析，并用“有放回抽彩球”模型进行理论解释和分析曲率。

Result: 回归分析结果显示，log-log尺度下，二次函数能完美拟合词型-词次关系，线性项系数略大于1，二次项约为-0.02。理论模型解释了log-log曲线的曲率等价于“伪方差”，且该伪方差为负数。

Conclusion: log-log尺度下，词型-词次关系并非严格幂律，而应使用二次函数拟合（存在轻微曲率）；理论分析给出了该曲率的数学来源，这为进一步理解和刻画语言统计规律提供了新工具。

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [178] [SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://arxiv.org/abs/2511.14684)
*Biaojie Zeng,Min Zhang,Juan Zhou,Fengrui Liu,Ruiyang Huang,Xin Lin*

Main category: cs.CL

TL;DR: 本文提出一种新方法SMRC，通过模拟教师批改学生解题过程来自动检测并纠正大语言模型在数学推理中的错误，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖模型自我纠错，无法满足教育情境中系统指导修正学生解题过程的需求，因此需要发展类教师的自动化纠错机制。

Method: 提出SMRC方法，将学生推理过程建模为多步决策问题，采用蒙特卡洛树搜索（MCTS）探索最优纠错路径，并结合广度优先搜索（BFS）与最终答案评估生成奖励，通过反向传播实现细粒度过程监督。此外，构建了包含158道高中数学题的MSEB基准，并提出兼顾最终答案正确率与中间步骤保留率的双指标评估方案。

Result: SMRC在两个公开数据集（ProcessBench与MR-GSM8K）及新构建的MSEB基准上，纠错效果和整体性能均明显超过现有方法。

Conclusion: SMRC为教育场景下自动化检测与纠正LLM数理推理错误提供了高效可行的方案，对推动AI赋能教育具有现实和应用价值。

Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

</details>


### [179] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 本论文探讨了大型语言模型（LLM）能否通过文本编码本应由科学测量获得的物理信息，并利用天体物理学为测试平台，通过稀疏自编码器分析LLM的嵌入表达物理统计量的能力及影响因素。


<details>
  <summary>Details</summary>
Motivation: LLM在多领域都表现出良好的泛化与上下文学习能力，促使研究者关注其在科学物理信息编码方面的应用潜力，尤其是能否通过自然语言文本表达复杂的科学测量结果。

Method: 作者以天体物理学为例，提出两大研究问题：提示词对物理信息在LLM中的编码有无影响，以及语言的哪些方面最能有效编码这些物理测量。方法上，通过稀疏自编码器从文本中提取可解释特征，对比不同提示及语言因素变化下的LLM嵌入对科学测量统计量的表达能力。

Result: 论文初步表明，LLM的嵌入在一定条件下可以编码部分物理统计特征，提示词设计与语言表达会显著影响其表现。

Conclusion: LLM有望作为一种辅助工具，通过合适设计的文本输入辅助科学物理信息的提取与表达；但其编码机制依赖提示和语言特征，需进一步细致研究和优化。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [180] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLM）自动生成历史法语和中文文本的标注数据，从而改进低资源时期的NLP（自然语言处理）任务表现。


<details>
  <summary>Details</summary>
Motivation: 历史文献和低资源语言的NLP面临标注数据稀缺及与现代语料领域不匹配的问题，限制了现有NLP模型的应用。该研究希望通过新的标注方式，提升古文处理效果。

Method: 借助LLM（大型语言模型）为历史时期（16-20世纪的法语、1900-1950年的中文）文本生成语料标注，再用这些合成数据微调spaCy模型，对比其在词性标注、词形还原及实体识别等任务上的表现。

Result: 在有限的LLM标注语料辅助下，微调后的spaCy模型在特定历史时期的各项语言处理任务中都取得了显著提升。

Conclusion: 面向特定领域和历史时期的模型对于提升低资源文本处理能力非常重要。即使数量有限的合成标注语料，也能有效改进现有工具在计算人文等领域的应用。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [181] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文提出了VALOR模型，用于多模态、多轮客户支持对话中的投诉细致分析，结合文本和图像信息，实现更精准的投诉方面与严重程度分类，并在新构建的数据集上显著优于以往基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的投诉分析方法多依赖单一模态（如文本）和短文本内容，如推文或产品评价，难以应对包含图片和多轮对话的复杂客户投诉场景。随着实际用户在投诉时常常提供文本和图像证据，亟需能融合多模态数据、细致理解投诉内容的分析手段。

Method: 作者提出VALOR（Validation-Aware Learner with Expert Routing）模型，采用多专家推理结构和大规模生成式模型，并结合Chain-of-Thought（CoT）提示实现更细致决策。模型通过计算语义对齐分数，融合文本和图像信息，经由元融合策略输出最终分类结果。

Result: 在精细标注的多模态投诉数据集上，VALOR在投诉方面和严重程度分类任务上均显著超越基线模型，尤其是在信息分布于图文中的复杂投诉案例中表现突出。

Conclusion: 多模态交互和专家验证有助于提升实际投诉理解系统性能。VALOR不仅在多模态投诉分析上取得进展，也促进了更可靠、负责任的服务基础设施和产品设计，为实现联合国可持续发展目标（SDG 9和12）作出贡献。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [182] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 本文比较了库尔德语词嵌入的不同分词策略（单词级、形态基、BPE），发现BPE虽然初看效果好，但覆盖率低，真实表现存在偏差。形态基分词在嵌入空间组织和语义结构上表现更优。


<details>
  <summary>Details</summary>
Motivation: 库尔德语等低资源语言的形态复杂且缺乏标准分词方法，现有分词策略对词嵌入效果和NLP任务有重要影响，因此需系统比较不同分词方法对词嵌入质量的影响。

Method: 开发了BiLSTM-CRF形态分割工具，利用少量人工标注结合自助训练；用Word2Vec训练词嵌入，通过相似性、聚类、语义结构多指标评估三种分词策略；分析评测中的偏差并比较覆盖率。

Result: BPE在相似性任务上表面效果最好，但实际覆盖率仅28.6%，导致结果偏高。形态基分词覆盖率达68.7%，整体上嵌入质量、语义邻域和各形态复杂度词的表现更佳。

Conclusion: 对低资源语言处理来说，分词方法选择应关注评测覆盖度，BPE虽然常见但有偏差风险，形态基础分词在整体嵌入表现和语义组织上更具优势。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [183] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在研发流程中的多重应用，突出其在推动创新和加速研发中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 近年来，研发过程面临知识碎片化、跨学科交流障碍和创新周期长等挑战。作者希望通过分析LLM应用，探讨其如何应对这些难题，提升研发效率。

Method: 研究通过广泛分析科学文献、专利数据库与实验数据，系统评估LLM在自动知识发现、假设生成和跨学科整合等方面的功能与表现。

Result: 结果显示，LLM能自动挖掘与集成信息，促进假设构建和跨学科研发协作，使研发流程变得更灵活高效。创新周期被显著缩短，新想法的市场转化速度提升。

Conclusion: LLM在研发中的多重作用可提升创新效率，加快科研成果落地，对推动未来技术进步和产业创新具有重要意义。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [184] [FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding](https://arxiv.org/abs/2511.13961)
*Jiarui Li,Alessandro Zanardi,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出了一个集规划与执行于一体的多智能体路径规划（MAPF）系统框架，采用新算法（FICO）实现高效、实时、可适应不确定性的闭环运行，实验显示大规模、多变环境下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF方法通常将规划与执行分开处理，对不同变体处理零散，缺乏针对不确定性与系统多样性的统一建模和高效应对能力。

Method: 提出MAPF系统级建模框架，将MAPF视为控制设计问题；进一步提出FICO算法，借鉴有限时域与递归式控制思想，通过系统结构分解实现高效闭环控制，支持实时、大规模和不确定环境下的路径规划。

Result: FICO显著降低计算时间（比开环方法提升高达两个数量级），能适应数千智能体以及随机延迟和动态入场等不确定性，吞吐率大幅提升，凸显其实时性和可扩展性。

Conclusion: 系统级、闭环与因子分解设计为MAPF研究带来统一且高效的方法基础，推动了理论与工程应用的发展。

Abstract: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.

</details>


### [185] [LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry](https://arxiv.org/abs/2511.13985)
*Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 本论文提出了一种基于LiDAR与IMU数据融合的新型实时定位与建图系统（LIO-MARS），通过多项创新性的算法设计，有效提升了自主机器人在复杂环境下的导航精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 面向自主机器人（如用于搜救的无人机），在复杂环境中安全导航对实时、精准的环境感知提出了高要求。单一传感器方案存在不足，因此结合IMU和LiDAR进行多源信息融合以提升感知能力尤为关键。

Method: 在现有LiDAR里程计MARS基础上，融合IMU数据，提出了一种多分辨率表面元（surfel）地图与高斯混合模型（GMM）联合对齐的新方法。利用连续时间B样条轨迹建模，采用非均匀时序节点确保轨迹连续。利用Kronecker和与积加速GMM计算，引入无迹变换对表面元去失真、扫描分段以优化运动补偿；额外引入IMU伪测量与相对位姿软约束提高鲁棒性。

Result: 所提LIO-MARS系统在多种手持、地面和空中平台公开数据集上进行了充分验证，实验结果表明其在精度与鲁棒性上优于现有多种主流LIO系统，且计算速度提升达3.3倍。

Conclusion: LIO-MARS方法在LiDAR-inertial融合定位领域达到了新的性能高度，尤其适合需要高精度导航的应用场景，如无人机搜救等任务。

Abstract: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.

</details>


### [186] [Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval](https://arxiv.org/abs/2511.14004)
*Taijing Chen,Sateesh Kumar,Junhong Xu,George Pavlakos,J oydeep Biswas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 本文提出了STAR（时空主动检索）框架，通过统一“记忆查询”与“具身动作”的决策过程，实现机器人在动态开放世界中更高效地查找对象，支持时间、空间和动作多维度互动，并优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法往往只能应对部分问题，如场景图仅能处理空间关系而忽视时间，时序推理模型不支持具身交互，动态场景图则词汇有限、无法应对开放世界。因此需要一种能同时处理对象的时间与空间信息，并与环境互动的统一框架。

Method: 提出STAR框架，将长时记忆与工作记忆结合，用于高效回忆相关信息，并借助视觉-语言模型根据任务动态选择时间性或空间性动作。框架将“记忆检索”和“具身操作”纳入单一决策循环，并设立了STARBench基准测试，在仿真和真实机器人环境中针对时空检索任务进行评估。

Result: 在STARBench基准和真实Tiago机器人上的实验表明，STAR方法在任务完成效果上持续优于仅使用场景图或仅用记忆的基线方法。

Conclusion: 将时间与空间查找统一到一个决策过程，能显著提升服务机器人在开放、动态环境下查询对象的能力。STAR框架验证了这种方法论的有效性，并推动了机器人主动对象查找技术的进步。

Abstract: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.

</details>


### [187] [FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities](https://arxiv.org/abs/2511.14024)
*Jaskirat Singh,Rohan Chandra*

Main category: cs.RO

TL;DR: 提出FACA方法，通过自然语言让多机器人实现灵活且高效的避碰，在复杂环境中明显提升任务完成效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 多机器人在复杂、约束空间里执行高优先级任务（如救援、配送）的需求日益增长，但在多种异质体且优先级动态变化的情况下，现有方法难以兼顾安全与灵活性。

Method: 提出FACA方法，机器人通过自然语言沟通协调任务，采用新型人工势场算法，在冲突时自动生成"环岛"避让效应，实现公平且灵活的避碰。

Result: FACA方法在实验中任务完成效率提升显著，速度比基线提升3.5倍，时间减少逾70%，同时保持良好安全边界。

Conclusion: FACA能在高动态、复杂场景下有效提升多机器人系统的安全性与完成任务的敏捷性，优于传统方法。

Abstract: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.

</details>


### [188] [BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation](https://arxiv.org/abs/2511.14037)
*Hesam Mojtahedi,Reza Akhavian*

Main category: cs.RO

TL;DR: 该论文提出了一种基于BIM差异的主动感知框架，用于无人机与无人车在施工环境中的协同导航，有效提升了导航安全性与自主性。


<details>
  <summary>Details</summary>
Motivation: 在动态施工场景中，现有导航方法仅依赖静态BIM信息或有限的机载感知，无法有效应对动态环境的不确定性和信息缺失，影响导航安全。

Method: 该方法融合无人机和无人车的实时LiDAR数据与BIM先验，维护动态2D占据图，并通过统一的走廊风险指标（综合占据不确定性、BIM差异和安全距离）量化导航安全性。当风险大于阈值时，无人机会主动重扫高风险区域，降低不确定性，实现安全重规划。

Result: 在PX4-Gazebo仿真和Robotec GPU LiDAR测试中，该方法将平均走廊风险降低58%，地图熵减少43%，同时保持0.4米以上的安全距离。与前沿探索法相比，用相同时间达成相似的不确定性降低效果。

Conclusion: 融合BIM先验和基于风险的主动感知能有效提升建筑机器人系统的可扩展性和不确定性自适应自主性。

Abstract: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.

</details>


### [189] [FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing](https://arxiv.org/abs/2511.14139)
*Junhao Gong,Shoujie Li,Kit-Wa Sou,Changqing Guo,Hourong Huang,Tong Wu,Yifan Xie,Chenxin Liang,Chuqiao Lyu,Xiaojun Liang,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出了一种具备多模态感知的新型无线吸盘FlexiCup，通过集成视觉和触觉双区域感知，实现复杂环境中自主且感知驱动的抓取与操作。实验结果显示其在不同吸附模式和任务中的表现优异，硬件与固件已开源。


<details>
  <summary>Details</summary>
Motivation: 传统吸盘在非结构化环境下操作时，缺乏感知能力，难以进行接触感知和灵活控制。这大大限制了其在智能机器人中复杂抓取和操作任务的应用。为解决上述问题，亟需将多模态感知集成到无线自主吸盘中。

Method: 设计了FlexiCup：一种完全无线、自主供电、集成双区（中央视觉-触觉动态切换+外周连续空间感知）多模态感知吸盘。通过机械模块切换，还可支持真空与伯努利两种吸附模式。系统采用机载计算和无线通信。抓取策略分为基于感知驱动和端到端学习，包括扩散模型实现任务学习，多头注意力融合多区域感知数据。

Result: 在具有不同障碍密度的结构化表面抓取任务中，真空与伯努利模式成功率分别为90.0%和86.7%；在倾斜搬运和橙子提取等任务中，端到端学习方法成功率分别为73.3%和66.7%。消融实验表明，多头注意力机制融合感知区域可提高13%的操作成功率。

Conclusion: FlexiCup在复杂、非结构化环境下实现了完全无线的多模态感知操作，显著提升了接触感知抓取的灵活性与成功率，推动了机器人智能操作的发展。相关硬件设计与固件已公开，为后续研究和应用提供了基础。

Abstract: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.

</details>


### [190] [AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models](https://arxiv.org/abs/2511.14148)
*Yuhua Jiang,Shuang Cheng,Yan Ding,Feifei Gao,Biqing Qi*

Main category: cs.RO

TL;DR: 提出了AsyncVLA，一种异步流匹配的视觉-语言-动作模型，可自我纠正动作错误，有效提升机器人在长任务中的稳定性与表现，达到了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在动作生成时依赖统一、固定的时间调度（同步流匹配，SFM），缺乏对动作上下文的感知与异步自我纠正能力，导致长时序任务中动作失误易级联导致失败。本研究旨在解决SFM的稳定性不足，提升VLA机器人在复杂任务中的表现。

Method: 提出AsyncVLA，将异步流匹配（AFM）引入VLA模型，实现动作生成的时序灵活性，具备动作上下文感知。创新点包括：1）非统一时序生成动作token，2）引入信心评估模块，判别初始生成动作的准确性，择机在执行前纠正不准确动作，3）统一训练SFM与AFM，提升KV-cache利用率，让单一模型兼容双模式。

Result: 在机器人操作基准测试中，AsyncVLA展现出更好的数据效率和自我纠正能力，长时任务表现稳定，且在多项常用评测中取得SOTA（最新最优）结果。

Conclusion: AsyncVLA通过引入异步流匹配与自我纠正机制，显著提升了VLA模型在长时序和复杂任务中的稳定性和表现，为通用机器人研究提供了新的有效范式。

Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

</details>


### [191] [RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action](https://arxiv.org/abs/2511.14161)
*Xiaoquan Sun,Ruijian Zhang,Kang Pang,Bingchen Miao,Yuxiang Tan,Zhen Yang,Ming Li,Jiayu Chen*

Main category: cs.RO

TL;DR: RoboTidy是一个针对家居整理任务的统一基准，支持语言引导的机器人综合评估，包括视觉、语言与动作、导航等多模态能力。


<details>
  <summary>Details</summary>
Motivation: 现有家居整理基准不能很好地建模用户偏好、缺乏移动性支持，且泛化性能差，难以系统评估端到端语言到行动能力，因此需要更统一且现实的测试平台。

Method: 提出了RoboTidy基准，包含500个高真实感3D场景，模拟了500种物体和容器的碰撞环境，将整理任务抽象为“动作（物体，容器）”序列，并提供大规模高质量的人机整理和导航示范轨迹，支持 few-shot 和大规模训练。该平台还可部署于真实世界，支持机器人实际测试。

Result: RoboTidy实现了端到端的家居整理基准，支持综合考察语言引导的机器人的整理能力和导航能力。

Conclusion: RoboTidy填补了智能体领域语言引导家居整理评估的关键空白，提供了现实且可扩展的综合评测平台，有助于推动具身智能体在实际场景下的应用。

Abstract: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.

</details>


### [192] [Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/abs/2511.14178)
*Zhuo Li,Junjia Liu,Zhipeng Dong,Tao Teng,Quentin Rouxel,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法VLA-Pilot，用于在无需再训练和额外数据的情况下提升视觉-语言-动作模型在机器人任务中的即插即用表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型虽然在机器人操作上表现出潜力，但在实际部署中表现会显著下降。常见的微调方法虽然能改善这一问题，但需要昂贵的人工演示和大量计算资源，实际应用难度大。作者希望提出一种无需额外成本即可提升VLA下游任务表现的新方法。

Method: 提出VLA-Pilot方法，这是一种在推理阶段直接对VLA模型决策进行引导的即插即用策略，不依赖后续微调或新收集数据。从而可以在零样本设置下部署预训练的VLA模型。方法在两个不同类型机器人的六个真实下游操控任务上进行了评估，覆盖分布内与分布外场景。

Result: 实验结果显示，VLA-Pilot能够大幅提升已有VLA模型的任务成功率，实现对不同任务和机器人硬件的鲁棒零样本泛化。

Conclusion: VLA-Pilot为视觉-语言-动作模型的实际部署提供了高效、普适的性能提升方法，避免了高昂的后处理与数据成本，展现出较强的实用价值。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.

</details>


### [193] [Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics](https://arxiv.org/abs/2511.14327)
*Felipe Ballen-Moreno,Pasquale Ferrentino,Milan Amighi,Bram Vanderborght,Tom Verstraten*

Main category: cs.RO

TL;DR: 提出一种双变量表征方法，更准确地模拟和分析可穿戴机器人绑带与人体软组织之间的物理交互。


<details>
  <summary>Details</summary>
Motivation: 现有针对可穿戴机器人和人体软组织交互的特性描述方法往往只考虑单一变量（如压力或一个方向的力），这限制了实际多自由度交互的准确建模，进而影响安全与舒适性评估。

Method: 作者提出一种新的双变量表征方法，能够同时考虑法向和切向力，评估材料参数，并通过归一化均方误差（NMSE）对不同场景和材料模型进行比较分析。

Result: 实验证明，双变量方法比单变量更能准确表征交互件（如绑带）的力和力矩响应，揭示采用单变量会带来较大误差。

Conclusion: 该方法为可穿戴机器人与人体交互的模拟和设计提供了更准确的基础，尤其是在关注绑带与软组织力学行为的场景中，改善了安全性和舒适性预测。

Abstract: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.

</details>


### [194] [MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning](https://arxiv.org/abs/2511.14330)
*Yizhen Yin,Yuhua Qi,Dapeng Feng,Hongbo Chen,Hongjun Ma,Jin Wu,Yi Jiang*

Main category: cs.RO

TL;DR: 本论文提出一种基于深度强化学习的地图感知主动建图与定位（MA-SLAM）系统，实现了对大规模环境下机器人的高效探索，在仿真和实际平台测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 主动SLAM能提升机器人对环境的感知和建图能力，但现有方法在大规模多样环境中探索效率低、路径不理想。如何提升大空间环境下主动探索的效率成为关键难题。

Method: 作者提出了一种新型结构化地图表示方法，将空间数据离散化，并整合边界点与历史轨迹，为基于深度强化学习的决策模块提供输入。同时引入全局规划器，不再逐步预测下一个动作，而是通过远距离目标点优化整体探索路径。

Result: 在三种仿真环境和一台真实无人地面车（UGV）上进行了测试，实验结果显示，与最先进的现有方法相比，所提方法大幅缩短了探索所需的时间和距离。

Conclusion: MA-SLAM系统充分发挥结构化地图和全局路径优化的优势，在大规模主动SLAM任务中显著优于当前主流方法，为移动机器人高效建图与探索提供了有效方案。

Abstract: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

</details>


### [195] [Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors](https://arxiv.org/abs/2511.14335)
*Jeryes Danial,Yosi Ben Asher,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种结合稀疏关键点与稠密边缘重建的轻量化单目SLAM系统，实现了低算力平台上的实时三维定位与地图构建。


<details>
  <summary>Details</summary>
Motivation: 现有单目SLAM方法要么几何信息不够丰富（稀疏方法），要么算力消耗过大（深度学习方法），且普遍存在尺度歧义，对无人机的导航精度造成影响。

Method: 算法结合稀疏关键点定位与深度学习预测得到的稠密边缘重建，通过优化保证几何一致性。融合IMU数据，用扩展卡尔曼滤波消除尺度歧义，且不依赖全局回环检测或重型神经网络。

Result: 该系统在DJI Tello无人机及TUM RGBD数据集上，展示了在低算力硬件上的实时运行能力，并具备鲁棒的自主导航和避障能力。

Conclusion: 所提出方法在资源受限环境下实现了精准、高效的实时SLAM，适用于实际无人机导航与三维建图应用。

Abstract: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.

</details>


### [196] [Going Places: Place Recognition in Artificial and Natural Systems](https://arxiv.org/abs/2511.14341)
*Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文综述了机器人、动物和人类在地点识别方面的研究进展，比较了不同系统在编码和回忆地点时的机制，并提出了统一的概念框架。


<details>
  <summary>Details</summary>
Motivation: 地点识别对导航至关重要，无论是生物还是自主机器人。整合不同领域的发现，有助于推动人工系统的创新和提升泛化能力。

Method: 作者综合分析了机器人、动物和人类地点识别的计算策略、表征方法，包括拓扑映射、线索整合和记忆管理，并对比了生物系统与人工系统的异同。

Result: 发现动物系统进化出多模态导航与环境适应机制，人类具备语义、文化和自省等独特空间表征，人工系统则展现出可扩展的架构与数据驱动模型。作者提出了用于理解和开发地点识别的统一概念体系，并明确了泛化性、鲁棒性与环境变异性等主要挑战。

Conclusion: 本文指出跨学科研究有助于推动人工定位技术的发展，建议未来人工地点识别系统应借鉴动物与人类的空间认知机制，以提升其性能与适应性。

Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.

</details>


### [197] [Perception-aware Exploration for Consumer-grade UAVs](https://arxiv.org/abs/2511.14393)
*Svetlana Seliunina,Daniel Schleich,Sven Behnke*

Main category: cs.RO

TL;DR: 本文将多无人机自主探索的方法拓展到了消费级无人机（如DJI Mini 3 Pro），提出了新颖的视觉与轨迹规划管道和半分布式通信方案，实现了资源受限情况下的高效探索与地图重建。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机自主探索的方法主要针对昂贵或高端无人机，缺乏适用于消费级（硬件受限）无人机的解决方案。随着消费级无人机的普及，扩展其自主探索能力具有实际应用价值。

Method: 提出了一种从视点对中估算深度与满足运动约束的轨迹规划流程。此外，设计了半分布式通信机制，实现任务负载的平衡分配。

Result: 在模拟实验中，验证了该方法可在多种无人机规模下实现安全探索和有效地图重建，且可应对消费级无人机硬件受限的挑战。

Conclusion: 本文方法提升了消费级无人机在自主多机探索和环境重建任务中的能力，具备良好的扩展性和实际应用前景。

Abstract: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.

</details>


### [198] [Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396)
*Xiuxiu Qi,Yu Yang,Jiannong Cao,Luyao Bai,Chongshan Fan,Chengtai Cao,Hongpeng Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种名为CCoL的新型行为克隆框架，通过多模态联合学习（视觉、语言和自身状态）实现连续且语义物理一致的机器人操控，获得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 行为克隆（BC）在机器人通过语言指令进行操控方面具有重要作用，但当前BC方法在处理动作序列决策时容易出现累积误差，尤其在动作连续性和语义-物理对齐方面存在显著问题，导致动作克隆不准确和执行不连贯。因此，研究如何增强BC性能、消除语义与物理执行的错位成为迫切需求。

Method: 作者提出了CCoL框架，将视觉、语言和机器人自身状态输入进行连续联合学习，并采用双向交叉注意力机制，将语言语义锚定在视运动表征上，强化上下文关联，从而生成更加平滑且语义对齐的动作执行轨迹。

Result: CCoL在三套仿真测试中平均提升8.0%，在人类示范的双臂插拔任务中最高提升19.2%。实际7自由度机器人测试显示，CCoL在未知和噪声物体状态下也具备良好泛化能力。

Conclusion: CCoL有效缓解了传统BC的语义-物理错位与动作执行中断问题，实现了更加鲁棒、流畅且语义对齐的人机交互操控，对提升实际机器人任务的BC表现具有积极意义。

Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

</details>


### [199] [Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning](https://arxiv.org/abs/2511.14427)
*Rickmer Krohn,Vignesh Prasad,Gabriele Tiboni,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 本文提出了一种名为MultiSensory Dynamic Pretraining（MSDP）的新框架，通过自监督预训练跨视觉、力觉和本体感受的融合表征，提高机器人在复杂接触操作中的学习效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的机器人在多传感器（视觉、力觉、本体感受）融合的复杂操作场景中，面对传感器噪声和动态变化时学习效果较差，需要提升其在信息融合和鲁棒性方面的能力。

Method: 提出MSDP框架，采用掩码自编码器预训练transformer编码器，使其能通过部分传感器嵌入重建完整多模态观测，实现跨模态预测与传感器融合。下游策略学习阶段，设计了不对称结构，critic通过交叉注意机制动态提取嵌入信息，actor则采用稳定的池化表示以指导动作。

Result: 在复杂、接触丰富的机器人操作任务（包括仿真和真实环境）中，方法展现出较快的学习速度和强大的鲁棒性，对传感器噪声及物体动态变化有良好适应性。

Conclusion: MSDP为多感知机器人控制提供了一种简单高效的解决方案，能以极少的在线交互在真实机器人上取得接近100%成功率，表现出极强的实用性和鲁棒性。

Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.

</details>


### [200] [Mutation Testing for Industrial Robotic Systems](https://arxiv.org/abs/2511.14432)
*Marcela Gonçalves dos Santos,Sylvain Hallé,Fábio Petrillo*

Main category: cs.RO

TL;DR: 本文提出了一种面向工业机器人系统（IRS）的领域特定变异测试方法，通过针对机器人动作与传感器读数设计专门的变异算子，更好地评价机器人控制软件的测试套件有效性。实验结果表明，该方法能生成更有意义、失效率更低的变异体，有助提升测试质量和系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统变异算子难以覆盖工业机器人软件的消息传递与物理交互特性，导致变异测试有效性不足。为提升机器人控制软件的可靠性，需要开发更适合IRS场景的测试方法。

Method: 针对IRS控制软件的高层次读写操作（如运动、夹爪动作和传感器噪声），设计了符合机器人语义的专用变异算子。通过注入这类变异体，评估测试套件的缺陷发现能力，并以拾取与放置场景为例进行了实证分析。

Result: 新设计的领域特定变异算子在实验案例中显著减少了无效或等价变异体的数量，提升了变异体的意义和覆盖率，对比传统方法更能发现测试套件弱点。

Conclusion: 适应于工业机器人领域的变异测试框架能够有效提升控制软件的测试质量，为IRS的安全与可靠性提供了新的技术保障。

Abstract: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.

</details>


### [201] [Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies](https://arxiv.org/abs/2511.14434)
*Marlow Fawn,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出了一种将基于信号时序逻辑(STL)的调和控制Lyapunov-Barrier函数（HCLBFs）与机器人已有策略结合的方法，从而用形式化保证将不安全的策略转变为安全策略。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习或其他任务导向的机器人策略容易产生不安全行为，而现有的安全约束与任务目标难以高效结合。研究者希望开发一种既能保证安全又不影响任务性能的通用方法。

Method: 首先利用STL规范自动导出HCLBFs，并采用这些函数作为系统的安全性证书。然后将HCLBFs与任意给定机器人策略结合，对原策略输出进行调整，使其在保持任务执行效率的同时，满足形式化的安全约束。

Result: 通过在静止机械臂上实现带有障碍物的物体操作任务，证明所提出的方法可以有效地将原本仅注重任务完成的策略转变为既完成任务又避免碰撞的安全策略。

Conclusion: 该方法不仅能有效提升机器人任务策略的安全性且具有形式化的安全保证，并且方法具有良好的泛化能力，可以应用于更复杂的规范和动态任务场景。

Abstract: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.

</details>


### [202] [Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy](https://arxiv.org/abs/2511.14458)
*Michelle Mattille,Alexandre Mesot,Miriam Weisskopf,Nicole Ochsenbein-Kölble,Ueli Moehrlen,Bradley J. Nelson,Quentin Boehler*

Main category: cs.RO

TL;DR: 本文提出了一种用于腔镜激光手术的柔性机器人平台，通过磁驱动柔性内窥镜及实时场景拼接，提升了手术时的灵活性与视野。实验在羊模型中验证了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有柔性机器人虽提升了微创手术的灵活性，但在开放腔体内因缺乏解剖支撑和内窥镜视野有限，导致设备控制和操作安全受限，影响了临床应用。

Method: 本研究设计了一套磁驱动柔性内窥镜机器人平台，具备远程操作和半自主导航功能，可用于目标激光消融，并通过实时场景拼接技术扩展手术视野。

Result: 系统在体内羊模型中进行了验证，证实其在开放腔体下提高手术可控性与视野连续性的能力。

Conclusion: 该平台有效解决了开放腔体腔镜手术中的控制与视野局限，有望推动微创手术在复杂场景中的临床应用。

Abstract: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.

</details>


### [203] [Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations](https://arxiv.org/abs/2511.14504)
*Jan Quenzel,Valerij Sekin,Daniel Schleich,Alexander Miller,Merlin Stampa,Norbert Pahlke,Christof Röhrig,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种结合无人机和机动喷水系统的自动化消防辅助系统，旨在提升工业设施火灾中的灭火效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有工业设施火灾中，由于厂房规模巨大和能见度差，消防员难以精准定位火源，延误灭火时间并增加损失。

Method: 系统结合装载在旋转升降梯上的机动喷水装备与配套无人机。无人机利用地理数据在无障碍飞行通道内自主飞行，侦测并定位热源。操作员通过手持终端监控并选择目标火点后，无人机自动在两点间执行三角定位，同时调整喷水装置，确保水柱对应热源。

Result: 初步实验表明，系统能成功定位多个热源，并将水柱准确导向火点。

Conclusion: 该系统可显著提升工业火灾定位和灭火效率，为消防员提供有力智能辅助，减少灭火时间和损失。

Abstract: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.

</details>


### [204] [Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language](https://arxiv.org/abs/2511.14565)
*Minyoung Hwang,Alexandra Forsey-Smerek,Nathaniel Dennler,Andreea Bobu*

Main category: cs.RO

TL;DR: 本论文提出了一种结合大语言模型与逆强化学习的方法，以提升机器人从示范和指令中学习奖励函数的泛化能力，并有效处理语言和数据不足导致的二义性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于示范的奖励学习在数据有限时容易过拟合，难以区分任务关键点和次要细节。虽然自然语言指令可以减少模糊性，但现有方法往往未能充分发挥语言的作用，且语言本身也可能含糊，因此单纯依靠某一输入仍存在不足。

Method: 提出了Masked IRL框架，利用大语言模型，从自然语言指令中推断与任务相关的状态掩码，将其与示范结合，保证奖励函数对不相关状态部分具有不变性。当指令中存在歧义时，利用大语言模型推理，通过示范上下文解析歧义。

Result: 在仿真和真实机器人实验中，Masked IRL相较于以往的语言条件逆强化学习方法，实现了最多15%的性能提升，并且数据需求降低至原方法的1/4.7，表现出更强的样本效率、泛化能力和对语言歧义的鲁棒性。

Conclusion: Masked IRL能够有效结合示范的操作性和语言的指示性，突破单一信息源的局限，显著提升机器人学习奖励函数的泛化与鲁棒性，为语言与示范双模态学习提供了新思路。

Abstract: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL

</details>


### [205] [Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks](https://arxiv.org/abs/2511.14592)
*Xianhui Meng,Yuchen Zhang,Zhijian Huang,Zheng Lu,Ziling Ji,Yaoyao Yin,Hongyuan Zhang,Guangfeng Jiang,Yandan Lin,Long Chen,Hangjun Ye,Li Zhang,Jun Liu,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出了DSBench，这是首个针对视觉语言模型（VLM）在自动驾驶安全场景下的全面评测基准，涵盖外部环境风险与车内行为安全，揭示当前VLM在复杂安全场景下表现不足，并通过新建大规模数据集实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）在自动驾驶领域的应用前景看好，但其在安全关键场景下的适用性和安全性尚未系统研究，主要原因是缺乏能同时覆盖外部环境与车内行为安全的综合评测基准。

Method: 作者构建了DSBench，首次系统评测VLM在自动驾驶场景中对多样安全风险的感知。DSBench分成外部环境风险与车内行为安全两大类，共10个关键类别、28个子类别。作者还建立了包含9.8万实例的新数据集，涵盖上述场景，并对主流VLM进行了测试和微调实验。

Result: 评测结果显示，无论是开源还是闭源VLM，在复杂安全关键环境下均有明显性能下降，暴露出潜在安全隐患。通过在新数据集上微调，VLM安全表现获得了明显提升。

Conclusion: 该研究为自动驾驶领域VLM的安全评估与改进提供了新基准和大规模数据资源，对推动安全可靠的自动驾驶技术具有重要意义。DSBench工具包、代码与模型将公开，有助于促进学术和工业发展。

Abstract: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.

</details>


### [206] [Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains](https://arxiv.org/abs/2511.14625)
*Qingwei Ben,Botian Xu,Kailin Li,Feiyu Jia,Wentao Zhang,Jingping Wang,Jingbo Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出了Gallant，一个基于体素网格（voxel grid）的全新感知与控制框架，可提升类人机器人（humanoid）在复杂3D地形中的行走与局部导航能力，通过引入体素化LiDAR感知和端到端优化，实现高成功率应对多种障碍与环境。


<details>
  <summary>Details</summary>
Motivation: 现有感知模块多依赖深度图或高程图，仅能获取局部、扁平的环境信息，难以满足类人机器人对3D环境全面、准确感知的需求，限制了其在多级结构、狭窄通道、横向或高空障碍等复杂场景中的导航与运动能力。

Method: Gallant采用体素化LiDAR数据为感知输入，将其作为结构化、轻量的3D表示，通过z轴分组的2D卷积神经网络（2D CNN），直接映射到运动控制策略，实现完全端到端的优化。为适应实际应用，开发了高保真LiDAR仿真环境，提供动态且真实的观测，提升训练的可扩展性和虚实一致性。

Result: 实验显示，Gallant拥有比以往方法更广的感知覆盖，可利用单一策略有效处理除地面障碍外的多种复杂情形，包括侧向杂物、上方限制、多层结构及狭窄通路等。尤其在爬楼梯和跨越高台等典型挑战场景中，通过改进的端到端优化，首次实现了接近100%的成功率。

Conclusion: Gallant显著扩展了类人机器人在三维约束环境下的行走与导航能力，突破了传统平面感知策略的限制，为实现更高级的自主移动提供了新的解决方案。

Abstract: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.

</details>


### [207] [NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards](https://arxiv.org/abs/2511.14659)
*Chia-Yu Hung,Navonil Majumder,Haoyuan Deng,Liu Renhang,Yankang Ang,Amir Zadeh,Chuan Li,Dorien Herremans,Ziwei Wang,Soujanya Poria*

Main category: cs.RO

TL;DR: 本文提出了NORA-1.5，一种增强版视觉-语言-动作（VLA）模型，通过引入流匹配的动作专家和基于奖励后训练策略，显著提升了模型在模拟和实际环境中的可靠性及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在不同平台或现实场景部署时，仍存在可靠性和泛化能力不足的问题。因此，作者希望通过改进模型架构和训练方法，提高VLA模型在实际任务中的表现和可靠性。

Method: NORA-1.5在预训练NORA基础上，增加了流匹配（flow-matching）动作专家。为提升鲁棒性和完成任务的能力，作者设计了两种奖励模型：一是动作条件世界模型，用于评价动作是否朝向目标前进；二是与真实动作偏差的启发式方法，区分优劣动作。通过奖励信号构建偏好数据集，并采用直接偏好优化（DPO）方式对NORA-1.5进行后训练，适应不同场景。

Result: NORA-1.5在多项模拟和真实机器人任务基准上，超越了原始NORA及其它先进VLA模型。基于奖励的后训练方案能在不同环境中稳定提升模型性能和任务成功率。

Conclusion: 该工作展示了NORA-1.5及奖励引导的后训练作为提高现实VLA模型可靠性与实用性的有效途径，为部署于真实环境的具身智能体提供了新的技术路径。

Abstract: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

</details>


### [208] [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2511.14755)
*Albert Lin,Alessandro Pinto,Somil Bansal*

Main category: cs.RO

TL;DR: 本文提出了RoVer-CoRe框架，为感知驱动自动系统基于Hamilton-Jacobi（HJ）可达性分析做形式化安全验证，有效兼顾感知不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前自主系统普遍采用基于感知的控制器，但由于控制器复杂性和感知不确定性的影响，对其形式化安全与性能验证具有挑战性。现有方法往往受限于处理的系统/控制器类型或分析过于保守。

Method: 作者提出利用HJ可达性分析，将系统控制器、观测函数和状态估计算法串联为等效的闭环系统，从而兼容通用可达性分析。提出新的正式安全验证与鲁棒控制器设计方法，并通过RoVer-CoRe框架实现。

Result: 作者在飞机滑行和基于神经网络的探测车导航案例中验证了所提方法的有效性，能够在存在感知不确定性时实现正式安全验证。

Conclusion: RoVer-CoRe是首个面向感知不确定系统、基于HJ可达性分析的验证框架，为更广泛的自主系统形式化验证提供支持。

Abstract: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.

</details>


### [209] [HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation](https://arxiv.org/abs/2511.14756)
*Lai Wei,Xuanbin Peng,Ri-Zhao Qiu,Tianshu Huang,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出HMC框架，通过融合多种控制模式，大幅提升机器人对复杂真实任务的适应能力和表现。


<details>
  <summary>Details</summary>
Motivation: 现实环境下的机器人演示学习在交互复杂、多变动力学场景下，传统位置控制器常常因接触或负载变化表现不佳，需要更为鲁棒和自适应的控制方法。

Method: 作者提出异构元控制（HMC）框架，将位置、阻抗及混合力-位置等多种控制模式自适应地结合。具体包括HMC-Controller（在力矩空间内混合不同控制动作，支持操作与策略部署）以及HMC-Policy（采用专家混合路由，统一不同控制器，实现从大规模位置数据和精细力演示中学习鲁棒策略）。

Result: 针对真实仿人机器人进行实验，HMC框架在如柔顺擦桌、开抽屉等挑战任务上相较基线方法获得50%以上的性能提升。

Conclusion: HMC框架能够有效融合多控制模式，显著提升机器人在现实复杂多变任务中的表现和适应性，具有实际应用前景。

Abstract: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.

</details>
