<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 295]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 本文提出了一种结合笔迹学与人工智能的新方法，通过分析学生手写考试试卷来量化其心理压力水平，利用高分辨率图像处理、TrOCR和基于RoBERTa的情感分析，最终生成压力指数。该方法采用多模型投票机制和异常检测，提升了鲁棒性，对教育领域具有创新意义。


<details>
  <summary>Details</summary>
Motivation: 传统的考试评价系统忽视了对学生在考试期间认知及情感状态的理解，仅关注成绩无法全面反映学生的心理压力。本研究旨在弥补传统评价体系这一不足，通过技术手段深入揭示学生考试时的心理状况。

Method: 本方法利用高分辨率的试卷图像采集，首先通过TrOCR实现手写文字的字符识别，再利用基于RoBERTa的情感分析模型测算情感熵，随后将这些指标融合形成学生的压力指数。最后，系统融合5个模型的投票机制，并用无监督异常检测提升结果的稳健性。

Result: 实验结果表明，该方法能够生成反映学生真实压力水平的数值指标，且在多个模型投票和异常检测的支持下，结果具备较高的稳定性和鲁棒性，优于单一情感分析方法。

Conclusion: 本研究开创性地将笔迹分析和AI结合，为教育测评与学术司法领域提供了一种新的压力测量手段，能够辅助更全面、科学地了解学生的心理状态，有助于后续心理健康干预和个性化教育方案制定。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [2] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 本论文提出了一种基于车辆车载传感器和SVM分类器的实时道路坑洞检测方法，取得了极高的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着道路车辆数量激增，道路状况需频繁监控以保障交通顺畅。路面裂缝和坑洞若未及时发现会迅速恶化，对出行安全造成隐患。因此需要便捷高效的大规模坑洞检测手段。

Method: 利用汽车上的传感器收集行驶过程中的路面数据，并应用支持向量机（SVM）分类器进行坑洞检测。研究在一段约2公里、分布有26个坑洞的道路上采集数据进行实验，并基于这些数据训练和评估检测算法。

Result: 所提出的方法达到了98.1%的检测准确率，有效识别了道路上的坑洞。

Conclusion: 基于车载传感器和SVM分类器的实时坑洞检测方法在实际道路测试中表现优异，可为大规模道路养护管理提供有力的数据支撑。相关代码已开源，便于成果复现与推广。

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [3] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: 本研究提出了一套适用于耕地生态系统的细致栖息地分类方法和超高分辨率遥感影像数据集，并基于动态加权特征融合网络（DWFF-Net）实现了高精度的多尺度栖息地分割，为精细化耕地栖息地监测提供了技术支撑。


<details>
  <summary>Details</summary>
Motivation: 目前耕地生态系统缺乏统一的栖息地分类体系，已有模型难以全面覆盖各类栖息地，并且现有分割模型难以有效融合语义与纹理特征，导致分割精度不足和边界模糊，尤其在大尺度地块与微小栖息地（如田埂）之间表现不佳。

Method: 研究构建了包含15类栖息地的标注超高分辨率遥感影像数据集，并提出了DWFF-Net：其编码器基于冻结参数的DINOv3提取基础特征，通过分析类别间特征关系实现数据级自适应动态加权特征融合，解码器利用动态权重计算网络融合多层特征，并结合混合损失函数提升训练效果。

Result: 在自建数据集上的实验结果表明，DWFF-Net在mIoU和F1-score上分别达到0.6979和0.8049，较基线网络分别提升0.021和0.0161。消融实验进一步验证了多层特征融合对提升微型栖息地（如田埂）IoU的效果。

Conclusion: 研究建立了基于自适应多层特征融合的耕地栖息地识别框架，实现亚米级精度且低成本的栖息地精细化监测，为耕地景观栖息地管理和保护提供了坚实的技术基础。

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [4] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了一种高效且精确的医学图像少样本分割方法AGENet，通过边界感知的测地距离学习和自适应特征提取，有效改善了有限标注下的分割精度，特别是在结构边界处优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割依赖大量标注数据，这成为限制其临床应用的瓶颈。传统少样本分割方法对医学结构边界的刻画能力有限，尤其在解剖结构相似区域缺乏空间上下文时效果不佳。作者希望在有限样本下提升分割模型的边界精度和鲁棒性。

Method: 提出了AGENet框架，主要包含三个组件：1）边界感知测地距离学习模块，利用快速行进算法迭代细化边界，2）自适应原型提取模块，通过空间加权聚合捕捉整体结构和局部边界特征，3）自适应参数学习机制，根据不同器官特性自动调整模型参数。方法以轻量级几何建模为主，避免了复杂深度网络。

Result: AGENet在多个医学影像数据集上进行了大量实验，与现有主流方法相比，表现出更高的分割精度，尤其在结构边界处的误差明显降低，同时保持了较高的计算效率。

Conclusion: 提出的方法在标注数据有限的情况下，实现了更好的边界分割效果，计算资源消耗低，适用于需要高精度分割且标注稀缺的临床实际应用场景。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [5] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: 本文提出了一种无需预训练的高效点云语义分割网络EPSegFZ，显著提升了零样本和小样本场景下分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有点云小样本分割方法普遍依赖预训练阶段，导致模型灵活性不足，并且大多未充分利用支持集中的文本等信息，限制了模型性能和零样本能力。

Method: 1. 构建不依赖预训练的EPSegFZ网络。2. 设计ProERA和基于DRPE的跨注意力机制以提升特征提取和查询-原型关系建立能力。3. 引入LGPE模块，充分利用支持集文本信息，增强小样本和零样本表现。

Result: 在S3DIS和ScanNet数据集上，该方法分别比最新方法提升5.68%和3.82%。

Conclusion: EPSegFZ无需预训练，通过多模态信息利用和创新注意力机制，有效提升了点云小样本和零样本分割性能。

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [6] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种高效准确的3D场景级可供性分割方法，结合了2D语义线索与3D几何信息，实现了从自然语言到空间动作的推理。


<details>
  <summary>Details</summary>
Motivation: 目前让机器人等智能体理解并执行自然语言下达的3D场景任务仍然具有挑战性，多数方法仅关注于物体级别或简单地将2D结果扩展到3D，既忽略了点云几何结构又算力消耗大。

Method: 提出了TASA（Task-Aware 3D Scene-level Affordance segmentation）框架：首先通过任务相关的2D可供性检测模块，根据语言和视觉信息在2D空间筛选出与任务相关的操控点，再通过3D可供性细化模块，将2D语义先验与局部3D几何特征融合，实现精确、一致的3D可供性掩码分割。全流程采用粗到细的联合推理方式。

Result: 在SceneFun3D数据集上的实验表明，TASA在场景级可供性分割的准确性和效率方面均显著优于现有主流方法。

Conclusion: TASA有效提升了场景级3D可供性理解的性能，对于上述任务具有实际应用意义，并能为智能体提供更高效的任务执行能力。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [7] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种新的轻量级Capsule Network变体LE-CapsNet，显著提升了推理速度和准确率，同时减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 尽管CapsNet相较于CNN在某些任务上有优势，但其速度慢、参数多、资源消耗大，且在某些基准上的准确率也低于CNN。研究者希望解决这些问题，提升CapsNet的实用性和性能。

Method: 提出LE-CapsNet，通过结构优化，减少参数（仅3.8M参数）、提升推理速度，并增强对仿射变换的鲁棒性。

Result: LE-CapsNet在CIFAR-10数据集上获得了76.73%的准确率，推理速度是传统CapsNet的4倍。在仿射变换数据集AffNIST上准确率达到94.3%，高于CapsNet的90.52%。

Conclusion: LE-CapsNet在参数量减少的同时，显著提高了推理速度和对变换的鲁棒性，在图像分类及变换适应性方面综合表现优于原始CapsNet。

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [8] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种改进的3D资产生成方法（TBSD），解决了传统SDS方法在纹理和形状之间的取舍问题，实现了纹理真实且形状准确的3D模型生成。


<details>
  <summary>Details</summary>
Motivation: 传统的Score Distillation Sampling（SDS）方法通过蒸馏2D扩散模型的先验进行3D资产生成，但存在过度饱和和过度平滑的问题。虽然引入了负向提示（Negative Prompts）来缓解该问题，但出现了纹理与形状之间难以兼得的矛盾：要么纹理不佳，要么纹理好但形状扭曲。作者希望系统性地分析并解决这一矛盾。

Method: 作者首先系统性分析了负向提示的用法，发现嵌入目标信息的Target Negative Prompts（TNP）虽然可以提升纹理真实性，但会导致形状失真。基于这一发现，提出了Target-Balanced Score Distillation（TBSD），将3D资产生成建模为多目标优化问题，并自适应调整负向提示的策略，以平衡纹理与形状之间的取舍。

Result: 大量实验表明，TBSD方法在纹理细节和形状准确度方面显著优于现有的最先进方法，同时有效避免了过度平滑和形状扭曲的问题。

Conclusion: TBSD为3D资产生成中的纹理与形状权衡提供了有效解决方案，能稳定生成纹理真实且几何形状准确的3D模型。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [9] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: 本文提出CompressNAS框架，用于在内存和精度约束下，通过全局搜索选择低秩分解参数，从而有效压缩CNN模型。


<details>
  <summary>Details</summary>
Motivation: 随着深度卷积神经网络（CNNs）逐渐变大，难以部署在微控制器和轻量级神经处理单元上，需要新的压缩方法。已有张量分解方法多为局部选择秩，忽视了全局的压缩与精度权衡。

Method: 作者提出CompressNAS框架，将分解秩的选择建模为全局搜索问题，借助快速精度评估器，能高效、全面地探索压缩方案，在给定内存和精度约束下完成模型压缩。

Result: CompressNAS在ImageNet上实现了ResNet-18 8倍压缩且精度仅损失4%；在COCO上，YOLOv5s模型实现2倍压缩几乎无精度损失，YOLOv5n模型2倍压缩精度仅下降2.5%。提出的STResNet系列模型与其它高效模型相比表现竞争力。

Conclusion: CompressNAS 能高效实现高比率CNN模型压缩，兼顾内存、精度，在多个数据集和模型上获得良好的压缩—精度平衡，并推出了新型高效模型用于边缘设备。

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [10] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种针对低空无人机网络的语义分割模型自适应方法，既能适应复杂环境，又轻量高效。方法在实际和基准测试中均显著提升了分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割基础模型在无人机实际应用中，容易受到天气、光照、视角变化等影响，导致性能显著下降。而无人机硬件资源有限，难以高效运行主流的自适应方法。此外，高算力无人机各自独立自适应，难以共享适应经验，存在资源浪费。

Method: 提出AdaptFly框架，无需梯度更新即可引导模型自适应。资源有限的无人机利用全局记忆池检索轻量级token-prompt；资源丰富的无人机采用基于协方差矩阵适应进化策略（CMA-ES）的梯度无关稀疏视觉prompt优化。通过激活统计检测器触发自适应，不同无人机间通过知识池高效共享prompt，通信开销极低，实现舰队级协同。

Result: 在UAVid和VDD基准，以及实际无人机多种恶劣天气部署中，AdaptFly显著优于静态模型和先进测试时自适应（TTA）基线方法，在分割准确率和模型鲁棒性方面均有大幅提升。

Conclusion: AdaptFly为无人机低空经济中的分布式感知、通信、高效协同提供了现实可行的新路径，提升了模型的泛化性和实际适用性，并且通信资源消耗很低。

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [11] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 本文提出了一种受生物学启发、结合人眼盲点知识的新型掩码策略，用于训练视觉表示，有助于自动学习词-指称物映射，对比实验显示其效果不亚于传统随机掩码。


<details>
  <summary>Details</summary>
Motivation: 儿童初学语言时，如何将听到的词与视觉对象关联起来仍是重要难题。现有的自监督视觉学习方法普遍使用随机掩码，缺乏生物学合理性。

Method: 作者基于一名儿童的真实长期视觉数据，采用Masked Autoencoder视觉主干网络，融入人眼盲点原理设计了新型掩码策略，模拟人脑填补视觉盲点的处理；并将预训练Encoder用于对比学习的视频-文本模型，进行词-指称物学习。

Result: 实验表明，新提出的生物学启发掩码策略在学习跨情境和时间延展的词-指称物映射任务中，表现至少与传统随机掩码相当。

Conclusion: 引入盲点掩码策略不仅更贴近人类生理特点，也有助于提升模型的生物合理性，为今后视觉表征和语言学习结合研究提供了新路径。

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [12] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: 本论文提出了GROVER框架，实现空间多组学数据（如转录组、蛋白组、表观遗传组与病理图像）的自适应整合，相较于现有方法效果更佳。


<details>
  <summary>Details</summary>
Motivation: 当前空间组学与病理图像融合分析因模态异质性高、分辨率不匹配及生物扰动等问题，难以高效建模疾病组织。为此，亟需开发能自适应挖掘和整合多模态信息的框架。

Method: 提出GROVER：基于Kolmogorov-Arnold网络的GCN对不同模态及空间结构作非线性建模，使用spot-feature-pair对比学习促进各模态特征对齐，并设计动态专家路由机制自适应筛选信息量大的模态，抑制噪声或低质输入。

Result: 在真实空间组学数据集上，GROVER在多模态整合任务中优于现有主流基线方法，表现出更强的鲁棒性和可靠性。

Conclusion: GROVER为多模态空间组学数据融合提供了一种新颖、高效且自适应的解决方案，可助力疾病组织多层次综合研究。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [13] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: 本研究提出HSI-Detect方法，通过将普通RGB图像重建为31通道高光谱图像，在高光谱域进行深度伪造检测，相较于RGB方法效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有生成式和扩散模型生成的高仿真图像能误导人眼和智能检测系统，目前大多数检测方法仅用于RGB空间，忽略了更多的光谱信息，导致检测难度增加。

Method: 提出HSI-Detect两阶段管线：第一阶段将RGB输入重建为31通道的高光谱图像，第二阶段在高光谱空间进行伪造检测，利用高密度光谱带扩大操控伪迹，使其在某些频段显现。

Result: 在FaceForensics++数据集上实验，HSI-Detect在伪造检测任务上相较于RGB基线获得了一致且显著的性能提升。

Conclusion: 利用高光谱域可显著提升深度伪造检测能力，表明光谱映射是未来Deepfake检测的重要方向。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [14] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: 本文探讨了点云神经网络在对称保持距离下的等价性，发现流行的不变网络在Procrustes Matching距离下不是bi-Lipschitz，并提出了可实现bi-Lipschitz保证的模型改进。


<details>
  <summary>Details</summary>
Motivation: 近期等变学习领域强调bi-Lipschitz模型的优势，促使作者研究点云领域神经网络是否保留对称相关的距离等价性。

Method: 作者讨论了两种对称敏感的点云距离度量（PM距离和Hard Gromov Wasserstein距离），分析两者的bi-Lipschitz等价性，并推导出当前主流点云不变网络的限制，随后提出模型修改以获得bi-Lipschitz属性，并做了初步实验。

Result: 发现PM距离和Hard Gromov Wasserstein距离本身不bi-Lipschitz等价，当前流行点云不变神经网络在PM距离下也不满足bi-Lipschitz条件，提出的改进网络在实验中表现优异于标准方法，尤其在3D点云对应任务上。

Conclusion: 标准的点云对称不变网络对某些自然的距离度量不bi-Lipschitz，改进后的网络能得到bi-Lipschitz保证，并提升对应性能。

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [15] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合多智能体与神经符号推理的视觉-语言模型系统（Concept-RuleNet），通过视觉概念与可解释规则，提升模型解释性与准确性，减少幻觉事实。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型尽管预测准确却缺乏解释性，还容易在分布外数据上生成幻觉事实。神经符号方法虽提升透明性，但其符号多源自标签，缺乏对真实视觉数据的扎根。作者希望构建既可解释又扎根于视觉事实的模型。

Method: 设计了Concept-RuleNet多智能体系统：首先由多模态概念生成器直接从训练图像中挖掘判别性视觉概念，并以此指导符号发现，减少标签引入的偏差；再由大语言模型将符号组合为可执行的一阶规则；最后在推理时，视觉验证器量化符号在图像中的表现，并结合传统黑盒神经网络输出，实现显式可解释的推理。

Result: 在两个医学影像和三个自然图像数据集上，系统相比最先进神经符号基线平均提升5%性能，同时规则中幻觉符号下降至一半。

Conclusion: 通过多智能体协作和视觉扎根的概念-规则链条，系统在解释性和准确性之间取得了良好平衡，是推进神经符号视觉-语言模型落地的重要进展。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [16] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 本文提出了一种隐式稀疏风格的Transformer新变种架构，将注意力机制应用于“重要”特征维度，实现特征选择，并在面部识别的图像生成任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对全部序列或批次实体的所有维度进行注意力处理，计算量大且效率较低。作者希望通过选取“重要”维度（主成分）进行注意力处理，减少计算瓶颈，提高效率，特别是在编码器-解码器神经网络结构中。

Method: 提出了一种Batch Transformers架构，在注意力机制中只关注挑选出来的“重要”特征维度或主成分，进行特征选择，进而显著缩小编解码器中的瓶颈规模。将该架构应用于带有化妆和遮挡的数据集上的人脸识别图像合成任务，评估其表现。

Result: 实验结果表明，该方法可以在原始数据有限的情况下提升合成图像的多样性，同时减少网络的编码解码瓶颈，提升效率。

Conclusion: 该Batch Transformers架构在需要有限数据合成多样性任务中表现良好，通过特征选择显著提高了编码器-解码器的效率，具有推广和实际应用价值。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [17] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: 提出了Image-POSER框架，通过反思性强化学习，将多个文本到图像、图像到图像专家模型协调起来，有效处理长且复杂的生成指令，显著提升了图片生成的对齐性、保真度与美学表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型尽管单次生成能力突出，但面对创意工作中常见的长、组合性描述指令时，往往难以应对，缺乏统一处理长文本生成流程的能力。

Method: 提出Image-POSER，通过反思性强化学习方式：（1）调度多样的预训练文本-图像、图像-图像专家模型；（2）动态任务分解，端到端处理长文本描述；（3）用视觉-语言模型形成结构化反馈，逐步监督对齐。将图像生成与编辑建模为马尔可夫决策过程，学习多模型协作的专家管道。

Result: 在行业标准与自定义基准测试中，Image-POSER在对齐性、保真度和美学性方面优于基线模型和前沿模型，并在人工评价中获得更高偏好。

Conclusion: 强化学习可赋能AI自主分解、重组与结合视觉模型能力，为未来通用视觉助手奠定基础。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [18] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: 本文提出了一种名为SOTFormer的新型时序Transformer方法，实现了单目标检测、跟踪和短期轨迹预测的一体化框架，兼具高精度和低内存占用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目标跟踪和短期运动预测在遮挡、尺度变化以及时间漂移等情况下依然充满挑战，很难保持时序连贯性，影响实时感知效果。作者希望解决这些问题，提升模型在困难场景下的表现，同时兼顾实时性和资源效率。

Method: SOTFormer是一个极致精简且恒定内存消耗的时序Transformer框架，将检测、跟踪、轨迹预测统一于端到端体系。创新点包括（1）用真实标签初始化的记忆模块，保证ID跟踪稳定；（2）burn-in anchor损失辅助初始化稳健；（3）仅使用单层时序注意力，在不同帧间优化特征，确保内存和计算高效。

Result: 在Mini-LaSOT (20%)基准集上，SOTFormer获得了76.3的AUC和53.7 FPS（AMP模式下，消耗4.3 GB显存），在快速运动、尺度变化、遮挡等难点下整体性能优于TrackFormer、MOTRv2等主流Transformer基线模型。

Conclusion: SOTFormer在保证高效运算和有限显存消耗的前提下，大幅提升了单目标跟踪和短程运动预测的准确性和稳定性，为实时多任务视觉感知提供了切实可行的解决方案。

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [19] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 本文提出了一种结合三维几何信息的动态图神经网络方法（MP-GFormer），用于提升机械加工过程规划中操作序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 在机械加工过程中，零件的特征和加工操作之间存在复杂的结构和几何依赖。现有的动态图学习方法虽然能建模时空依赖关系，但未能纳入零件的立体几何信息，导致操作序列预测缺乏领域感知能力。因此，迫切需要一种集成三维几何信息的新方法。

Method: 作者提出MP-GFormer方法：通过注意力机制，将每次加工后零件的三维几何表面网格（STL表示）纳入动态图学习，并结合初始的三维边界表示，实现动态场景下更精确的序列预测。

Result: 在合成数据集上评估，MP-GFormer相比现有先进方法，主操作预测提升24%，子操作预测提升36%。

Conclusion: 集成三维几何信息的动态图学习模型，可以显著提升机械加工操作序列的预测性能，为自动化工艺规划提供更有效的智能方法。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [20] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: 提出了一种防止未经授权的模型融合的新方法，可以显著削弱合并后的模型性能，而不影响原模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型数量激增，开放模型库盛行，导致模型融合变得便捷，这也使部分人可以未经授权整合多个微调模型形成新模型，侵犯知识产权并威胁模型归属权和责任。

Method: 提出MergeGuard框架，包括两步：第一步采用L2正则化优化，使任务相关信息在模型不同层均匀分布；第二步注入结构化扰动，使任务子空间错位，从而破坏损失曲线上的兼容性。这样即便进行模型合并，参数结构不再匹配，内聚失效。

Result: 在视觉和多种语言大模型上进行测试，MergeGuard可使合并后模型精度最多下降90%，而被保护模型自身性能损失小于1.5%。

Conclusion: MergeGuard有效防止未经授权的模型融合，保护模型知识产权和归属，应用前景广阔。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [21] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: 提出了一种新的基于有符号距离函数（SDF）的损失函数FocusSDF，用于改善医学图像分割中边界保持问题，并在多种任务和模型上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割为临床诊断和治疗提供基础，但现有分割模型普遍未能显式编码边界信息，导致分割边界不精确。边界的精确分割对于疾病诊断和治疗计划至关重要，因此需要一种更关注边界的分割方法。

Method: 提出了一种名为FocusSDF的损失函数，利用图像中目标边界的有符号距离函数，对靠近边界的像素自适应赋予更高权重，引导神经网络聚焦于边界区域，从而提升边界分割的准确性。

Result: 通过在脑动脉瘤、中风、肝脏及乳腺肿瘤等多种分割任务及多模态数据集上，对比五种主流分割模型及四种基于距离损失函数的方法，FocusSDF在分割精度上均表现优异，优于现有基于距离变换的损失函数。

Conclusion: FocusSDF有效关注医学图像中的目标边界区域，提升了分割精度，在多任务和模型下展现出较强的泛化性和优越性，为医学图像分割带来新的进展。

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [22] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究提出利用合成图像（SI）扩充有限的遥感真实图像数据，以提升深度学习目标检测模型（ODM）在北极麝牛监测中的检测表现，尤其适用于零样本和少样本场景。


<details>
  <summary>Details</summary>
Motivation: 北极麝牛等稀疏分布物种的种群估算依赖空中目视或GNSS追踪等传统手段，但这类手段耗时、成本高且受环境限制。深度学习ODM依赖于大量真实标注数据，对于稀有物种实际很难获得，因此需要低成本、可扩展的数据补足方案。

Method: 本文设计了对比实验，将完全基于真实图像训练的基线模型与两类融合了不同量级SI的ODM模型（零样本和少样本）进行评估，考察合成图像数量对检测性能（Precision、Recall、F1分数）的影响。

Result: 在零样本模型中，仅用合成图像训练能显著提升检测表现，且随着合成图像数量增加，各项指标提升但随后趋于平台期（>100%基线数据量时收益递减）；在少样本模型中，合成图像与真实图像混合训练可提高召回率和总精度，但提升不具统计显著性。

Conclusion: 合成图像可有效补充稀缺数据集，提升ODM在稀有野生动物监测中的初始和持续训练效果，适于无真实图像或早期实时数据不足时先启动检测流程，并可随真实数据积累不断优化模型。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [23] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 论文提出了一种名为Harpia的CUDA加速处理库，集成于Annotat3D中，能够高效处理超大三维体数据，实现更快的分割和交互式探索，适用于HPC和远程访问场景。


<details>
  <summary>Details</summary>
Motivation: 高分辨率体数据（如X射线断层成像、先进显微成像）越来越大，已有工具无法高效处理、分割和交互，尤其是面临HPC和远程协作时的扩展性和资源受限问题。

Method: 开发了Harpia库，支持原生分块执行、严格内存控制、GPU加速的过滤、标注与定量分析，实现超过单GPU内存容量的数据处理，并在Annotat3D平台中实现交互式、增强型分割流程。

Result: 实验结果表明，相较于广泛应用的NVIDIA cuCIM和scikit-image等现有框架，Harpia在处理速度、内存效率和扩展性方面有显著提升。

Conclusion: Harpia结合交互式界面和高效GPU资源管理，非常适用于共享HPC基础设施下的科学影像协作分割与分析，被证实是处理大规模三维数据的有效工具。

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [24] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文通过自动化提示优化，显著提升了视觉语言基础模型（VLM）在医学影像任务中的表现，减少了对人工提示工程和大规模领域数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多种图像任务中表现优异，但在医疗领域尤其是医学基准任务中表现不佳。现有改进手段如微调或人工提示工程，难以拓展且对医疗机构使用存在较高门槛，因此亟需无需手动提示优化而又能高效提升模型能力的新方法。

Method: 作者将Declarative Self-improving Python（DSPy）框架应用于医学视觉语言系统，实现结构化的自动化提示优化。在放射、胃肠、皮肤等五类医学影像任务上，评测了10款开源VLM和4种提示优化技术，并建立了系统的评测流程。

Result: 优化后的流程在五项医学影像任务中，模型中位相对性能提升53%，部分任务零样本基线下达300%–3400%的惊人提升，表明自动化提示优化对医疗AI系统有巨大促益。

Conclusion: 自动化提示优化显著提升了医学VLM在临床影像解释任务中的准确性，减轻了提示工程负担，有助于医疗机构部署与扩展，提高临床效率，且方案具备扩展性和数据隐私安全性。评测流程已开源助力医学领域可复现研究。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [25] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: 本文提出了一种针对医疗影像及多模态临床场景中缺失模态问题的新型样本自适应补全框架PI-NAIM，显著提升补全精度及下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 多模态临床诊断中常常遇到部分模态或数据缺失的问题，现有补全方法要么表现力有限，要么计算成本高，难以兼顾效率与精度。因此，亟需一种兼顾高效与高精度的解决方案。

Method: 提出PI-NAIM双路径结构，根据样本缺失复杂度智能分流：低缺失则用高效统计补全（MICE），高缺失或复杂模式由神经网络GAIN（带时序分析）处理；设计路径间交互注意力机制自动融合结果，并采用端到端联合优化提升补全和下游任务表现。

Result: 在MIMIC-III及多模态基准数据上，PI-NAIM补全RMSE达0.108（相比其他方法0.119-0.152），下游任务（死亡预测）AUROC也提升到0.812，均达到当前最优水平。

Conclusion: PI-NAIM具备模块化和高扩展特性，可无缝集成进多模态医学视觉流程，适用于实际传感不全、数据缺失或损坏等复杂场景，提供一体化补全过程。

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [26] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的视觉token选择模块QTSplus，显著提升多模态大模型对长视频的理解能力，并减少了计算和延迟。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频理解特别是长视频理解方面仍然面临挑战，主要由于视频长度增加导致视觉token数量线性增长，极大消耗计算与存储资源。

Method: QTSplus模块基于输入文本查询，动态对视频token进行筛选，具体过程包括：（1）通过跨注意力机制为视觉token打分；（2）预测与查询复杂度相关的实例化保留budget；（3）用可微ST估计器与推理时硬门控选择Top-n token；并通过小型重编码器利用绝对时间信息保持时间顺序，实现秒级定位。

Result: 集成至Qwen2.5-VL后，QTSplus可将视频流压缩多达89%，整体推理延迟减少28%。在8个长视频理解基准上取得与原Qwen模型近似的准确率，并在TempCompass方向和顺序准确率上分别提升20.5和5.6个百分点。

Conclusion: QTSplus是高效且通用的视觉token选择机制，能帮助MLLMs扩展到实际长视频场景，并保持与任务相关的证据。

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [27] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 本文首次将事件相机应用于图像去雾，通过引入事件引导扩散模型实现对有雾场景的高动态范围去雾重建，取得了领先的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法多基于RGB图像，受限于动态范围，导致去雾仍是病态问题，存在结构和光照细节丢失。需要更高动态范围的信息辅助去雾任务。

Method: 提出一种事件引导的扩散模型，将事件相机提供的高动态范围、稀疏的结构特征（如边缘、角点）映射到扩散模型的隐空间，实现HDR信息向RGB帧的有效迁移，从而提升结构精度和视觉效果。

Result: 在两个基准数据集以及自采集的极端雾霾无人机数据集（AQI=341）上，所提出方法均取得当前最优效果。

Conclusion: 事件相机的高动态范围信息能够有效辅助去雾任务，结合扩散模型可显著提升有雾场景下的成像质量，具有实际应用潜力。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [28] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 本文对三种基于U-Net的岩画语义分割架构进行了对比分析，证明引入注意力机制能显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提升对巴西考古遗址岩画图像的自动化语义分割效果，从而促进文物数字化保护。

Method: 采用BEGL-UNet及其两种增强变体（Attention-Residual BEGL-UNet和Spatial Channel Attention BEGL-UNet），并引入注意力机制及边界增强损失函数，在实际考古图像数据集上用5折交叉验证进行实验对比。

Result: Attention-Residual BEGL-UNet获得最优性能（Dice Score 0.710, recall 0.854），其余增强模型也优于基线，Dice提升2.5-2.9%。

Conclusion: 注意力机制结合边界增强损失能有效提升岩画分割，在考古遗产数字保护中具有广阔应用前景。

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [29] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本论文研究临床中肾小球精细分型在数据极度稀缺情况下的计算病理学实现，比较了多种视觉-语言模型的适应性和表现，并提出在样本极少的情景下使用领域专用的视觉-语言模型且采用基础微调最为有效。


<details>
  <summary>Details</summary>
Motivation: 肾小球分型对于肾脏活检诊断至关重要，但有价值的分型标签稀缺且获取成本高。现有方法多关注粗粒度全监督分类，缺乏对如何用视觉-语言模型（VLM）解决有限标注下临床有意义的细分问题的探索。

Method: 将肾小球精细分型建模为临床现实中的few-shot问题，系统评估了病理专用与通用视觉-语言模型，比较不同的模型架构、先验知识、适应方式对分类表现和表示结构的影响，分析了图像-文本嵌入对齐和类别可分性等表现。

Result: 领域专用VLM配合基础微调能在极少标注样本（每类4-8例）下有效区分不同亚型，较通用模型表现优越。模型随标注量提升持续改善。发现正负样本判别能力和图像-文本对齐同等重要。

Conclusion: 监督方式和模型适应策略共同决定诊断精度与多模态结构特性。为临床实际中模型选型、训练策略和标注投入提供了参考。

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [30] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的身份保持个性化生成（IPPG）方法，克服现有方法过度聚焦人脸特写的问题，实现身份和场景语义的协同优化。所提方法有效提升了生成内容的视觉叙事性和语义一致性，可以无缝集成到现有IPPG框架中。


<details>
  <summary>Details</summary>
Motivation: 现有IPPG方法侧重面部特征，导致生成的图像多为面部特写，视觉叙事性弱、复杂文本语义一致性不足，主要因为身份特征嵌入削弱了生成模型的语义表现力。作者希望解决这种限制，实现身份和场景的更好融合。

Method: 作者设计了“双线推理”（Dual-Line Inference, DLI）架构，实现身份特征与语义特征的分离；引入身份自适应融合（IdAF）策略，将ID与语义的融合延后到噪声预测阶段，并采用自适应注意力融合和噪声决策掩码，避免手动掩码；还提出身份聚合预插（IdAP）模块，用于在初始阶段整合ID信息，提升身份保持能力。

Result: 实验显示，该方法在避免面部特写的前提下，依然可稳定高效地完成身份保持和场景生成，无需手动掩码或额外微调。该方法具有良好的通用性，可作为插件快速应用于现有IPPG系统。

Conclusion: 本研究突破了身份个性化生成领域对于面部特写的依赖，实现了更具叙事性和语义一致性的高质量身份-场景生成，为影视制作和相关领域提供了更丰富的个性化生成手段。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [31] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 本文提出了一种高效的动态参数优化方法（DPO），显著提升了基于变换的对抗攻击在不同模型、迭代次数和任务上的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实际应用中容易受到对抗性攻击，尤其是基于变换的迁移攻击已表现出较强能力。然而，现有方法在参数优化上存在盲区，如仅考虑低迭代次数、各模型参数一致以及传统网格搜索计算量大，导致无法发挥最大潜能。因此，亟需更有效的参数优化方法以提高迁移攻击效果。

Method: 作者首先通过实证研究分析不同变换参数对攻击迁移性的影响，发现了三种动态变化模式，并提出了同心衰减模型（CDM）来解释这些现象。在此基础上，借助“先升后降”模式，提出了动态参数优化（DPO）方法，将参数优化的复杂度从O(m^n)降至O(nlogm)。

Result: 实验表明，DPO方法在不同的模型、迭代次数和任务上均能显著提高现有变换攻击方法的迁移效果，且优化效率大幅提升。

Conclusion: 该工作通过理论模型与实证验证，为基于变换的对抗攻击参数优化提供了新思路，提升了其在实际应用中的可用性和效率。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [32] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为LithoSeg的两阶段方法，针对光刻SEM图像的精准分割和测量问题，在保证高准确度和精度的同时降低了人工标注需求，对半导体制造具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 目前用于光刻SEM图像分割的方法在精度和鲁棒性上存在不足，无法满足实际半导体制造过程中对各种图形和工艺窗口的高一致性需求，因此亟需更高效、更准确的分割方案。

Method: 提出了一种粗到精的网络LithoSeg。第一阶段结合了Human-in-the-Loop Bootstrapping与Segment Anything Model(SAM)，以达到较强鲁棒性和减少人工标注。第二阶段将2D分割转化为1D回归问题，利用粗分割掩模进行法向采样，并通过轻量级MLP点对点精细化处理。

Result: LithoSeg在分割准确率和计量精度上均超越了传统方法，同时对人工监督的需求更低，实验结果显示其具有优异性能。

Conclusion: LithoSeg在提高分割精度与计量精度的同时，也兼顾了低人工标注成本，显示出其在半导体制造等实际场景中的广阔应用前景。

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [33] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的显微镜图像领域自适应方法，主要通过只调整网络前几层来提升模型在不同条件下的泛化能力，并实现了自动适配深度选择，在多种场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在显微镜图像分析中表现突出，但面对不同仪器或采集参数时，模型性能常因分布变化显著下降。现有方法通常需对整个网络进行再训练，破坏已学习到的语义信息，急需新的适应策略提升模型迁移能力。

Method: 作者提出了SIT-ADDA-Auto方法，仅对网络前几层进行对抗式领域自适应，并自动选择适配深度，无需目标域标签，将浅层特征对齐与不确定性预测相结合。

Result: 在多种曝光、照明和仪器、染色条件下，该方法在重建与分割任务上均优于全编码器自适应和非对抗性基线，并能保持更好的语义特征。通过多指标评估、专家盲测和消融实验验证了方法的鲁棒性。

Conclusion: 仅对早期网络层进行领域适应是一条实用且有效的设计准则，SIT-ADDA-Auto为无标签显微镜领域适应提供了新思路和实用方案，且公开了代码供社区使用。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [34] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多摄像头视觉框架，实现了交通信号交叉口实时安全评估，通过计算车辆间的穿越后置时间（PET）来识别高风险区域，具备高精度、实时和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交叉口安全分析因数据稀疏和延迟而受限，亟需一种更高效、实时的方法以减少事故发生，尤其是在交叉口等事故多发地带。

Method: 作者采用了四个同步摄像头，利用YOLOv11进行车辆检测，将检测到的车辆多边形通过单应性矩阵映射到统一的鸟瞰图，再提出基于像素级的PET算法进行危险区域精细可视化，并在边缘设备（NVIDIA Jetson AGX Xavier）上实现端到端实时处理和后续数据库存储。

Result: 系统可在子秒级精度和实时吞吐下，准确识别交叉口高风险区，产生分辨率为800x800像素的对数热力图，平均帧速率达到2.68 FPS，空间精度达3.3平方厘米。

Conclusion: 研究验证了分布式视觉系统进行基于PET的交叉口安全分析的可行性，并为智能交通系统提供了可复制的方法，实现更高分辨率、实时性和可扩展的安全评估。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [35] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 本论文提出了WGREC新任务和LIHE新框架，有效解决了以往弱监督指代理解无法处理复杂多目标情况的问题，在多个基准上取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代理解方法局限于“一对一”映射，现实中常出现零/多目标表达，这对提升模型泛化和实用性带来挑战。

Method: 提出Weakly-Supervised Generalized Referring Expression Comprehension（WGREC）任务，针对弱标注带来的监督信号歧义和语义崩塌，设计了两阶段LIHE框架：第一步语言实例分解，预测目标数并将复杂表达拆分为简单子表达；第二步采用HEMix混合相似度模块结合欧氏与双曲几何进行分层及细粒度语义对齐。

Result: LIHE在gRefCOCO及Ref-ZOM建立首个有效WGREC基线，并在常规REC基准上，HEMix模块将IoU@0.5指标提升最多2.5%。

Conclusion: LIHE显著推动了弱监督指代理解向更实际、泛化能力更强的方向发展，并为复杂语义结构场景提供了可行的技术路线。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [36] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种无需配对监督、速度快、效果优异的无镜头成像反演方法NSDD，在保证测量一致性的前提下，实现了高质量的照片级重建。


<details>
  <summary>Details</summary>
Motivation: 现有镜头-无镜头配对监督方法由于域不匹配，导致模型性能有限。无监督扩散先验虽有吸引力，但在高噪声、强耦合的无镜头反演问题中效果不佳，因此需要新的无监督高质量重建方法。

Method: 本文观察到，将测量空间（range-space）约束与空域扩散先验(null-space diffusion-prior)分离的方法稳定且生成真实，进而提出了NSDD。该方法使用单次推理，将迭代DDNM+的null-space部分蒸馏为学生网络，在无镜头测量和测量空间锚点的条件下进行重建，无需配对监督。

Result: 在Lensless-FFHQ和PhlatCam数据集上，NSDD重建速度仅次于Wiener方法，感知质量（LPIPS）接近教师网络（DDNM+），效果明显优于DPS和传统凸优化方法。

Conclusion: NSDD为无需真值、计算高效的无镜头照片级成像提供了切实可行的路径，兼顾速度、感知质量和测量一致性，推动了无镜头成像实用化。

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [37] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: 本文提出了VL-SurgPT，这是首个将手术场景视觉追踪与点状态文本描述相结合的大规模多模态数据集，并通过引入语义信息显著提升了手术点追踪的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术追踪数据集仅提供坐标信息，缺乏描述追踪失败原因的语义上下文，导致在烟雾、反光、组织变形等复杂环境下点追踪效果不理想。

Method: 1) 构建VL-SurgPT多模态数据集，包含908段手术视频及详尽的点和文本描述标注；2) 针对多种复杂场景和器械类型进行追踪注释；3) 建立八种主流追踪方法的基准，并提出TG-SurgPT，利用文本指导的语义信息对追踪进行增强。

Result: 实验证明，融合点状态的文本信息后，追踪方法在烟雾、反光等困难场景下的准确率和可靠性显著优于传统纯视觉方法。

Conclusion: 该研究通过视觉与语言信息融合，推动了语义感知的手术点追踪技术发展，为复杂手术环境下的计算机辅助手术奠定了基础，有助于提升临床实际中的追踪系统鲁棒性。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [38] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: 本文提出了GCAgent，一种全新具备全球上下文感知能力的智能体框架，通过结构化记忆机制显著提升了跨多个模态长视频理解的表现，并在主流基准上取得了领先。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在长视频理解方面受制于token长度和难以捕捉长期时序依赖问题，现有方法未能有效建模全局语境和事件复杂关系，导致深层次视频推理能力有限。

Method: 提出GCAgent框架，核心创新是结构化的“图示与叙事情节记忆”，通过分阶段的感知-行动-反思循环，以及记忆管理器有针对性地回溯场景，实现对事件及其因果与时序关系的建模，从而突破长时依赖瓶颈。

Result: 大量实验显示，GCAgent在Video-MME Long数据集上相较于强baseline提升了最高23.5%的准确率，并在7B规模模型中表现最佳，在Long split和整体平均分（71.9%）均取得最高成绩。

Conclusion: GCAgent以结构化记忆和智能体推理机制，为具认知启发的长视频理解建立了新范式，有效缓解大模型token和长期依赖难题，在多模态视频推理领域迈出重要一步。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [39] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉和物理信息的手-物体三维姿态估计算法，有效提升了估计的准确性与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 传统的单RGB图像手-物体三维姿态估计方法主要依赖视觉特征，常常会导致姿态结果违反物理约束（如部分穿插或无法接触）。现有尝试引入物理信息的方法多数采用后处理或不可微的物理引擎，难以端到端训练且与视觉特征不自然结合。为了解决这些问题，本文提出了一体化的视觉-物理学习框架。

Method: 本文提出了联合视觉-物理线索学习的框架。首先，模型同时学习2D视觉特征和3D物理特征，从而更全面地表示手-物体交互。其次，引入了候选姿态聚合模块，将多个基于扩散模型生成的候选姿态，通过结合视觉与物理预测结果进行聚合，得到最终既符合视觉一致性又具备物理合理性的姿态。

Result: 实验表明，所提出的方法在姿态准确率和物理合理性方面都较现有最先进方法有显著提升。

Conclusion: 联合视觉与物理信息学习与聚合，有效提高了手-物体三维姿态估计的准确性和合理性，对增强现实和人机交互等应用具有重要意义。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [40] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: 本文提出了一种知识增强的Masked Image Generation（KA-MIG）框架，通过引入与视觉token相关的知识图谱（先验），显著提升了图像生成的语义质量。


<details>
  <summary>Details</summary>
Motivation: 现有MIG方法只依赖模型自身从数据中直接学习token间的语义依赖，但由于token缺乏明确语义且序列较长，造成学习困难，影响生成效果。作者希望通过引入显式的知识先验弥补这种不足。

Method: 作者提出KA-MIG框架，首先从训练数据中构建三类token级别知识图谱：共现图、语义相似性图和位置-token不兼容图。利用这三种图谱，通过设计图感知编码器学习融合token与位置的嵌入表示，接着采用轻量级融合机制将其与现有MIG方法结合。

Result: 实验证明KA-MIG可显著提升现有MIG在ImageNet等数据集上的类条件图像生成质量。

Conclusion: 通过引入token层面的知识图谱作为先验显式建模token间的语义关系，有效提升了MIG方法的生成能力，框架适用于现有MIG的改进。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [41] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 该论文提出了CalMRL方法，有效解决多模态学习中因缺失模态造成的对齐偏移问题，显著提升了在不完整数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态表示学习要求所有模态齐全，现实中常常存在模态缺失，导致无法充分利用类似这样的不完整数据集。作者针对因此引发的“anchor shift”理论问题进行分析，试图提出有效解决缺失模态下的对齐问题。

Method: 作者提出了CalMRL方法，通过建模各模态间的先验和联系，在表示层面对缺失模态进行填充，并引入双步学习法，推导出共享潜变量的后验分布的闭式解，实现了对不完整对齐的校准。

Result: 实验证明，CalMRL在缓解anchor shift、优化收敛性方面有理论和实验支持，能使现有先进方法处理本来无法吸收的缺失模态数据。

Conclusion: CalMRL改善了多模态对齐的灵活性和性能，能有效支持处理缺失模态情形，拓展了强多模态模型的应用范围。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [42] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat是一种用于从少量低分辨率图像重建高分辨率3D场景的新方法，通过结合场景特定的高质量参考图像和低分辨率输入，提高了三维重建的精细纹理还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有从稀疏、低分辨率图像进行三维重建的方法，难以恢复场景的精致纹理细节，主要原因是低分辨率输入缺乏高频信息，而这对于实际如自动驾驶和智能体应用尤为关键。

Method: SRSplat方法首先通过多模态大模型和扩散模型为每个场景生成高质量参考图像库，然后提出参考引导特征增强（RGFE）模块，将低分辨率输入与参考图像的特征对齐融合；融合后通过解码器预测高斯元，并用纹理感知密度控制（TADC）模块根据输入纹理丰富度自适应优化高斯元密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上，SRSplat均优于现有方法，并展现出良好的跨数据集、跨分辨率泛化能力。

Conclusion: SRSplat通过结合参考图像信息和输入自身纹理特征，在稀疏低分辨率输入场景下实现了高分辨率三维重建，为实际应用提供了更优解决方案。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [43] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedSDA的联邦学习方法，通过对各客户端的数据分布，尤其是组织病理图像的染色特征分布进行对齐，以缓解非独立同分布（non-IID）带来的联邦学习性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能够保护数据隐私进行分布式训练，但在面对各客户端数据分布不一致（non-IID），尤其是在组织病理图像中存在特征分布偏移时，现有方法效果有限，亟需新的解决思路。

Method: 利用扩散模型对染色分离后的特征进行分布对齐，提出FedSDA方法，在联邦学习框架下将每个客户端的染色分布对齐到统一目标分布。为规避扩散模型在原始数据上的隐私泄露风险，设计了相关隐私保护机制。

Result: 大量实验表明，FedSDA不仅能提升现有关注模型更新差异的基线性能，还优于以数据分布为视角处理non-IID问题的其他基线方法。

Conclusion: FedSDA为处理组织病理领域的非IID分布问题提供了有效且实际的方法，对计算病理社区具有重要应用价值。

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [44] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: 提出了DCMM-Transformer，在医学影像分析中通过可微分的方式引入解剖结构社群信息，提升了模型性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前主流ViT方法未能充分利用医学图像中存在的潜在解剖分组结构，已有尝试如SBM-Transformer引入结构信息但难以训练，且难以建模复杂结构。

Method: DCMM-Transformer通过在自注意力机制中加入Degree-Corrected Mixed-Membership (DCMM) 模型形成加性偏置。不同于依赖二元Mask的非可微方法，此法可微、可解释，能够在建模时引入社群结构和度异质性。

Result: 在包括脑部、胸部、乳腺和眼部等多种医学图像数据集上进行实验，显示该方法优于现有方法，具备更好泛化性。

Conclusion: DCMM-Transformer有效提升了医学影像分析的效果，并通过结构化关注提升了解释性，注意力分布与解剖结构高度对应。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [45] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于DeiT的深度伪造检测方法（DeiTFake），结合创新的两阶段训练策略，在权威数据集上取得了优异的检测效果。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对数字媒体的真实性构成了严峻威胁，现有检测方法易受伪造手段干扰。因此，亟需更鲁棒有效的检测模型。

Method: 提出DeiT（Distilled Vision Transformer）基础的检测模型，并设计了两阶段训练流程。第一阶段用常规增强进行迁移学习，第二阶段用更复杂的仿射与针对深度伪造的增强精细调整模型，增强对伪造特征的捕捉能力。

Result: 在OpenForensics大数据集上，第一阶段检测准确率达98.71%，经过二阶段训练后准确率提升至99.22%，AUROC达0.9997，均优于现有基线方法。

Conclusion: 所提方法在深度伪造检测中实现了性能突破，同时分析了增强与训练设置影响，为实际人脸伪造检测提供了有效参考。

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [46] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的无监督跨视角地理定位框架UniABG，通过对抗性视图桥接和图结构校准，提高跨视角图像匹配的精度，性能超越现有无监督甚至部分有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前跨视角地理定位在无监督场景下依赖伪标签的方法精度有限，主要由于视角差异导致标注噪声高，迫切需要提升无监督学习下的跨视角配准能力。

Method: 提出UniABG框架，分为两阶段：第一阶段采用视图感知的对抗性桥接（VAAB）捕获视图无关特征并增强伪标签可靠性；第二阶段采用异构图滤波校准（HGFC），通过构建双视图间结构图，细化图像对应关系。

Result: 在University-1652和SUES-200数据集上，UniABG在Satellite→Drone任务中AP分别提升10.63%和16.73%，并超过了一些有监督的基线方法。

Conclusion: UniABG有效提高了无监督跨视角地理定位性能，证明了对抗性特征桥接和图结构校准的有效性，为减少标注依赖提供了新思路。

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [47] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: 本文提出了PipeDiT管道化框架，加速基于DiT的视频生成，显著提升推理速度并降低显存消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管DiT等扩散模型在视频生成中表现优异，但其推理速度慢、内存占用高，难以实际部署，因此亟需高效的视频生成加速方案。

Method: 1）提出PipeSP算法，实现序列并行的管道化，减少推理延迟；2）将扩散模块和变分自编码器（VAE）模块按GPU分组，解耦并管道化执行，降低显存占用和延迟；3）在VAE组提出Attention协同处理方法，进一步提升并行度。上述方法集成到OpenSoraPlan和HunyuanVideo两个开源框架中。

Result: 在两个8卡GPU系统上的实验表明，PipeDiT在多种分辨率和时间步配置下，相比于原生框架可实现1.06到4.02倍的速度提升。

Conclusion: PipeDiT通过管道化和资源优化，极大提升了基于DiT架构的视频生成推理效率，适合大规模和高效的视频生成实际应用。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [48] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: 该论文提出MovSemCL方法，通过运动语义对比学习框架提升轨迹相似度计算的准确性与效率，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在轨迹语义和层级建模、计算效率以及数据增强合理性方面存在不足，亟需更好方法提升轨迹相似度计算。

Method: 提出MovSemCL框架，将原始GPS轨迹转换为运动语义特征，再分割为片段。通过片内和片间注意力机制编码轨迹的局部与全局模式，实现高效层级表示。设计基于曲率的增强策略，保留有用片段、屏蔽冗余片段，生成物理合理的增强视图。

Result: 在真实数据集上的实验显示，MovSemCL在相似度检索任务中，平均排序接近理想值1，在启发式近似性能上提升最多20.3%，推理延迟降低43.4%。

Conclusion: MovSemCL兼具高效性和准确性，能有效提升轨迹相似度计算，在实际任务中优于现有方法。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [49] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的紫边去除方法DCA-LUT，通过构建CA-CT模块和5D LUT，有效消除了镜头色差引起的紫边现象，实验结果表现领先。


<details>
  <summary>Details</summary>
Motivation: 紫边是由于镜头轴向色差导致的常见数字图像伪影，传统处理方法依赖昂贵硬件或手工特征，难以普及。基于学习的数据驱动方法尚未有系统解决方案。

Method: DCA-LUT采用深度学习，核心为CA-CT模块，能自适应学习颜色空间，将紫边特征分离为单独维度，进而学习‘紫边通道’，辅助恢复亮度。最后通过5D LUT进行高效非线性色彩校正。构建了大规模合成紫边数据集PF-Synth用于训练和评估。

Result: 在合成和真实紫边数据集上，DCA-LUT显著优于现有方法，实现了最优的紫边去除效果。

Conclusion: 本文提出的DCA-LUT为紫边去除提供了全新、有效的解决方案，具备广泛实际应用潜力。

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [50] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉锚定音视情感大模型（VAEmotionLLM），能够在有限音频预训练下实现多模态情感理解，在全新艺术情感基准集上取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在情感理解方面仍有局限性，尤其在艺术作品中，情感常通过视觉和听觉多模态共同呈现。现有方法要么聚焦于人本或单一模态，要么依赖大规模音频预训练，导致扩展性受限，并未充分捕捉艺术作品所表达的复杂情感。

Method: 提出两阶段框架：第一阶段用视觉引导音频对齐（VG-Align），通过视觉路径对预训练音频路径的知识蒸馏，无需大规模音频数据，实现“看”学“听”；第二阶段引入跨模态情感适配器（EmoAdapter），由情感增强模块和情感监督模块组成，提升跨模态情感表征能力。同时构建了艺术情感基准集（ArtEmoBenchmark）用于评测。

Result: VAEmotionLLM在ArtEmoBenchmark上达到了当前最优水平，显著优于单模态和现有多模态基线。消融实验表明各组件互补、均有积极作用。

Conclusion: 该方法为大模型多模态情感理解提供了新思路，减少了对大规模音频预训练的依赖，对艺术领域的情感感知任务尤为有效。

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [51] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态提示的量化框架用于点云分析，通过引入文本嵌入与多模态提醒，实现更具表现力和可解释性的矢量量化，显著提升了点云任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的矢量量化方式（如可训练向量或聚类中心）在表示能力和可解释性上存在不足，尤其是在多模态（视觉-语言）任务中，对齐效果受到限制。因此亟需设计更健壮有效的码本，提高矢量离散编码的表示性能。

Method: 1）利用预训练文本嵌入作为强鲁棒的原型先验，实现视觉语义对齐；2）通过多模态提示使原型自适应细化，缓解视觉与语言语义不一致问题；3）引入紧致性与分离性正则，创建受双重约束的量化空间，促进几何与语义信息的融合；4）采用Gumbel-Softmax，实现可微分的离散化以及稀疏量化。

Result: 该方法在点云分析的ModelNet40和ScanObjectNN数据集上，实验结果显示在准确率等关键指标上优于现有方法，验证了方法的有效性和优越性。

Conclusion: 多模态提示驱动的量化框架能联合编码几何和语义信息，具备更佳代表性、更强多模态对齐能力及更高解释性，为点云等多模态任务带来性能突破。

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [52] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种结合概率推理的多标签图像分类新方法，在COCO-2014数据集上的表现优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类在计算机视觉应用中需求旺盛，但标签之间的依赖性和不确定性增加了分类难度，传统方法在这方面表现有限。

Method: 采用改进的ResNet-101网络架构，融合概率推理以模拟标签之间的依赖关系和不确定性，对多标签图像进行分类。

Result: 在COCO-2014数据集上，所提出模型的mAP达到0.794，超过ResNet-SRN（0.771）与Vision Transformer（0.785）等基线方法，精确召回表现突出。

Conclusion: 将概率推理整合到深度学习模型中，能够有效提升多标签图像分类的性能，解决标签相关性与不确定性带来的挑战，具有实际推广意义。

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [53] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 本文提出了一个统一的端到端框架，将目标检测与位姿估计无缝集成，无论是否有传统3D CAD模型都能适用。核心方法结合Transformer和NOCS，能高效地在BOP等基准上达到优良效果。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测和位姿估计往往分离，且对高质量3D模型依赖强。缺乏一个能统一适应不同场景（有CAD模型或仅有多视角图像）的通用方法。

Method: 设计了一个端到端流水线，第一步通过上游模块从CAD模型或用NeRF从多视角图片重建物体表示。检测阶段用CNOS检测器定位目标，然后用新提出的OPFormer模块进行6D位姿估计。OPFormer基于Transformer结构，融合基础大模型进行特征提取，通过联合编码多模板视角并引入NOCS空间几何先验，实现鲁棒的2D-3D特征匹配。

Result: 在BOP等挑战性数据集上评测，系统在准确率与效率之间取得良好平衡，既适合基于模型也适合无模型场景。

Conclusion: 提出的方法能够在不同应用场景下统一处理目标检测与位姿估计任务，展现出很好的实用性，有望推广到实际工业等多领域。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [54] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的图像拼接新方法SemanticStitch，可以更好地保持前景物体的完整性，并提升拼接后图像的视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的拼接方法忽略了前景语义信息，往往导致物体断裂、视觉连贯性差，无法适应复杂实际场景的需求。

Method: 提出SemanticStitch框架，通过引入前景物体的语义先验和创新的损失函数，加强对显著物体语义完整性的约束，并引入两个真实场景数据集进行评估。

Result: 实验结果表明，所提方法在保证拼接质量方面显著优于传统方法，在多种实际应用场景下有良好表现。

Conclusion: SemanticStitch有效提升了图像拼接的效果，尤其是在保持前景物体连续性和整体视觉一致性方面，具有较强的实用价值。

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [55] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种零样本手-物体交互时序定位方法EgoLoc，无需对象掩码与类别注释，在头戴视角视频中自动精准识别手与物体接触/分离的关键时刻，并在多个应用与数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟/增强现实及人机协作应用中，对手部与物体交互的精细时序感知（如接触与分离时刻）需求迫切，直接影响沉浸式体验与机器人控制精度；但现有方法仅泛泛处理动作过程，精准时刻识别尚未得到有效关注和解决。

Method: 本文提出EgoLoc方法，通过手部动态引导的采样方式生成高质量视觉提示，引入视觉-语言模型辨别接触/分离属性，无需依赖对象掩码与动作类别标注；系统通过闭环反馈不断优化，最终可直接定位关键交互时刻，实现零样本适应。

Result: EgoLoc在公共数据集和自建基准上实现了合理准确的时序交互定位，效果优于传统掩码和类别辨识依赖方法；此外，该技术在多个下游任务（如机器人操作、头戴视角分析）中验证了其实用性。

Conclusion: EgoLoc方法拓宽了时序交互识别的研究边界，实现了高泛化性的前景式时刻定位，对于增强现有的VR/AR及机器人领域应用具有积极推动作用。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [56] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种分层分组提示调优方法，在持续学习中提升模型对新任务的适应能力，同时缓解灾难性遗忘问题。新方法在多个基准上优于最新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的持续学习方法虽然能高效适应新任务，并减少忘记，但每层独立的提示可能导致一些层出现过度调整，增加遗忘风险。为此，需要一种既能灵活调整，又能保持模型稳定性的调优方法。

Method: 作者提出分层分组提示调优方法。具体包括：(1) 将同一组内的层共享相似提示，并通过位置编码调整，保持组内特征关系；(2) 利用单一的任务特定根提示生成各组子提示，提高组间协同，减少独立性。

Result: 在四个基准数据集上进行的大量实验显示，该方法在性能上优于多种最新的持续学习方法。

Conclusion: 分层分组的提示调优方法通过结构性共享和层内协同，有效提升了持续学习中模型的稳定性和表现，缓解了灾难性遗忘问题。

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [57] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffPixelFormer的新型Transformer架构，用于RGB-D室内场景分割，显著提升了分割精度，并在多个公开数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D融合方法依赖于计算量大的跨注意力机制，并且在建模模态内外特征关系时不足，导致特征对齐不精确和判别力有限。因此，寻求更有效的融合和建模方式以改进室内场景分割。

Method: 提出DiffPixelFormer架构，核心为IIMIB模块，结合自注意力机制捕获模态内长距离依赖，并通过DSIM模块将模态专属与共享特征解耦，实现像素级、细粒度的跨模态对齐。同时，采用动态融合策略平衡各模态贡献，充分利用RGB与深度信息。

Result: 在SUN RGB-D和NYUDv2两个基准测试集上，DiffPixelFormer-L的mIoU分别达到54.28%和59.95%，相比于DFormer-L提升了1.78%和2.75%。

Conclusion: DiffPixelFormer有效提升了RGB-D室内语义分割的性能，在现有基准任务上取得了优化结果，验证了其在跨模态特征对齐和表征能力方面的优势。

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [58] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出PACE框架，首次将数据集蒸馏方法应用于SNN和事件视觉任务，极大减少了SNN的训练成本，同时保障性能。


<details>
  <summary>Details</summary>
Motivation: 受事件相机和SNN能效高的触发，SNN实际应用受限于昂贵的训练代价。该工作致力于通过数据集蒸馏降低训练数据量和算力需求，提升SNN的实用性。

Method: 提出PACE框架，核心包括ST-DSM和PEQ-N两个模块：ST-DSM通过残差膜电位实现稀疏脉冲特征的致密化和精细时空匹配，PEQ-N为标准事件流管线提供可插拔的概率整数量化器。通过将原始训练集压缩成合成小数据集，实现快速训练。

Result: 在DVS-Gesture、CIFAR10-DVS、N-MNIST等数据集上，PACE优于现有的核心集选择和数据集蒸馏方法。其中N-MNIST任务上，PACE仅用6000分之一的数据与一半以下的训练时长达到接近完整数据集的84.4%准确率。

Conclusion: PACE大幅提高SNN训练效率，使分钟级训练成为可能，为事件相机和SNN在边缘设备上的落地提供了有效方案。

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [59] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了PhysX-Anything框架，可以从一张普通图片生成可用于物理仿真的高质量三维模型，极大提升了3D生成在智能体和仿真任务中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法主要关注视觉效果，忽略了物理属性和关节结构，导致其难以直接应用于需要物理交互的场景（如智能体训练等）。作者希望填补这一空白，实现更实用的物理三维资产生成。

Method: 提出首个基于视觉语言模型（VLM）的物理3D生成模型，设计了高效的3D几何表示，将token数量降低了193倍，使模型能在标准token预算下学习显式几何结构。同时，构建了包含丰富物理属性和类别更广的新3D数据集PhysX-Mobility。

Result: 实验证明，PhysX-Anything在新构建的数据集和自然图片上表现出强大的生成能力和泛化能力。仿真实验显示生成的资产可被直接用于机器人控制等接触丰富的物理任务。

Conclusion: PhysX-Anything显著提高了3D模型在物理仿真和智能体任务中的可用性，可广泛推动下游人工智能和仿真应用的发展。

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [60] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本论文提出SpikeNM，第一个面向SNN的半结构化N:M剪枝框架，实现无损精度的高效稀疏化，兼具硬件友好和灵活性。


<details>
  <summary>Details</summary>
Motivation: SNN因能效高而受关注，但深层网络参数和计算复杂，难以在边缘设备应用。现有的权重剪枝方法要么稀疏度高但难以硬件加速（非结构化），要么易部署但精度损失大且灵活性差（结构化）。为兼顾硬件友好性、稀疏度与精度，有必要探索一种新的剪枝方式。

Method: 提出SpikeNM，首次实现针对SNN的N:M半结构化剪枝框架，利用M-way basis-logit参数化结合可微的top-k采样器，将剪枝组合空间从指数级降为线性复杂度。同时，提出神经科学启发的'eligibility-inspired distillation'（EID）机制，将时间累积的“信号”转为区块软标签，引导掩码学习更符合脉冲动力学，减小采样方差，提升稳定性。

Result: 在主流数据集上，SpikeNM于2:4稀疏度下基本无损甚至提升精度，同时产生的剪枝模式适合硬件实现，对脉冲本身的稀疏性有补充。

Conclusion: SpikeNM实现了高效、硬件友好的SNN高稀疏化，弥补了结构化和非结构化剪枝方案的兼容性与实用性短板，对SNN在边缘设备上的部署具有实际意义。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [61] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: 本文提出并构建了SenseNova-SI系列多模态基础模型，通过规模化高质量空间智能数据集，显著提升了模型的空间感知与推理能力，展现了多项空间智能基准的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型在空间智能方面仍存在明显不足，研究者希望通过扩展数据和模型规模，系统提升模型的空间感知与空间推理能力。

Method: 作者基于现有多模态模型（如Qwen3-VL、InternVL3和Bagel），系统整理了SenseNova-SI-8M大规模空间智能数据集，通过严密的空间能力分类规范进行数据选取和标注，训练SenseNova-SI模型，并在多项空间智能基准任务进行评估。

Result: SenseNova-SI模型在多个空间智能基准数据集上均创下领先成绩，在VSI-Bench、MMSI、MindCube等评测中取得大幅提升。同时模型依然保持了强大的通用多模态理解能力。

Conclusion: 规模化多样空间智能数据的汇集与精细建模能够显著提升多模态模型的空间智能表现。SenseNova-SI展现了出色的空间推理能力和一定的泛化潜力。该系列模型已开源，对后续空间智能及多模态领域研究具有重要价值。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [62] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种利用 DINOv3 Transformer 和 CNN 相结合的新型框架（DGCF），用于将CBCT或MRI影像合成CT影像，以提升放疗中的剂量规划效率和质量。实验表明该方法达到当前最佳的指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的合成CT方法缺乏全局语义理解能力，而Transformer虽然具有全局感知能力，但在小型医学数据集上易过拟合且归纳性较弱。因此亟需一种将两者优势结合，提升合成CT图像质量的新方法。

Method: 提出了DINOv3-Guided Cross Fusion (DGCF) 框架，将冻结参数的DINOv3 Transformer用于全局特征抽取，并与可训练的CNN编码-解码器结合，通过可学习的跨融合模块实现Transformer全局表示与CNN局部特征的层次融合。同时，引入多层级DINOv3感知损失（MLDP loss），促进合成CT与真实CT在Transformer特征空间的语义相似性。

Result: 在SynthRAD2023骨盆数据集上，DGCF在MS-SSIM、PSNR以及基于分割的评测指标方面，实现了MRI到CT和CBCT到CT任务上的最新最优性能。

Conclusion: DGCF首次将自监督DINOv3 Transformer引入医学影像合成，显著提升了合成CT的语义理解和成像质量，展现了Transformer全局特征与CNN局部特征互补的巨大潜力。

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [63] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: 该论文针对现有基于扩散的视频生成模型在生成长视频时容易出现一致性削弱、动态表现差等问题，提出了Adaptive Begin-of-Video Tokens（ada-BOV）和若干改进策略，有效提升了长视频生成的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的视频生成模型可生成高保真的短视频，但向长视频扩展时，主流的自回归方式（如chunk拼接和stream denoising）分别存在延迟、误差累积、一致性和动态表现差等难题。为此，需要新的方法实现长视频内的全局一致性、灵活条件引导和局部动态表现。

Method: 提出了适应性Begin-of-Video Tokens（ada-BOV），通过自适应层归一（adaptive-layer-norm-like）调节，将已去噪的前帧信息融合进可学习的BOV特征，实现灵活且保持全局连贯的条件引导。为增强局部动态，引入流式去噪的精细化策略，解耦采样轨迹长度与注意力窗口，提升局部引导和成像。用扰动增强的训练噪声调度，平衡模型收敛速度与弹性。

Result: 在多项定性和定量指标上证明该方法效果优秀，生成的长视频在全局一致性和局部动态表现上均优于现有方法。

Conclusion: ada-BOV与新颖的训练与推理策略，有效解决了基于扩散的自回归长视频生成中的一致性、灵活性和动态性问题，是实现高质量长视频生成的重要进展。

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [64] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于归因分析的反事实数据增强方法（SS-CA），通过识别并替换模型做出预测的最小关键区域，从而提升模型在分布内与分布外的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型训练容易只依赖于部分充分原因，对分布变化和关键特征缺失较为敏感。作者发现模型对关键区域的依赖性不足以支撑稳健的因果推断，而人类则更具鲁棒性，因此寻求通过归因和数据增强改进模型的因果学习能力。

Method: 提出Subset-Selected Counterfactual Augmentation (SS-CA)方法：基于归因方法LIMA与其变体Counterfactual LIMA，自动识别并移除对模型预测有决定性影响的最小空间区域，生成反事实样本（将该区域替换为自然背景），并采用原始与增强数据共同训练模型。

Result: 在ImageNet及其多种变体（如ImageNet-R, ImageNet-S）上的实验表明，SS-CA提升了模型在分布内（ID）和分布外（OOD）测试集的泛化能力。同时，在各种扰动噪声下，也能显著提升模型鲁棒性。

Conclusion: SS-CA通过结合可解释性归因与反事实数据增强，纠正了模型的因果依赖缺陷，增强了性能与稳定性，表明可解释性技术能用于指导和改进深度模型的训练。

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [65] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: 本文提出了BdSL-SPOTER，一种基于姿态的Transformer框架，实现了高效且准确的孟加拉手语识别，在BdSLW60基准测试中大幅优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉手语资源有限，现有方法准确性和效率不足，急需一种效率高、能够泛化的小型深度学习模型，满足实际无障碍应用需求。

Method: 提出了BdSL-SPOTER，结合文化专属预处理、四层Transformer编码器、优化的位置编码，并利用课程学习提升在数据有限场景下的泛化能力和收敛速度。

Result: 在BdSLW60基准测试获得97.92%的Top-1准确率，比Bi-LSTM基线提升22.82%，同时参数量少、计算量低、帧率高，具备实际应用优势。

Conclusion: BdSL-SPOTER可高效识别孟加拉手语，适合实际无障碍应用，也为其它低资源区域手语识别提供了可扩展的模型。

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [66] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: 本文提出了TEMPO，一个基于高分辨率卫星影像和深度学习模型，能够全球、季度尺度监测建筑密度和高度的时空数据集。


<details>
  <summary>Details</summary>
Motivation: 现有全球建筑密度与高度数据时空分辨率有限，缺乏对城市化、发展模式和气候影响动态变化的高频监测工具。为推动全球韧性与适应性研究，需要高时空分辨率的建筑动态数据。

Method: 研究人员收集全球建筑轮廓与高度数据，并与PlanetScope提供的季度卫星影像配对；随后利用多任务深度学习模型，在每像素37.6米分辨率下，同时预测建筑密度和高度，进而叠加2018年Q1至2025年Q2的全球图像，获得全球季度动态建筑分布和高度图。模型成果与现有建筑数据集及人工标注做对比，验证准确性和时序一致性。

Result: TEMPO模型在不同人工标注样本中F1得分达到85%-88%，五年趋势一致性得分高达0.96。能够以极低算力成本，每季度动态捕捉全球建筑变化。

Conclusion: TEMPO极大提升了全球建筑密度与高度的动态监测能力，为研究开发模式变化、气候影响评估及强化全球适应性提供了关键基础数据。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [67] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: 本文提出了一种轻量化高效的深度伪造检测方法DFF-Adapter，显著提升了DINOv2在伪造检测与伪造类型细粒度识别上的能力，且参数量极低，检测准确率媲美甚至超过复杂现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于DINOv2的深度伪造检测方法多作为通用二分类任务，未能充分利用不同伪造方法固有的特定伪造痕迹，难以针对性提升检测性能。

Method: 作者提出DFF-Adapter，将多头LoRA模块以轻量化方式集成到DINOv2的每个transformer块，联合训练检测真伪与伪造类型两个任务，通过共享分支传递细粒度伪造信息，用多任务协同优化提升模型对伪造特征的敏感性。

Result: 仅需350万可训练参数，DFF-Adapter实现了媲美或超越当前主流复杂深度伪造检测方法的检测准确率。

Conclusion: DFF-Adapter以极高参数效率提升了DINOv2模型在伪造检测与分类任务中的能力，对提升深度伪造检测的实用性与准确性具有重要意义。

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [68] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: 提出了一种支持多轮实体级推理的医学图像分割新任务，并构建了相关大规模数据集及基线模型，还引入了纠错机制，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割方法大多任务专一且缺乏交互性，基于文本提示的新方法虽增强了交互但受限于单轮对话，难以完成多轮推理任务，因此需要能支持多轮推理和实体级分析的新分割框架。

Method: (1) 定义了Multi-Round Entity-Level Medical Reasoning Segmentation（MEMR-Seg）新任务，要求模型通过多轮实体级推理对话生成分割掩膜；(2) 构建了包含17.7万条多轮医学分割对话的大规模数据集MR-MedSeg；(3) 提出MediRound基线模型，并在推理时引入轻量级判决与纠错机制，减少链式多轮分割中的误差传播。

Result: 实验表明，提出的方法能有效完成MEMR-Seg任务，并在性能上超越了现有医学指令式（referring）分割方法。

Conclusion: 该工作为医学图像分割引入了多轮对话和实体级推理，为交互式和智能医学影像分析打开了新方向，相关数据集和模型为后续研究提供了坚实基础。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [69] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: 本文提出了RadarMP，一种利用低级雷达回波信号，联合完成3D场景运动感知和目标检测的新方法，通过自监督损失进行联合建模，相较于现有方法具有更优的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在恶劣天气下依赖毫米波雷达，但雷达数据稀疏且噪声大，现有方法将目标检测与运动估计分离，导致感知效果受限。因此需要一种更精准且能适应各种环境的3D运动感知方案。

Method: RadarMP利用来自连续两帧的低级雷达回波信号，将雷达点云生成与3D场景流预测统一在一个架构中。方法基于毫米波雷达的多普勒和回波强度设计了专用自监督损失函数，提升空间与运动一致性，无需手工标注。

Result: 在公开数据集上，RadarMP在不同天气和光照条件下展现了优异的运动感知可靠性，超越了传统的分离式雷达运动感知管线。

Conclusion: RadarMP提升了毫米波雷达在极端环境下的运动感知能力，有助于实现全天候、高可靠性的自动驾驶感知系统。

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [70] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: OAD-Promoter方法通过减少语言偏见和增强领域泛化能力，显著提升了大语言模型在VQA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在VQA任务中表现突出，但由于对大量数据训练的依赖，它们容易继承语言偏见，并在处理分布外样本时泛化能力不足，影响了预测可靠性和灵活性。

Method: 提出OAD-Promoter方法，包括对象集中示例生成模块（OEG）、记忆知识辅助模块（MKA）和OAD提示，将全局及区域视觉信息与检索到的相关知识结合输入LLM，有效减少偏见并提升对未见领域问题的推理能力。

Result: 实验证明，采用OAD-Promoter后，大语言模型在小样本或零样本VQA任务上的表现明显优于现有方法，达到了新的SOTA水平。

Conclusion: OAD-Promoter通过多模块集成，有效提升了LLM在VQA中的鲁棒性和泛化能力，减轻了偏见与领域迁移问题，为知识密集型视觉问答提供了更优解决方案。

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [71] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 本文提出了一种面向边缘设备的轻量级SNN（脉冲神经网络）推理运行时，并通过多种优化大幅提升了速度与内存效率，同时保持准确率不变。


<details>
  <summary>Details</summary>
Motivation: SNN具有能效高和适合时序处理的优势，但在资源受限的硬件上训练和部署很有挑战。作者希望解决SNN模型在低功耗嵌入式设备上高效部署的问题。

Method: 1. SNN模型用SNNTorch训练并导出后，自动转化为紧凑的C语言表示。
2. 设计了静态、缓存友好的数据布局与预分配，以避免解释与动态分配的开销。
3. 通过利用神经元稀疏激活，自动剪枝非活跃神经元和突触，减少前层卷积计算负担，从而实现模型压缩和加速。

Result: 在N-MNIST和ST-MNIST两个任务中，压缩优化的C实现与Python基线准确率一致，在桌面CPU上实现约10倍提速，加上剪枝后效果更佳，并显著降低内存消耗，使模型能在如Arduino Portenta H7等微控制器部署。

Conclusion: 经过优化的SNN可以在常规嵌入式平台上高效运行，兼具速度、节能与准确性，证明事件驱动模型适合低功耗硬件部署。

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [72] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文提出了MAVIS，这是首个用于评估多模态源归因系统的基准，涵盖视觉问答、证据检索和带引用的长文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有AI答案的来源归因主要关注文本场景，忽视了多模态（如图像+文本）在提供可靠答案和出处中的作用，因此需要新的多模态评测基准。

Method: 作者构建了含15.7万条视觉问答数据集，每条答案有细粒度多模态文档引用；设计了三类自动化评测指标（信息性、扎根性、流畅性）；对比了不同多模态检索生成方法、提示词设计与结果。

Result: 实验发现：多模态RAG模型比单模态生成更具信息性和流畅性，但在图像文档扎根性较弱，且信息性与扎根性之间存在权衡。提出减缓多模态理解时的上下文偏差是未来关键方向。

Conclusion: MAVIS推动了多模态源归因系统的评估方法，为更可靠的AI生成答案提供了实验工具和改进线索。

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [73] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: 本文针对事件相机与脉冲神经网络（SNN）在视觉智能领域中的知识迁移难题，提出了一种时间步混合知识迁移（TMKT）方法，通过在不同时间步混合RGB和DVS数据，有效提升了跨模态迁移的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机与SNN组合可实现高能效视觉感知，但受事件数据稀缺和分布差异影响，训练效果受限。RGB到DVS的知识迁移因模态差距大，效果通常较差，亟需新方法提升跨模态迁移性能。

Method: 提出时间步混合知识迁移（TMKT）框架，核心是概率性时间步混合（TSM）策略，在不同时间步下混合RGB和DVS输入，形成平滑的训练过程。配合两种轻量辅助目标：帧级源监督（MAG）和序列级混合感知（MRP），显式对齐时间特征和混合计划，改善迁移优化。

Result: 在多个基准数据集与多种SNN骨干网络上，通过大量实验证明TMKT显著提升了脉冲图像分类任务的性能，并验证了其机制有效性。

Conclusion: TMKT方法能缓解模态不匹配，有效促进跨模态知识迁移，为事件相机和SNN结合下的视觉任务提供了性能更优的新方案。

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [74] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了FIA-Edit，一种高效且高保真度的文本引导图像编辑方法，通过频率交互注意力机制提升编辑质量，并扩展到医学应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流式（flow-based）且无需反演的图像编辑方法，虽高效但往往无法有效集成源图像信息，导致背景保留差、空间一致性不足以及过度编辑等问题。因此，亟需一种能够提升保真度和语义准确性，并兼顾效率的编辑方法。

Method: 提出了FIA-Edit新框架。方法包括：（1）频率表示交互（FRI）模块，通过在自注意力机制中交换源和目标特征的频率分量，提升跨域对齐能力；（2）特征注入（FIJ）模块，将源端query、key、value及文本嵌入显式注入目标端的交叉注意力，增强结构与语义保留。

Result: FIA-Edit实现了在RTX 4090上每张512×512图像大约6秒的低计算开销，同时在多种任务中在视觉质量、背景保真和可控性方面持续优于现有方法。此外，首次将文本引导图像编辑应用于临床，通过合成解剖结构一致的出血变化，有效增强了医学数据，并提升了下游的出血分类性能。

Conclusion: FIA-Edit具备高保真、高语义精度和高效率优势，在常规和医学场景中均展现出优越性能，为数据增强等需求提供了新工具。

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [75] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种动态重分配哈希中心的端到端深度哈希方法，通过多头机制提升语义表达能力，并在三个基准数据集上取得了领先的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于哈希中心的深度哈希方法虽然优于传统的对/三元组方法，但初始化中心时往往忽略类别间的语义关系，且两阶段优化方法存在复杂性高、效率低和性能不佳等问题，需要一种高效且能捕捉语义关系的哈希中心分配方法。

Method: 作者提出了Center-Reassigned Hashing（CRH）方法，从预设的码本动态分配哈希中心，并和哈希函数联合端到端优化。同时，引入多头机制提升哈希中心的表达能力，无需显式的中心优化阶段，能自动适应数据分布、融入语义关系。

Result: 在三个基准检索任务上，CRH方法能够学习到具有语义意义的哈希中心，且性能优于当前最先进的深度哈希方法，证明了方法的有效性。

Conclusion: CRH通过动态哈希中心分配和多头机制，不仅简化了训练流程，还增强了哈希表达与语义结构的关联性，在大规模检索任务中表现出色。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [76] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云补全范式——从修正入手，通过利用预训练的图像到3D模型生成的先验形状，并在特征空间对其进行校正，提升了结构一致性和观测对齐效果。所提出的PGNet方法在ShapeNetViPC数据集上相对于现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有多模态点云补全方法大多采用“补洞”范式，仅通过特征融合合成缺失部分，容易出现结构不一致和拓扑异常。缺乏有效的几何和语义约束，导致重建效果受限。

Method: 本文提出“修正式补全”新范式，以预训练图像到3D模型输出的完整形状为先验，通过特征空间修正其与观测局部相对齐。具体实现的PGNet包括双特征编码、生成结构对齐初稿和分层几何细化等多阶段流程。

Result: 在ShapeNetViPC数据集实验中，PGNet在平均Chamfer距离降低23.5%、F-score提升7.1%，优于各类最新方法。

Conclusion: 通过引入结构先验与特征修正，创新性的Completion-by-Correction范式有效提升了点云补全过程的结构一致性与观测对齐能力，PGNet展现出先进的性能表现。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [77] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: 现有的自回归（AR）图像生成方法因量化和码本有限而损失细节。本文提出MixAR，用离散token结合连续空间来提升生成质量，并提出多种混合策略，取得了更好的生成表现。


<details>
  <summary>Details</summary>
Motivation: 离散token方法易损失图像细节，连续空间虽质量高但难以高效建模。需要一种兼具细节保留和高效性的AR生成方案。

Method: 提出MixAR框架，将离散token作为连续自回归预测的先验引导，探索了自注意力混合（DC-SA）、交叉注意力混合（DC-CA）和简单混合（DC-Mix）多种策略。同时提出TI-Mix方案，解决训练和推断token分布不一致的问题。

Result: DC-Mix策略在计算效率和生成质量之间取得了良好平衡，TI-Mix方案也带来了持续的改进。实验表明整体方案优于传统方法。

Conclusion: MixAR有效结合了离散与连续自回归建模优点，提升了图像生成质量，并具备良好的效率和泛化能力。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [78] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级脑肿瘤分割网络MMRINet，可在资源受限环境下高效运行，取得了准确的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在多参数MRI中，3D深度神经网络虽然分割效果好，但在计算资源有限（如部分临床环境）时难以实用。该工作旨在设计一种既高效又准确的脑肿瘤分割网络，解决3D网络高算力需求和推广受限的问题。

Method: 作者提出MMRINet，主要创新包括：将传统高复杂度的注意力机制替换为线性复杂度的Mamba状态空间模型以实现高效体素上下文建模；引入双路径特征细化模块(DPFR)增强特征多样性且不增加数据需求；采用渐进式特征聚合(PFA)，实现多尺度特征融合。模型参数量仅约2.5M。

Result: 在BraTS-Lighthouse SSA 2025挑战中，MMRINet获得了平均Dice系数0.752、平均HD95为12.23，并且模型参数量非常小，仅约2.5M，显示出优异的效率和准确性。

Conclusion: MMRINet实现了在资源受限环境下对脑肿瘤的高效、精准自动分割，对临床实际应用具有很强的现实意义。

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [79] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 本论文提出了一个新的两阶段跨视角、跨模态无监督领域自适应框架，实现了更有效的驾驶员分心行为识别，并在真实环境和多模态数据集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的驾驶员行为识别方法在实际部署时，因摄像头视角变化和领域转移（如传感器模态或环境变化）而效果受限。多数方法只能针对单一挑战，缺乏可扩展且健壮的解决方案，因此需要一个能同时应对两者的新方法。

Method: 提出了两阶段无监督领域自适应框架。第一阶段通过多视角数据上的对比学习，获取视角无关、动作判别特征。第二阶段利用信息瓶颈损失，对新模态（如不同类型传感器）进行领域适应，不需要新领域的标注数据。在Video Swin、MViT等先进视频Transformer和Drive&Act多模态数据集上评估框架效果。

Result: 新方法在RGB视频数据的Top-1准确率上，相较于基于监督式对比学习的跨视角方法提升近50%；与仅进行无监督领域自适应的方法相比，同样视频Transformer骨干网络下，性能提升可达5%。

Conclusion: 所提出的联合式跨视角、跨模态领域自适应方法，能显著提升驾驶员行为识别在实际不同视角和模态下的泛化能力与准确率，对智能驾驶监控系统的推广具有重要意义。

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [80] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种分层语义学习框架（HSL），通过引入双重风格随机化和分层语义挖掘，有效提升了跨域小样本分割对新类别的适应性和语义区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域小样本分割方法主要关注源域与目标域之间的风格差异，往往忽略了分割粒度（granularity）的差异。这导致模型在目标域新类别上的语义区分能力不足，因此需要新方法来解决分割粒度适应性的问题。

Method: 作者提出了层次化语义学习（HSL）框架，包括：1）双重风格随机化模块（DSR），通过前景与全局双重风格变换，模拟多样化的目标域风格情况；2）分层语义挖掘模块（HSM），利用多尺度超像素，引导模型在不同粒度下学习类内一致性与类间辨别性；此外，作者还设计了基于原型置信度调节的阈值法（PCMT），以解决前景和背景差异不足导致的分割歧义。

Result: 在四个主流目标域数据集上进行了大量实验，结果显示所提出方法在各项指标上均取得了当前最优（state-of-the-art）性能。

Conclusion: 分层语义学习框架不仅有效提升了模型对不同粒度新类别的分割能力，还显著增强了模型处理风格与语义差异的泛化能力，为跨域小样本分割提供了新的有效方案。

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [81] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: 本论文提出OmniSparse框架，实现了基于细粒度动态分配的稀疏注意力机制，兼顾模型训练与推理，加速了长视频多模态大模型（MLLMs）注意力计算，并显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法主要关注于推理加速，只在特定稀疏结构下选关键token，忽略训练与推理阶段的差异，同时难以实现跨query、KV、head的细粒度选择，从而性能和加速效果有限。

Method: 提出OmniSparse框架，包含三大机制：1）通过惰性-活跃分类，实现query选择，保留语义丰富的活跃query，丢弃冗余的惰性query；2）基于最平坦head动态分配KV预算，并在全部head间均匀应用，该策略记作KV选择；3）根据解码query分布精细化挑选并访问视觉KV缓存，实现KV cache瘦身，从而减少冗余。

Result: 在实验中，OmniSparse的推理速度提升最高达2.7倍、解码内存降低2.4倍，同时保持全注意力算法的性能水平。

Conclusion: OmniSparse能够解决训练与推理之间的稀疏性鸿沟，通过训练期和推理期一致的动态token分配与多维度细粒度选择，实现对长视频多模态大模型的高效加速和资源节省，无明显牺牲模型表现。

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [82] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的高质量图像到3D重建方法LSS3D，通过可学习的空间平移机制，有效缓解了多视角生成中常见的形状与纹理不一致等问题，在非正面视角输入下表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 多视角扩散模型在3D生成领域受到关注，但现有方法在多视图形状与纹理对齐方面存在不足，导致3D结果质量低，且对非正面视角输入鲁棒性差。因此，亟需一种能同时处理多视角不一致与非正面视角输入的新方法。

Method: 提出LSS3D，将可学习的空间平移参数分配给每个视角，通过重建网格指导每个视角向空间一致目标对齐，提升几何与纹理质量；同时将输入视角作为优化约束，增强对非正面（特别是高角度）视角输入的鲁棒性，并建立了全面的定量评测流程。

Result: 大量实验证明，LSS3D在更灵活的输入视角下，于几何和纹理评测指标上均取得领先结果，显示方法的优越性能。

Conclusion: LSS3D有效解决了多视图3D生成中常见的不一致问题，显著提升了多视角下的3D生成质量和方法的适用性，并为社区提供了先进的评估工具。

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [83] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: 提出了一种几何引导的多视角扩散生成模型，有效提升了多视角图像生成中的一致性与细节丰富度。


<details>
  <summary>Details</summary>
Motivation: 现有多视角图像生成方法存在跨视角一致性差与高分辨率输出难的问题，尤其是在3D重建、虚拟现实等领域，这极大限制了其应用价值。

Method: 提出一种几何引导的多视角扩散模型。核心包含多视角几何信息提取模块（结合深度图、法线图和前景分割掩码提取共享几何结构）、解耦的几何增强注意力机制（增强几何细节特征关注）、自适应学习策略（提升空间关系与可视一致性）、多阶段迭代优化及动态几何信息强度调节机制。

Result: 该方法显著提升了生成图像的跨视角一致性与细节表现，同时经过多阶段优化后整体输出质量更高，生成结果更加真实自然。

Conclusion: 通过引入几何信息与相关机制，提出的模型有效解决了多视角生成中的一致性与细节难题，为3D重建等领域提供了更优的解决方案。

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [84] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 本文提出并验证了一种基于AI的自动交通违法检测系统，用于检测骑行安全帽、机动车后视镜缺失等交通违规行为，并自动识别车牌。系统性能优异，实现了高效、自动化的道路安全管理。


<details>
  <summary>Details</summary>
Motivation: 传统人工执法道路安全措施效率低、成本高且不一致。需要一种智能、自动化的技术手段提升交通安全法规的执行效率和广度。

Method: 基于YOLOv8实现目标检测，采用EasyOCR对车牌进行识别。系统利用定制标注数据集训练模型，并通过数据增强提升泛化能力。采用高阶图像预处理提升恶劣环境下的识别准确性。实时界面基于Streamlit实现监控和违规记录。

Result: 模型整体精度达到0.9147，召回率0.886，mAP@50为0.843，mAP@50-95为0.503，表明其在多种复杂环境下具有较强的检测能力。

Conclusion: 该系统有效提升了交通违法检测和执法自动化水平，展示了实际部署的可行性和应用前景，为提升道路安全管理提供了有力工具。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [85] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: 本文提出了一种名为MoS（Mixture of States）的多模态扩散模型融合新范式，通过灵活、基于状态的方式高效整合多种模态信息，性能达到甚至超越了更大规模的同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散模型在模态融合和计算效率方面存在瓶颈，难以在保证高性能的同时减少参数和计算消耗。因此，作者希望提出一种新的融合方法，实现灵活高效的多模态特征交互。

Method: MoS的核心是一个可学习的、逐token的路由模块，根据噪声去除时间步和输入自适应地在不同模态的隐状态间建立联系。该路由器利用$ε$-greedy策略稀疏选择最相关的top-k隐状态，以较少参数和极低计算开销实现高效融合。

Result: 在文本生成图像（MoS-Image）和图像编辑（MoS-Editing）等任务上，MoS模型以3B到5B参数规模实现了与最大规模大4倍的对比模型相当或更优的性能，达到了当前最佳水平。

Conclusion: MoS为多模态扩散模型提供了一种灵活且计算高效的扩展路径，能够在更少参数下实现强大的融合能力，对于多模态生成与编辑任务展现了良好应用前景。

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [86] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: 本文提出FaNe框架，解决医学视觉-语言预训练中的误负样本与跨模态对齐不足的问题，显著提升多项医学影像任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法存在误将语义相近文本视为负样本（误负样本）以及图文细粒度对齐不足的问题，限制了模型性能。

Method: 1）提出基于文本语义相似度的自适应标准化正样本挖掘策略；2）设计文本条件的稀疏注意力池化模块，实现由文本引导的视觉局部表征对齐；3）开发难负样本感知的对比损失函数，针对语义相近负样本自适应加权。

Result: 在五项下游医学影像任务（涵盖图像分类、目标检测和语义分割）中，FaNe框架均取得了当前最佳表现。

Conclusion: FaNe通过有效缓解误负样本和提升跨模态对齐能力，显著提升了医学视觉-语言模型的综合表现，验证了方法的有效性。

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [87] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: 本文提出了光谱表征过滤（SRF）方法，用于抑制视觉语言模型中的幻觉生成。SRF无需训练或修改结构，通过特征协方差的分析和调整，有效减少了虚假描述。该方法在多个主流VLMs和数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常因过度依赖语言先验和跨模态对齐不准确，产生“幻觉”——即描述图像中不存在的内容。抑制这类幻觉对提升模型实际应用的准确性和可靠性极为重要。

Method: SRF通过分析真实和幻觉描述对应特征的协方差结构，利用特征差异的协方差矩阵进行特征的特征值分解，识别出导致幻觉的低秩模态。随后，通过软光谱滤波在深层的前馈层权重中削弱这些模态，实现特征方差均衡，并保留语义信息。整个过程为推理后处理，无需额外训练，无推理延时，不改动模型结构。

Result: SRF方法在LLaVA-1.5、MiniGPT-4、mPLUG-Owl2三类VLMs及MSCOCO、POPE-VQA等基准测试中，有效降低了幻觉发生率，并取得了信实性指标的新SOTA，同时保持生成内容质量。

Conclusion: SRF是一种高效、实用、无需重训练的新方法，能够后处理地抑制视觉语言模型的幻觉倾向，显著提升模型输出的真实性和可靠性。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [88] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 本文提出了一种针对深度哈希模型的扩散式模型反演攻击DHMI，即使在黑盒条件下也能从哈希码重构高质量原始图像，暴露出深度哈希在隐私上的严重风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希通过紧凑的二值编码提升检索效率，但易造成数据重建风险，可能导致生物识别伪造等隐私威胁。目前缺乏专门针对深度哈希模型的反演攻击研究，主要因哈希空间离散和真实训练哈希码不可得。

Method: 提出DHMI框架：1）用辅助数据集聚类获得语义哈希中心作为代理锚点；2）设计了融合分类一致性和哈希接近度的新攻击度量，动态优化候选样本去噪；3）采用代理模型集引导，持续精细化重建过程，生成高保真、语义一致的图像。

Result: 在多个数据集上，DHMI可以在黑盒场景下（无训练哈希码）准确重构高分辨率、高质量图像，优于现有模型反演方法。

Conclusion: DHMI的出色攻击能力表明，深度哈希系统存在重大隐私风险，须对其安全性加以关注并采取防护措施。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [89] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: 本文介绍了Fusionista2.0视频检索系统，在提升速度和易用性的同时，大幅优化了效率。核心模块全面重构，检索时间降低75%，精度和用户满意度提升。


<details>
  <summary>Details</summary>
Motivation: 视频浏览竞赛（VBS）要求系统在严苛时间限制下快速准确地检索视频，推动了高效、高速的视频检索系统的需求。

Method: Fusionista2.0通过多项技术更新，包括用ffmpeg加快关键帧提取、采用Vintern-1B-v3.5提升多语言OCR性能，以及借助faster-whisper实时语音识别。此外，选用轻量化视觉-语言模型进行问答，重构前端UI以优化响应性和易用性。

Result: 系统检索时间缩短了最高可达75%，同时检索准确率和用户满意度均得到了提升。

Conclusion: Fusionista2.0兼具竞争力和用户友好性，是大规模视频检索任务的有力工具。

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [90] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 本论文提出了一种基于MedSigLIP的prompt引导框架，通过FiLM和多尺度池化，将文本先验有效注入到医学图像质量评估中，显著提升了低剂量CT影像质量评估的表现，超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像质量评估面临数据有限、泛化能力弱的问题，传统方法难以快速适应多样化的临床需求。论文旨在引入有针对性的文本提示，提升模型的泛化效率和临床实用性。

Method: 方法将文本提示通过FiLM调制和多尺度池化引入模型主干结构，使用全局、局部和纹理感知池化的多头回归结构，并以轻量级MLP融合。训练过程中采用pairwise ranking loss提升表现。

Result: 在LDCTIQA2023公开挑战的1,000张训练图像上，模型取得了PLCC = 0.9575、SROCC = 0.9561、KROCC = 0.8301的成绩，超过了挑战赛上的公开最佳方案。

Conclusion: 框架通过引入prompt指导，证明了其在医学图像质量评估中的有效性和优越性，可实现更高效、适应性更强的临床应用。

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [91] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的、疾病感知的双阶段胸部X光报告生成框架，通过引入针对疾病的视觉语义对齐机制，大幅提升了生成报告的临床准确性和语言质量。


<details>
  <summary>Details</summary>
Motivation: 现有的X光影像报告生成方法，疾病感知能力不足且视觉-文本对齐不充分，难以满足医学影像分析对病理特征识别与报告准确性的高要求。因此，急需设计专为医学场景优化的新模型。

Method: 提出了双阶段方法：第一阶段通过交叉注意力机制和多标签分类，学习与具体病理类别对应的“疾病感知语义Token”，并用对比学习提升视觉与文本的对齐；第二阶段，引入疾病-视觉注意力融合模块（DVAF），整合疾病相关信息与视觉特征，同时用双模态相似性检索机制（DMSR）在生成报告时提供示例参考。

Result: 在CheXpert Plus、IU X-ray和MIMIC-CXR等权威数据集上，该方法取得了当前最优的报告生成效果，显著提升了在临床正确性与文本质量方面的表现。

Conclusion: 所提疾病感知框架能更好捕捉关键病理信息，实现高准确度、高质量的自动化胸部X光报告生成，具有重要的实际应用价值。

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [92] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: 本文提出了一种用于加速扩散式多模态大语言模型（Diffusion MLLMs）推理的新方法D$^{3}$ToM，即由判定器引导的动态 Token 合并，用于在各去噪步动态合并冗余视觉 token，从而降低计算量并加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大语言模型在视觉与语言任务中表现优异，但推理速度远逊于自回归模型，主要由于每个去噪步骤都需全序列双向自注意力计算，特别是视觉 token 多时，其计算复杂度呈三次方增长，导致大规模应用受到限制。

Method: D$^{3}$ToM方法在每个去噪步骤中利用上一步生成的判定器 token 构建视觉 token 的重要性图谱，保留最重要的一部分 token，将剩余的通过相似性聚合合并。此方法以即插即用方式融入变换器的单层，对后续各层均有效并可变动合并比例，无需修改原有模型参数。

Result: 实验表明，D$^{3}$ToM在不降低任务表现的前提下，显著加速了Diffusion MLLMs的推理速度，并能在相同计算预算下获得更优表现。

Conclusion: D$^{3}$ToM作为一种简单高效的推理加速模块，能够有效解决扩散式多模态大语言模型的计算瓶颈，为其大规模应用和部署提供可行方案。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [93] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: 该论文提出了全新的CrossVid基准，用于全面评估多模态大语言模型（MLLMs）在跨视频推理（CVR）方面的能力，发现现有模型普遍表现不佳。


<details>
  <summary>Details</summary>
Motivation: 目前视频理解的研究多以单视频为主，与现实中跨视频综合推理的复杂场景不符，缺少评估MLLMs跨视频推理能力的系统基准。

Method: 作者构建了CrossVid基准，包括四个高层次维度和十项具体任务，涵盖5,331个视频及9,015个多样化问题，题型包括单选、多选和开放问答。同时对多种开放源和闭源的MLLM进行了实验评测。

Result: 在CrossVid基准上，Gemini-2.5-Pro取得了最佳表现，但平均准确率仅为50.4%。案例分析显示，多数现有MLLM在CVR任务中难以有效整合与对比多个视频间的信息。

Conclusion: CrossVid能够更全面地评估并推动MLLMs在跨视频推理能力上的进步，当前模型距离真实场景需求仍有较大提升空间。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [94] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 本论文提出了一种基于语音的在线密集注释平台DenseAnnotate，能够高效生成丰富、多语言的图像和3D资产注释数据集，并显著提升多模态大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLMs）应用不断扩展，需要更高质量、更贴合任务的密集训练数据。当前数据集注释普遍稀疏，只能覆盖图片内容的一小部分，且传统文本注释方式难以捕捉丰富视觉细节，特别是在多元文化及3D资产场景中更为突出。

Method: 提出DenseAnnotate平台，支持标注者通过语音讲述同步标记图片区域或3D场景部件，结合语音转文本和关注区域标注，实现便捷、丰富的密集注释。通过两大领域的案例研究（多元文化图片和3D场景），邀请1000余名标注者，构建包含3531张图片、898个3D场景、7460个3D对象及20种语言、音频对齐注释的数据集。

Result: 模型在多语言、多文化对齐和3D空间能力三项指标上，分别提升了5%、47%和54%。生成的数据集包含8746条图片描述、2000个场景描述、19000个对象描述，是目前少有的密集、多语言多模态高质量数据集。

Conclusion: DenseAnnotate平台为未来视觉-语言研究、不同任务和多样数据类型的注释提供了可行方案，能够极大丰富和改善多模态大模型的训练数据基础。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [95] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新范式的主动感知方法用于超高分辨率遥感图像处理，并发布了大规模基准数据集LRS-GRO，以及新方法ZoomEarth，在多个任务上表现优异且具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的动态分辨率与Token剪枝方法在面对超高分辨率遥感图像时，普遍受限于被动感知范式，导致处理精细视觉输入时冗余增多。在实际需求下，针对遥感图像中的信息丰富区域主动关注与多次审视显得尤为重要，因此亟需一种能实现主动、动态关注信息的处理方法。

Method: 作者构建了一个面向主动感知的大规模基准数据集LRS-GRO，包含全球、区域和目标等17类问题。基于该数据集，提出了Adaptive Cropping-Zooming框架ZoomEarth，引入了Region-Guided奖励信号用于细粒度指导。该方法通过监督微调（SFT）和Group Relative Policy Optimization算法训练。

Result: ZoomEarth在LRS-GRO数据集上以及三个公开UHR遥感基准中（零样本设置下）均取得了新的SOTA性能。此外，ZoomEarth能无缝接入云移除、去噪、分割、图像编辑等下游模型，展现出很强的适应性和扩展性。

Conclusion: 主动感知范式显著提升了超高分辨率遥感图像的处理效果，ZoomEarth方法不仅性能突出，还能方便地与各种遥感下游应用结合，推动了领域发展。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [96] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种融合大语言模型（LLM）与基于网格的整数规划的新型自动化室内设计框架，实现房间布局与家具布置的联合优化，在速度和效果上均明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化室内设计方法多为两阶段流程，难以同时优化空间布局与家具布置，且对复杂设计约束的表达能力有限，因此需要新的方法提升设计质量和效率。

Method: 1. 利用大语言模型从文本指令中提取结构化的设计约束；2. 将这些约束编码到受“Modulor”启发的统一网格表示中；3. 引入整数规划模型以联合优化房间与家具布局，兼顾走廊连通性、空间独占性等关键需求；4. 采用由粗到细的分辨率优化策略，先在低分辨率下规划再细化至高分辨率。

Result: 在多样化场景下，所提出方法在设计质量方面显著优于现有两阶段流程，且通过粗到细的优化策略有效提升了计算效率。

Conclusion: 结合大语言模型与网格整数规划的联合优化框架，能够更好地满足复杂室内设计需求，实现高质量、计算高效的自动化设计方案。

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [97] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: 提出了一种新的轻量级医学图像分割方法TM-UNet，在保持高精度的同时大幅度降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的医学图像分割方法虽然效果好，但计算量大，难以在临床实际部署。因此亟需一种既高效又轻量的分割算法。

Method: 提出TM-UNet框架，核心创新是多尺度token-memory (MSTM)模块，将2D特征转化成token序列，通过矩阵记忆单元有选择地保留和传递区分性上下文信息，同时引入指数门控（exponential gating）和多尺度并行池化操作以提升表征能力。该模块具备线性复杂度，可高效捕获全局依赖关系。

Result: 在多种医学图像分割任务上，大量实验证明TM-UNet在性能超过现有最先进方法的同时计算量大幅降低。

Conclusion: TM-UNet通过引入高效的token-memory机制，实现了更快、更轻量的医学图像分割，为临床实际应用提供了有前景的新方案。

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [98] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新型多模态外参标定框架，可同时实现事件相机、激光雷达与RGB相机之间的相对位姿估计，基于自定义三维标定靶实现一站式高精度标定。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等多传感系统中，不同类型传感器的数据融合非常依赖于精确的空间对齐，目前针对事件相机的外参标定尤为困难，主流方法只能逐对或分步进行，效率低且误差大。

Method: 设计并制造了能同时被三种传感器感知的三维标定靶，分别用平面、ChArUco和主动LED编码适配激光雷达、RGB相机和事件相机；设计了一套一站式联合外参标定流程并开发实验验证平台。

Result: 在自建高级自动驾驶传感器数据集上进行了充分实验，结果表明新方法在精度与鲁棒性方面均优于现有方法。

Conclusion: 该方法实现了复杂多模态视觉系统的高效高精度联合标定，特别解决了事件相机的标定难题，具有重要应用意义。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [99] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种频率再校准（FreRec）方法，解决了AI生成医学图像时因高频信息失配导致的下游任务性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在医学AI中，数据稀缺是普遍难题，虽然生成式数据增强（GDA）可以合成虚拟医学影像来缓解，但生成图像与真实图像在频率分布上的差异会影响这些合成数据的有效性，甚至带来偏差。

Method: FreRec包含两个步骤：（1）统计性高频替换（SHR）用于大致对齐高频成分；（2）重建性高频映射（RHM）用于提升图像质量与高频细节。该方法作为生成图片后的独立处理步骤，可兼容任意生成模型。

Result: 在脑MRI、胸部X光和眼底等多种医学图像数据集上的实验表明：FreRec校准后的合成样本用于数据增强，显著提升了下游医学图像分类任务的表现。

Conclusion: FreRec能有效缓解生成式数据增强中的频率偏差问题，提升AI模型在医学影像领域的泛化能力，可无缝集成至现有GDA流程。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [100] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: 本文提出LiDAR-GS++方法，通过引入扩散先验和生成一致的额外点云扫描，大幅提升了LiDAR数据的三维场景重建质量和新视角合成能力，优于现有GS和NeRF方法。


<details>
  <summary>Details</summary>
Motivation: GS方法虽优于NeRF，但基于单次采集扫描存在重建不完整，导致新视角合成出现伪影，尤其在外推视角下。作者希望解决这种不可见区域的重建问题，实现高保真、实时的新视角重建。

Method: 提出LiDAR-GS++，将高斯溅射法与扩散模型相结合。具体包括：1）设计可控的LiDAR生成模型，基于粗略外推渲染生成几何一致的补充扫描数据；2）通过蒸馏机制扩展重建范围；3）保证全局几何一致性和局部高细节。

Result: 在多个公开城市场景数据集上，LiDAR-GS++在插值及外推新视角合成任务中均取得了最先进的表现，效果和速度超过现有GS和NeRF方法。

Conclusion: LiDAR-GS++有效提升了城市道路场景LiDAR数据在三维重建和新视角合成中的质量和广适应性，为实时高保真的场景重建提供了优异方案。

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [101] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 本文提出了一种简单有效的方法，通过数据组织和损失函数改进，让普通前馈分类器具备时序推理能力，无需改变模型结构或引入循环模块。


<details>
  <summary>Details</summary>
Motivation: 实际视觉数据是动态变化的，但传统分类器假设数据是静态、独立的，无法捕捉时间动态。作者希望提升分类器对时间变化的建模能力，同时保持架构的简单和通用性。

Method: 引入Support-Exemplar-Query（SEQ）训练范式，将训练数据组织为时序连贯的轨迹。利用这些轨迹学习类别特定的时序原型，通过可微分的soft-DTW损失对预测序列进行对齐。多重目标函数鼓励语义一致性和时间平滑性，实现强时序归纳偏置。

Result: 方法不仅提升了细粒度和超细粒度静态图像分类性能，还在视频异常检测等时序任务中实现了准确且时序连续的预测。

Conclusion: 该方法能够通过设计损失函数，在无需改变原有分类器结构的基础上，实现静态和时序学习的有效结合，具有模块化和数据高效性。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [102] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练的方法，通过在VLMs嵌入空间中建模否定子空间，显著提升视觉-语言模型对否定指令的理解能力，同时不损害原有模型的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 主流视觉-语言模型（如CLIP）在处理否定指令（如“不包含某物”）时表现不佳。虽然现有方法用大规模否定数据进行微调，但往往会牺牲模型对正向指令的零样本能力，因此亟需解决如何在提升否定理解的同时，不损害零样本性能。

Method: 发现VLM的嵌入空间可以被划分为语义一致的子空间，进而提出一种无训练框架，将否定建模为嵌入空间中的一个子空间而非单一向量。具体做法是，对“A但不是N”类型提示，通过在A和N分别构造球面帽区域，根据既接近A且远离N的方向来为图片打分，实现更好地处理否定指令。

Result: 在检索、多项选择题（MCQ）和文本生成图像任务上，该方法整体提升了约30%的否定理解能力，且不会损害对正面提示的零样本性能，相较于先前基于微调的方法有明显优势。

Conclusion: 该方法无需对VLMs进行再训练，在显著提升模型否定理解能力的同时，保持了对正向指令的零样本效果，展现了拓展VLMs指令理解范围的新思路。

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [103] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 研究表明，通过将交通摄像头检测到的车辆投影到地面平面进行分析（而不是直接在图像平面上分析），可以显著提升车辆转向流量的计数精度，特别是在多摄像头数据融合的情况下效果更佳。


<details>
  <summary>Details</summary>
Motivation: 交叉口准确的交通转向流量统计对于信号控制、交通管理和城市规划至关重要。原有基于图像平面分析的视觉系统在精度上存在局限，因此本研究探究了转向车辆轨迹映射到地面平面后的潜在优势。

Method: 将摄像头检测到的车辆轨迹通过后投影，映射到现实世界的地面3D坐标上，并分别在单摄像头和多摄像头（采用弱融合策略）场景下评估轨迹分类和转向流量计数的准确性。

Result: 单摄像头场景下，地面平面后投影方法能够提升车辆轨迹分类和转向流量计数的准确性；多摄像头场景下，通过弱融合进一步提升了精度。

Conclusion: 分析交通流量时，应该在地面平面而非图像平面进行车辆检测与分析，能有效提高转向流量计数的准确性。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [104] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: 本文提出了CLAReSNet，一种结合多尺度卷积与Transformer风格注意力机制的混合框架，并在高光谱图像分类上取得了突破性的精度表现。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高光谱维度、复杂的光谱－空间相关性及少量且类别不平衡的训练样本等挑战。现有CNN和Transformer各有优劣，单独应用效果受限，需要新的方法更好地融合空间和光谱特征，并提升强类间可分性和弱类内聚性。

Method: 本文提出CLAReSNet，采用多尺度卷积、深残差网络及改进版卷积注意力模块提取空间层次特征，然后引入融合LSTM/GRU与多尺度光谱潜在注意力（MSLA）的光谱编码器。MSLA通过自适应的潜在分配实现复杂度从二次降低到近线性。最后，采用分层交叉注意力融合多层表示，提高分类鲁棒性。

Result: 在Indian Pines与Salinas两个高光谱数据集上，CLAReSNet取得了99.71%和99.96%的总体分类准确率，显著优于HybridSN、SSRN和SpectralFormer等主流方法。其嵌入特征表现出更好的类间可分性和类内紧凑性。

Conclusion: CLAReSNet结合了多尺度卷积、注意力机制与高效的光谱建模，能够在样本少且类别不平衡的高光谱图像场景下取得极高精度，并展现出优越的特征表征能力，适用于严苛的遥感分类任务。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [105] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出了XAIGID-RewardBench，这是首个用于评估多模态大语言模型（MLLMs）判别AI生成与真实图片解释能力的基准数据集。结果显示，现有模型距离人类水平仍有差距。


<details>
  <summary>Details</summary>
Motivation: 传统检测AI生成图片的方法只能分类图片真假，无法像人类专家那样解释判别理由，影响了其实际说服力和可信度。当前趋势是利用MLLMs生成并评判判别解释，但MLLMs对此类解释评判能力尚未系统研究。

Method: 作者构建了XAIGID-RewardBench基准数据集，包含约3000组三元组，涵盖多种图片生成模型和MLLMs作为策略模型，专门评估MLLMs担任奖励模型（解释判别“裁判”）的能力。

Result: 目前最佳MLLM在此基准上得分88.76%，而人类注释者间一致性为98.3%，揭示了MLLMs与人类推理能力仍有明显差距。作者还分析了MLLMs常见的判别失误。

Conclusion: 现有MLLMs在解释AI生成图片判别的理由上仍显不足，无法达到人类水平，未来需提升其可解释性和推理能力。

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [106] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: 当前视觉推理任务往往需要针对具体任务和输出方式设计专用模型，难以实现统一和泛化。该论文提出DT-R1，通过强化学习训练大模型构建数字孪生高层视觉表征，并在多个视觉推理任务上实现领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理模型高度依赖具体任务的架构和有监督微调，限制了一体化应用和跨任务、跨模态的泛化能力。因此亟需一种统一的视觉推理方法。

Method: 作者提出DT-R1框架，采用强化学习方式（GRPO），训练大语言模型基于多模态视觉输入构建数字孪生表征，然后在此基础上实现统一的高层推理。方法中设计了新颖的奖励函数，同时考验结构完整性和输出准确性。

Result: 在六个视觉推理基准（涵盖两种模态和四类任务）上的实验表明，DT-R1在各项任务中均优于当前最好的专用模型。

Conclusion: DT-R1为利用数字孪生表征和强化学习实现视觉推理提供了新方向，能够突破以往任务专属和泛化受限的局面。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [107] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了一种新型高效的推理分割方法FastReasonSeg，通过蒸馏大模型推理链，实现了小模型在边缘设备上的高效推理分割能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法依赖超大参数量的多模态大语言模型，难以在资源受限的边缘设备部署，同时传统蒸馏方法难以保留多步推理能力。

Method: 提出了基于数字孪生表示的FastReasonSeg方法，将感知和推理解耦，实现更有效的知识蒸馏。具体包括：先利用教师模型生成的推理链进行有监督微调，再通过结合分割精度与推理一致性的联合奖励进行强化微调。

Result: 在两个视频（JiTBench, RVTBench）和两个图像（ReasonSeg, LLM-Seg40K）基准上，FastReasonSeg实现了推理分割的最新性能。其0.6B参数量模型在速度和内存占用上远优于大模型，且精度超越参数量大20倍的模型。

Conclusion: FastReasonSeg支持在受限环境下的实时推理分割，为智能体的实际部署提供了可行方案，并推动小模型推理能力的发展。

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [108] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: 提出了一种新的在线场景变化检测方法，在保证高效率的前提下，首次超越了离线方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前在线场景变化检测准确率远逊于离线方法，难以满足实时、高效、多视角一致性和无需标签等需求，因此亟需更优的在线方案。

Method: 该方法无需标签，能够适应任意姿态视角，通过自监督融合损失、多源信息推理变化，并结合PnP快速姿态估计和基于变化引导的3D高斯Splatting场景表示更新，实现了10 FPS以上高效检测。

Result: 在多个复杂真实数据集上，提出的方法不仅超过了所有在线基线，还首次在准确性上超越了最优的离线方法。

Conclusion: 该研究突破了在线SCD的性能瓶颈，首次实现高效、姿态无关、多视角一致且无需标签的场景变化检测，并为后续研究提供了全新思路和方法。

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [109] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出通过推理能力提升文本-视频检索，能够处理蕴含推理需求的隐式查询，并提供对象级标注。方法以数字孪生场景表征和大语言模型推理相结合，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本-视频检索主要针对显式查询，难以处理需要推理和理解视频细节的隐式查询，限制了实际应用场景。

Method: 方法创新地用专家视觉模型将视频内容解析为结构化“数字孪生”表征，再以两阶段框架：第一阶段将复杂查询分解并与视频对象进行组合对齐，第二阶段用大语言模型进行推理，必要时动态调用其他模型补足信息缺失。提供了含隐式查询的新基准数据集支持实验。

Result: 提出方法在ReasonT2VBench-135上取得81.2% R@1，较最强基线高出50个百分点；在1000视频扩展集上也维持81.7% R@1。并在三类主流基准（MSR-VTT、MSVD、VATEX）获得SOTA表现。

Conclusion: 结合数字孪生与大语言模型推理的新方法极大提升了隐式查询下的文本-视频检索性能，对视频理解和多模态推理具有重要推动作用。

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [110] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: 本文提出了一种新框架AGGRNet，通过提取有效和无效特征，提升了对复杂医学图像中细粒度视觉模式的理解，在多个医学影像任务中实现了对比最先进模型，准确率最高提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制模型虽能提取复杂医学图像特征，但在区分微小和细致类别上表现不佳，导致误诊，迫切需要更有效的方法解决细粒度分类难题。

Method: 作者提出AGGRNet，通过区分和提取信息性与非信息性特征，增强模型对细粒度视觉模式的表达能力，从而更好地区分相似类别，提高医学影像任务中的分类准确性。

Result: AGGRNet在多个医学影像数据集上进行了实验，取得了业界领先水平的准确率，特别是在Kvasir数据集上的最佳提升达5%。

Conclusion: AGGRNet有效提高了复杂医学影像分类的准确性，显著优于现有主流模型，展示了其在实际医学图像分析中的应用前景。

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [111] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: 本研究提出一种结合经典与量子技术的混合方法，利用CT图像高效诊断和区分肾结石、囊肿及肿瘤，并取得了优异的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 肾脏疾病（结石、囊肿、肿瘤）影像诊断常依赖医生主观判断，存在误诊率高、效率低等问题，高效、自动化且高精度的诊断技术临床需求迫切。

Method: 采用ResNet50作为编码器提取深层特征，并通过角度编码将潜在特征转化为量子比特输入到量子卷积神经网络（QCNN）；图像先进行降噪和直方图均衡预处理，利用数据增强和加权采样解决类别不平衡，分别在8-和12-量子比特配置下训练评估。

Result: 两种QCNN模型均实现了训练-验证一致且快速收敛，测试集准确率高达0.99；12量子比特模型在囊肿识别召回率、肿瘤F1分数上更优，混淆矩阵显示各类别分类可靠且误判极少。

Conclusion: 该方法融合经典与量子计算，显著提升了肾脏疾病影像自动诊断性能，证明量子辅助模型在医学诊断领域的潜力和可行性。

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [112] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 本文提出了一种新型的不确定性引导的推理选择框架，无需额外计算，能更好地区分和利用数据与模型不确定性，从而显著减少推理资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有估计方法经常将所有类型的不确定性合并为一个分数，导致难以判断何时需要更多计算或调整推理策略，影响模型在资源受限环境下的效率和可靠性。

Method: 作者提出将推理时的不确定性分解为两类：源于数据的Aleatoric不确定性（通过全局密度模型估计）、源于模型的Epistemic不确定性（通过局部支持、流形谱塌缩和跨层特征不一致三个部分估算）。这些分量彼此正交，无需采样或集成。分解后结果用于分布无关的共形校准过程，以获得更紧致的预测区间。

Result: 该方法集成于模型自适应选择，应用于MOT17数据集，在计算资源减少约60%的情况下依然保持几乎不变的精度，同时所有序列上计算节省幅度较总不确定性基线提升13.6个百分点。

Conclusion: 通过正交分解不确定性，并在视觉推理中自适应分配计算资源，可以在准确率几乎不下降的情况下显著提升推理效率，证明了方法的实用性和有效性。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [113] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA是一种无需修改主干网络、参数高效的适配器，能使不同视觉架构（如CNN与ViT）高效统一地适应新任务，其通过特征加权而非权重微调，取得了优异迁移学习表现。


<details>
  <summary>Details</summary>
Motivation: 目前参数高效微调方法大多集中在ViT模型，难以适用于CNN等其他架构，且通常需要对主干权重进行微调，限制了泛化能力和迁移效率。

Method: MSLoRA结合低秩线性投影与多尺度非线性变换，对空间与通道关注进行联合调制，两部分通过逐点乘法和残差连接融合；整个模块轻量且可在主干权重冻结下实现高效适配。

Result: 实验证明，MSLoRA在分类、检测、分割等任务上，使用不到5%的主干参数就能显著提升迁移性能，且优化稳定、收敛快、架构泛化性强。

Conclusion: MSLoRA通过特征加权而非微调主干参数，实现了一种通用、简洁、高效的视觉主干冻结适配方法，适用多种架构并带来优良迁移效果。

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [114] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉与语言模型的新型自动驾驶框架，实现了在未见过的开放世界环境中自主决策和驾驶，并在真实机器人平台上验证了其优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶难以应对开放、未结构化环境下的新奇情况，主要受限于感知与决策的泛化能力。为增强在开放世界中的适应性和通用性，亟需开发能融合多模态感知（视觉与语言）并实现有效行动检索的框架。

Method: 提出了VLA-R框架，结合冻结的视觉-语言模型（实现开放世界检测与分割，无需特定领域微调），通过Q-Former整合多尺度、语言引导的感知信息，以及引入视觉-动作对比学习，直接对齐感知特征与动作，实现开放世界下的端到端驾驶决策。

Result: 在实际机器人测试平台上，VLA-R展现出在未见过、复杂环境中的良好泛化与探索能力，即便仅有有限训练数据，依然能有效完成驾驶任务，且对各类未训练情境有较好的适应。

Conclusion: 利用视觉-语言模型和视觉-动作对齐机制，可大幅提升端到端自动驾驶在开放世界、非结构化环境下的泛化与决策能力，为实际应用中的自动驾驶带来新的突破。

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [115] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自监督道路缺陷检测方法，能够不依赖标注数据适应新环境，在多个基准上优于现有方法，并实现了强健的零样本和小样本迁移。


<details>
  <summary>Details</summary>
Motivation: 目前自动化道路缺陷检测在跨域应用时泛化能力弱。传统有监督方法虽然精度高但依赖昂贵的人工标注，现有自监督方法虽省标注但易受领域漂移影响，亟需在无标注条件下能自适应新环境的方法。

Method: 提出了一种自监督框架PROBE，包括自监督提示增强模块(SPEM)，利用无标签目标域数据生成缺陷相关的提示，引导ViT主干模型的特征提取；以及领域感知提示对齐(DAPA)目标，通过对齐源域和目标域的提示条件表征以减少领域差异。

Result: 在4个具有挑战性的基准上，PROBE在零样本传递、小样本适应等任务中均超越了有监督、自监督和领域适应的主流方法，展现出对领域变化的鲁棒性和高数据效率。

Conclusion: 自监督提示机制为视觉缺陷检测系统提供了实用、高效且可扩展的新方向。

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [116] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种仅基于旋转的SfM优化框架，仅利用旋转参数表达场景和摄像机运动，大幅简化了表示和优化过程，提升了3D重建和相机姿态估计的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SfM方法通常同时估计三维结构和相机的旋转、平移参数，参数冗余且耦合。近期的pose-only方法通过仅调整姿态取得了更优效果。为进一步简化问题，希望探索如何仅用旋转参数表达并优化整个SfM问题，提升效率和精度。

Method: 作者发现摄像机平移可以用旋转参数表示，因此提出将成像几何映射到旋转流形上的方法。构建了一个基于重投影误差的旋转优化框架，适用于两视角和多视角SfM场景，并对比了主流方法。

Result: 实验表明，该方法在旋转估计精度和鲁棒性方面优于当前最先进的旋转估计方法，在多次束束调整后也能取得相媲美甚至更优的效果。

Conclusion: 本工作证明了仅用旋转参数进行优化在3D视觉任务中具备很高的精度和效率，有望推动更准确、高效和可靠的3D视觉计算发展。

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [117] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型和高频引导的新方法DHGM，用于在去除恶劣天气影响的同时提升图像分辨率和细节，特别适用于小目标检测等视觉任务。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，恶劣天气（如雨天）会严重影响图像质量，进而影响小目标检测等对细节依赖强的视觉任务。传统的图像去天气和超分辨率方法相结合时，二者的目标存在矛盾：去噪追求去除高频噪声，而超分辨率则试图增加高频细节，导致还原内容不一致。

Method: 作者以去雨（deraining）为案例，提出了基于扩散模型的高频引导模型DHGM。该方法将预训练扩散模型先验与高通滤波器结合，能够在去除雨滴伪影的同时增强图像结构细节，一步实现图像清晰化和分辨率提升。

Result: 实验表明，DHGM在清晰度恢复和高分辨率细节方面均优于现有的方法，并且具有更低的计算成本。

Conclusion: DHGM能有效解决去除恶劣天气和提升超分辨率之间的矛盾，在去雨和高分辨率图像恢复方面表现突出，适用于需要高细节准确性的视觉任务。

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [118] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种基于MeanFlow生成模型和ResNet相结合的新架构MFI-ResNet，在参数更少的情况下取得了更高的识别精度。


<details>
  <summary>Details</summary>
Motivation: ResNet通过残差连接机制在视觉任务上表现优异，而ResNet的残差块本质上可看作特征变换的多步离散迭代。最新的MeanFlow模型能够通过一阶速度场在一步内完成分布变换，启发了如何将生成建模思想引入判别模型设计，以提升模型参数效率和性能。

Method: 提出MFI-ResNet，首先在每个ResNet stage将多层残差结构简化为一两个MeanFlow模块，构建轻量级Meta模型（压缩阶段）；然后对前3个stage按原ResNet残差块数量扩展（膨胀阶段），最后保持最后一层为MeanFlow结构，并对全模型进行微调。

Result: 在CIFAR-10和CIFAR-100上，MFI-ResNet在参数减少46%左右的前提下，相比ResNet-50，精度分别提升0.23%和0.17%。

Conclusion: 生成型流动场在ResNet的特征变换中具有很强的表达能力，揭示了生成建模与判别学习的内在关联，为高效判别模型设计提供了新思路。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [119] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: 本文介绍了一种新颖的视觉token剪枝方法（RedVTP），极大提升了扩散式视觉语言模型（DVLMs）的推理效率，显著减少推理延迟，并在部分情况下还能提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: DVLMs因能实现并行token解码而具吸引力，但大量视觉token输入使推理速度受限。视觉token剪枝技术虽在自回归VLMs（AVLMs）上有研究，但对DVLMs几乎未涉及，因此有必要提出专为DVLMs设计的token剪枝方法以提高其推理效率。

Method: RedVTP是一种响应驱动的视觉token剪枝策略。通过利用masked response token的注意力来估计各视觉token的重要性，并利用其跨步稳定性，在首次推理步骤后即剪枝掉不重要token，从而提高推理效率。

Result: 实验表明，RedVTP能将LLaDA-V和LaViDa模型的token生成吞吐量分别提高186%和28.05%，推理延迟分别降低64.97%和21.87%，且精度没有下降，部分场景还能提升。

Conclusion: RedVTP无需损失准确率即可显著提升DVLMs的推理速度和吞吐量，验证了响应驱动剪枝方法在此类模型中的有效性，有望推动其实际应用。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [120] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于通道扰动与预训练知识融合的多模态图像融合统一框架UP-Fusion，通过系列模块有效提升融合质量，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前多模态图像融合中的统一模型在应对模态间巨大差异时因梯度冲突而效果有限，使用模态专属编码器虽提升了融合质量，但牺牲了泛化能力。作者旨在提升统一模型的融合质量同时保证泛化性。

Method: 方法上，提出UP-Fusion框架：包括三个核心模块——1）借助预训练模型语义能力，筛选增强多模态特征通道的SCPM；2）利用原始模态特征对融合特征进行仿射变换，保持编码器模态可辨识性的GAM；3）在解码器阶段引入文本引导的通道扰动模块TCPM，以调整通道分布，减少对模态专属通道的依赖。

Result: 大量实验证明，UP-Fusion在多模态图像融合及下游应用任务中，性能优于现有最新方法。

Conclusion: UP-Fusion框架通过充分利用语义感知与通道扰动机制，在模态区分性和特征泛化能力间取得更好平衡，推动了多模态图像融合领域发展。

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [121] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度卷积神经网络（DCNNs）和OpenCV的人脸特征分析方法，实现对驾驶员疲劳驾驶的实时检测，能在检测到困倦时发出警报。算法在相关数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 长时间驾驶容易导致驾驶员疲劳甚至打瞌睡，这对自己和他人都构成了极大安全隐患。因此，急需一种实时、有效的疲劳检测方案来提高行车安全。

Method: 通过实时摄像头采集驾驶员面部图像，使用OpenCV识别眼睛睁开程度、打哈欠等面部特征，借助预训练的DCNN模型对疲劳状态做出判定，并在检测到疲劳时发出持续警报。该系统嵌入于智能汽车中，属于非侵入式且低成本方案。

Result: 该检测系统在NTHU-DDD数据集上的准确率为99.6%、在Yawn-Eye-Dataset上的准确率为97%。

Conclusion: 提出的基于DCNN的驾驶员疲劳检测系统能够高效、准确、低成本地实现智能汽车中的实时疲劳预警，有望显著减少由疲劳驾驶引发的事故，具有实际推广应用的潜力。

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [122] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需标签、推理时即可适配的医学视觉问答系统，提升了模型在领域转移情况下的表现和答案的图像证据关联性。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答系统在实际应用中常常因领域迁移而表现不佳，且模型生成的答案与医学影像的实际证据关联性弱，难以直接支撑临床决策。内在原因在于模型可能关注无关区域，而部署时重新训练或提供额外标注往往不可行。

Method: 提出了一种称为CoTBox-TTT的方法，在推理阶段通过更新少量软提示（soft prompt），而冻结全部主干网络权重，利用视觉chain-of-thought线索聚焦于与问题相关的图像区域，并保持原图与局部裁剪图之间的答案一致性。整个过程无需标签，并可适配不同主干模型，作为插件灵活应用。

Result: 实验证明该方法在医学VQA任务中具有实用性。例如，将CoTBox-TTT应用于LLaVA模型，可使pathVQA数据集的封闭式问题准确率提升12.3%。

Conclusion: 该方案可显著提升医学VQA在实际部署和领域迁移时的鲁棒性，增强模型回答的证据关联性，且实现简单（无需标签和主干微调），适合实际医疗场景落地。

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [123] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了MOON2.0框架，有效缓解了当前多模态商品理解模型在电商场景下关于模态失衡、模态对齐不足及噪声干扰等问题，在多个公开及新提出的基准上实现了零样本识别的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有电商多模态大模型虽然能表征商品视觉与文本信息，但存在模态失衡、信息对齐利用不充分、以及对噪声数据处理能力有限等问题。

Method: MOON2.0方法包括：1. 基于模态分驱的专家混合（MoE）模块，根据模态成分自适应处理输入，有效缓解模态失衡；2. 商品内部的双层语义对齐方法，提升视觉与文本信息的对齐效果；3. 结合MLLM的图文联合增强策略，并用动态样本筛选提升数据质量。此外，还提出了MBE2.0多模态商品理解评测基准。

Result: 实验证明，MOON2.0在MBE2.0及多个公用数据集上，零样本场景下均取得了最优性能。可视化结果也展示了其对多模态对齐能力的提升。

Conclusion: MOON2.0解决了电商多模态表征学习中的主要挑战，极大提升了商品理解及模型泛化能力，并以MBE2.0为新基准推动了该领域的发展。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [124] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: 本文针对传统图像掩码方法浪费有效像素和忽略关键特征的问题，提出将掩码区域内容作为辅助知识进行利用，并设计了MaskAnyNet方法，通过对可见和被掩码区域共同学习，提升网络对语义多样性和细粒度特征的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习中的图像掩码方法存在两个主要问题：被掩码像素未被充分利用，以及可能丢失细粒度或关键信息。因此，作者希望改进掩码方法，更好地发掘被掩码区域的知识价值。

Method: 提出了MaskAnyNet方法，将被掩码内容视为辅助知识，设计额外分支，在模型训练时对掩码区域进行重组学习。该方法可灵活扩展至任何模型架构，并同时利用可见和掩码信息进行特征学习，从而增强语义表达和细节捕捉能力。

Result: 在多种CNN和Transformer骨干模型上的实验结果显示，MaskAnyNet在多个基准数据集上均带来一致性能提升。进一步分析表明，该方法通过复用掩码内容，有效提升了特征的语义多样性。

Conclusion: 通过将掩码内容从被忽略的信息转变为辅助学习资源，MaskAnyNet能够丰富网络的表示能力，保持细粒度语义特征，提升模型在多个任务中的泛化和表现能力。

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [125] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D场景理解方法C3DFusion，有效提升了相机感知的3D语义场景补全能力，特别是对历史帧可见但当前帧不可见区域的补全效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机的3D语义场景补全方法越来越重视时间信息以增强当前帧建模，但多数方法无法很好重建车辆两侧这些当前帧不可见但历史帧曾拍到的区域。

Method: 提出C3DFusion模块，显式对齐当前与历史帧的三维点特征，采用历史上下文模糊与当前特征稠密化相结合，既抑制历史帧错误点的干扰，又增强当前帧的空间贡献。

Result: C3DFusion简单插入主流结构后，在SemanticKITTI和SSCBench-KITTI-360数据集上表现优异，大幅超过现有最佳方法，并对各主流基线模型均有显著性能提升。

Conclusion: C3DFusion高效集成进常规SSC网络，显著提升了历史信息利用、3D场景补全准确率，并展现良好泛化性，是推进基于相机3D场景理解的有效方案。

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [126] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的利用神经网络直接从图像预测可见3D结构点的方法，以提升摄像机定位效率并减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的定位方法在大场景下因2D-3D匹配问题，通常需依赖图像检索或启发式搜索，但这引入了复杂流程和随观测数量增长的存储问题。

Method: 作者设计了一种可见结构检索网络。该网络输入查询图像后，能直接输出该图像可见的3D结构点子集，从而显著减少所需的2D-3D匹配搜索空间。

Result: 实验表明，该方法在定位精度上可与当前最先进方法媲美，并且大幅降低了计算与存储资源消耗。

Conclusion: 该方法为结构化重定位任务提供了新的范式，极大提高了效率，为实际大规模场景部署带来可能。

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [127] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种对运动模糊鲁棒的AI生成图像检测方法，通过知识蒸馏有效提升了检测器在真实世界模糊场景下的检测性能，达到了当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在现实世界中由于运动模糊等退化影响，检测效果大幅下降。运动模糊常见于手持拍照、快速运动或视频压缩，极大干扰了检测器对高频伪影和纹理的识别。为此，亟需一种具备模糊鲁棒性的检测方法。

Method: 采用教师-学生知识蒸馏框架：以在清晰图像上训练好的高能力教师模型（DINOv3）为特征和语义基准，将其在清晰图像中的特征和输出通过知识蒸馏，传递给针对模糊图像训练的学生模型。教师参数冻结，保证泛化，学生则学习在模糊影响下保持一致判别能力。

Result: 实验显示，该方法在运动模糊和清晰条件下均取得了当前最优的检测性能，显著提升了AI生成图像检测器的泛化能力和真实应用效果。

Conclusion: 所提方法有效克服了运动模糊带来的检测退化问题，在实际场景下展示了优越的稳定性和适用性。源码已开源，有望推动AIGI检测实用化进程。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [128] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型的新颖多退化场景红外与可见光图像融合框架MdaIF，能在多种恶劣天气下实现更优的融合结果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在恶劣天气导致可见光图像质量退化时表现较差，且网络结构固定，难以适应不同退化情景，这限制了实用性和融合质量。

Method: 提出了一个多退化感知、可处理多种天气场景的融合框架MdaIF。其核心包括：（1）针对不同退化场景，引入Mixture-of-Experts系统帮助网络适应各类天气退化；（2）借助预训练视觉-语言模型自动提取与天气相关的语义先验知识和场景特征；（3）设计退化感知信道注意力模块（DCAM），利用退化原型分解增强多模态特征在通道域的互动；（4）通过语义先验和特征对MoE实现智能路由，提升复杂场景下的融合效果。

Result: 大量实验验证了MdaIF框架在多种天气退化场景下图像融合性能优于当前主流方法。

Conclusion: MdaIF能够根据不同天气退化类型自适应提取和融合多模态特征，在复杂环境下有效提升融合鲁棒性与质量，具有较好的实用价值。

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [129] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉定位方法$D^{2}$-VPR，结合了知识蒸馏和可变形特征聚合，有效提升了模型效率并适用于资源有限的设备，在减少模型参数和计算量的同时实现了与最先进方法媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管DINOv2等视觉基础模型在地标识别等任务上表现优异，但其模型复杂度和计算开销较大，难以在资源受限的设备上高效部署。因此，亟需在保持高性能的同时，减少模型体积和推理计算量，实现更优的性能与效率权衡。

Method: 文章提出了$D^{2}$-VPR框架，包含两大核心创新：1）设计两阶段训练流程，结合知识蒸馏（从大模型传递能力到轻量化模型）与精细微调，并提出特有的蒸馏恢复模块（DRM）进一步减少知识转移损失；2）设计了基于顶层注意力的可变形聚合器（TDDA），根据全局语义动态调整聚合的兴趣区域，以适应不规则结构环境。

Result: 实验表明，与当前主流方法相比，该框架在多项视觉定位基准上实现了竞争性的性能，同时模型参数减少了约64.2%，计算量（FLOPs）降低约62.6%（以CricaVPR为参照）。

Conclusion: $D^{2}$-VPR兼顾了视觉基础模型的特征表达能力和轻量化需求，显著提升了部署友好性并保持了主流方法的性能水平，是视觉定位优化中的有效方案。

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [130] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: 提出了一种新的视频关键帧选择方法ReaSon，结合因果信息瓶颈理论和强化学习，有效提升在有限帧条件下的视频理解效果。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在视频理解任务中受限于输入token数量，而视频帧中相关信息稀疏，高效地选择信息丰富且因果决定性的关键帧对于提升理解能力尤为重要。

Method: 提出Reinforced Causal Search with Information Bottleneck (ReaSon)框架，将关键帧选择建模为一个优化问题。方法包括：1）提出因果信息瓶颈（CIB），要求关键帧同时具备预测充分性和因果必要性；2）设计可学习的策略网络，从候选帧中选出预测能力强的帧；3）采用反事实干预评估帧的因果必要性；4）构建复合奖励函数，以强化学习引导帧的选择策略。

Result: 在NExT-QA、EgoSchema和Video-MME等数据集上，ReaSon在仅使用有限帧的情况下，相较于现有主流方法取得了持续领先的性能。

Conclusion: ReaSon方法能有效实现符合因果信息瓶颈理论的关键帧选择，在有限帧约束下显著提升视频理解能力，且具有较强的泛化性。

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [131] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于细粒度数据增强的新方法HiGFA，有效提升了生成模型在细粒度分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型被广泛用于数据增强，但在细粒度任务中难以保证生成样本对类别特征的准确刻画，现有基于文本的指导方法无法充分捕捉细致特征，甚至可能引入误导性样本，降低分类器性能。

Method: 作者提出了分层引导的细粒度增强方法（HiGFA）。该方法利用扩散采样的时序特性，在采样早期到中期采用强力的文本和轮廓指导以确保整体场景风格和结构，采样末期则激活专门的细粒度分类器引导，并根据预测置信度动态调节所有指导信号强度，实现对全局结构和细节精度的有效平衡。

Result: 在多个细粒度视觉分类（FGVC）数据集上进行了实验证明，HiGFA能够生成兼具多样性和真实性的合成图像，并提升了下游分类器的性能。

Conclusion: HiGFA通过分阶段动态调控多种指导信号，实现了高质量细粒度数据增强，能有效提升扩增数据对下游任务的价值。

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [132] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: 本文提出了EmoVerse，一个大规模、开放源代码、可解释性强的视觉情感分析数据集，通过多层次知识图谱注释和创新的方法提升了情感理解的可解释性和研究基础。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感分析数据集大多为单标签注释，缺乏细粒度和可解释性，难以揭示视觉元素如何共同影响情感。缺少公开可获取、可解释性的数据集制约了领域进展。

Method: 作者构建了EmoVerse数据集，将情感分解为背景-属性-主体三元组（B-A-S），每个元素都定位到具体视觉区域，实现词层和主题层的情感推理。数据集包含219k+图片，并双重标注了离散（CES）和连续（DES）情感状态。为了提升标注可靠性，采用了多阶段注释流程。作者还提出了一个可解释性模型，将视觉特征映射到DES空间，并提供详细归因解释。

Result: EmoVerse提供了精细的可解释性情感注释，支撑离散与连续两种情感表达。新颖的标注流程在保证高可靠性的同时显著降低人工消耗。所提出的情感分析模型可做细致归因，有助于推动基于视觉的高层情感理解。

Conclusion: EmoVerse及其配套标注流程和模型为可解释的视觉情感分析提供了坚实的基础，对推动该领域取得更多进展具有重要意义。

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [133] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出结构增强混合专家对比学习框架（SEMC），通过结构感知特征融合与专家引导的对比学习方法，显著提升超声标准切面识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有超声标准切面识别方法难以有效利用浅层结构信息，对比样本生成存在不足，难以区分细粒度语义差异，导致结构与判别细节识别受限。

Method: 提出SEMC框架，包括两个创新模块：（1）语义-结构融合模块（SSFM），利用多尺度结构信息，有效对齐浅层与深层特征，增强细粒度结构感知能力；（2）混合专家对比识别模块（MCRM），借助混合专家机制，在多层次特征上进行分层对比学习和分类，提高类别区分和识别性能。同时自建大规模带注释的肝脏超声切面数据集（六类标准切面）。

Result: 在自建数据集和两个公开数据集上广泛实验，SEMC在多项评价指标上均优于最新方法，表现突出。

Conclusion: SEMC有效提升超声标准切面识别的结构感知和判别性能，有望为相关临床应用提供更准确的辅助支持。

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [134] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: 本文提出一种新方法，通过信号处理和机器学习相结合，实现穿越森林植被遮挡的地表温度重建。该方法可应用于无人机对野火的自动化监测，提前发现森林火灾。研究还解决了训练数据稀缺和温度信号被模糊等难题，显著提高了表面热点检测的精度和完整性。


<details>
  <summary>Details</summary>
Motivation: 传统的热成像或合成孔径成像在植被遮挡下难以准确监测地表火情，且受限于信号模糊以及训练数据稀缺等问题，影响无人机早期发现野火的能力。研究动机在于开发能克服上述难题的自动化监测方法，以提升火灾预警和响应效率。

Method: 采用信号处理与机器学习结合的方式，利用视觉状态空间模型来从模糊热成像数据中复原被部分遮挡的地表温度和热点。在训练上，引入潜变量扩散模型与向量量化生成丰富且真实的温度模拟数据，再结合温度增强和程序化森林热模拟进行扩充。

Result: 在多种环境和场景下，实验显示该方法的均方根误差（RMSE）相比传统方法降低了2到2.5倍。在野外高温热点的实际测试中，RMSE比传统热成像降低12.8倍，比未校正的合成孔径成像降低2.6倍。该模型还能泛化用于检测如搜救任务中的人体热信号。

Conclusion: 提出的方法有效克服了林冠遮挡和信号模糊问题，不仅大幅提升了热信号检测和重建的精度，也能完整还原火源和人体等复杂热信号的形态，为自动化野火监测和搜救提供了强有力的技术支撑，优于传统成像方法。

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [135] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的文本干扰方法，通过在图片外部添加欺骗性文本，有效保护社交媒体用户的地理隐私，且不影响图片视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）能够直接通过用户分享的图片推断其地理位置，带来严重的隐私风险。现有的对抗性扰动方法需强烈改变图片，影响图片美观和可用性，因此亟需更高效且“无损”的隐私保护方法。

Method: 作者提出了一种基于文字的typographical攻击方法，即在图像之外添加具有欺骗性的语义文本，并针对哪些文本语义最能影响地理定位进行了分析，设计了分两阶段的、具备语义干扰的文本生成流程，以迷惑LVLMs地理定位。

Result: 在三个数据集和五个主流商用LVLMs上进行大量实验，结果显示该方法在显著降低地理位置预测准确率的同时，几乎不改变图像的视觉效果。

Conclusion: 文本扩展攻击方法能有效降低LVLMs推断用户地理位置的能力，是实际可行且美观的地理隐私保护策略。

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [136] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster是一种新的视频生成框架，通过分阶段提升帧率，逐步细化视频细节和运动连贯性，兼顾效率和质量，在长视频生成任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 长视频生成在视觉和时间连贯性上具有较高挑战。现有方法在保持高质量画面与长时序一致性间难以兼顾，且效率有限。作者希望解决长视频生成的质量、连贯性和效率难题。

Method: TempoMaster首先低帧率生成一个粗略蓝本视频序列，然后逐步提升帧率细化视频细节和运动。框架在每一帧率层级内采用双向注意力机制，层级间则通过自回归方式实现时序依赖，实现了远程时序连贯与高效并行合成。

Result: TempoMaster在多个长视频生成实验中表现优异，无论在视觉质量还是时间连贯性上都优于现有方法，树立了新标杆。

Conclusion: TempoMaster有效兼顾了长视频生成的画面细节和时序连贯性，同时提升了生成效率，适合更广泛的长视频合成应用场景。

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [137] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: 提出了一种Rank-Aware聚合框架（CountIHC），通过从多个基础模型中有选择地学习知识，实现了高效且紧凑的多分类细胞计数，对重叠细胞和IHC异质性具备优秀表现。


<details>
  <summary>Details</summary>
Motivation: 当前IHC图像的细胞计数面临染色重叠、标记物变异和细胞形态多样等挑战。回归式方法对重叠细胞计数更优，但很少支持端到端的多分类计数。此外，基础模型在该领域应用不足，需要创新方法来提升细胞计数的准确性和适应性。

Method: 提出Rank-Aware Agglomeration框架，从多个基础模型中有选择地蒸馏知识，设计Rank-Aware Teacher Selecting（RATS）机制，根据全局到局部的图像patch排名进行教师模型选择。针对多分类计数，引入视觉-语言对齐的微调，利用结构化文本提示的离散语义锚点编码类别和数量信息，引导回归类别密度图，从而增强重叠细胞的分类与计数能力。

Result: CountIHC在12种IHC生物标记物、5种组织类型上超越了当前最先进的方法，并与病理学专家的评估高度一致。同时，该方法在H&E染色情况下仍表现优秀，展现了模型的可扩展性。

Conclusion: CountIHC方法能有效整合基础模型优势，融合视觉与语言信息，在多分类细胞计数任务上实现了高准确率，对不同组织和染色类型有良好的适应性，显示出在病理计数领域的广泛应用前景。

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [138] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出了TopoFG细粒度车道拓扑推理框架，通过细粒度查询和三阶段处理方法，有效提升了复杂车道结构的拓扑预测准确性，并在OpenLane-V2数据集上取得了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有车道拓扑建模方法多用单一查询表示单条车道，通过查询之间相似性推理拓扑关系，难以精准建模复杂车道结构，导致拓扑预测不可靠。因此，亟需改进方法，提升在实际复杂场景下的表现。

Method: TopoFG框架将BEV特征到拓扑预测过程划分为三阶段：1）Hierarchical Prior Extractor（HPE）从BEV mask中提取全局空间先验，从车道关键点序列中提取局部顺序先验，为后续细粒度查询建模提供指导；2）Region-Focused Decoder（RFD）融合空间和顺序先验，构建细粒度查询，通过在感兴趣区域采样参考点并与BEV特征跨注意力交互，精细化各车道的查询表示；3）Robust Boundary-Point Topology Reasoning（RBTR）基于边界点特征建模车道连通性，并引入拓扑降噪策略降低匹配歧义。

Result: TopoFG在OpenLane-V2基准集上进行了大量实验，在subsetA和subsetB分别取得了48.0%和45.4%的OLS分数，达到了新的SOTA水平。

Conclusion: TopoFG通过空间和顺序先验与细粒度查询的结合，以及边界点拓扑推理中的降噪策略，有效提升了复杂车道拓扑结构的建模能力和预测可靠性，在主流数据集上展现出优越表现。

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [139] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的图像分割方法Seg-VAR，将分割任务转化为条件自回归掩码生成问题，取得了优于以往方法的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉自回归建模（VAR）在图像生成上已有进展，但其在需要精细空间感知的分割任务中的潜力尚未被挖掘。本文希望探索自回归模型在图像分割领域的应用，并提升分割性能。

Method: 方法提出Seg-VAR，将经典Mask2Former的多尺度思想与自回归建模结合，通过用潜在学习替代判别式学习。整体框架包括：1）图像编码器生成潜在先验；2）空间感知seglat编码器将分割掩码映射为离散潜在token，引入位置敏感的颜色映射区分实例；3）解码器利用这些潜在变量重构掩码。训练分为多阶段：先联合训练seglat表示，再细化潜在变换，最后对齐图像编码器和seglat的分布。

Result: 实验结果显示，Seg-VAR在多项分割任务和验证基准上优于现有的判别式与生成式方法。

Conclusion: 论文证明了将分割建模为顺序的分层预测任务是有效的，自回归推理方式可增强空间感知视觉系统，并为分割领域带来新的方法方向。

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [140] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的单张图像变脸攻击检测方法(S-MAD)，结合教师-学生框架和低秩适应(LoRA)，在保证高准确率同时大幅提升检测效率，并在多个公开数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统在安全领域应用广泛，但当前系统难以抵御变脸攻击，即合成融合多个人生物特征的虚假图像，因此亟需更精准和高效的检测方法。

Method: 本方法采用教师-学生框架：以卷积神经网络(CNN)为教师，细化视觉Transformer(ViT)为学生模型；通过引入低秩适应(LoRA)技术进行微调，在维持高检测准确率的前提下显著降低计算成本。实验构建了融合3个公开人脸数据集的变脸数据集，涵盖10种变脸算法，全面验证了方法的鲁棒性。

Result: 与6种SOTA单张图像变脸检测方法对比，本文方法在检测性能和计算效率两方面均取得最优表现。

Conclusion: 本文提出的S-MAD方法结合了新型网络结构和高效微调策略，实现了对变脸攻击的高效、准确检测，建议在实际安全场景中推广应用。

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [141] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: 本论文提出了一个新的大规模多模态数据集SoccerNet-GAR，并对基于视频和跟踪（位置）两种主流小组行为识别方法进行了系统性对比，发现跟踪端利用图神经网络的分类器在准确率和训练效率上均优于视频端方法。


<details>
  <summary>Details</summary>
Motivation: 现有小组行为识别研究主要集中于视频数据，而对跟踪数据（运动员位置和轨迹）的利用较少。两种模态优劣未被广泛系统探讨，且缺乏统一基准对比平台。明确哪种模态更优，对未来研究方向有重要指导价值。

Method: 作者构建了SoccerNet-GAR数据集，包含世界杯64场比赛、9万余条同步标注的团体活动、10类行为。采用标准化评测协议，分别用先进的视频分类模型和引入角色感知的图神经网络的跟踪分类模型进行对比。跟踪端模型通过位置边和时序注意力直接建模战术结构。

Result: 跟踪端模型在平衡准确率上达到了67.2%，视频端最佳结果为58.1%；同时，跟踪模型训练速度快4.25倍，参数量仅为视频模型的1/438（19.7万对86.3万）。

Conclusion: 跟踪数据（位置）在小组行为判别任务上明显优于传统视频像素数据，尤其在效率、参数和战术结构表达方面更具优势。今后相关任务需重视模态选择并注重角色感知建模。

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [142] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: 本文提出一种新方法以提升模型在测试时面对分布变化和未知类别(OOD)时的适应能力，有效改进了在分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前许多TTA（测试时自适应）方法在实际环境下遇到未知类别（OOD）时表现不佳，可能导致模型预测精度和适应性下降。因此，提升模型对于分布外样本的检测与自适应能力是未来发展的关键需求。

Method: 作者提出了Hierarchical Ladder Network，从Transformer各层的聚合类token中提取OOD特征，并与原有模型预测结合，通过加权概率融合提升OOD检测性能。同时，提出了Attention Affine Network以动态调整自注意力机制，应对领域漂移。最后，利用加权熵机制动态抑制置信度低样本对自适应过程的负面影响。

Result: 在多个知名分类基准数据集上进行实验，结果显示所提方法在处理分布变化和未知类别时，显著提升了模型的整体性能。

Conclusion: 基于所提的新网络结构和机制，模型在应对分布外样本和领域漂移等实际问题时表现更为鲁棒，为TTA研究提供了有效方案。

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [143] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 本文提出了一种新的网络C3Net，专门用于应对伪装物体检测（COD）任务中的六大核心挑战，并在多个数据集上实现了当前最优的检测性能。


<details>
  <summary>Details</summary>
Motivation: 伪装物体因与环境在色彩、纹理和模式上的高度相似而难以被检测到，现有分割方法和基础模型效果不佳，因此需解决该领域的多重复杂挑战。

Method: 方法上，作者提出了C3Net网络，采用双通路解码器架构。其中，边缘细化通路采用梯度初始化的边缘增强模块提取精细边界；语境定位通路通过新颖的基于图像的上下文引导机制，实现无需外部模型的显著性抑制，并用注意力融合模块结合两条通路。

Result: C3Net在COD10K、CAMO和NC4K三大主流伪装物体检测数据集上，S-measure指标分别达到0.898、0.904和0.913，超过以往方法，同时保持高效处理。

Conclusion: 多重且复合的检测挑战需要架构创新，通过专门组件协同作用，才能整体性提升伪装物体检测效果。C3Net验证了这一观点，并为领域提供了通用、高效的新方法。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [144] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: 本文提出了MDiTFace框架，通过统一的token化策略和新颖的变换器模块，实现更高效的语义蒙版和文本多模态人脸生成，并在显著降低计算开销的同时提升生成质量和条件一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态人脸生成方法中，语义蒙版和文本特征融合较弱，导致生成效果不佳，需要突破现有modalities特征混合方式的局限，实现更优质的多模态融合。

Method: 提出MDiTFace定制扩散变换器框架，采用统一token化方式处理语义蒙版与文本输入，引入了全新的多变量变换器模块以加强多模态信息同步交互，并设计了解耦注意力机制，将mask token和时间嵌入的依赖分离，实现动态路径与静态路径分开，静态路径特征支持缓存复用，显著减少因mask condition带来的计算消耗。

Result: 实验表明，MDiTFace在面部真实性和条件一致性方面均超越现有方法，且计算成本大幅降低（由mask条件引入的额外消耗减少94%以上）。

Conclusion: MDiTFace通过高效统一的跨模态信息交互机制，同时兼顾性能与效率，为多模态人脸生成任务提供了一种更优的方法，具有很强的应用和推广潜力。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [145] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: 这篇论文分析了高维VAE潜空间中冗余高频成分会影响扩散模型训练，并提出频谱自正则化方法优化生成质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 目前VAE常用高维潜变量提升重构效果，但这会对生成模型（如扩散模型）的优化造成障碍。已有方法需借助外部视觉基础模型来正则化，这增加系统复杂性且机制不明，因此需要系统分析高维潜空间的影响及更优正则化方式。

Method: 作者首先分析高维潜空间冗余高频噪声对扩散模型生成优化的负面影响。随后提出频谱自正则化（spectral self-regularization）策略，通过压制这些高频噪声，同时不牺牲重构能力。实现了无须依赖外部VFM的基于ViT的Denoising-VAE，并进一步提出频谱对齐策略提升训练优化效果。

Result: 提出的方法生成的潜空间噪声更低，有效提升扩散生成模型的质量和优化收敛速度。实验表明，方法可使扩散模型收敛速度提升约2倍，同时在ImageNet 256×256数据集上取得了最优重构质量（rFID=0.28，PSNR=27.26）和有竞争力的生成性能（gFID=1.82）。

Conclusion: 高维潜空间中的冗余高频成分阻碍生成模型优化。频谱自正则化可有效降低噪声，提升重构和生成性能，无须外部VFM。该策略对扩散模型应用潜空间的研究具有重要参考价值。

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [146] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: 提出了一种结合大语言模型（LLM）与视觉语言模型（VLM）进行医疗图像分类的高效提示调优新方法——CILMP，显著提升了不同数据集上的任务表现。


<details>
  <summary>Details</summary>
Motivation: VLM虽然强大但参数量大，微调成本高；提示调优虽高效，但难以区分具体医疗概念，捕捉病种细节不足。作者发现LLM在医学知识表达上优势明显，因此希望利用其知识提升VLM任务表现。

Method: 提出CILMP方法：将LLM作为医学知识源，提取疾病特征表示，并在低秩线性子空间中干预VLM提示，生成与疾病相关的自适应提示。方法还引入了条件机制，使提示能根据每张医疗图像进行实例化自适应调整。

Result: 在多个医学影像数据集上，CILMP优于现有先进的提示调优方法，验证了方法的高效性和优越性。

Conclusion: CILMP能够有效将LLM的专业医学知识无缝迁移到VLM的下游任务提示中，实现更高效、更具适应性的医学图像分类。

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [147] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: 本文提出了DPVO-QAT++，一种用于深度视觉里程计的分层量化优化框架，实现了大幅提升运行效率并降低显存占用，同时保留原模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的视觉SLAM系统虽然几何推理强大，但算力消耗高，难以部署到资源有限的自动化平台。因此亟需能兼顾精度与效率的方法。

Method: 提出分层量化优化框架，前端采用异构精度（FP16/FP32浮点fake量化），后端采用全精度，并结合GPU原生CUDA内核融合技术，优化量化训练过程。

Result: 在TartanAir和EuRoC数据集上实验，显示速度提升约30-50%，显存占用降低约30-65%，同时准确率几乎无损。

Conclusion: DPVO-QAT++有效平衡高精度与高效率，实现了深度视觉里程计在实际嵌入式设备上的落地应用。

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [148] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: 当前文本图像伪造定位（T-IFL）方法泛化能力不足，主要因为真实数据稀缺和合成数据分布与现实差异较大。本文提出了基于傅里叶级数的篡改合成方法（FSTS），能够合成更具真实性和多样性的伪造数据，提升模型在真实场景下的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本图像伪造定位方法泛化能力差的问题，原因在于真实伪造数据有限和现有合成数据难以还原真实篡改复杂性。

Method: 作者收集了16750个来自五类主要伪造类型的真实篡改案例，利用结构化流程记录人工编辑轨迹，分析这些数据提取参数和行为模式，建立分层建模框架，通过类似傅里叶级数的方式将个人编辑参数和群体分布建模为紧凑的基函数加权组合，然后从该分布采样合成训练数据。

Result: 在四种评测协议上的广泛实验表明，用FSTS合成数据训练的模型在真实世界数据集上的泛化能力显著增强。

Conclusion: 通过引入基于傅里叶级数的伪造合成框架，能够合成更贴近真实的伪造数据，从而大幅提升伪造检测模型在真实场景下的实用性和鲁棒性。

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [149] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一个高保真、实时响应的对话式数字人系统，整合了高真实性3D头像、富有个性的语音合成和基于知识的对话生成，适用于沉浸式交互应用。


<details>
  <summary>Details</summary>
Motivation: 数字人在交互应用中日益重要，实现视觉真实性和实时响应性仍存在巨大挑战。作者希望克服性能与效果之间的平衡难题。

Method: 提出了异步执行管线以协调包括3D渲染、语音合成和对话生成在内的多模态组件。该系统包括唤醒词检测、情感语调表达、上下文相关的高精度对话回复，并引入了检索增强、历史补充和基于意图的知识路由等创新方法。

Result: 系统实现了低延迟、自然且智能的交互，可以实时动态地生成逼真的数字人响应，适合通信、教育和娱乐等多种沉浸式场景。

Conclusion: 综合的新型模块和优化管线能大幅提升数字人的交互性和可信度，为各类沉浸式应用提供了有效解决方案。

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [150] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: 本文提出了一种融合非因果选择状态空间的模型，实现了高效且实时的光流与视差估计，兼顾精度与资源利用率，适用于3D密集感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有光流和视差估计算法往往难以兼顾实时性、准确性和资源消耗。针对密集3D感知任务，亟需高效、低延迟并且准确的统一估计模型，满足实际应用（尤其是嵌入式场景）需求。

Method: 作者提出了一种基于非因果Mamba模块的模型，能高效地在一对输入图像之间，融合信息后估计光流和视差。该模型重点利用非因果选择状态空间，有效实现了快速推断，并降低GPU资源占用。

Result: 实验结果表明，所提出模型大幅提升了推理速度，准确率高且GPU占用低。在真实场景下进行了验证，结果证明模型适用于实际3D感知应用。代码及模型已开源。

Conclusion: 提出的非因果Mamba模块模型能够统一、实时并高效地估算3D密集感知任务中的光流和视差，兼顾性能与资源消耗，具有广泛应用前景。

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [151] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: 本文提出了一种新的新视角合成(NVS)评价框架，基于Zero123特征和轻量调优，能更好地评估生成图像的真实性和变换的忠实性, 同时设计了可参考与无参考的评价指标，并在多基准测试中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型提升了NVS生成质量，但现有评估指标难以全面评价生成图像是否既真实又忠于源视角与变换。标准指标如像素相似度常常错误判断结果。因此，急需新的方法更好评测NVS结果的可靠性。

Method: 作者提出结合Zero123强大NVS模型的特征和一个轻量调优步骤，设计出两种新评价指标：参考式分数$D_{PRISM}$和无参考分数$MMD_{PRISM}$，前者需要参考图像，后者仅依赖特征分布，能更准确区分和排序不同模型效果。

Result: 通过在Toys4K、GSO和OmniObject3D等三大基准上对六种NVS方法评测，$MMD_{PRISM}$表现出良好的区分能力，且评分稳定准确，低分对应更优的生成模型，并与人工偏好高度一致。

Conclusion: 本研究提出的评价框架为新视角合成提供了原理性且实用的评估工具，可靠性高，有助于社区对NVS方法的客观比较和后续发展。

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [152] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 本文提出BridgeEQA基准，用于评估具身智能体在基础设施巡检领域中的开放式问答能力，并提出EMVR方法提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现实环境下具身问答（EQA）应用受限于缺乏真实任务基准，尤其是在需要多尺度推理和复杂语义理解的场景。基础设施巡检（如桥梁检查）正好符合对多图像证据整合、空间推理及客观评分的需求，因此是检验EQA系统实用性的理想领域。

Method: 构建BridgeEQA基准，收集200个真实桥梁场景、平均每个场景约48张图片，并基于专业检查报告制作2,200组开放式问答。设计了Image Citation Relevance新评价指标，要求模型在回答时引用相关图片。提出EMVR方法，将场景建模为图像节点的有向图，通过强化学习方法引导智能体在图中导航、比较、推理完成问答任务。

Result: 主流视觉-语言模型在EQA 任务下表现仍有限，在需要顺序记忆和跨视角推理时性能有明显差距。EMVR方法在基准上效果显著优于其他基线方法。

Conclusion: BridgeEQA推动实际具身问答研究，为基础设施巡检等现实任务提供了新基准和评测指标。EMVR方法有效提升了具身智能体的视觉推理能力，为相关应用提供了有力支持。数据集与代码已公开发布。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [153] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: 提出一个无需训练的新框架R$^{2}$Seg，用于在分布外数据（OOD）上对肿瘤进行稳健分割，通过推理和拒绝（Reason-and-Reject）过程，有效抑制碎片化误报，大幅提升主流基线性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗图像分割的大模型在分布外数据上容易产生碎片化的假阳性，影响实际应用，亟需一种无需重新训练、可直接应用的O​​OD鲁棒分割方法。

Method: 提出了R$^{2}$Seg框架，包含两个阶段：Reason阶段—利用LLM指导的解剖推理规划，定位器官锚点并生成多尺度ROI；Reject阶段—在ROI中对基础模型（如BiomedParse）生成的候选区块进行两样本统计检验，保留与正常组织显著不同的候选，从而过滤误报。整个方法无需参数更新，适配零更新测试时增强（test-time augmentation）。

Result: 在多中心、多模态肿瘤分割基准上，R$^{2}$Seg显著提升了Dice、特异性和敏感性指标，优于强基线和原基础模型。

Conclusion: R$^{2}$Seg在无需训练和参数更新的前提下，有效提升了分布外肿瘤分割的准确性和可靠性，具有良好的通用性和实用前景。

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [154] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 本文提出了HEDGE框架，对视觉-语言模型（VLMs）的幻觉现象进行检测，通过组合视觉扰动、语义聚类与不确定性度量实现统一的检测流程，证明其适用多种模型与数据集并在部分设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: VLMs虽然能回答开放性视觉问题，但容易出现幻觉（hallucination），即输出与视觉内容无关或错误的信息。目前缺乏通用且稳健的检测方法。作者希望开发一种通用、重现性强的框架，提升多模态模型的可靠性。

Method: HEDGE框架集成了采样、多种视觉扰动合成、基于蕴含和嵌入的语义聚类，以及幻觉不确定性度量。其流程适用于不同VLM架构，通过可控扰动及聚类方法分析模型输出的稳健性。

Result: 在VQA-RAD和KvasirVQA-x1数据集上，HEDGE对不同VLM（LLaVA-Med, Med-Gemma, Qwen2.5-VL）进行了评估。发现统一融合、密集视觉分词的架构（如Qwen2.5-VL）对幻觉更易检测；而受限分词的模型（如Med-Gemma）最难。嵌入聚类直接应用于生成答案时分离效果最佳；而NLI聚类对LLaVA-Med和长句输出更有效。VASE指标尤其在嵌入聚类和中等采样数下，最能体现幻觉信号。简明标签式问句较为有利。

Conclusion: HEDGE通过将幻觉检测建模为几何鲁棒性问题，并综合采样、提示设计、模型结构和聚类策略，提出了可复现、面向计算效率的多模态鲁棒性评估方法。附带的hedge-bench工具和完整开源资源有助于社区复现和扩展。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [155] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: 该论文提出了一种新的可解释性框架，用于分析和理解状态空间模型（SSMS），尤其是Mamba架构在视觉序列建模中的空间信息处理方式。


<details>
  <summary>Details</summary>
Motivation: 尽管SSMs如Mamba在序列建模中表现优异，并具备线性计算复杂度，但由于缺乏类Attention的机制，其空间信息处理过程较难理解。因此，亟需开发一种有效的解释方法来揭示SSMs内部状态如何受到输入的影响。

Method: 提出了两种互补的可控性分析方法：1）基于Jacobian的方法，适用于任意SSM，通过完整的状态传播链衡量各输入部分的影响力；2）基于Gramian的方法，专用于对角SSM，通过封闭解实现更快速度。两种方法均只需单次前向传播，线性复杂度，无需修改模型结构或调整超参。

Result: 在三种不同医学影像模态上进行了实验验证，发现SSMs在网络不同层中能自发地实现分层特征提炼：浅层聚焦较为弥散的低级纹理，深层则关注更聚焦、具临床意义的模式，同时揭示了领域特定以及扫描策略带来的影响。

Conclusion: 该框架为各类SSM提供了统一的可控性解释视角，有助于医疗影像、计算机视觉、自然语言处理等多领域模型的可解释性分析。代码和分析工具将在论文发表后开放。

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [156] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: 本文提出了一种名为CountOCC的新方法，实现了对部分遮挡目标的鲁棒计数，并在多个数据集上取得了显著超越现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的目标计数方法在遮挡环境下表现极差，原因在于特征主干网络更倾向于编码遮挡表面，导致目标信息被污染，影响计数的准确性。

Method: 提出了CountOCC框架，利用层级式多模态引导，将可见区域的空间信息与语义先验（来自文本和视觉嵌入）相融合，显式重建被遮挡目标的完整特征。在多尺度金字塔层级生成类别判别性的特征，并设计视觉等价性损失，保证遮挡和非遮挡视图在注意力分布上一致。

Result: 在遮挡版本的FSC 147数据集上，CountOCC在验证集与测试集的MAE分别减少26.72%和20.80%；在CARPK和CAPTUREReal数据集上，分别减少了49.89%和28.79%的MAE，均大幅超过现有方法。

Conclusion: CountOCC能够实现鲁棒的无模态（amodal）目标计数，在多种复杂现实场景下均取得了新的最优结果，为实际应用中的遮挡场景目标计数提供了有效解决方案。

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [157] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM是一种仅需约100个标注样本的驾驶员注意力建模方法，能同时预测注意力位置并生成解释性文字，具备很强的数据效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶员视线与注意力建模依赖大规模凝视数据集，收集和整理非常费力不现实，因此需要在极少标注数据下实现注意力建模与可解释性的系统。

Method: 提出了FSDAM框架，采用双通路架构：一部分做空间注意力预测，一部分做生成式解释（caption），通过跨模态对齐保证语义一致，在大约100个标注数据下进行联合训练。

Result: FSDAM在很少监督下达到了注意力预测竞争性能，能生成连贯、有语境意识的解释说明，并在多个驾驶基准测试上展现出强的零样本泛化。

Conclusion: 在少量数据监督下，基于注意力的信息生成和可解释性完全可行，为数据有限场景下的可解释驾驶员注意力系统部署带来了新可能。

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [158] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: 本论文首次研究了开放词汇目标检测器（OVODs）上的后门攻击，提出了一种新型多模态后门注入方法TrAP，并验证了其高效攻击能力及对干净图片性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着OVODs在机器人、自动驾驶与安防等领域的重要性提升，其安全风险尤为关键。目前尚缺乏针对这类模型后门攻击的系统性研究，特别是针对由prompt tuning带来的新攻击面。

Method: 作者提出TrAP（一种触发感知式prompt调优），该方法在无需重新训练基础模型权重的前提下，同时对图像和文本prompt参数与视觉触发器进行联合优化。在训练过程中采用递进式策略，逐步缩小触发器尺寸，实现小型触发补丁在推理阶段的有效激活。

Result: TrAP在多个数据集上对目标误分类与目标消失的后门攻击均表现出高成功率，同时，在干净图片上的下游任务性能也优于零样本设置。

Conclusion: 本文揭示了开放词汇目标检测器在prompt tuning下新出现的后门攻击威胁，提出的TrAP方法在不牺牲泛化性的前提下施加隐蔽后门，强调了需审慎应对此类系统的安全隐患。

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [159] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 现有视觉语言模型（VLMs）在关注与问题相关的视觉token时，存在注意力分布异常，导致视觉问题回答不准确。本文提出直接监督视觉token与语言token之间注意力分布的新损失函数KLAL，将其与常规的NTP损失结合，显著提升模型视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: VLMs在融合视觉与文本信息时，现有LLM模块对视觉和文本token一视同仁，导致模型未能充分关注与问题最相关的视觉信息，进而影响视觉相关问题的回答准确性。标准的NTP损失无法有效引导模型将关注点落在相关视觉token上，因此需要改进注意力的指导机制。

Method: 作者提出一种新的KL注意力损失（KLAL），直接监督生成答案token时其注意力应更集中于相关视觉token。具体做法是，利用来自合成数据任务几何或真实图像标注（如框或点）的ground truth注意力图，通过KL散度对比真实与模型注意分布，用作额外损失项指导模型学习。该过程无需引入新标签。

Result: 结合KLAL和NTP，模型在几何任务、指点和指代理解上无论在合成还是真实数据集上都取得了显著提升。此外，作者还提出了一套用于考察线条追踪能力的新数据集，实验证明甚至一些商用VLM在此任务上表现也很差。

Conclusion: 通过引入对视觉token注意力的直接监督，VLM在需要视觉理解的任务上取得了突出的效果。这暗示着现有主流VLM在视觉信息对齐上的固有限制，进一步的改进可显著提升其实际视觉推理能力。

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [160] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: 本文探讨了在LiDAR点云体素化处理过程中，如何从高层体素化数据推断低层体素内部细节（如占据百分比），并使用KPConv和不均衡学习改进多目标回归。结果表明，体素大小对误差有显著影响，选择依赖具体应用。


<details>
  <summary>Details</summary>
Motivation: 体素化可以降低LiDAR数据处理的计算成本，但会丢失细粒度结构信息。本研究旨在解决如何在此情况下估算体素内不同目标的实际占据比例，满足精细林地结构分析和相关生态应用需求。

Method: 提出基于KPConv的多目标回归方法，通过密度相关（DBR）不均衡学习，采用带权重MSE、FocalR与正则化进行损失优化；并对不同体素尺寸做灵敏度分析，以评估对点云细节还原的影响。

Result: 较大体素（2米）误差较小，较小体素（0.25、0.5米）在林冠部位误差增加明显，尤其是在树皮和树叶目标中。表明细粒度体素下内部内容难以准确估算，误差随体素尺寸增减。

Conclusion: 体素大小的选择需根据实际应用目标权衡。本文在3D林地点云上推进了不均衡多目标回归模型研究，为后续精细生态建模与算法设计提供了方法基础。

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [161] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAGE的新方法，通过将人类显著性指导嵌入模型的对比嵌入空间，提高神经网络的泛化能力和与人类认知的一致性。


<details>
  <summary>Details</summary>
Motivation: 目前将人类视觉显著性（saliency）信息融入神经网络训练能提升模型表现，但大多数方法是在图像空间操作，且依赖不可靠的内部机制。论文为解决此类不足，致力于更好地将显著性信息引入网络训练。

Method: 提出SAGE（Saliency-Guided Contrastive Embeddings）方法：将显著性相关操作转移到模型的隐空间，通过对比三元组损失引导模型关注显著特征，弱化对非显著特征的关注。采用显著性增强和削弱的数据增强操作，并以对比学习的方式，配合对输出logits分布的检测，增强训练指导。

Result: SAGE方法在多个分类任务中（包括开放集和封闭集）优于现有的显著性引导方法，并在多种主流模型骨干上验证了广泛适用性和更强泛化能力。

Conclusion: 将显著性指导迁移至隐空间，结合对比损失，可以有效提升模型对显著特征的关注度和整体性能，SAGE方法在广泛任务中具备推广潜力。

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [162] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 本文提出将CLIP嵌入视为Wasserstein空间中的点云，并用最优传输方法优化Stable Diffusion中的文本嵌入插值，从而获得更平滑的生成图像。


<details>
  <summary>Details</summary>
Motivation: 传统的Stable Diffusion方法将文本嵌入简单地视为欧氏空间的矩阵，在插值时忽略了其几何特性，导致插值质量有限。作者发现Stable Diffusion对CLIP嵌入的排列不变性，启发了以点云的方式重新解读嵌入空间，从而提升插值效果。

Method: 作者将CLIP嵌入解释为Wasserstein空间中的点云。插值任务被重新表述为最优传输问题，通过求解两组点云之间的最短路径（测地线），以获得更自然、平滑的嵌入插值，并将其输入Stable Diffusion模型生成图像。

Result: 实验证明，采用基于最优传输的插值方法，相比传统插值算法生成的过渡图像更加平滑、连贯，视觉效果更好。

Conclusion: 将嵌入视为点云并利用最优传输进行插值，能更好体现和利用嵌入空间的几何特性，从而提升生成模型的插值图像质量。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [163] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文提出了首个大规模罗马尼亚手语识别数据集RoCoISLR，并用主流视频识别模型建立了基准，发现Transformer类模型表现最好，数据集为后续罗马尼亚手语研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前大多数手语识别数据集集中于美式手语，缺乏针对罗马尼亚手语的标准化大规模数据集，严重限制了相关研究进展。因此急需一个公开标准的数据集来推动罗马尼亚孤立手语识别（RoISLR）领域发展。

Method: 作者构建了包含6,000个标准词条、9,000余段视频的新数据集RoCoISLR，并采用I3D、SlowFast、Swin Transformer、TimeSformer、Uniformer、VideoMAE及PoseConv3D共7种先进视频识别模型，进行了统一实验，对比了其与经典WLASL2000语料库下的表现。

Result: Transformer类架构模型显著优于卷积类模型，其中Swin Transformer取得了34.1%的Top-1准确率。此外，实验揭示了低资源手语识别领域存在长尾类别分布等难题。

Conclusion: RoCoISLR数据集为罗马尼亚手语识别提供了首个系统性研究基础，有望推动该领域的模型与方法进一步发展。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [164] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的颜色协调算法MKL-Harmonizer，可实现AR环境下对插入对象的实时色彩匹配，使复合图像更自然无缝，并开源了专门的数据集及工具。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实（AR）技术的发展，将对象无缝地插入现实世界背景成为实际需求。然而，插入对象的颜色无法自然融入场景，导致视觉违和。当前缺少适用于实时AR的高效色彩协调算法，因此亟需轻量级、可在终端设备实时推理的解决方案。

Method: 作者基于经典的最优传输理论，设计了一个紧凑型编码器网络，预测Monge-Kantorovich传输映射，实现端侧推理的色彩协调（MKL-Harmonizer）。

Result: 在实际AR图像复合的基准测试中，所提算法在多项评分指标上均优于当前先进方法，获得最佳综合得分。

Conclusion: MKL-Harmonizer不仅提升了AR场景下的复合图像自然度，还具备实时性和端侧部署能力。此外，论文还公开了高质量数据集和采集工具，推动后续研究。

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [165] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的不确定性感知分割框架，能够在分割脑肿瘤的同时输出每个体素的不确定性估计，并将肿瘤与正常脑组织进行统一分割，提高了预测安全性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分割深度学习方法未能提供预测错误的不确定性估计，且通常无法同时分割周边健康脑结构，这在手术规划和临床应用中是个重大缺陷。

Method: 本研究在nnUNet中增加一个用于体素级不确定性输出的通道，实现单次推理获取分割及其不确定度，并通过联合正常及肿瘤影像数据集实现全脑结构与肿瘤的统一分割。采用BraTS2023数据进行训练和评估。

Result: 增强后的模型能在不影响肿瘤分割准确率的前提下，准确预测分割不确定性（相关系数0.750、RMSD 0.047），全脑结构分割DSC为0.81，肿瘤分割DSC为0.86，重要区域均表现稳健。可输出直观的不确定性叠加分割结果。

Conclusion: 提出的模型首次实现了对肿瘤在大脑自然结构背景下的分割及对应不确定性输出，为临床提供更可靠的术前评估工具，不确定性可帮助评估预测可信度和纠错，有助于人工智能辅助下的精准外科决策。

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [166] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度递归网络（MSRNet），针对伪装目标检测中复杂场景下目标难以识别的问题，显著提升了检测小目标和多目标的能力，在多个基准数据集上取得了最佳或次优表现。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法在复杂背景、低光照、小尺度及多目标等场景下表现不佳，原因在于传统方法难以提取多尺度和全局上下文信息。本文旨在提升模型在上述困难场景下的检测准确性。

Method: 提出MSRNet网络，包括以Pyramid Vision Transformer为骨干提取多尺度特征，采用Attention-Based Scale Integration Unit实现选择性特征融合，设计Multi-Granularity Fusion Unit用于特征递归细化，并引入递归反馈解码策略加强全局语义理解，实现多尺度联合和特征递归优化。

Result: 在两个主流伪装目标检测数据集上获得了最优性能，在其他两个数据集上排名第二，验证了方法在小目标和多目标感知能力上的提升。

Conclusion: 联合多尺度信息提取与递归特征优化机制，显著提升了复杂场景下的伪装目标检测效果，为相关领域提供了新的技术方案。

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [167] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了SAGA，一个能够大规模追踪AI生成视频源的架构，不仅能区分真伪，还能追踪到具体的模型、团队和生成器，实验显示该方法高效且可解释。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，合成视频变得极为逼真，现有的真伪二分类检测方法已无法应对日益增长的滥用风险和需求，因此迫切需要对AI生成视频的来源进行精准溯源。

Method: SAGA框架提出多粒度归因方法，涵盖真实性、生成任务、模型版本、开发团队以及具体生成器。核心采用基于视觉基础模型的高效视频Transformer架构，结合创新的数据高效预训练与归因策略，仅需极少量标注数据即能取得与全监督相当的效果。同时提出T-Sigs方法，实现视频生成器可区分性的可视化解释。

Result: 在公开数据集上，SAGA显著优于现有方法，其0.5%的标注样本即可达到全监督性能，支持跨域归因分析。T-Sigs直观展示了不同生成器之间的时序差异，有效提升了解释性。

Conclusion: SAGA成为AI生成视频归因的新基线，有效提升了法证分析和合规监管的能力，并首次提供了可解释的归因特征，为打击AI视频滥用带来更强工具。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [168] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 该论文提出了Visual Chain-of-Thought（vCoT）方法，通过生成帧间事件描述提升多模态大模型对视频时序推理能力。vCoT显著提升了仅用图像训练的大模型在长视频问答任务的表现，而对视频微调模型提升有限，表明后者已隐式掌握时序信息。视频模型在静态任务中也展现出更优关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 以往多模态大模型主要通过拼接帧信息扩展至视频理解，但未有效利用视频的时序推理能力。作者希望探索视频微调对模型能力提升的本质，并提出新方法使图像大模型具备时序推理能力。

Method: 作者提出Visual Chain-of-Thought（vCoT），让模型在相邻帧间生成过渡事件描述，从而显式建模时序推理。通过系统性实验对比图像大模型与视频微调模型，在有无vCoT提示下对视频问答和静态推理任务进行评测。

Result: 使用vCoT能大幅提升图像模型在长视频问答任务的表现；而对于经过视频微调的模型，提升有限，说明其已隐式获得时序推理能力。此外，视频模型在静态关系视觉推理任务上也超越了图像模型基线。

Conclusion: vCoT能够显著提升仅基于图像训练的多模态大模型对视频时序推理的能力，而视频微调过程中，模型已自然掌握部分时序推理，无需额外vCoT提示。视频模型具备较强的关系推理泛化能力。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [169] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 本文提出了一种用于多视角动作识别的知识蒸馏框架ViCoKD，在部分视角重叠和有限标注条件下显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 多视角动作识别在多传感器系统普及的背景下很重要，但现实场景中各视角往往只部分重叠、输入模态有限、序列级标签稀疏，现有方法对此情境研究不足。

Method: 提出View-aware Cross-modal Knowledge Distillation (ViCoKD) 框架，包含：1）跨模态蒸馏机制，从完全监督多模态教师模型向只具有限定模态/标注的学生导知识；2）跨模态注意力适配器，使学生有效利用多模态相关性；3）视角感知一致性模块，通过人体检测掩码和加权Jensen-Shannon散度，在动作在不同视角部分可见时对其分布对齐。

Result: 在MultiSensor-Home真实数据集上，与当前主流蒸馏方法、不同主干网络及多种环境下对比，ViCoKD均表现优异，在有限条件下还能超越教师模型。

Conclusion: ViCoKD能显著提升现实中多视角、模态受限、标注稀疏情形下的动作识别性能，为实际部署多传感器分析系统提供了新思路。

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [170] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化、可解释的绘画创意评估框架，结合内容和风格两个维度，并通过多模态多任务学习方法实现，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前绘画创意评估过于依赖人工主观打分，既耗时又缺乏一致性。因此，有必要开发自动化、客观且可解释的评估方法，以提升效率和公正性。

Method: 作者基于已有的带有创意分数的数据集，新增了内容类别标签。然后，提出了一个融合多模态（内容与风格）、多任务学习的深度模型，能同时预测创意得分、分类内容类型，并提取风格特征。创新点在于引入了条件学习机制，使模型能依据风格和语义信号自动调整特征提取方式。

Result: 实验结果显示，该方法在创意分数预测上达到了目前最佳水平，并且能给出与人类评价高度一致的可视化解释，优于现有回归方法。

Conclusion: 本文的方法为绘画创意评估提供了高效、自动化且可解释的解决方案，展现出广泛应用前景。数据和代码将公开，有助于推动相关领域发展。

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [171] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR是一种新颖的动态稀疏机制，针对视觉自回归模型的计算效率问题，通过智能分配权重和token计算，提高了图像生成效率且几乎不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉自回归模型随着序列长度增加，计算量急剧上升；而静态裁剪会影响模型性能，因为它永久移除了权重或token，破坏了预训练的依赖关系。需要一种方法在保证性能的同时提升计算效率。

Method: 提出ActVAR：1）将前馈神经网络（FFN）分解为多个轻量专家子网络，并用可学习路由器根据输入内容为每个token动态选择专家组合；2）通过门控token选择器，只选择高价值token进行计算，并对未选中的token进行重构，以保持全局一致性；3）用两阶段蒸馏，利用原始VAR模型引导路由与门控策略学习，从而继承其知识。

Result: 在ImageNet 256x256基准测试上，ActVAR能够将FLOPs减少21.2%，同时模型性能基本无损失。

Conclusion: ActVAR可有效提升视觉自回归模型的推断效率，在保持模型容量和准确性的情况下减少计算量，在密集推理场景具有应用价值。

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [172] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种首创的直接基于本机HDR数据完成的3D场景重建方法，相较于以往依赖LDR或复杂预处理的技术，NH-3DGS可实现专业级高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法主要基于LDR数据，这不利于专业影视、虚拟制作等需求，且现有HDR重建方法依赖多曝光融合或逆色调映射，过程复杂且效果有限。随着原生HDR相机出现，亟需能直接利用这类数据的高效3D重建技术。

Method: 作者提出Native High dynamic range 3D Gaussian Splatting（NH-3DGS）方法，核心创新在于颜色表示采用亮度-色度分解，使得能够直接从原生HDR相机数据进行优化并在整个流程中保持HDR。

Result: 在合成和真实多视角HDR数据集上的实验表明，NH-3DGS在重建质量和动态范围保持方面明显优于现有方法。

Conclusion: NH-3DGS实现了从原生HDR捕获直接进行高保真3D重建，推动了专业数字媒体制作领域的HDR三维场景重建，并承诺开源代码及数据集。

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [173] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文针对脑MRI异常检测提出了一种基于频域分解预处理的新框架，通过频域特征提升无监督异常检测效果，在多种模型中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 脑部解剖结构多样且标注数据稀缺，使得有监督异常检测难以推广，当前主流的无监督方法通过对健康MRI添加噪声模拟异常，但这类异常缺乏临床真实病变的生物物理特征和形态复杂度，难以获得理想检测效果。因此，需要新的方法提升无监督异常检测在脑MRI中的表现。

Method: 作者首次系统性地分析了脑MRI中病理异常在频域下的表现，发现异常具有独特的可分辨频谱特征，且低频信号在人类健康扫描中保持一致。基于这一发现，提出了频域分解预处理（FDP）框架，将其与已有模拟异常方法结合，利用频域重建达到病变抑制和解剖结构保留的平衡。FDP可与主流异常检测方法无缝集成。

Result: 在多个主流无监督异常检测模型中（如LDM等）引入FDP，均显著提升了异常检测性能。以LDM为例，DICE分数提升了17.63%，并在多种基线方法中保持稳健提升。

Conclusion: 文中提出的FDP方法为MRI无监督异常检测提供了全新视角，通过频域分析和预处理，有效提升了检测性能和诊断可信度。FDP具备通用性和可集成性，为未来相关研究提供了新方向。

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [174] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: 该论文提出了DeepSport，这是首个专为多任务、多运动场景视频理解设计的端到端多模态大模型MLLM框架，并通过创新机制实现了对体育视频复杂性的高效推理和理解，取得了当前最优表现。


<details>
  <summary>Details</summary>
Motivation: 体育视频因其动态复杂、高速变化和规则推理需求，对现有大模型提出了很高挑战。而现有方法多局限于单一运动、特定任务或无训练推理，缺乏强泛化和推理能力。因此亟需能适应多运动、多任务的新一代视频理解框架。

Method: 提出DeepSport框架，通过主动、迭代式推理实现“与视频共思考”，并利用特制帧提取工具提升内容分析效率。提出数据蒸馏流水线，从10种数据源合成高质量CoT推理数据，创造78k训练集。训练上采用两阶段：有监督微调SFT+创新门控工具奖励的强化学习RL，优化模型推理过程。

Result: 在拥有6.7k问题的基准测试中，DeepSport显著优于主流专有与开源模型，刷新了多维度体育视频理解任务的最优成绩。

Conclusion: DeepSport树立了体育视频领域多任务推理与理解新基线，为面向多样化运动及复杂体育场景的视频大模型研究提供了坚实基础。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [175] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督和曲率增强的3D异常检测方法CASL，在无需专用异常检测设计的情况下，表现优越且具备良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前3D异常检测多为专门设计，难以泛化到其它3D任务；而通用自监督方法在异常检测中效果不佳，因此需研发兼具检测和泛化能力的新方法。

Method: 基于U-Net架构，提出曲率增强自监督学习框架CASL，引入多尺度曲率提示指导解码器重建点云坐标，通过简单的微调即可完成异常检测。

Result: 仅用每个点的曲率即可超过多种传统方法，CASL在无专用机制情况下，异常检测效果领先且在其它3D任务（如点云分类）同样泛化良好。

Conclusion: CASL框架利用点云曲率实现异常检测与通用表征，在提升检测性能同时兼顾多任务适应性，具有很强的应用前景。

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [176] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种针对多模态大语言模型（MLLMs）的新型微调策略，通过引入有益随机噪声进行高效的模态微调，有效提升模型跨模态表示能力，且参数开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs微调方法往往忽视了跨模态异质性，导致模型未能充分发挥潜力，因此需要一种能够更好适配各模态关系的微调新方法。

Method: 作者从变分推断角度重塑MLLMs的推理流程，设计了多模态噪声生成器（MuNG），该模块可分析图文对的跨模态关系并动态生成任务自适应的噪声，然后向冻结的MLLMs注入这些针对性的噪声，从而在极少参数增加的前提下实现高效微调。

Result: 在两个主流多模态大语言模型QwenVL和LLaVA上进行实验，结果表明该方法在仅需1~2%附加参数的情况下，超越了全参数微调及其他现有微调方法。

Conclusion: 注入有益噪声能有效抑制无关语义成分，显著提升跨模态对齐与下游任务表现，MuNG方法高效实用，且开源代码已公开。

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [177] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoordAR的自回归框架，用于仅凭单一参考视图而不是完整3D模型，进行未知物体的6D姿态估计。该方法在多个基准测试上表现优异，尤其在对称性和遮挡等难题下也表现出强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的物体6D姿态估计需依赖完整的3D模型，而新物体往往缺乏3D模型，限制了实际应用。虽然单参考视图方法被提出，但受卷积结构局限及不确定性建模不足，现有方法在全局一致性、对称或遮挡场景中表现不佳。

Method: CoordAR提出将参考图和查询图间的3D-3D对应关系表征为离散token地图，通过自回归和概率建模方式生成；创新点包括：1）提出对坐标图离散化的token化机制，使得对3D空间进行概率性预测；2）采用模态解耦编码，将RGB外观与坐标信息分别编码；3）设计自回归transformer解码器，结合位置信息和生成中的token序列进行姿态回归。

Result: CoordAR显著优于现有主流方法，在多个数据集基准测试取得更好成绩。其在对称物体、被遮挡或真实应用环境下依然表现出强鲁棒性。

Conclusion: CoordAR为单参考视图的6D姿态估计提供了有效和强鲁棒性的解决方案，为未建模新物体的相关任务、机器人和AR应用带来新可能。

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [178] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了CineCtrl框架，实现了对生成视频中的景深、曝光等专业摄影参数的独立精细调控。通过创新机制和数据集，提升了视频生成的艺术表现力。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型大多只支持相机运动控制，缺乏对景深、快门速度等摄影效果的细致调控，这限制了视频的艺术表达和氛围营造。

Method: 提出了CineCtrl框架，使用解耦的交叉注意力机制，将相机运动与摄影输入分离，实现参数的独立精细控制。此外，还设计了综合数据生成策略，包括仿真摄影效果和真实数据采集流水线，构建大规模训练数据集。

Result: 实验结果表明，CineCtrl能够生成高保真、具备用户所设定摄影效果的高质量视频，且保持场景一致性和细节丰富度。

Conclusion: CineCtrl首次实现了对生成视频中专业摄影参数的独立控制，显著拓展了生成模型在艺术与表达维度的应用潜力。

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [179] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种文本驱动的交通场景图像生成与编辑统一框架，提升了语义丰富度、视角多样性及小目标生成质量，在相关任务上性能领先。


<details>
  <summary>Details</summary>
Motivation: 当前在交通场景图像生成与编辑领域，存在语义不丰富、视角受限、图像质量低、文本与图像内容对齐差等难题，制约了智能交通应用场景的视觉数据支持。

Method: 提出统一框架，利用可控mask机制整合生成与编辑；引入车载和路侧多视角数据以丰富几何多样性；采用两阶段训练：大规模粗粒度数据预训练，细粒度数据微调；并设计面向小区域的mask加权损失，提升小目标生成质量。

Result: 大规模实验证明，该方法在交通场景下的文本驱动图像生成与编辑任务中取得了领先性能，表现优于现有方法。

Conclusion: 该方法显著提升了交通场景中的图像生成和编辑效果，特别是在细节保真和语义对齐方面，具有重要应用价值。

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [180] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: 该论文提出了PFAvatar方法，实现了从日常穿搭（OOTD）照片快速高质量地重建3D虚拟人像，具备更好的细节、鲁棒性和适用性。


<details>
  <summary>Details</summary>
Motivation: 目前从OOTD等复杂背景、多姿态照片重建3D虚拟人像仍难以兼顾细节保留、高还原度和操作效率，且现有方法多依赖分割与拼接，容易导致不一致与错误。作者旨在突破这一瓶颈，简化流程、提升质量和效率。

Method: 方法分两阶段：第一阶段基于少量OOTD照片微调一个感知姿态的扩散模型，结合ControlNet做姿态估计，并引入条件先验保持损失（CPPL）防止语言漂移，以端到端方式重建精细全身外观。第二阶段使用SMPL-X采样和多分辨率3D-SDS优化NeRF，作为虚拟人连续化表示，保证纹理与遮挡。整个流程无需资产拆分，且大幅提速。

Result: PFAvatar在重建质量、细节还原、对遮挡与截断的鲁棒性等方面优于现有方法，个性化建模时间缩短为5分钟，相较以往提速48倍。实验覆盖真实性、细节性和可应用性测试，均取得领先表现。

Conclusion: PFAvatar极大推进了从真实OOTD照片到高质量3D虚拟人重建的实用性，不仅提升技术指标，还支持虚拟试衣、动画复现等下游应用，展现出广泛实际价值。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [181] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于原型学习的新颖工业异常检测及分类方法ProtoAnomalyNCD，能够有效发现和聚类多类型未见异常，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法仅判断是否存在异常，无法发现和区分多种异常类型，且未充分利用图像先验，导致聚类方法效果不佳。本研究旨在解决这两大难题。

Method: 提出ProtoAnomalyNCD框架：1）利用Grounded SAM结合文本提示，定位物体区域以抑制背景杂波并作为异常分类的先验；2）设计Anomaly-Map-Guided Attention模块，通过引入Region Guidance Factor，使注意力能区分背景、物体及异常区域；3）在统一的原型学习框架下，实现未见异常类别的发现、聚类与多类型异常分类，并扩展到检测未见异常的统一任务。

Result: 在MVTec AD、MTD和Real-IAD等主流数据集上，所提方法在发现和分类多类型异常任务上，性能优于当前最先进方法（SOTA）。

Conclusion: ProtoAnomalyNCD有效利用区域先验与异常特征增强，不仅实现了多类型未见异常的发现和分类，还能够统一处理异常检测任务，具有较强的泛化与应用价值。

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [182] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出了一种基于半监督学习的HDR图像重建方法，通过不确定性掩膜筛除伪标注的不可靠区域，即便只使用6.7%的HDR真值也能达到与全监督方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的HDR重建方法依赖大量LDR-HDR图像对获取真实标注，但由于高动态范围图像采集困难，获取标注代价高，因此需研究如何在极少HDR真值下实现高性能的HDR重建。

Method: 采用teacher-student半监督框架，由teacher网络对无真值的LDR样本生成伪HDR标签，student模型用于学习。但为避免伪标签假阳性污染，利用不确定性掩膜机制，在像素和小块两个层面“屏蔽”不可信区域，仅用可信区域训练student模型。

Result: 该方法在仅用6.7%的HDR真值数据时，不仅超过了以往的低标注量算法，并且几乎达到最新全监督方法的表现。

Conclusion: 通过创新的不确定性掩膜半监督框架，大幅降低了对HDR标注数据的需求，推进了高效HDR图像重建方法的发展。

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [183] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: 本文提出了一种新的融合RNN和扩散模型的视频生成方法，显著提升了长视频生成的记忆保持能力和时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成超出注意力窗口大小的超长视频时，容易遗忘历史信息且时空一致性较差，主要原因是缺乏高效的记忆压缩与检索机制。

Method: 作者在扩散Transformer框架中引入了LSTM（RNN），将LSTM与注意力机制结合，实现了高效的历史信息保留和检索，并提出了新的Recurrent Autoregressive Diffusion（RAD）方法，在训练和推理阶段统一采用逐帧自回归方式维护记忆。

Result: 在Memory Maze和Minecraft等数据集上的实验表明，RAD框架比现有方法生成的长视频表现更优，且LSTM在序列建模方面展现出高效率。

Conclusion: RAD框架有效提升了超长视频生成中的记忆能力和整体一致性，为长时序生成任务提供了新的思路。

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [184] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: 提出了一种新方法DiffSign，用于生成对交通标志识别系统更具隐蔽性和通用性的物理对抗样本，显著提升了攻击的有效性和迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有物理世界对抗攻击方法要么伪装性弱、转移性差，要么泛化能力有限，难以威胁实际交通标志识别系统。作者希望设计一种更强、更实用的攻击手段。

Method: 提出DiffSign，结合T2I扩散模型、CLIP损失和mask prompt以聚焦和可控攻击，并加入两种新的风格定制方法以提升泛化和隐蔽性。系统在各种真实环境（距离、角度、光照、标志类型）下进行评测。

Result: DiffSign在多种真实世界条件下物理攻击平均成功率达83.3%，攻击转移性、隐蔽性、泛化能力均有显著提升。

Conclusion: DiffSign提供了对自动驾驶关键感知环节的更大安全挑战，说明T2I结合新机制可实现高效、强隐蔽、可迁移的物理对抗攻击。

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [185] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: 本文提出了EndoSight AI，一种基于深度学习的胃肠道息肉实时检测与分割系统，在Hyper-Kvasir公开数据集上取得了高精度和高速的出色表现。


<details>
  <summary>Details</summary>
Motivation: 在内镜操作过程中，对胃肠道息肉进行精准、实时检测对于结直肠癌的早期诊断和预防至关重要，但现有方法在检测准确率和实时性方面存在瓶颈。

Method: 构建了一种新的深度学习架构EndoSight AI，结合了多尺度检测和详细边界分割。系统训练过程中纳入了临床相关指标，还引入了新颖的温控训练，提升了模型的鲁棒性和效率。

Result: 在Hyper-Kvasir数据集上，息肉检测mAP达到88.3%，分割Dice系数最高达69%，且在GPU上实时推理速度超过35帧/秒。

Conclusion: EndoSight AI能够无缝集成至内镜工作流，有望提升胃肠道息肉的诊断准确率及临床决策效率，对胃肠道医疗具有重要推动作用。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [186] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: 本文提出了一种名为CalibrateMix的半监督学习（SSL）方法，通过有针对性的混合“易学”和“难学”样本，提高了模型的校准性能，同时保持甚至提升了分类精度。实验表明该方法在多个图像基准数据集上优于现有SSL方法。


<details>
  <summary>Details</summary>
Motivation: 当前半监督学习在图像分类中表现优异，但常面临模型预测置信度过高、校准性差的问题。虽然supervised mixup方法在校准方面取得了不错效果，但在SSL中简单混合伪标签会因伪标签不可靠导致性能下降。因此需要创新的SSL校准方法。

Method: 作者提出CalibrateMix方法，首先利用训练过程中数据表现动态，识别出“易学”和“难学”的样本；随后，有针对性地将这两类样本进行混合训练，而不是随机混合，旨在改善模型的校准能力，控制过度自信。

Result: 在多个标准图像数据集上的实验结果显示，CalibrateMix方法在降低期望校准误差（ECE）的同时，还能获得比现有主流SSL方法更高的分类准确率。

Conclusion: CalibrateMix能够有效提升半监督学习模型的校准性能，并保持或提升分类性能，为SSL模型的实际部署和可靠性提供了更优方案。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [187] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无须额外训练的图结构指导的概念擦除方法GrOCE，能够高效且精确地从文本到图像扩散模型中移除不良或受限内容，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的概念擦除方法依赖于成本高昂的微调，或采用粗略的语义分离方式，导致无法精准去除目标概念，而且容易损害无关内容，且适应性较差。因此，需要一种无需重新训练且能精细控制擦除效果的方法。

Method: 作者提出Graph-Guided Online Concept Erasure (GrOCE)方法，将概念及其联系建模为动态语义图，并包含三大模块：动态拓扑图构建用于递增构建语义图、适应性聚类识别通过多跳遍历计算相似性和识别群组、选择性边缘切断则对特定概念的边进行切断以实现保留整体语义的精细擦除。整个流程无须训练过程，完全基于图推理实现。

Result: 在概念相似度（Concept Similarity, CS）和Fréchet Inception Distance (FID)两项指标的大量实验中，GrOCE方法都展现了超过现有方法的性能，实现了高效、精确且稳定的概念擦除。

Conclusion: GrOCE无需重新训练，能够精细、适应性强地移除特定概念内容，同时很好地保持其他语义信息，是概念擦除任务的有效新方案。

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [188] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: 论文提出HiFusion框架，通过更细粒度和更具上下文感知的深度学习方法，从常规H&E染色切片中更加准确地预测时空转录组表达，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术能够在组织切片中对应基因表达与形态结构，但受限于高昂的成本和技术门槛，临床应用受阻。现有从H&E切片预测基因表达的计算方法存在对细胞异质性刻画不足、受形态噪声干扰等问题。

Method: 该方法提出HiFusion深度学习框架，包括两个核心模块：一是分层点内建模模块，通过多分辨率子块分解和特征对齐损失实现细粒度形态特征提取；二是上下文感知跨尺度融合模块，采用交互注意力机制，选择性采集相关的区域上下文信息，从而增强特征表达能力。

Result: 在两个空间转录组基准数据集上，HiFusion在2D切片级和更具挑战性的3D样本级场景中均取得了当前最优的预测性能。

Conclusion: HiFusion具备鲁棒、准确、高可扩展等优势，有望成为从常规组织病理切片推断空间转录组表达的有效工具，助力相关临床应用的落地。

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [189] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于形态复杂度的YOLO量化方法（MCAQ-YOLO），通过自适应分配比特数实现更高效、精度更高的目标检测。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络量化方法通常对图像不同区域采用统一的比特精度，未能利用视觉数据在结构和纹理上的异质性，降低了效率与精度。

Method: MCAQ-YOLO采用五种形态学指标（分形维数、纹理熵、梯度方差、边缘密度、轮廓复杂度）来度量局部图像的复杂度，并据此分配不同的量化比特数。同时引入基于课程的量化训练方案，逐步提升量化难度以优化训练过程。

Result: 实验证明形态复杂度与量化敏感性高度相关，MCAQ-YOLO在检测精度、收敛速度上均优于统一量化。在安全装备数据集上，MCAQ-YOLO以平均4.2比特和7.6倍压缩率取得85.6%的mAP@0.5，比统一4比特量化高3.5个百分点，平均只增加1.8ms计算开销。在COCO和Pascal VOC跨数据集实验中也表现出一致优越性。

Conclusion: 基于图像形态学复杂度自适应量化可以在计算受限和安全关键的视觉识别任务中提升效率与鲁棒性，是比统一量化更优的选择。

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [190] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种自动识别和转换场景中可关节化物体为可交互URDF模型的场景感知方法ArtiWorld，极大简化了三维资产转为机器人可用关节物体的流程。


<details>
  <summary>Details</summary>
Motivation: 当前大多数仿真3D资产是刚体，手动将其转换为带关节物体代价高且耗时，急需自动化工具提升机器人学习与交互仿真的资产规模与多样性。

Method: 作者提出了ArtiWorld流程，通过从文本场景描述定位候选可关节对象，结合3D点云数据和大型语言模型先验，以及为URDF设计的prompt，实现刚体到URDF关节物体的自动、高效重构，并保留原有3D形状。

Result: 方法在3D仿真单物体、全场景以及实物扫描数据三个层次进行了评估，均超越现有方法，在保持物体几何形状和捕捉正确交互性的同时，输出实际可用的URDF关节模型。

Conclusion: ArtiWorld为将大规模现有3D资产直接转化为机器人仿真环境中的可交互资产提供了可行方案，有望推动机器人模拟和学习环境的建设。

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [191] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: 本文提出Cluster-based Concept Importance（CCI）方法，利用CLIP模型自身的patch嵌入对不同区域聚类并分析其对预测的影响，在解释性和鲁棒性分析中创下新性能记录。还引入COVAR基准区分前景背景等因素，系统评估多种CLIP变体。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等对比式视觉-语言模型在零样本识别上表现优秀，但仍对背景等虚假相关性敏感，缺乏对模型决策的细致解释和诊断工具。现有的一些基准对性能下降的归因过于简单，不能区分背景、视角变化等多种影响。

Method: 提出CCI方法，利用CLIP的patch嵌入将图像空间区域聚为语义一致的簇、进行掩码，并量化不同区域对模型预测的贡献；结合GroundedSAM，实现判别模型依赖于前景还是背景。还新建COVAR基准，系统控制对象前景与背景，便于分析各因素对模型表现的影响。

Result: CCI在解释性相关的faithfulness基准上显著优于现有方法，在MS COCO检索的deletion-AUC等指标上提升超过两倍。结合COVAR基准，对18种CLIP变体做了细致对比，揭示了背景等以外的多种误差来源。

Conclusion: CCI与COVAR为VLMs模型的可解释性和鲁棒性研究提供了新方法和新工具，能够更精细地诊断模型依赖，并推动更稳健的视觉-语言模型的设计和评测。

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [192] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了UNSEEN框架，从泛化视角进行数据集筛选，通过未见样本的分数进行样本选择，取代传统拟合阶段分数方法。在多个公开数据集上实验表明，该方法能在减少30%训练数据的情况下保持性能无损，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习数据集规模不断扩大，训练成本急剧上升，数据集剪枝（即用更精简但保持信息量的子集训练模型）成为重要问题。传统方法多依赖于模型训练时期的拟合能力进行评分，导致分数区分性差，影响样本选择效果。

Method: 论文提出UNSEEN，可以无缝嵌入现有数据集剪枝方法中，其核心思想是通过模型在未见过样本上的表现对样本进行评分，避免了过拟合对分数分布的影响。同时提出多步增量式采样策略，不断用新模型对不同子集打分，动态优化核心集质量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K等基准测试中，UNSEEN在数据集规模显著减少时依然保持甚至提升模型性能。在ImageNet-1K上实现了训练数据缩减30%且性能无损。

Conclusion: 通过泛化视角评分与多步增量式采样，UNSEEN极大地提升了数据集剪枝的效率和效果，为大规模深度学习任务的高效数据利用提供了新思路。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [193] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种创新的方法WSAE-Net，用于提高非生成式视觉反事实解释的效率和语义相关性，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统非生成式视觉反事实解释方法在用替代图像片段替换目标图像时，往往忽视了替代区域与目标对象的语义相关性，导致模型可解释性下降且编辑流程低效。为了解决这个问题，作者提出了新方法。

Method: 提出加权语义图（Weighted Semantic Map）和自适应候选编辑序列（Auto-adaptive Candidate Editing Network, WSAE-Net）。该方法通过加权语义图优化特征单元的选择，减少非语义特征的计算量，同时自适应编辑顺序决定特征单元处理的最优顺序，以保证反事实生成的效率和语义相关性。

Result: 通过大量实验验证，所提出的WSAE-Net在语义相关性和解释性方面显著优于传统方法，且提升了计算效率。

Conclusion: WSAE-Net方法提升了视觉反事实解释的效率和语义合理性，有助于更清晰深入地解释模型决策，推动了该领域的发展。

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [194] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了一个基于扩散模型的统一图像润饰框架PerTouch，实现了细致、个性化的图像美化，并有效结合了用户主观审美和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前图像润饰任务难以同时兼顾用户个性化主观审美和编辑操作的可控性。因此，作者希望提出一个新的方法，提升个性化控制能力与视觉美学的双重满足。

Method: 作者设计了PerTouch框架，利用扩散模型支持语义级别的图像润饰，并通过参数图实现对特定语义区域属性的精细调整。训练过程中引入了语义替换和参数扰动机制加强模型对语义边界的感知。此外，设计了一个VLM（视觉语言模型）驱动代理，实现了自然语言指令到视觉控制的无缝连接，并配备了根据用户反馈重新思考和场景记忆模块，更好地对齐用户长期偏好。

Result: 大量实验表明，PerTouch框架在个性化图像润饰任务上显著优于现有方法，其各组件在提升主观性与可控性方面均取得了积极成效。

Conclusion: PerTouch方法在个性化、可控图像润饰领域表现优异，成功实现了语义感知、自然语言交互和用户长期偏好的融合，推动了高水平智能图像处理的发展。

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [195] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S是一个医疗分割领域的大模型，能够原生支持空间和文本多模态提示，并在多种成像模式下展示出色多类别分割效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有医学分割模型大多仅支持文本提示，空间感知能力不足，导致空间与语义信息对齐不精确，且多类别任务效率低。为此，研究者希望设计一个同时支持原生空间和文本提示的基础模型，提升精度与推理效率。

Method: Medal S采用端到端可训练架构，核心创新包括：1) 支持通道级对齐，将体积空间提示与文本嵌入匹配，缓解分辨率失配；2) 保留原生3D上下文，实现多类别、高分辨率并行分割；3) 轻量级3D卷积模块精准整合空间和文本信息；4) 支持243类别、全模态数据；5) 两种提示模式：自主文本提示和混合人工提示；6) 动态重采样、数据增强、多阶段推理等新式优化。

Result: 在BiomedSegFM数据集五种成像模式、243类任务中，Medal S显著优于传统方法（如SAT和nnU-Net），表现为DSC、NSD、F1、DSC TP指标全面提升，24类分割时推理速度提升90%以上。

Conclusion: Medal S结合空间精度与文本语义，引领多类别医学分割准确性与效率，适用于多种医学影像场景，将作为基础模型开放。

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [196] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story是一种无需训练即可实现多提示一致性文本到图像生成的新框架，解决了现有方法中人物身份和风格不统一的问题，速度远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多提示文本到图像生成方法在人物身份和风格一致性方面存在明显不足，且推理速度较慢，无法很好地满足实际视觉叙事的需求。

Method: 提出了基于尺度自回归模型的训练自由架构，并引入三项核心技术：身份提示替换（缓解文本编码器的上下文偏见，保证身份一致）、自适应风格注入及同步引导自适应（通过统一关注机制保证风格和身份外观在多提示下的一致性），无须微调，仅在测试阶段应用。

Result: 在多个实验中，Infinite-Story不仅在身份和风格一致性方面取得了最优效果，而且推理速度比现有最快的一致性T2I模型快6倍以上（每张图像1.72秒）。

Conclusion: Infinite-Story在保证生成质量、身份和风格一致性的同时大幅提升了推理效率，适用于实际的多提示视觉故事生成场景，具有很高的应用价值。

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [197] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调或额外知识的新方法SAGE，通过引导式提示选择缓解CLIP等大规模视觉-语言模型在零样本分类中的多模态虚假偏差，从而提升模型鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: CLIP等大规模视觉-语言模型在零样本分类时容易学习到多模态虚假偏差，即倾向于依赖于非本质特征（如背景），遇到分布外数据时表现极差，现有缓解方法普遍需要微调或先验知识，影响模型开箱即用性。

Method: 本文提出Spuriousness-Aware Guided Exploration（SAGE）方法，通过在无监督情况下探索提示模板空间，选择能最大化类别语义区分度的提示，从而引导模型减少依赖虚假特征，整个流程无需训练、微调或外部标注。

Result: 在四个真实数据集和五种主流模型上的大量实验表明，SAGE能够在不使用外部知识或对模型进行更新的前提下，持续提升零样本识别性能及泛化能力，优于此前的无监督方法。

Conclusion: SAGE为缓解大模型多模态虚假偏差提供了简单有效的新途径，提高了视觉-语言模型开箱即用的鲁棒性和可靠性。

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [198] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: 提出DTGS方法，通过与Retinex理论结合和加入热成像信息，实现了在极低光照下的新视角合成和三维重建，相较于现有方法，显著提升了几何、色彩和辐射一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的新视角合成方法在极端低光照情况下性能大幅下降，出现几何畸变、色差和辐射不稳定，常规的3D Gaussian Splatting等管线无法有效处理多视角低照度输入。此外，简单的预处理增强方案无法保证各视图间一致性，亟需更强鲁棒性的解决方法。

Method: 提出DTGS框架，通过Retinex启发的光照分离、热感指导的三维高斯分布优化联合训练，实现增强、几何重建和热成像监督的协同优化；循环增强-重建机制和嵌入的物理可解释Retinex模块确保不同视角下颜色、质感一致；专设热感分支动态平衡增强、结构及热损失，增强整体稳定性。

Result: 在自建的新RGBT-LOW多视角低光-热数据集上，DTGS方法在颜色一致性、几何准确性和辐射稳定性方面都大幅超越当前低光增强及三维重建主流方法。

Conclusion: DTGS框架有效解决了极端低光新视角合成中的一致性和准确性难题，为低光环境下的三维建模和重建提供了新思路，并通过热感辅助极大提升了复杂照明环境下的重建质量。

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [199] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对红外小目标检测任务的全新特征金字塔网络BP-FPN，改进了特征学习方式，并在多个数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测由于目标信号弱、与背景对比低，传统深度学习方法在特征表征上存在瓶颈，尤其是每帧的目标特征不明确。作者认为，突破点应在于优化基础特征表征而不是增强时空特征聚合。

Method: 作者提出BP-FPN网络，引入梯度隔离低层捷径（GILS），提升细粒度目标信息的整合效率，同时用方向性梯度正则化（DGR）在反向传播中保证分层特征一致性。该设计理论有据，计算开销小，易于集成到现有框架。

Result: 在多个公开红外小目标检测数据集上的大量对比实验显示，BP-FPN相较现有主流方法取得了持续、显著的性能提升，刷新了最新的最佳结果。

Conclusion: BP-FPN为红外小目标检测任务提供了全新的特征学习思路，是首个从反向传播视角设计的特征金字塔结构，为后续研究提供了新方向。

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [200] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: 该论文提出了GeoUniPS，一种结合了合成监督和三维大模型几何先验的通用光度立体网络，有效提升了在复杂、现实场景下的表面法线恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的通用光度立体方法在光照信息不足（如偏置光照、阴影或自遮挡）时表现不佳，尤其是在真实复杂场景下恢复表面法线的准确性受到限制。

Method: 作者提出GeoUniPS网络，将合成数据训练与大规模三维重建模型（在海量真实场景下预训练）中的高层几何先验结合。核心设计是一个光照-几何双分支编码器，分别从多光照线索和冻结的三维重建模型中提取信息。此外，作者提出新的PS-Perp数据集，采用更加真实的透视投影，替代原有的正交投影假设，从而学习空间变化的视角方向。

Result: GeoUniPS在多个公开数据集上，尤其是在复杂的野外场景下，无论定量还是定性评估均取得了当前最优的表面法线恢复性能。

Conclusion: 整合三维重建大模型的几何知识和多光照信息，可以有效弥补传统光度立体方法在难以判别区域的不足，提高通用性和准确性，是实现现实场景下鲁棒表面法线恢复的有力方案。

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [201] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 本文提出了一种高效的MeanFlow（MF）方法，结合Representation Autoencoder（RAE）潜空间与一致性训练和蒸馏技术，大幅提升了生成质量并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有MF在用于高维数据生成时，因训练不稳定且计算量大，推理消耗高且依赖复杂参数，限制了其实用性。

Method: 在RAE潜空间中训练MF，结合一致性中期训练、两阶段蒸馏和自举训练以稳定梯度并加速收敛，配合轻量解码器和无需引导的简化生成流程。

Result: 在ImageNet 256实现1-step FID 2.03，优于普通MF的3.43，采样计算量减少38%，总训练成本降低83%；在ImageNet 512上亦取得有竞争力的性能且GFLOPS最低。

Conclusion: 本方法在提升生成质量的同时大幅度降低了计算资源消耗与配置复杂度，实现了更高效可扩展的生成模型。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [202] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出了SpectralAdapt框架，通过半监督领域自适应方法，实现从RGB等常见图像模态重建高光谱图像，解决医疗领域人类高光谱数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像具备丰富的光谱信息，有很大医疗潜力，但高成本和数据采集难限制了其应用。现有公开数据多为通用场景，而专注于生物医学的人体高光谱数据稀缺，亟需利用现有资源，提升医疗方向高光谱图像重建能力。

Method: 提出半监督领域自适应（SSDA）框架SpectralAdapt。创新包括：1）光谱密度掩码（SDM）：基于RGB信道复杂度自适应掩码，引导一致性训练下信息丰富区域的重建；2）光谱端元表示对齐（SERA）：从少量有标签像素提取端元，作为无域差异锚点，指导无标签预测，并采用动量更新提升稳定性。两者结合，有效利用有限标签与大量无标签数据。

Result: 在基准数据集上实验，SpectralAdapt显著提升了光谱重建精度、跨域泛化能力与模型训练稳定性。

Conclusion: SpectralAdapt为高光谱图像医疗应用带来低成本、高效率的解决方案，有效缓解了领域差异、数据稀缺及光谱退化等难题，SSDA方法在医疗高光谱成像领域前景广阔。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [203] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: REVISOR是一种增强多模态大模型处理长视频理解能力的新框架，克服了仅依赖文本反思的局限，显著提升了模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态反思机制大多基于文本，难以有效理解和推理长视频内容，因为长视频信息丰富且动态，单纯文本反思无法充分利用视觉信息，且缺乏跨模态的信息整合能力。因此，需要一种既能处理长视频视觉特性、又能跨模态反思的新机制。

Method: 提出REVISOR框架，将工具增强的多模态反思机制集成到多模态大模型（MLLMs）中，实现文本与视觉信息的协同反思。设计了Dual Attribution Decoupled Reward（DADR）机制，在强化学习中引导模型重点回顾与问题高度相关的视频片段，实现模型推理与证据选择的因果一致。通过GRPO训练策略提升模型反思与推理能力。

Result: REVISOR在VideoMME、LongVideoBench、MLVU和LVBench四个长视频理解基准上取得了显著提升，无需辅助监督微调或外部模型。

Conclusion: REVISOR可显著提升MLLMs长视频理解和多模态推理能力，是解决仅文本反思不足的有效方法，并具备良好通用性和实用价值。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [204] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于视觉的3D语义场景补全方法Ocean，通过关注对象级别的特征，有效提升了复杂环境下的语义和几何预测精度，实验达到了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景补全方法大多基于自车视角，注重整体特征聚合，忽视了细粒度对象层面的信息，导致在复杂场景中语义和几何的不确定性增加。该研究旨在克服上述不足，通过对象中心化的方法提升语义占据预测的准确性。

Method: 1. 利用轻量级分割模型MobileSAM对输入图像提取实例掩码。2. 设计了3D语义组注意力模块，在3D空间中以线性注意力方式聚合对象级特征。3. 针对分割误差与缺失实例，引入全局相似性引导的注意力模块，实现全局特征交互。4. 提出实例感知的局部扩散模块，通过生成式提升对象特征，并优化BEV空间场景表达。

Result: 在SemanticKITTI和SSCBench-KITTI360数据集上，Ocean方法分别取得了17.40和20.28的mIoU得分，均达到或超越了当前最好水平。

Conclusion: Ocean方法通过对象中心化的特征处理与多级注意力机制，有效提升了3D语义场景补全任务的性能，验证了面向对象方法在复杂场景中的优越性。

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [205] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了Uni-Inter，一个统一的人体动作生成框架，能够应对多种互动场景，并在多个任务中展示了高效的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的人体动作生成方法多为面向特定任务，且泛化能力有限，难以同时支持人-人、人-物和人-场景等多样的交互场景。为了解决这个问题，作者希望构建一个能够统一泛化多交互类型的通用框架。

Method: Uni-Inter提出了一种统一交互体积（UIV）的体素表示方法，将各种异构的互动实体编码在同一空间域中。基于UIV，通过关节级的概率预测，实现动作生成，捕捉空间细粒度依赖，建模一致且复合的互动关系。整个系统无任务特异性，能够兼容多种互动对象。

Result: 在三个具有代表性的互动任务上进行实验，Uni-Inter表现出有竞争力的性能，并且对新的实体组合具有良好的泛化能力。

Conclusion: 统一建模复合互动关系能够有效推动复杂环境下的可扩展动作生成，对未来的大规模交互动作合成具有重要意义。

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [206] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种高效、轻量级的多语言视觉-语言对齐方法，仅需训练一个小型投影模块，在低资源语言上获得了显著的跨模态检索性能提升。


<details>
  <summary>Details</summary>
Motivation: 受限于获取高质量多语言图文数据，现有多语言视觉-语言模型在诸如捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语等低资源语言上的表现较差，有必要探索无需大规模图文配对的多语言对齐新方法。

Method: 提出一种无需图文对或文本对的对比学习框架，冻结预训练的图像编码器和多语言文本编码器，仅训练一个1.7M参数的紧凑投影模块，利用英文表征作为语义锚点进行对齐。

Result: 在多个多语言检索基准（尤其是XM3600等低资源语种）上，该方法相比现有模型在五种代表性低资源语言上取得了显著性能提升。

Conclusion: 基于pivot的高效参数对齐策略可在缺乏大规模监督的情况下实现更包容的多模态学习，尤其对低资源语言效果突出。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [207] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: 本文提出了一种用于开放词汇时序动作定位的新模型MGCA-Net，能更好识别和定位视频中任意动作类别，并在主流数据集上取得了最新的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇时序动作定位方法只在单一粒度下识别动作类别，导致对已知和新类别的识别精度下降。因此，迫切需要有效区分并精确识别多粒度的动作类别。

Method: 提出Multi-Grained Category-Aware Network（MGCA-Net），包括动作定位器、动作存在预测器、传统分类器和粗到细分类器。该方法通过多粒度类别意识：对基础类别用传统分类器处理，对新类别采用从粗到细的分级方式，首先在视频级判断动作，再在候选提案上精细分配类别，实现更精确的动作定位和识别。

Result: 在THUMOS'14和ActivityNet-1.3两个主流数据集上进行了全面实验证明MGCA-Net达到了SOTA效果。在Zero-Shot Temporal Action Localization场景下，MGCA-Net同样表现优异。

Conclusion: MGCA-Net实现了多粒度类别感知，大幅提升时序动作定位的性能，特别是在面对新类别时依旧保持优异表现。

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [208] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: 论文提出了两种新方法（Pretext-GRPO和ViSS-R1），提升多模态大语言模型对复杂视频推理任务的视觉理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视频推理任务中，常受限于以文本为主导的推理方式，导致未能充分利用视频中的丰富视觉信息，容易出现投机取巧或幻觉式错误。为强化模型的视觉中心理解能力，作者有意设计算法使模型被迫深入处理视频视觉信息。

Method: 1）提出自监督强化学习算法Pretext-GRPO，在视频输入被转换后设立前置任务，模型仅在正确解决前置任务时获得奖励，从而提升视觉信息处理能力。2）发展了ViSS-R1框架，将前置任务自监督学习融入多模态大模型后训练流程，要求模型不仅处理用户问题，还需识别并逆推视频转换过程、重建原视频，有效提升模型对视频本身的推理精度。

Result: 在六个主流视频推理与理解基准测试上，Pretext-GRPO和ViSS-R1方法均表现出明显优于现有方法的性能。

Conclusion: 引入前置任务驱动的自监督学习并明确强化对视觉变换的处理，能大大提升多模态大模型对复杂视频推理任务的视觉理解和生成准确性。此方法有望成为多模态视频推理领域的新范式。

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [209] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: 该论文提出了MonoUnc，一种新型的单目3D车道检测方法，能够更好地建模局部结构和观测噪声引起的不可约不确定性，并在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目3D车道检测往往受限于观测噪声带来的不可约不确定性（Aleatoric Uncertainty），而现有方法对几何和结构建模过于简化，难以精准应对真实场景中的结构变化和不确定性。

Method: 提出了MonoUnc方法，无需鸟瞰视角（BEV）变换，直接在前视图中以参数曲线建模3D车道线。通过曲线预测动态生成嵌入，用于3D空间中的车道点预测。每一段车道用3D高斯分布建模，并引入了结合局部结构和不确定性的匹配损失函数（3D Gaussian matching loss）。

Result: 在ONCE-3DLanes和OpenLane数据集上，MonoUnc的性能在更严格评测标准下全面超越了当前最新方法。同时，提出了更综合的评价指标，使用双向Chamfer距离来衡量全局与局部的检测误差。

Conclusion: MonoUnc有效提升了单目3D车道检测精度，能够更好地处理结构变化和噪声不确定性，推动了相关技术进步。

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [210] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种将自然图像视觉大模型（SAM2）知识迁移到电子显微镜（EM）神经结构分割的新框架，通过引入特征引导注意力模块与解码器，实现性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: EM神经结构分割任务面临形态复杂、信噪比低和标注稀缺等难题，限制了现有方法的准确率和泛化能力。本文致力于利用视觉基础模型（如SAM2）在大规模自然图像中学到的先验知识，提升EM分割性能。

Method: （1）利用在自然图像上预训练好的SAM2提取通用的高质量特征；（2）提出特征引导注意力模块，借助SAM2语义线索，引导轻量级的细粒度编码器（FGE）聚焦分割难点区域；（3）使用双亲和性解码器，分别生成粗粒度和精细亲和图，实现更优的区域分割。

Result: 模型在冻结SAM2权重的情况下表现与现有SOTA方法相当；经过对EM数据微调后，性能大幅超越目前所有主流方法，分割准确率等指标明显提升。

Conclusion: 本研究表明，将自然图像上预训练的大模型知识迁移，并配合有针对性的领域自适应改进，能够显著提升复杂EM神经结构的分割质量。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [211] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: RobustGait提出了一个细粒度的步态识别健壮性评测框架，从多个维度系统性评测主流外观特征步态识别方法在真实世界腐蚀与扰动下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于外观的步态识别方法虽然在受控数据集上表现良好，但其对于真实世界各种扰动和轮廓变化的鲁棒性评估尚不充分。本工作旨在填补该领域系统性健壮性评测的空白。

Method: 作者设计了RobustGait评测框架，针对数字扰动、环境扰动、时间扰动和遮挡四类干扰因素，结合不同的轮廓提取方法与步态识别模型结构，在多种部署场景和公开数据集下，引入15种扰动类型和5个强度等级，对六种顶级步态识别系统进行了评测，并分析扰动如何影响系统各个模块。

Result: 发现RGB层面的噪声更真实反映实际退化，并揭示轮廓提取器偏差是步态识别准确率敏感且易被忽视的误差源；不同扰动类型和模型结构影响系统鲁棒性，并验证噪声感知训练和知识蒸馏等策略可显著提升系统性能。

Conclusion: RobustGait揭示了步态识别系统在真实场景下面临的多重鲁棒性挑战，并提供了一套系统性的评测与提升方案，为部署级步态识别应用奠定基础。

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [212] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: 本文提出了一种名为AdaptiveAD的新型架构，通过解耦场景感知与自车状态，有效提升自动驾驶的泛化能力，并减少对自车状态的过度依赖。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶端到端系统模块化设计取得进展，但过度依赖自车状态，限制了场景理解能力和泛化性。作者发现问题根源在于现有架构设计，信息过早融合导致下游规划模块过多依赖自车状态。

Method: 提出AdaptiveAD架构，采用多上下文融合策略和双分支结构。一条分支专注于多任务驱动的场景理解，显式去除BEV编码器输入的自车状态；另一分支侧重于基于自车状态的任务规划。两路决策通过场景感知融合模块自适应整合。此外，引入路径注意力机制和两个辅助任务（BEV单向蒸馏、可自回归的在线映射）保障多任务学习。

Result: 在nuScenes数据集上，AdaptiveAD在open-loop规划任务取得了最优性能。评估显示该架构大幅度削弱了对自车状态的依赖，并在多场景中展现出很强的泛化能力。

Conclusion: AdaptiveAD通过结构性解耦和自适应融合，显著改善了现有方案过度依赖自车状态的问题，是提升自动驾驶系统场景泛化能力的有效方法。

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [213] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 该论文提出RFxG分类法系统整理显著性解释方法的目的和粒度，指出现有评估标准只关注点式信度，忽视对比推理和语义层次，并据此设计了四个新信度指标系统评估显著性方法。


<details>
  <summary>Details</summary>
Motivation: 显著性图作为深度学习可视化解释工具，但当前其用途与不同用户需求未达成一致，解释方法的目的和评估标准往往模糊，影响其实用性。

Method: 提出以“参考框架（点式/对比式）”和“粒度（类级/组级）”为两轴的RFxG分类法，并据此设计四个新的faithfulness（信度）评估指标。用这些指标对10种主流显著性方法、4种网络结构和3个数据集进行系统评估。

Result: 应用RFxG分类和新指标发现，主流显著性评估仅侧重点式信度，普遍忽略对比性和语义粒度。新框架和指标展示了更全面、贴合用户需求的解释能力。

Conclusion: 推动从用户意图出发评估解释方法，RFxG为解释方法的开发和评估提供更科学的体系和实践工具，促进深度学习可视化解释与人类理解需求更紧密结合。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [214] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: MergeSlide框架将数字病理全景切片图像的终身学习问题转化为模型合并问题，通过视觉-语言基础模型实现更高效的多任务学习，并在TCGA六个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 在病理切片分析领域，随着任务不断增多和数据体量庞大（WSI为GB级），现有方法需频繁传输和处理数据，效率低、资源消耗大。因此，亟需一种能够不断引入新任务同时规避灾难性遗忘，且适用于大规模数据的终身学习方法。

Method: 提出MergeSlide框架：（1）每有新任务时，用类感知的prompt定义任务，采用无MLP主干微调数轮；（2）借助正交连续合并策略，将新任务高效地合并进统一模型以保留历史性能，缓解灾难性遗忘；（3）针对类增量学习的推断场景，创新提出Task-to-Class Prompt-aligned（TCP）推断流程，先任务识别再用相应prompt预测类别。

Result: 在六个TCGA数据流场景下，MergeSlide显著优于基于重放的持续学习方法和视觉-语言零样本方法。

Conclusion: MergeSlide为病理全景切片的终身学习提供了高效、易用、性能优良的统一框架。该方法有效缓解灾难性遗忘，且推断高效，在病理图像分析多任务持续学习中具有明确应用价值。

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [215] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: 论文指出现有CAPE方法采用静态文本关键点描述作为语义先验，在鲁棒性和灵活性提升的同时存在跨类概念歧义和细粒度区分性不足。作者提出CapeNext框架，通过层次化跨模态交互和双流特征优化，融合类别和实例层面信息，有效改善性能。实验证明CapeNext在MP-100数据集上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分析发现基于静态文本嵌入的现有CAPE方法存在两个核心问题：一是同一语义点（如“腿”）因多义性导致不同类别（人、家具）间的区分混淆；二是对于同类内部的细粒度关键点变异（如体态、毛色）区分力不足。为突破这两项限制，亟需创新融合更丰富语义和实例区分信号的方法。

Method: 作者提出CapeNext框架，创新性地将层次化的跨模态语义交互和双流特征细化机制结合进静态嵌入流程，通过文本和图像分别提取类别级与实例级信息，并进行联合优化，使得匹配更加精准和具有区分性。

Result: 在MP-100数据集上，CapeNext在不同网络骨干结构下，均显著优于现有最先进的CAPE方法，表现出更强的泛化能力和细粒度区分力。

Conclusion: CapeNext有效解决了静态嵌入导致的语义歧义和区分性不足问题，为类无关姿态估计领域提供了更鲁棒和灵活的解决路线，具有较大应用和研究价值。

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [216] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出了一种名为PlugTrack的新型多目标跟踪方法，可以自适应融合经典Kalman滤波器和数据驱动的运动预测器，显著提升了不同数据集上的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪主要依赖Kalman滤波器（适合线性运动），但对于非线性运动效果不佳，而数据驱动预测器虽然能处理复杂运动却存在泛化能力差和计算代价高的问题。实际场景中两类运动模式共存，二者各有优劣。

Method: 提出PlugTrack框架，利用多感知运动分析，根据场景动态自适应融合Kalman滤波器和数据驱动预测器，通过自适应混合因子来实现优势互补。PlugTrack无需修改现有运动预测模块，即可大幅提升跟踪效果。

Result: 实验证明即使在非线性运动占主导的数据集中，Kalman滤波器在34%的案例中仍优于数据驱动预测器。PlugTrack在MOT17/MOT20、DanceTrack等基准上达到新的SOTA表现，显著增强了跟踪准确率。

Conclusion: PlugTrack首次实现了经典与现代运动预测范式的自适应融合，在保证高效性的同时提升了泛化能力，为多目标跟踪的实际应用提供了更强的解决方案。

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [217] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对医学图像增强的低层次数据集蒸馏方法，通过构建解剖结构先验和结构保持个性化生成模块，实现高效隐私保护的数据压缩和共享。


<details>
  <summary>Details</summary>
Motivation: 医学图像增强依赖大量高质量训练数据集，但大规模数据集的存储和训练成本高且涉及隐私风险。已有的数据集蒸馏方法多用于高层任务，难以处理对像素级保真度要求高的低层任务。作者希望找到一种适用于低层任务、能够缓解数据负担并兼顾隐私保护的数据集蒸馏方案。

Method: 作者首次提出医学图像增强领域的低层次数据集蒸馏方法。先利用患者间的解剖相似性，以具有代表性的患者数据构建共享的解剖学先验，并作为不同患者蒸馏数据的初始化。设计了结构保持个性化生成（SPG）模块，将患者特异性解剖信息集成到蒸馏数据中以保证像素级保真度。根据不同任务需要，利用蒸馏数据合成高低质量训练对，通过梯度对齐方式将患者知识注入蒸馏数据。

Result: 该方法能够在不泄露原始隐私数据的条件下，为医学图像增强等低层任务生成高效、小规模且具有代表性的数据集。

Conclusion: 本工作首次实现了适用于低层像素级医学图像增强的隐私保护数据集蒸馏方法，在有效压缩数据的同时保证了隐私安全和像素级表现，为实际部署提供了新的解决思路。

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [218] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出了一种新的方法DGS-Net，有效检测AI生成图片，在保持预训练优势的同时提升泛化能力，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着GANs和扩散模型等生成式模型的发展，AI生成图片日益泛滥，带来误导信息、隐私泄露和信任危机。当前使用CLIP等模型检测此类内容，但细调易导致遗忘原有能力，影响跨领域泛化，需要新方法解决这一问题。

Method: 提出Distillation-guided Gradient Surgery Network（DGS-Net），通过梯度空间分解分离优化过程中的有益和有害方向，将任务梯度投影到有益方向，并利用冻结的CLIP编码器蒸馏获得有益梯度，实现预训练知识保留和任务无关信息抑制的统一优化。

Result: 在50个生成模型上进行大量实验，DGS-Net比最新方法平均提升6.6个百分点，在检测性能和跨领域泛化上均取得更好表现。

Conclusion: DGS-Net能在保持预训练知识优势的同时更好检测AI生成图像，为生成内容检测领域提供了更强大的解决方案。

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [219] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的图像去雾方法，通过隐式神经退化表示，提升了在复杂场景下的去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理雾分布不均的复杂场景时，精细特征表示和全局一致性建模难以兼得。此外，期望能更好学习空间变化下雾的通用退化特征。

Method: 1. 基于Kolmogorov-Arnold表示定理，提出结合通道独立与通道相关机制的网络结构，增强非线性依赖建模能力。2. 设计隐式神经表示，将雾退化建模为连续函数，减少对显式特征提取和物理模型的依赖。3. 引入稠密残差增强模块，进一步提升隐式特征建模和去除冗余信息的能力。整体方法为无监督学习。

Result: 在多个公开和真实数据集上，所提方法在去雾任务中获得了具有竞争力的性能表现。

Conclusion: 所提出的隐式神经退化表示去雾方法有效提升了复杂场景下的去雾质量，解决了传统方法在细粒度和全局建模上的不足，并且具有实际应用价值。

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [220] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: 本文提出了MPHM网络，通过多种先验信息结合，显著提升了图像去雨任务中的语义与结构还原能力，实验验证了其在benchmark数据集及真实雨景下的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像去雨方法在语义和空间细节还原方面存在不足，特别是在涉及复杂语义与结构信息场景下表现不佳，因此需要一种能够融合多源先验以提升还原质量的方法。

Method: 作者提出了多先验分层Mamba（MPHM）网络，创新性地融合了CLIP提供的语义先验和DINOv2提供的结构先验，通过渐进式先验融合注入（PFI）机制在不同解码层有策略地融合，结合层次型Mamba模块（HMM）实现全局与局部特征的同时建模。

Result: MPHM在Rain200H数据集上较现有方法提升了0.57 dB PSNR，并在真实雨景图像上展现出更好的泛化能力。

Conclusion: MPHM网络能够有效融合异构语义和结构先验，通过创新的网络设计和多级注入方式，在图像去雨任务上实现了业界领先的性能和优良泛化性。

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [221] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种旋转不变特征（RIF）框架，用于提升3D点云异常检测在旋转和位移情况下的鲁棒性，并在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法在点云方向和位置变化时，提取的特征不一致，导致检测性能下降，因此亟需一种对旋转和位移鲁棒的方法来保证检测效果。

Method: 提出Point Coordinate Mapping (PCM)方法，将点云映射到旋转不变的表示空间，再设计轻量级卷积特征网络（CTF-Net）提取旋转不变特征，并利用3D数据增强和迁移学习对特征提取器进行预训练。

Result: 在Anomaly-ShapeNet和Real3D-AD两个数据集上，分别获得平均P-AUROC提升17.7%和1.6%；融合传统特征方法进一步验证了方法的通用性。

Conclusion: RIF框架有效提升了3D异常检测的鲁棒性与准确率，在工业场景具备广阔应用前景。

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [222] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: 本文提出了CloseUpShot，一种基于扩散模型的稀疏视角近距离新视图合成框架，有效提升了3D重建和近距离新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 面对稀疏输入视角的3D场景重建和新视角合成任务，现有视频扩散模型在近距离、细节丰富的场景下表现不佳，主要受限于输入信息稀缺和像素条件失真。因此，迫切需要一种针对近距离场景的新方法，提升重建和新视点生成质量。

Method: 提出CloseUpShot框架：1）采用分层像素变换与遮挡感知噪声抑制增强条件图像的质量和完整性；2）引入全局结构引导，将稠密点云信息嵌入扩散过程，提供全局几何一致性，实现对稀疏条件输入下信息缺失的补偿。

Result: 在多个数据集上的实验表明，与现有方法相比，CloseUpShot在近距离新视点合成任务上有明显性能提升，合成出的图像质量和细节均优于对手方法。

Conclusion: CloseUpShot框架通过多层次像素变换和全局结构引导，有效解决了稀疏视角下新视点合成面临的条件图像稀缺和细节丢失问题，是解决近距离新视点生成的有效途径。

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [223] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: 该论文提出了RePo方法，通过联合编码轨域（region-wise）和点级（point-wise）特征，大幅提升了轨迹相似性建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有基于学习的方法已经减少了传统轨迹相似性计算的复杂度，但最先进的方法仍未能充分利用轨迹信息的全貌，影响了相似性建模的效果。

Method: RePo方法包括两大核心：一是将GPS轨迹映射为网格序列，通过结构与视觉特征获取空间和语义上下文；二是通过三个轻量专家网络提取点级的局部、相关性和连续运动模式，最后用router网络自适应融合并与区域特征用交叉注意力结合，获得最终轨迹嵌入。训练时采用带有难负样本的对比损失函数。

Result: 实验显示，RePo在所有评估指标上较SOTA基线平均提升了22.2%的准确率。

Conclusion: RePo能够显著提升轨迹相似性计算的效果，对空间轨迹分析具有实际和广泛的应用价值。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [224] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的针对文本生成视频（T2V）模型的越狱（jailbreak）攻击方法，能够通过伪装为正常的文本提示，诱使模型生成语义上不安全的视频内容，从而绕过传统的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有T2V越狱攻击通常添加明显的对抗性扰动，容易被检测防御。作者想研究更隐蔽、难以察觉的提示，利用T2V模型的跨模态关联偏置，探索隐藏式攻击方式，以揭示模型的深层安全隐患。

Method: 作者提出VEIL攻击框架：通过组合“三段式”模块化提示（中性场景锚点、潜在听觉触发器和风格调节器），以引导T2V模型隐晦地产生不安全内容。具体地，将屏蔽意图中的场景描述作为表层，中性音效描述作为潜在诱因，电影风格指令加强触发效果，并用引导搜索优化提示的隐蔽性与攻击效果。

Result: 在7个主流T2V模型上进行大量实验，结果显示VEIL攻击能够有效绕过安全防护，使商用模型的攻击成功率平均提升23%。

Conclusion: 论文证明主流T2V模型存在利用跨模态关联实现隐蔽安全威胁的漏洞，现有防护手段对该类型攻击防御有限，提示未来需重视跨模态安全性及防御技术。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [225] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于室内照明的对抗性攻击方法，用以揭示视觉-语言导航（VLN）智能体在真实照明变化下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 目前对VLN智能体的鲁棒性研究不足，相关对抗攻击大多依赖非常规纹理，这些与现实环境不符，对实际应用价值有限。照明作为室内环境的内在属性，对导航影响却被忽视。

Method: 作者提出了一种黑盒框架——室内照明对抗性攻击（ILA），通过调控环境的全局照明来影响VLN智能体，包括静态（SILA, 灯光强度恒定）和动态（DILA, 灯光在关键时刻切换）两种攻击模式。

Result: 实验在两种主流VLN模型、三项导航任务上验证了ILA，有效提升了智能体失败率并降低了路径效率，显示出VLN模型对现实照明变化的显著脆弱性。

Conclusion: VLN智能体在真实室内照明变化下具有未被重视的弱点，未来相关系统需关注并提升对照明变化的鲁棒性。

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [226] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: 本文提出了MedGEN-Bench，一个综合性的医学多模态评测基准，用于推动医学视觉-语言模型（VLMs）的研究和应用。


<details>
  <summary>Details</summary>
Motivation: 目前广泛使用的医学视觉基准存在问题，包括查询语句与图像内容相关性不足，过于简化复杂诊断推理，将评价重点置于文本生成而忽略图像生成能力。这影响了医学AI在真实临床流程中的应用价值。

Method: 作者建立了MedGEN-Bench，包含6422组专家验证的图文配对，涵盖6种成像模态、16大临床任务和28个子任务。该基准包括视觉问答、图像编辑和情境多模态生成三类任务，强调上下文相关指令和开放式生成输出。作者还设计了结合像素级指标、语义文本分析和临床专家评分的三层评估体系，系统评测了多种模型。

Result: 通过MedGEN-Bench，对10种组合框架、3个统一模型和5个VLM进行了系统评估，初步结果反映出现有方法在跨模态推理和开放式生成等方面的优势与不足。

Conclusion: MedGEN-Bench能够更好地反映医学AI系统在真实场景下的能力，推动更加贴近临床需求的模型发展，并为后续模型设计和评估提供了标准工具。

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [227] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba是一种新颖的Mamba架构3D特征提取骨干网络，通过多尺度自适应和窗口偏移策略在保证效率的同时增强了空间依赖捕捉能力，并在主流数据集（KITTI与Waymo）上显著提升了3D目标检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统的3D目标检测方法在效率与长距离空间依赖捕捉之间难以取得兼顾，尤其是现有Mamba方法由于只在固定窗口轴对齐扫描，丢失了空间信息。该工作旨在兼顾检测性能与计算资源，解决现有Mamba方法空间信息损失的问题。

Method: 提出了WinMamba骨干网络，其中每个WinMamba块内含窗口尺度自适应模块（可在采样时自动平衡各分辨率体素特征），并引入可学习位置编码与窗口移动策略，有助于在线性状态空间中捕获丰富上下文。

Result: 在KITTI和Waymo公开数据集上的大量实验显示，WinMamba在准确率方面明显优于基线模型。消融实验进一步证明WSF和AWF模块对提升检测精度具有重要作用。

Conclusion: WinMamba能够在保证高效推理的前提下增强3D空间依赖表达，显著提升了3D目标检测表现，是高效和精度兼容的新型解决方案。代码即将开源。

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [228] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: 这篇论文提出利用计算机视觉方法提升道路损坏检测效率，并验证了GAN生成的合成数据对模型训练的有效性，MaskFormer模型在mAP50和IoU指标上优于CNN。


<details>
  <summary>Details</summary>
Motivation: 美国基础设施状况普遍不佳，道路系统尤其落后，而传统人工或激光检测方法效率低、成本高，亟需新技术提升道路监测和维护水平。

Method: 首先利用GAN生成道路损坏的合成数据以增强训练数据集，然后采用卷积神经网络（CNN）进行道路损坏分割，最后引入Transformer架构的MaskFormer模型进行实验对比。

Result: 实验显示，加入GAN生成的数据能提升分割模型的检测效果，且MaskFormer在mAP50和IoU两项评估指标上均显著优于传统的CNN模型。

Conclusion: 基于GAN增强数据集和MaskFormer模型的道路损坏分割方法能有效提升检测性能，为基础设施智能养护和管理提供了技术方案。

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [229] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的基于骨骼驱动的多模态预训练框架，显著提升了视频行人再识别的精度，突破了传统依赖文本的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频行人再识别多依赖视频-文本对，但文本难以精细捕获时序运动细节，同时缺乏真正的多模态预训练，影响了模型效果和泛化。

Method: 提出CSIP-ReID方法，通过两阶段策略：第一阶段用对比学习在序列层面对齐骨骼与视觉特征；第二阶段引入动态原型融合更新器（PFU），融合运动和外观信息精炼多模态身份原型。同时提出SGTM模块，将骨骼的时序信息引入视觉特征。

Result: 该方法在MARS、LS-VID、iLIDS-VID等主流视频ReID基准上取得了新的SOTA，并在骨骼ReID任务（如BIWI，IAS）上展现出强泛化能力，显著优于现有方法。

Conclusion: CSIP-ReID开创了无标注、关注运动特征的多模态预训练新范式，为行人再识别及多模态表征学习开拓了全新方向。

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [230] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法SOMA，通过融合结构梯度先验和混合匹配策略，有效提升了SAR与光学影像的像素级配准精度。


<details>
  <summary>Details</summary>
Motivation: SAR与光学影像因成像原理和视觉特性差异巨大，导致像素级配准极具挑战。虽然深度学习在多模态任务中有成功应用，但对SAR-Optical配准的效果欠佳，尤其没有充分利用梯度信息。

Method: 作者提出了SOMA框架，包括两大核心组件：一是特征梯度增强器（FGE），通过多尺度、多方向梯度滤波器结合注意力机制，将结构梯度信息注入深度特征；二是全局-局部仿射流匹配器（GLAM），在粗到细架构下结合仿射变换与流的精细调整，实现结构一致与局部精度提升。

Result: 在SEN1-2与GFGE_SO数据集上，SOMA将CMR@1px指标分别提升了12.29%和18.50%，展现出精度的显著提升。同时方法对不同场景和分辨率具有良好的泛化能力与鲁棒性。

Conclusion: SOMA框架有效融合梯度信息和混合配准策略，为SAR-光学影像配准任务带来了明显的性能进步，并具有较强的实用性和鲁棒性。

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [231] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: 本文提出了一种基于持久同调拓扑数据分析的新型医学图像检索方法THIR，无需监督训练即可高效、准确地对乳腺组织病理图像进行检索，在BreaKHis数据集上超越现有方法，并大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的重要原因，早期诊断和精准决策至关重要。现有基于深度学习的图像检索方法依赖大量标注数据和高性能硬件，限制了在实际临床环境中的推广。因此需开发高效、资源友好且无需监督的方法。

Method: 提出THIR框架，基于拓扑数据分析，利用持久同调中Betti数对图像的结构特征进行建模。具体做法为：直接对RGB病理图像用cubic persistence提取拓扑指纹（演化的循环结构），编码为可解释的特征向量，通过拓扑描述符间距离实现图像相似性检索，无需标注或训练。

Result: 在BreaKHis乳腺组织病理图像公开数据集上的实验显示，THIR在检索准确性上优于当前主流的有监督和无监督方法。整个数据集在标准CPU上处理时间不到20分钟，显示出极高的效率和可扩展性。

Conclusion: THIR为临床病理图像检索提供了无需训练、快速且精确的解决方案，资源需求低，有望推进CBMIR系统在实际医疗环境中的广泛应用。

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [232] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波分解的高频引导扩散网络（HDW-SR），通过专注高频残差图和小波频域分解，实现了更精细的单幅图像超分辨率重建，在合成及真实数据集上表现突出，特别擅长恢复细节。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在单幅图像超分辨率任务中对高频信息引导不足，容易导致图像细节模糊。因此有必要设计新结构以增强网络对高频细节的重建能力。

Method: HDW-SR用小波分解替换了常规U-Net骨干，并对残差图实施扩散以聚焦高频信息。采用小波分解获得多尺度高低频子带，并通过稀疏注意力机制将预超分辨图的高频子带与扩散图像的低频子带结合，提升显式高频细节恢复。引入动态阈值模块优化稀疏注意过程中高频选择，并利用小波可逆性在上采样阶段实现低损重构。

Result: 在合成和真实超分辨率数据集上，HDW-SR表现出与当前最优水平竞争的性能，尤其在细粒度图像细节还原方面有明显提升。

Conclusion: 通过在扩散框架中引入高频引导与小波域处理，HDW-SR能更加有效地恢复高频图像细节，是高质量单幅图像超分辨率的一项有前景的方法。

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [233] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 本文提出了GenTract，一种用于全局纤维束追踪的生成式模型，相比现有方法显著提升了精度，尤其在低分辨率和噪声数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有局部追踪方法在噪声或低分辨率下易积累误差，误报率高；而全局方法虽精度高但计算复杂，缺乏高效且精确的全局追踪解决方案。

Method: 将白质纤维束追踪任务建模为生成任务，利用生成模型从dMRI直接生成符合解剖结构的纤维束。对比了基于扩散和flow matching的范式，并与SOTA基线方法进行了评估。

Result: GenTract精度为现有最优方法TractOracle的2.1倍，在低分辨率和噪声环境下甚至高出数量级，性能优于同类方法。

Conclusion: GenTract兼具高分辨率条件下高精度和低分辨率下的鲁棒性，为全局追踪提供了高效、可靠的新方案。

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [234] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文提出了一种结合大规模解码器及视觉信息的极端多标签分类方法ViXML，通过高效整合视觉基础模型，实现了多模态能力并显著超越此前SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在诸多AI领域产生了革命性影响，其在极端多标签分类（XMC）领域的潜力尚未被充分挖掘，尤其是在如何利用大规模解码器和融合视觉信息方面。既要追求性能，也要兼顾计算效率。

Method: 1. 研究如何在XMC任务中有效利用大规模解码器；2. 设计ViXML框架，将视觉基础模型通过单一图像嵌入的方式高效整合到XMC流程，控制计算开销；3. 扩展现有数据集以包含视觉元数据，支持多模态学习与评测。

Result: 大规模解码器可在保持合理计算量的同时带来显著性能提升。ViXML利用小型编码器与视觉信息融合，优于绝大多数文本专用大解码器，部分任务P@1提升高达8.21%。

Conclusion: 有效利用大规模解码器和视觉信息能大幅提升XMC任务表现。ViXML框架在保持效率的前提下，充分挖掘多模态基础模型潜力，为领域发展指明了方向。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [235] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Object-Centric 3D Rollout (OCR)的新方法，通过结构化扰动视频中对象的3D几何结构，提升多模态大模型在动态场景下的空间推理能力。实验结果表明，该方法在VSI-Bench数据集上取得了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视觉-语言理解上表现强大，但对动态3D场景下对象位置、朝向与关系的空间推理仍存在不足。现有方法存在 '查询锁定' 问题，难以全面利用场景信息，这成为进一步提升空间推理能力的瓶颈。

Method: 作者提出OCR方法：在训练阶段，有选择性地对视频中目标对象进行3D几何扰动，并投影到2D空间，从而削弱对象具体的视觉线索，促使模型需全局性地推理场景。同时，训练流程结合原始和局部扰动的视频，加深模型对空间关系的理解。

Result: 实验结果显示，所提出的3B参数模型在VSI-Bench上达到了47.5%的准确率，优于多个7B参数的主流基线。此外，消融实验进一步验证了OCR相较于先前rollout策略（如T-GRPO、NoisyRollout）的优势。

Conclusion: OCR极大提升了视频场景中对象间空间推理的能力，为多模态大模型在复杂动态场景下的理解能力提供了新思路，具有重要的研究和应用价值。

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [236] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 本论文提出了一种可微分的笔触重建框架，实现了高质量、多风格的数字绘画仿真，可还原人类绘画与晕染流程，结合笔触、材质与色彩优化，效果超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的生成模型在绘画仿真中多仅关注最终成图或简单模拟绘画过程，难以表现真实笔触结构与流畅、真实的色彩晕染，因而无法完整还原人类的绘画过程。

Method: 提出一个可微分笔触重建框架，综合绘画、风格化纹理与晕染模拟。方法先用可微的并行绘画渲染器优化贝塞尔曲线单色和双色笔触，后用风格生成模块基于几何信息生成多样纹理。引入可微分的晕染算子，实现自然的色彩混合与过渡。采用粗到细的联合优化，整体兼顾笔触几何、色彩与纹理表达。

Result: 在油画、水彩、墨画及数字绘画等多种数据集上实验，结果显示该方法可重建真实且富表现力的笔触、平滑的色调过渡与丰富的风格化效果，优于以往仅关注最终成图的生成方法。

Conclusion: 本方法实现了笔触、材质、色彩的统一建模，能够高质量地还原人类绘画—晕染循环，为数字绘画提供了表现力强且多风格的生成工具。

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [237] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: 本文提出了一种新的单目3D目标检测框架MonoDLGD，通过难度感知的标签引导去噪方法提升检测效果，并在KITTI数据集上取得了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测因深度信息不充分导致本质上的问题，现有方法对检测难度的实例特性关注不足（如遮挡、距离、裁剪），从而影响精准检测。

Method: MonoDLGD是一种难度感知的标签引导去噪框架。其核心思想是根据检测难度对标签进行扰动：对容易检测的实例施加强烈扰动，对难检测的实例施加较弱扰动，并通过重建过程让网络学习几何信息，提升不同复杂度目标的检测鲁棒性。

Result: 实验在KITTI基准测试集上进行，MonoDLGD在所有难度等级上均取得了新的最优检测性能。

Conclusion: MonoDLGD能够有效提升单目3D目标检测的表现，尤其在处理不同检测难度实例时表现更为鲁棒，体现出强泛化能力。

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [238] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: 本论文提出了一种自监督方法，可从超声设备显示器的照片中自动提取图像，实现无需DICOM格式的超声图像获取。


<details>
  <summary>Details</summary>
Motivation: 传统超声图像依赖DICOM格式传输到医院系统，这一流程繁琐且不利于快速测试和算法原型开发。为解决DICOM瓶颈，提升算法开发效率，提出新的图像获取方法。

Method: 通过自监督的学习方法，从拍摄显示器的照片中提取和校正超声图像，并进行处理和评估。

Result: 校正后的照片图像在心脏视图分类任务上的平衡准确率达到0.79，与原生DICOM图像相比具有较高的视觉保真度。

Conclusion: 该方法可以绕过DICOM流程，实现快速高效的超声图像获取，促进新算法的测试和开发。

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [239] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出了RefineVAD框架，利用运动感知和类别引导两大模块，以更好地进行弱监督视频异常检测，突破将所有异常简单归类的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督视频异常检测方法只用视频级标签，但通常将所有异常事件看成一个类别，无法细致地区分实际场景下多样的异常类型与时间动态，因而检测效果有限。

Method: 提出RefineVAD框架，包括两个核心部分：1）MoTAR模块通过运动显著性和Transformer动态调节时间焦点；2）CORE模块通过交互注意力将类别先验注入到特征表示中，使得模型可按类别细化异常片段特征。二者结合模拟人类观察异常时的运动模式识别和语义结构推断双重过程。

Result: 在WVAD基准数据集上，RefineVAD展现出比现有方法更好的表现，尤其在区分不同类型和结构的异常事件时具有优势。

Conclusion: 论文证明了结合运动时序与语义类别信息可显著提升弱监督视频异常检测的效果，并强调通过语义上下文引导特征细化的重要性。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [240] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的端到端多人人体姿态估计方法PAVE-Net，摒弃传统检测与后处理流程，实现时空关联提升了准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多人人体姿态估计方法多采用两阶段流程，依赖检测、裁剪与NMS等启发式操作，限制了模型的准确率与推理效率，并且跨帧个体的时空关联十分复杂。

Method: 提出了一套端到端的视频多人人体姿态估计框架PAVE-Net，包括：1)空间编码器用于分析帧内关系；2)具备空间-时序结构的姿态解码器来捕捉跨帧全局依赖。核心创新为姿态感知注意力机制，使每个姿态查询可跨帧聚合属于同一人的特征；此外还显式建模了关键点的时空关联。

Result: PAVE-Net在PoseTrack2017数据集上，相比早期的端到端方法mAP提升6.0，且效能优于主要的两阶段方法；准确率与主流两阶段方案相当，但效率显著提升。

Conclusion: PAVE-Net首次实现了端到端多帧视频多人2D姿态估计，有效提高了准确性和效率，摆脱了传统启发式操作。

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [241] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 提出3DAlign-DAER，一个可以高效对齐文本与3D几何结构的统一框架，具备动态注意力机制和高效检索策略，显著提升大规模3D数据库跨模态检索与分类效果，并发布了大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3D-文本跨模态对齐方法在细粒度对齐和大规模数据库检索时表现不佳，难以捕捉文本和3D结构间的细微对应关系，且扩展到大规模数据集时性能明显下降。

Method: 提出动态注意力策略(DAP)和分层注意力融合(HAF)模块，通过可学习的token-to-point注意力实现细粒度对齐，并引入蒙特卡洛树搜索动态调整权重。推理阶段引入高效检索策略(ERS)，在大规模嵌入空间内实现更快速、准确的检索。同时自建Align3D-2M数据集，包含200万条文本-3D配对数据。

Result: 3DAlign-DAER在多项跨模态检索和分类基准测试中取得了优越表现，检索准确率和效率均超越传统方法。

Conclusion: 3DAlign-DAER有效提升了文本与3D几何的细粒度对齐能力，在大规模跨模态任务下表现卓越，对3D-文本跨模态检索与分类研究具有重要推动作用。相关代码、模型和数据集将公开。

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [242] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: 该论文提出了一种新的混合域自适应表征学习（HARL）框架，以提升基于外观的凝视估计的跨域鲁棒性，并在多个公开数据集上取得了最新的准确率表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于外观的凝视估计方法在跨域应用时常受到表情、可穿戴设备、图像质量等和凝视无关因素的干扰，导致性能显著下降。如何在不同质量和来源的数据中获得稳定鲁棒的凝视表征是值得解决的问题。

Method: 提出混合域自适应表征学习（HARL）框架，利用多源混合数据集进行鲁棒凝视特征学习。具体做法是通过无监督的域自适应方式，将高质量近眼图像中提取的特征，与低质量全脸图像中的凝视相关表征进行特征对齐，从而实现凝视特征的有效解耦，且几乎不增加计算和推理成本。此外，分析头部姿态作用，通过设计稀疏图融合模块以探索凝视方向与头部姿态的几何约束，实现密集且鲁棒的表征。

Result: 在EyeDiap、MPIIFaceGaze和Gaze360三个数据集上的凝视估计误差分别达到了5.02°、3.36°和9.26°，均优于现有方法，并在跨数据集实验中也表现出较强泛化能力。

Conclusion: 所提出的HARL框架能够有效提升基于外观的凝视估计在不同域和不同图像质量下的鲁棒性，并实现了行业领先的性能。

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [243] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D条件扩散模型的MRI图像质量迁移（MRIQT）方法，用于提升便携式超低场（uLF）MRI的新生儿脑部图像质量，使其接近高场（HF）MRI的诊断水平。


<details>
  <summary>Details</summary>
Motivation: 超低场（uLF）便携式MRI可为新生儿神经成像提供便利，但成像信噪比较低、诊断质量较差，因此急需提升uLF MRI图像质量的方法。

Method: 提出MRIQT框架：结合真实K-space退化过程以模拟物理一致的uLF数据，采用无分类器引导的v-prediction技术稳定进行图像到图像的扩散生成，利用SNR加权3D感知损失保证解剖结构保真。模型基于3D volumetric attention-UNet架构，实现有结构约束的图像翻译。

Result: MRIQT模型在新生儿不同病理队列上训练测试，PSNR提升15.3%，比最新方法高1.78%。医生评估显示85%输出图像质量良好且病灶清晰。

Conclusion: MRIQT实现了对便携式uLF MRI的高保真增强，为新生儿脑部可靠评估提供了有力工具，优于已有GAN与CNN基线方法。

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [244] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: 本文提出了MMD-Thinker，一个用于多模态虚假信息检测的两阶段框架，通过自适应多维思维提升检测和推理能力，并构建了包含推理过程的大型数据集，实现了业界领先的检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC时代的到来，社交媒体上的多模态虚假信息泛滥且不断演化，其低成本高迷惑性对社会构成重大威胁。现有通用多模态大模型虽在检测任务取得一定成绩，但存在推理能力不足和推理模式单一的问题，难以应对复杂、快速变化的多模态虚假信息。

Method: 本文提出MMD-Thinker，分两阶段实现：(1) 设计专门的多模态虚假信息检测思维模式；(2) 通过任务定制的指令微调，将该思维模式注入通用多模态大模型。进一步引入混合优势函数的强化学习策略，激励推理能力的提升。此外，作者还构建了包含8000多对图文、推理过程及分类标签的大规模多模态虚假信息推理数据集（MMR）。

Result: 实验结果表明，MMD-Thinker在多组内外领域数据集上均取得了业界领先的表现（state-of-the-art），同时具备推理灵活性和高效的token使用。

Conclusion: MMD-Thinker有效弥补了通用大模型在多模态虚假信息检测中的推理不足和模式单一的问题，通过专属思考模式和强化学习方法，极大提升了检测准确性和泛化能力，并推动了该领域数据集和方法的发展。

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [245] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: 本文提出了一种新的网络结构RFMNet，用于提升基于参考信息的隐蔽目标检测（Ref-COD）性能，通过多阶段特征融合与创新注意力机制取得了目前最佳效果。


<details>
  <summary>Details</summary>
Motivation: 以往工作常将参考显著目标图片简单编码为一维提示，未能充分利用丰富的显著目标特征用于增强隐蔽目标检测。提高多上下文特征融合能力，有望提升检测性能。

Method: 1）设计RFMNet，提取参考显著图像在多个编码阶段的特征，并与隐蔽目标特征在相应层级进行交互融合；2）引入Overlapped Windows Cross-attention机制，使特征在局部区域内进行更有效的信息匹配和融合，关注更细粒度空间对应关系；3）提出Referring Feature Aggregation（RFA）模块，对融合后的特征逐步进行解码分割，实现隐蔽目标精准提取。

Result: 在Ref-COD评测基准上进行了大量实验，结果表明所提方法性能超过先前方法，实现了当前最优（state-of-the-art）检测表现。

Conclusion: 充分利用参考显著图像的多阶段、多区域特征信息，并通过创新的注意力与特征融合模块，有效提升了隐蔽目标检测的准确性，对相关领域具有启发和借鉴意义。

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [246] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了GeoX-Bench数据集与基准，用于系统评估和提升大规模多模态模型在跨视角地理定位与姿态估计任务上的能力，发现现有LMMs在地理定位表现优秀，但姿态估计较弱，指令微调有助于提升能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模多模态模型（LMMs）在诸多任务展现强大能力，但其在跨视角地理定位和姿态估计领域的知识和技能未被系统研究，而这些领域对于导航、自动驾驶及户外机器人等应用至关重要，因此需要设计相应基准推动该方向发展。

Method: 作者创建了GeoX-Bench基准，包含10859组全景-卫星图像对与75万余组问答，覆盖全球128座城市。基于该基准，系统评估了25个主流LMMs在跨视角地理定位和姿态估计任务上的表现，并探索了指令微调对其感知与推理能力的提升。

Result: 实验显示，当前LMMs在地理定位任务上已取得显著成绩，但在复杂的姿态估计任务上的表现明显较弱。同时，经过GeoX-Bench上指令微调后，模型在跨视角地理感知方面有明显提升。

Conclusion: GeoX-Bench为LMMs的跨视角地理定位与姿态估计提供了评测和优化工具。尽管LMMs现阶段在地理定位已具备较强能力，但针对高难度的姿态估计还需持续改进，指令微调是一条有效的提升途径。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [247] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本论文提出了面向第一视角日常任务的智能助手EgoProceAssist，系统梳理并评估了相关任务、技术和挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉语言模型（VLMs）和第一视角感知的快速发展，为辅助用户完成日常流程性任务提供了新机遇。现有VLM助手在该场景下存在功能和性能上的不足，因此有必要系统梳理、分析并推动该领域的发展。

Method: 作者提出EgoProceAssist的概念，并定义了三项核心任务——第一视角流程性错误检测、流程性学习、流程性问答。同时，系统性回顾与对比了相关技术、数据集和评测指标，并设计新实验对典型VLM方法进行评测，分析其与EgoProceAssist之间的差距，最后对领域未来挑战与方向进行讨论。

Result: 论文全面总结了现有技术在三大核心任务下的应用情况，系统对比实验揭示了当前VLM助手在流程性任务中的不足与潜力，并基于上述发现提出了研究建议。

Conclusion: 本文为第一视角流程性AI助手奠定了系统化的基础，明确了技术壁垒与发展空间，为推动VLM在实际流程性助手中的应用提供了方向和资源，并开放了最新研究的持续性汇总库。

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [248] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种新的对称性感知压缩框架 SymGS，通过引入可学习镜像（mirror）以消除3D Gaussian Splatting场景中的局部和全局反射冗余，实现更高效的内存压缩，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting 在新视角合成领域以高效和高保真著称，但其内存需求随场景复杂度急剧增加，成为大规模应用的瓶颈。现有压缩方法主要依赖于原始冗余的相似性检测和量化，但压缩效果存在瓶颈。作者希望突破这些压缩限制，进一步降低存储需求。

Method: 作者提出 SymGS 框架，通过在场景中插入可学习的镜像（mirrors），利用镜面对称性识别并消除重复的高斯基本体（primitive），减少存储冗余。该方法能与如HAC等现有压缩技术无缝协作，提高压缩比。

Result: 在基准数据集上，SymGS 在与HAC结合时取得了1.66倍的提升，在大型场景下压缩比最高可达3倍。综合来看，SymGS平均实现了108倍的压缩率，且几乎不影响渲染质量。

Conclusion: SymGS 显著提升了3D Gaussian Splatting场景的压缩效率，为高质量、大规模新视角渲染提供内存友好型解决方案，可作为现有压缩方法的重要增强模块。

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [249] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: 该论文针对无人机（UAV）任务中视觉-语言模型（VLM）空间智能的不足，提出了专门的评测基准SpatialSky-Bench，并构建了大规模数据集SpatialSky-Dataset；同时提出新模型Sky-VLM，有效提升了VLM在无人机场景下的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在无人机应用中表现出良好的视觉理解和推理能力，但其空间智能能力，尤其是在动态和复杂环境中的表现尚未被系统化测评和提升。因此，亟需标准化基准和更好的数据支持，推动其在实际无人机任务中的应用。

Method: 1. 构建SpatialSky-Bench基准，设计涵盖环境感知和场景理解的13类无人机场景下的空间智能任务；2. 系统评测主流VLM，暴露其不足；3. 开发包含百万级样本和多维注释的SpatialSky-Dataset；4. 基于该数据集提出专为空间推理优化的Sky-VLM模型。

Result: 主流开源和闭源VLM在Benchmark上表现不佳，存在明显空间智能短板。新提出的Sky-VLM在所有测试任务上大幅领先，达到了当前最优水平。

Conclusion: 本文工作的SpatialSky-Bench与SpatialSky-Dataset推动了VLM空间智能系统化评测，并通过Sky-VLM实现了显著性能提升，为无人机场景下VLM开发和应用奠定了坚实基础。

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [250] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 本论文提出了一种仅使用视频级标签，检测监控视频中罕见且多样异常的新方法，并在UCF-Crime数据集上取得了90.7%的AUC。


<details>
  <summary>Details</summary>
Motivation: 监控视频中异常事件种类繁多且罕见，精细标注（帧级）的获取成本极高，因此亟需无需精细标签即可高效检测异常的方法。

Method: 论文提出了双主干架构，结合卷积和transformer特征，通过top-k pooling聚合实现异常检测，全过程仅需视频级监督。

Result: 所提出方法在UCF-Crime这一标准监控异常检测数据集上取得了90.7%的AUC，超过了已有方法。

Conclusion: 该方法验证了仅依赖视频级标签通过多模态特征结合和top-k池化也能有效检测监控视频异常，具有实际应用价值。

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [251] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: 论文提出了一种无需繁琐简化步骤，可直接从多视角图像重建轻量级建筑表面的新方法SF-Recon。其核心是结合3D高斯展开、法线-梯度优化和多视角的边一致性处理，最终通过三角剖分得到结构化轻量网格，在效率和模型简洁性上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角建筑重建方法依赖于密集重建、网格化及其简化，流程复杂且对输入质量敏感，效率和灵活性有限。研究动机是为数字城市、导航等应用高效获取轻量级建筑表面模型，避免传统流水线中的瓶颈。

Method: 1) 首先训练3D高斯展开得到视图一致的场表示；2) 通过法线-梯度引导的高斯优化，筛选与结构边界对齐的点原语；3) 利用多视角的边一致性剪枝，增强结构清晰度、压制伪影；4) 最终基于多视角约束的三角剖分，生成结构化轻量建筑网格。无需外部监督和后处理简化。

Result: 在自建SF数据集上的实验表明，SF-Recon可直接从多视角图像重建出顶点和面数极少、高度轻量的建筑模型，且保持结构忠实和计算高效，显著优于常规方法。

Conclusion: SF-Recon实现了从多视角图像到轻量结构化建筑表面的端到端高效建模，为数字城市等需求带来更快、更轻、更精确的三维模型重建方案。

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [252] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: 本文提出了针对多人的人类网格恢复任务的新方法，解决了因训练数据稀缺导致的场景一致性问题，并构建了新的高质量多人人体网格数据集和端到端网络。


<details>
  <summary>Details</summary>
Motivation: 当前多人的3D人体网格恢复主要依赖单人伪标签，忽略了同一图像内人物间的深度和尺度一致性，导致重建结果在场景层面缺乏一致性，因此亟需能生成场景一致多人人体网格伪标签的新方法。

Method: 作者提出Depth-conditioned Translation Optimization（DTO），利用人的身高统计学先验和单目深度估计，全局优化同一场景内所有人的摄像机空间平移，实现群体级的一致重建。进而在大规模数据集上生成了新的多人人体网格伪标签。还提出Metric-Aware HMR网络，通过引入相对量纲损失与摄像机分支直接估计度量尺度下的网格和参数。

Result: 实验结果显示，基于DTO生成的数据在深度推理、多人人体网格恢复等任务上均取得了当前最优的表现，验证了方法的有效性和优越性。

Conclusion: DTO方法显著提升了多人人体恢复的场景一致性，并大幅拓展了可用高质量数据规模，为后续相关研究奠定数据和方法基础。

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [253] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出一种高效且精简的多模态大模型TabFlash，用于提升对表格图像的理解能力，兼顾性能和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 表格图像理解需要关注问题相关信息并去除冗余背景，但现有多模态大模型在视觉特征上存在无效和冗余问题，影响理解效率与效果。

Method: 1）设计渐进式问题条件注入机制，将问题逐步嵌入到视觉Transformer的不同层以生成问题感知的视觉特征；2）提出剪枝策略，剔除图像中的背景token以提升效率；3）提出token聚焦的训练策略，确保关键信息集中在保留的token中，从而缓解剪枝导致的信息损失。

Result: TabFlash在表格理解任务中取得了SOTA表现，相较于第二好模型减少了27%的计算量和30%的内存消耗，超越开源及闭源同类模型。

Conclusion: TabFlash通过高效的信息注入与压缩机制，有效提升了表格图像理解的准确性和效率，在业界处于领先水平。

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [254] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种全新的字体可控海报文本编辑框架SkyReels-Text，可以高精度地对多区域多字体文本进行编辑，显著提升了字体一致性与视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑工具难以在保持视觉和字体风格的前提下，精细地、多区域地修改艺术设计类文本内容，如海报设计中多种字体的文本编辑需求尚未得到有效解决。

Method: 作者提出SkyReels-Text框架，实现可控字体的多文本区域编辑。用户仅需提供所需字体的字形样本，无需字体标签和推理阶段微调，即可针对不同非标准库字体进行精准控制与编辑，且保持未编辑区域的视觉一致性。

Result: 在多个数据集（包括手写文本基准）上，SkyReels-Text在文本保真度和视觉真实感方面均达到了当前最优性能，并能实现对字体家族和风格细节的前所未有的掌控。

Conclusion: SkyReels-Text有效弥补了通用图像编辑与专业排版设计之间的空白，为高质量多字体设计提供了强有力的技术支持。

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [255] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的视频生成与3D结构结合的自动驾驶系统自纠正方法，通过仿真和生成技术提升对罕见失败场景的鲁棒性，在主流数据集上有效减少了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶方法对数据驱动极为依赖，但面临长尾问题，即极少但关键的失败场景难以充分学习。本文旨在改善这些失败场景下的鲁棒性。

Method: 1）提出PM-Agent，模拟产品经理提出采集与失败案例相似数据的需求；2）利用生成模型（DriveSora）根据需求生成高质量、符合3D布局的数据和标注；3）将生成的数据用于训练纠错系统CorrectAD，该系统可无缝用于各种端到端方案。

Result: 在nuScenes和更具挑战性的数据集以及多种端到端规划器上，CorrectAD对失败案例的纠正率达62.5%和49.8%，碰撞率分别减少了39%和27%。

Conclusion: 结合扩散视频生成和结构化3D信息的生成系统能有效增加稀有失败场景的数据，实现端到端自动驾驶规划器的高效自纠正，提高系统整体安全性和鲁棒性。

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [256] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: 本论文提出DriveLiDAR4D，一种能够同时生成时序一致、前景可控、背景真实的LiDAR点云全场景生成方法，在nuScenes和KITTI数据集上大幅领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云生成方法无法实现时序连续、多模态条件下的生成，且难以精确控制前景物体和背景真实性，影响自动驾驶系统的开发和评估。

Method: 提出了DriveLiDAR4D管线，包含多模态生成条件和新型顺序噪声预测模型LiDAR4DNet，实现端到端的时序一致LiDAR点云生成，以及对前景和背景的精细可控全场景操作。

Result: 在nuScenes数据集取得FRD 743.13（提升37.2%）和FVD 16.96（提升24.1%），优于现有SOTA方法UniScene，在KITTI数据集上同样表现出色。

Conclusion: DriveLiDAR4D首次实现了可端到端、全场景操作、时序一致的LiDAR点云序列生成，为自动驾驶系统的仿真和评估提供了更实用的技术手段。

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [257] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: 该论文提出了一种结合深度学习和图神经网络的新框架，用于多人的群体活动识别和动作定位，实验验证了其在群体数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 多人物场景中的群体活动检测面临需求，如人群中的复杂互动、遮挡和外观变化，传统方法难以应对。因此，作者希望设计一种更有效处理个体、交互和整体群体行为的视觉识别方法。

Method: 首先使用Mask R-CNN进行演员定位和实例分割，通过多种特征提取网络（Inception V3、MobileNet、VGG16）抽取特征图，RoIAlign保持空间对齐。将分割掩膜与特征图融合，获得每个演员的精细特征表示。通过外观相似度与位置信息（归一化互相关、绝对差值、点积）构建演员关系图，再利用图卷积网络（GCN）进行关系推理，最终实现动作识别和群体活动识别。

Result: 在Collective Activity数据集上进行实验，证明融合掩膜特征强化、鲁棒的相似度搜索和图神经网络推理的方案，在拥挤和非拥挤场景下均提升了识别指标，取得了更优性能。

Conclusion: 将分割、特征提取与关系图推理三者结合为统一框架，能显著提升复杂视频中的人群行为识别，是群体活动理解任务的重要进展。

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [258] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 本文提出了一种新型专家混合（Mixture-of-Experts, MoE）框架，通过集成多个YOLOv9-T专家模型，并采用自适应路由机制，使各专家在特征提取上实现动态分工，从而提升目标检测的性能。实验结果表明，该方法在mAP和AR指标上均优于单一YOLOv9-T模型。


<details>
  <summary>Details</summary>
Motivation: 现有的单一目标检测模型（如YOLOv9-T）在面对多样化的目标或复杂场景时，可能难以兼顾所有特征类型。为提升检测器的泛化能力和性能，亟需一种能够针对不同特征子空间自动优化的架构。

Method: 构建多个YOLOv9-T作为专家子模型，并引入自适应路由机制，根据输入特征动态分配不同专家进行处理，实现深度网络的特征专门化。

Result: 多专家混合框架在多个目标检测数据集上测试后显示，mAP和AR均较单YOLOv9-T有显著提升。

Conclusion: 专家混合模型能有效提升目标检测性能，表明自适应专家分工是一种值得进一步研究的高效策略。

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [259] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 本文提出了一种半监督学习方法，通过结合整体人工标签与伪标签，在不增加标注成本的前提下，提高了视网膜图像质量评估的解释性和准确性，并能指导图像重采集。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜图像质量评估工具仅能判断整体质量，无法定位具体采集缺陷，原因在于详细标注成本高。该论文旨在解决这个不足，降低对人工标注的依赖，同时提升模型对具体缺陷的解释能力。

Method: 作者采用混合半监督学习方法，利用少量人工整体质量标签和通过Teacher模型生成的细节伪标签，在多任务框架下微调ResNet-18主干网络，从而既保证了性能又增强了解释性。

Result: 实验证明，所提方法在EyeQ和DeepDRiD数据集上较单任务基线有明显改进，并可与现有方法媲美或超越；模型对细节的预测与专家相当，且在新标注EyeQ子集上的表现与专家一致。

Conclusion: 这种半监督多任务方法有效提升了视网膜图像质量评估精度和责任性，无需额外人工成本即可输出具临床指导意义的采集反馈，有望实用化应用于自动化临床辅助诊断。

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [260] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 本文提出了更通用的图像压缩扩散模型gDDCM，将原本Denoising Diffusion Codebook Models（DDCM）用于DDPM的方法推广到了更多扩散模型，并验证了有效性。


<details>
  <summary>Details</summary>
Motivation: DDCM只适用于DDPM，无法泛化到其它主流扩散模型，限制了其实用性。作者希望扩展其适用范围，提高通用性和压缩效果。

Method: 通过设计gDDCM，作者将DDCM方法泛化，使其可以支持包括DDPM、Score-Based Models、Consistency Models以及Rectified Flow等多种主流扩散模型，并在这些模型上实现图像压缩。具体做法是将逆过程中的噪声采样机制推广为适用于不同模型的策略。

Result: 在CIFAR-10和LSUN Bedroom数据集上的实验表明，gDDCM不仅成功地将DDCM泛化到了其他扩散模型，并且在性能上有所提升。

Conclusion: gDDCM扩展了DDCM的应用范围，实现了跨扩散模型的图像压缩方法，为扩散模型在压缩领域的应用提供了更广阔的可能性，并带来了更优的压缩性能。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [261] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 提出了一个新的视觉-语言模型(VLMs)交通感知能力评估基准——DTPQA，用于衡量模型在不同距离上的交通场景感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型在多个任务上表现卓越，研究者希望其应用于自动驾驶领域。但由于自动驾驶对感知能力要求极高，尤其是对远距离交通场景中关键物体和参与者的识别，因此需要一个专门评价VLM感知能力而非推理能力的基准。

Method: 作者提出DTPQA基准，该基准通过视觉问答(VQA)方式，分为两部分：仿真环境生成（DTP-Synthetic）和真实交通图像（DTP-Real），并为每个样本标注了问题对象距离。数据集含有图片、问题、真实答案及对象距离，可系统分析模型在不同距离感知能力的变化。

Result: 提供了DTPQA数据集及生成脚本，使得研究者能扩展数据集并深入评估VLMs在交通场景中特定距离的感知表现。

Conclusion: DTPQA为VLMs在自动驾驶中的安全性评估提供了重要工具，有助于量化其多距离交通感知能力，促进VLM在自动驾驶等安全关键场景中的进一步发展和信任。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [262] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出了一种新颖的场景文本编辑（STE）框架TripleFDS，实现文本风格、内容与背景三属性的有效解耦，显著提升编辑灵活性与视觉一致性，并配套发布了多样化训练集SCB Synthesis。


<details>
  <summary>Details</summary>
Motivation: 当前场景文本编辑方法无法同时有效编辑文本风格、内容和背景三者，属性耦合导致可控性差与视觉一致性不足，只能部分满足STE需求。

Method: 提出TripleFDS框架，基于SCB Synthesis数据集的“SCB Group”结构实现三属性（风格、内容、背景）模块化解耦；通过组间对比正则化确保语义准确性，组内特征正交抑制属性冗余；采用特征重映射技术减少重建捷径和属性泄露。

Result: TripleFDS在SCB Synthesis上训练，STE主流基准上获得SOTA表现（SSIM 44.54，ACC 93.58%），且支持风格替换、背景迁移等灵活编辑功能。

Conclusion: TripleFDS在STE任务实现了属性高度可控的自然文本编辑，提升了视觉一致性和算法灵活性，并通过SCB Synthesis推动该领域的进一步发展。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [263] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文针对多模态大模型（MLMs）在视觉感知，尤其是颜色感知方面容易出现信息干扰和幻觉风险的问题，提出了一个专门的数据集和相关研究方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型的快速发展，涌现出大量融合文本与视觉信息的多模态大模型，但这些模型在视觉感知（如颜色）上仍易受干扰并产生错误输出（幻觉），因此需要验证并深入研究其成因及改进方案。

Method: 作者构建了一个新数据集“What Color Is It”，通过简单方式诱发MLMs在单一模态（视觉）上的幻觉行为。基于该数据集，系统分析了MLMs中视觉模态幻觉问题的成因，并进一步提出了增强鲁棒性的潜在解决方案。

Result: 实验证明，现有MLMs在颜色感知上确实易受到干扰并产生视觉幻觉；该数据集能有效触发并观察模型中的此类问题。初步提出的改进方法显示出一定缓解效果。

Conclusion: 本文揭示并验证了MLMs在视觉模态尤其是颜色感知方面存在幻觉风险，并通过新基准推动问题研究，为提升多模态大模型可靠性和鲁棒性提供了新思路。

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [264] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: 本论文提出了DelAnyFlow方法，可高效、精确地从卫星影像中提取大范围农业地块边界，并支持国家级规模应用。该方法在精度和速度上都远超现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有地块边界提取方法存在边界不完整、地块融合、扩展性差等问题，影响土地管理和作物监测。迫切需要一种高效、扩展性强的方法来自动化、准确地 delineate 大范围农田边界。

Method: 提出了DelAnyFlow方法，融合了基于YOLOv11骨干的DelAny实例分割模型和后处理、矢量化流程，利用大规模多分辨率FBIS 22M数据集训练，无缝处理不同分辨率影像，并生成具有拓扑一致性的矢量成果。

Result: DelAny模型在准确率和推理速度上均大幅领先主流方法（mAP提升100%，速度提升400倍）；可零样本通用至不同场景。例如在单台工作站上6小时内完成乌克兰全国地块边界提取（603,000km2），边界完整性优于Sinergise Solutions和NASA Harvest，尤其在小农和碎片化田块区域表现突出。

Conclusion: DelAnyFlow为数字地籍数据缺乏地区，提供了可扩展、低成本、高精度的地块边界提取方案，推动了土地管理数字化进程。

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [265] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法VOPE，用于评估大视觉-语言模型（LVLMs）在需要主动想象的新内容生成任务中产生幻觉的情况，发现现有模型普遍存在问题，现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 以往关于LVLMs幻觉的研究主要集中于客观描述任务，而对于模型在自由创作（如写故事）时想象内容的幻觉问题关注不足。仅将所有新想象内容视为幻觉并不恰当，因此有必要开发一种新方法进行更合理的评估。

Method: 提出VOPE方法，通过向模型提出与想象对象相关的回查性问题，评估模型在自由创作任务中其自己所理解的想象对象是否真的出现在图像中。通过模型解释与图像内容的一致性来判断是否产生幻觉。

Result: 用VOPE评估多种主流LVLM及幻觉缓解方法，发现：1）在自由想象任务中，大多数LVLM对想象对象有较严重的幻觉问题，表现较差；2）现有的幻觉缓解方法对该类任务效果有限。

Conclusion: 目前LVLM在自由想象任务中幻觉问题突出，评价和缓解幻觉的现有方法还需针对这类任务改进，这是未来研究的重要方向。

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [266] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于流匹配模型的神经表示方法，实现3D形状间高效、可逆、无需大规模训练的数据匹配，并支持多种3D表达（点云、网格、SDF、体素）的互配。方法在多个基准任务上表现优异，并扩展至UV映射和点云配准等应用。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状之间的映射方法往往需要大规模训练，且对数据表达有依赖性，难以通用，故需要一种高效、统一且无需数据驱动的新方法。

Method: 将3D形状看作通过可逆流（flow）将固定锚点分布映射而来，通过计算源形状到锚点的逆向流与锚点到目标形状的正向流的复合，实现两个形状间点的连续匹配，每个形状都以任务特定的点嵌入编码以获得可逆、跨模态的形状映射。

Result: 该方法在多个复杂基准和挑战性环境下，实现了高准确性与高覆盖率的形状匹配效果，同时还能用于UV映射和人体点云配准，表现优异。

Conclusion: 提出的基于流匹配的神经表示能跨多种3D数据类型实现高效、可逆的形状映射，为传统形状匹配和相关领域带来统一而强大的新解法。

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [267] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: 本论文提出了Foresee，一个基于多模态大模型(MLLM)的无训练图像伪造检测与定位管道，有效提升了定位准确率和可解释性，且具备更强的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测与定位方法在多样化数据集上泛化能力有限，且解释性不足；利用MLLM大规模训练虽能提升性能，但计算资源消耗大，且未揭示原生MLLM的泛化潜力。

Method: 提出了Foresee，无需额外训练，通过类型先验信息和灵活特征检测(FFD)模块，直接利用原生MLLM进行轻量级推理，尤其针对copy-move等多种伪造类型进行分析。

Result: Foresee在多个伪造类型（如copy-move、拼接、删除、局部增强、deepfake和AIGC编辑）上实现了更高的定位准确率和更丰富的文本解释，实验结果表明其泛化性能优于现有方法。

Conclusion: Foresee无需训练即可精准检测和定位多种类型的图像伪造，具有更高的泛化性和可解释性，为法证领域的MLLM应用提供了新思路，未来代码还将开源。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [268] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: 本论文提出SliDer框架，利用视觉-语言模型将幻灯片等多媒体文档的光栅图像自动还原为可编辑的结构化SVG格式，显著提升了语义结构恢复和用户主观偏好。


<details>
  <summary>Details</summary>
Motivation: 当前幻灯片和海报等多媒体文档常以静态光栅图像格式分发，限制了后续编辑和定制。现有几何化矢量还原方法只关注低层图形，丢失了文档高层语义结构，导致文本与图片等元素分离错乱。如何恢复可编辑的高语义矢量格式成为亟需解决的问题。

Method: 提出SliDer，一个结合视觉-语言模型的语义文档反渲染框架。首先检测并区分输入光栅图中的文本和图片元素，提取其属性，并构建带语义结构的SVG表示。推理过程中采用模拟人类设计者迭代微调预测，逐步优化SVG代码使其更接近原图。并新建了Slide2SVG数据集，为研究提供标准评测数据。

Result: SliDer在重构质量上取得了0.069的LPIPS指标，在与主流VLM零样本基线比较时，获得了82.9%的主观偏好率。

Conclusion: SliDer有效提升了复杂文档的结构化还原和可编辑性，为多媒体文档的编辑及相关研究带来新的方法和数据集。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [269] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了InterMoE框架，实现了高质量且具备个体特征的3D人类交互生成，FID指标领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高质量人类交互时，往往不能充分保留个体特征或忠实还原文本描述，影响实际应用如虚拟现实和机器人。

Method: 采用动态时序选择专家混合（Dynamic Temporal-Selective Mixture of Experts）机制，同时结合文本语义和运动上下文，将运动特征动态分配给专门的子模型（专家），使其专注于重要的时序特征，以此提升个性化和语义一致性。

Result: 在InterHuman和InterX数据集上，InterMoE分别将FID降低9%和22%，在个体特征保留和语义一致性上均达到SOTA性能。

Conclusion: InterMoE能够有效生成高保真度、个体化强的人类3D交互，为虚拟现实等应用提供更真实和定制化的数据基础。

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [270] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: 本文提出LGIP基准，以评估主流视觉-语言模型（如CLIP、OpenCLIP等）在图文匹配任务中对语言扰动的稳健性，包括对同义语句的容忍度和对语义变更的敏感度。结果发现部分模型在语言鲁棒性上存在明显差异，LGIP可作为传统指标之外的重要补充。


<details>
  <summary>Details</summary>
Motivation: 当前流行的视觉-语言模型在零样本任务上表现优异，但模型是否能够稳定应对语言的细微变化（如同义改写或语义翻转）尚不明确。作者旨在检测和量化这些模型的语言鲁棒性，而现有评价指标难以揭示此类问题。

Method: 作者设计了LGIP基准，基于COCO数据集40k图片及其五条人工描述，自动生成语义保持的同义句和语义翻转语句（如更改单词类别、颜色或数量），并用新的衡量指标（不变性错误、语义敏感性差距及正例率）来量化模型对于不同语言扰动的表现。

Result: 九种主流VLM实验结果表明，EVA02-CLIP和大型OpenCLIP在稳健性和敏感性之间表现最佳；而SigLIP系列表现较差，特别是在对象和颜色变更时容易偏向语义错误的描述。这类鲁棒性问题用常规检索指标难以发现。

Conclusion: LGIP能有效揭示VLM对语言扰动的敏感性和鲁棒性，弥补了传统准确率评估的不足，对于提升模型语言理解稳健性具有重要意义。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [271] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: 本文通过深度学习框架分析中国城市村落（UVs）拆迁后的土地利用情况，发现在拆迁后，土地再开发过程复杂且存在拖延，需更加科学分层和本地化的规划。


<details>
  <summary>Details</summary>
Motivation: 城市村落大规模拆迁改造，但对其土地是否有效再利用缺乏系统评估，引发对现行更新实践成效和可持续性的担忧。

Method: 提出基于深度学习的多时相遥感影像语义分割框架，监测UV的时空变化，并将拆后土地利用分为六类，选取中国四大经济区域典型城市（广州、郑州、西安、哈尔滨）进行实证研究。

Result: 1）UV再开发进程常被拖延；2）转型多发生在城市边缘，核心区较为稳定；3）揭示了三种时空转型路径：同步、迟延逐步和渐进优化。

Conclusion: UV再开发呈现碎片化、复杂和非线性特征，需采取分层和情境敏感的规划。研究为包容、高效和可持续的城市更新提供了实证依据，也为全球理解非正规住区转型做出贡献。

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [272] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 本文提出了一种用于多目标预测的保守且紧致的不确定性量化方法，可广泛适用于与医学成像等安全相关的领域。


<details>
  <summary>Details</summary>
Motivation: 现有的保守预测方法大多只能处理单一预测目标，而实际应用往往涉及多个指标，需要针对多目标问题有理论保障的精准不确定性量化方法。

Method: 作者提出了一种渐近极小极大（asymptotically minimax）的多目标合规预测（conformal prediction）方法，同时兼顾预测区间的紧致性和联合覆盖率的理论保证。并进一步阐述该方法在多指标盲图像质量评估、多任务不确定性量化、迭代采集等多场景下的适用性。

Result: 通过合成数据和实际MRI数据的实验，作者的极小极大方法在预测区间紧致性和联合覆盖率等方面，相比现有多目标合规预测方法表现更优。

Conclusion: 该工作实现了对多目标问题的理论与实际突破，能在保持联合覆盖率的前提下大幅收窄不确定性区间，具有广泛的实际应用价值。

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [273] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: 本文揭示了一种可破坏模型可解释性的全新攻击方法，即在联邦学习场景中，攻击者通过微小色彩扰动影响模型的显著性图，却不改变预测结果，这暴露了解释性算法的潜在安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型越来越多地应用于安全关键领域，可视化解释技术被广泛用作提高模型透明度的工具。当前社区普遍假定，只要模型做出正确预测，其解释就是可信的。但这种假设尚未被严密检验。本文旨在挑战这一假设，探究即便模型预测准确，解释机制本身仍可被恶意操作的可能性。

Method: 作者提出了一种名为Chromatic Perturbation Module（色彩扰动模块）的攻击框架：攻击者在联邦学习环境中对颜色进行微小调整，改变前景与背景间的色彩对比。尽管这些扰动不会影响模型的分类准确率，但会系统性地将显著性图（如Grad-CAM的关注区域）从语义相关部分转移，从而破坏模型解释的可信度。这种攻击随训练轮次积累，导致全局模型的特征归因持续被投毒。

Result: 实验表明，在多个数据集上，该攻击能够显著降低显著性图与实际语义区域的重叠度。例如，Grad-CAM的峰值激活重叠度最多下降35%，但模型在所有数据集上的分类准确率仍保持在96%以上。此外，标准训练流程难以检测或缓解此类解释退化，尤其是在联邦学习背景下，细微颜色扰动不易被察觉。

Conclusion: 本文挑战了预测准确即解释可信的普遍认知，首次表明模型可解释性自身是潜在攻击面。作者提出的色彩扰动攻击能够隐蔽且持久地降低模型的可解释性，暴露了解释性算法和联邦学习系统在安全性方面的重要隐患。

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [274] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督OOD检测方法BootOOD，在无须额外OOD样本的情形下，有效提升了对语义相近OOD样本的检测能力，表现优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在面对与ID类别语义上相近的OOD样本时效果不佳，因此急需一种能够更准确分辨此类样本的方法，确保安全敏感应用中模型的可靠性。

Method: 提出BootOOD框架，仅利用ID数据，通过对ID特征的简单变换合成pseudo-OOD特征，并结合神经收敛(Neural Collapse)现象。引入一个附加的分类头，对特征范数进行基于半径的分类，将OOD检测与主分类器解耦，仅需学习OOD样本的特征范数小于ID特征。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200等数据集上的实验表明，BootOOD优于现有后处理方法、超过无需暴露外部异常样本（outlier-exposure）的方法，对比现有SOTA暴露式方法也有竞争力，并且保持或提升了ID精度。

Conclusion: BootOOD无需外部OOD样本，通过新颖的特征范数判别方法，有效提升了语义相近样本的检测能力，是安全敏感环境下具有实际应用价值的OOD检测进展。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [275] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: 提出了一种创新方法，提升多模态对比学习模型（如CLIP）抵抗后门攻击的能力。通过引入图像分割oracle，能够有效识别并修正被投毒模型。


<details>
  <summary>Details</summary>
Motivation: 多模态模型如CLIP在实际应用中容易受到后门攻击，现有防御方法成本高、目标不明确，亟需高效、精准的防御策略。

Method: 提出利用图像分割oracle监督CLIP输出，开发两种算法：一是通过对比CLIP与oracle的知识识别潜在后门触发器，二是定位受害标签和样本，生成精简微调数据集，最终修复被投毒的CLIP模型。

Result: 在视觉识别基准上进行大量实验，验证了该方法对基于CLIP的后门防御效果显著。

Conclusion: 该策略能高效识别后门触发器与受害样本，修正投毒模型、消除后门影响，推动多模态模型防御能力提升。

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [276] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本论文提出了一种半监督的单目高度估计算法，通过整合教师-学生-考核网络结构，有效利用大量未标注的遥感数据，提升高度预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目高度估计相比多视角和LiDAR方案更经济，但受限于高质量标注数据的稀缺，导致当前深度学习方法的性能和泛化能力受阻。作者希望利用未标注数据，突破标注数据不足带来的瓶颈。

Method: 提出TSE-Net半监督学习框架，包括教师、学生和考核（exam）网络。教师网络结合回归和分类预测，为未标注数据生成伪标签。学生网络用这些伪标签进行训练，考核网络作为学生网络的时间集成，提升稳定性。采用分层bi-cut策略应对高度的长尾分布，通过Plackett-Luce模型校准类别概率，过滤低质量伪标签。

Result: 在三种不同分辨率和成像模态的数据集上实验，结果表明提出的TSE-Net在单目遥感高度估计任务中取得了更优的表现。

Conclusion: TSE-Net能有效利用未标注数据，大幅提升单目高度估计模型的性能和泛化性，为遥感3D感知提供了更经济高效的解决方案。

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [277] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: 本文提出了Opt3DGS，一种改进3D高斯泼溅（3DGS）优化过程的新框架，通过两阶段优化方法提升新视角合成效果，并在多个数据集上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS在新视角合成领域表现突出，但其核心的优化过程仍然存在陷入局部最优和收敛质量不足等问题，本文针对这些未被充分研究的问题进行探索。

Method: 提出Opt3DGS框架，包括两个阶段：1）探索阶段使用自适应加权随机梯度Langevin动力学（SGLD）以增强全局搜索能力，帮助跳出局部最优；2）开发利用曲率信息的本地拟牛顿方向引导的Adam优化器，实现更精确且高效的收敛。

Result: 在多个具有代表性的数据集上进行了大量实验，验证Opt3DGS能够在不改变底层表示的前提下，显著提升3DGS的渲染质量，达到当前最优水平。

Conclusion: 优化方法本身对于3DGS的表现有重要影响，Opt3DGS通过系统性两阶段优化显著提升了最终质量，为高质量的新视角合成任务提供了更稳健有效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [278] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种新型统一框架HPL，同时优化图片与图片（I2I）和文本与图片（T2I）两类行人重识别任务，取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法大都将I2I和T2I任务分开处理，导致特征表征混杂及性能有限。该文希望设计能同时处理两任务、实现高效表征和跨模态对齐的统一方法。

Method: 1) 构建任务路由Transformer，复用视觉编码器并引入双分类token，分别引导I2I与T2I特征提取分支；2) 设计层次化prompt学习，结合身份级可学习token和实例级伪文本token，后者通过模态特定逆向网络从图像或文本特征获得，注入细粒度语义信息；3) 提出跨模态prompt正则化，促使伪prompt在保持原模态特性的同时提升跨模态对齐能力。

Result: 在多组主流行人重识别数据集上，HPL均在I2I与T2I任务获得领先性能，验证了方法有效性。

Conclusion: 提出的HPL框架为多模态行人重识别提供了统一高效解决方案，可兼顾判别性和语义对齐，在未来相关任务中具有应用和拓展潜力。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [279] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass 是一种结合核形态和微环境上下文、针对细胞类别细分的多尺度深度学习框架，通过新颖的数据集和不确定性引导机制，实现了比传统方法更高的细胞表型识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的组织病理图像分析方法通常只能获得粗粒度的人为标注，且只关注细胞核形态，忽视了广义的组织环境对细胞功能和身份的影响，导致难以精细区分亚型。

Method: 提出了NuClass框架，包括两条路径：Path local（224×224像素，关注核形态特征）和Path global（1024×1024像素，捕捉细胞周围微环境）。通过可学习的门控模块自适应平衡局部和全局信息，并利用不确定性引导机制提升整体学习效果。此外，构建结合空间转录组数据的标注数据集，实现大规模、高分辨率的标签生成。

Result: 在三个独立测试队列上，NuClass模型某类别F1分数高达96%，显著优于强基线方法。并能输出有效的置信度估计与可解释性可视化。

Conclusion: 多尺度、不确定性感知的特征融合方法能够兼顾细胞核局部信息和组织环境全局特征，为细胞级表型预测提供了高准确率且具解释性的新路径。

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [280] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉自回归推理加速方法VVS，在保持生成质量的同时大幅减少模型推理步数，从而显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型在图像生成方面表现突出，但其逐步预测范式导致推理延迟高。尽管投机解码（SD）可加速该过程，但其“生成一步、验证一步”的限制阻碍了加速潜力。因此，作者希望突破验证过程的瓶颈，进一步降低推理延迟。

Method: 作者分析了起草阶段的特点，提出利用视觉token可替换性，首次在视觉自回归模型的SD过程中探索跳过验证（verification skipping）。具体方法VVS包括：1）带动态截断的免验证token选择器；2）token级特征缓存与重用；3）细粒度跳步调度，从而实现部分步骤免验证。

Result: VVS方法可将目标模型前向推理步数减少2.8倍，并且在生成质量上与传统方案持平，速度和质量的平衡优于以往SD框架。

Conclusion: VVS有潜力显著重塑视觉自回归生成的投机解码范式，兼顾高效性和生成质量，为视觉生成模型推理加速提供了新思路。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [281] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: 本文针对低光照图像增强任务提出了一种名为ICLR的交互增强框架，包含双流交互增强模块与协方差校正损失，在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有HVI色彩空间虽提升了低光照图像的增强效果，但其色度与亮度分支之间存在显著分布差异，导致特征互补受限，而且像素级损失函数在色度分支间相关性较弱区域引发梯度冲突，影响增强质量。

Method: 提出了ICLR框架，其中包含双流交互增强模块DIEM实现亮度与色度分支特征的有效融合与增强，协方差校正损失CCL通过亮度残差统计来惩罚色度误差并在色度分支间做梯度平衡。

Result: 在多个公开数据集上的实验结果表明，该方法在低光照图像增强任务中优于其他主流方法。

Conclusion: 提出的ICLR框架有效提升了低光照图像的细节恢复与色彩还原能力，能够更好地处理中低相关性区域，具有较好的应用前景。

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [282] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络的机器学习框架，用于高效地生成与研究对象特征（如年龄和性别）相关的医学图像模板，有效提升医学影像在注册与分割任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像模板生成过程非常耗费计算资源，因此可用模板数量有限，无法满足多样化研究人群的定制需求，尤其是在存在显著人口变异时，模板往往不能代表研究群体。

Method: 作者提出了一种利用卷积式配准神经网络的机器学习框架，能快速学习由对象特征（如年龄、性别）条件生成模板的函数。如有分割数据，还能同时生成解剖分割图，并可用该模型对个体影像与模板进行自动配准。

Result: 在多个3D脑MRI数据集上的实验表明，该方法能学习到高质量、能代表目标人群的模板。基于条件属性注解生成的模板在配准表现上优于无条件模板和其他主流模板构建方法。

Conclusion: 此方法能高效生成代表目标人群特征的医学影像模板，显著提升了影像配准及分割的准确性，为计算解剖等研究提供了更有力的工具。

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [283] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 本文提出了TAND框架，将组织信息与细胞核检测和分类结合，在减少精细标注需求下取得了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 细胞核检测和分类对于计算病理学非常关键，但现有方法依赖于精细的专家标注并且没有充分利用组织背景，导致效果受限。

Method: 作者提出了一种新颖的TAND框架，用点级监督结合组织掩膜条件，采用ConvNeXt编码器-解码器结构，并集成了冻结的Virchow-2组织分割分支，利用空间特征层次线性调制（Spatial-FiLM）实现组织概率对分类流的选择性调制。

Result: 在PUMA基准上，TAND超越了无组织知识和仅用掩膜监督的基线，在上皮、内皮、间质等组织依赖型细胞类型检测和分类上提升显著。

Conclusion: 这是首次实现基于学习到的组织掩膜对每细胞分类进行调节的方法，显著减轻了人工标注负担，具有广泛实际应用前景。

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [284] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: 该论文提出了一种基于普通摄像头、利用深度学习识别面部特征和眼部参数的驾驶员疲劳检测系统，并通过实验验证了其实用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是造成交通事故的重要原因，因此需要可实时监测驾驶员状态并及时预警的实用方法，以提升道路安全。

Method: 系统通过标准网络摄像头采集驾驶员面部图像，采用MediaPipe的Face Mesh算法进行人脸关键点检测，并利用EAR（眼部纵横比）方法检测眼部运动（如长时间闭眼或眨眼频率降低），结合OpenCV进行图像处理，实时判断驾驶员疲劳状态并通过声音进行提醒。

Result: 实验结果表明，该系统识别准确率高，响应速度快，能够低成本、高性能地实现疲劳检测，适合集成至现有驾驶辅助系统。

Conclusion: 该研究证明，利用常见的硬件和高效的图像处理算法可以实现经济实用的驾驶员疲劳检测，对提升行车安全具有现实意义。

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [285] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文提出了两种新型基于α-散度的带边界损失函数，并有效结合了角度边界，对人脸和说话人验证表现出显著提升，尤其是在低误接受率场景下效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸与说话人验证多依赖如CosFace和ArcFace这类基于边界的softmax损失，但这些方法难以与α-散度损失函数整合，而后者在稀疏解方面有天然优势，且能带来不同特性。本文致力于将角度边界机制有效结合进α-散度损失中，解决实际验证任务需求，特别是提升高安全场景的性能。

Method: 作者分析了将角度边界引入α-散度损失的两种途径：一是在参考分布（先验概率）上引入边界（Q-Margin），二是在logits（未归一化对数似然）上引入边界（A3M）。特别地，A3M在训练过程中出现了由于对logits和稀疏性的共同惩罚导致的不稳定，作者提出了原型重初始化策略加以解决。

Result: 所提Q-Margin和A3M方法在IJB-B和IJB-C人脸验证基准以及VoxCeleb说话人验证任务上均取得了显著领先于现有方法的表现，尤其是在低FAR（误接受率）指标下，优于强基线。

Conclusion: 论文创新性地提出了结合α-散度与角度边界的监督损失，提升了验证任务的准确性和安全性，尤其适用于需要极低误接受的高安全性应用场景，如金融身份认证等。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [286] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow是一种训练无关的方法，通过动态令牌丢弃和压缩型长期记忆机制，使视觉语言模型能够高效处理长视频问答任务，大幅减小计算开销，同时提升理解能力。


<details>
  <summary>Details</summary>
Motivation: 长视频问答任务对现有视觉语言模型而言极具挑战，因为随着视频长度增长，模型的注意力和缓存也线性增长，导致推理开销陡增或只能使用窗口滑动补救，二者均效率低下。创新的方法能够缓解长视频推理时的性能和资源瓶颈，是实现实际应用的关键。

Method: 提出了一种无需训练的新流程CacheFlow，该流程结合动态令牌丢弃（对每帧token根据与前一帧余弦相似度在线裁剪）和压缩型长期记忆。模型将有价值token打包进定长块，每个块用递归编码器摘要其关键信息后建索引保存，块的详细内容则转为离线存储，推理时再加载。推理阶段通过共识机制只检索最相关块并同时关注检索块与当前帧，实现高效且长距离推理。

Result: 在离线和流式VQA基准测试上，CacheFlow相较强基线方法有更好表现，同时处理的token数量减少最高达87%。验证了该方法显著提升视频问答任务效率且保持答案准确性。

Conclusion: CacheFlow为长视频内容理解提供了一种高效、架构无关且无需微调的新范式，使视觉语言模型兼具效率和上下文感知能力，为长视频理解实际应用奠定了基础。

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [287] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: 本文提出了Part-X-MLLM，一种原生3D多模态大模型，通过结构化语法将多样化的3D任务统一为可执行程序，实现了复杂3D编辑与生成的统一接口。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态任务高度碎片化，缺乏统一、可扩展的接口来同时处理结构化编辑、语义描述和指令执行等问题。作者希望通过统一的语言驱动方式简化3D任务交互、增强模型灵活性和可用性。

Method: 提出了一种双编码器架构，输入为RGB点云和自然语言提示，模型以自回归方式生成结构化token序列，包含局部框、语义描述和编辑指令等，输出可被下游几何引擎直接解析。模型先进行结构与语义解耦预训练，再进行大规模部件级任务微调。

Result: 实验表明，Part-X-MLLM能够高质量生成结构化计划，且在3D问答、部件组合生成与局部编辑等任务上实现了先进性能，支持统一的多3D任务接口。

Conclusion: 该方法证明了将3D多模态任务结构化为程序和与几何引擎解耦的有效性，为复杂3D任务提供了统一、强大且可扩展的语言接口。

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [288] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出了DMDR框架，将强化学习（RL）与分布匹配蒸馏（DMD）结合，实现多步扩散模型向少步推理模型的高效蒸馏，不仅提升了推理效率，还突破了学生模型性能受教师模型限制的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统的分布匹配蒸馏方法，虽然能够将多步扩散模型压缩为少步模型，提高推理速度，但学生模型的性能通常被教师模型所限制。研究动机在于突破这一性能上限，实现更高效并且表现优异的少步扩散模型。

Method: 作者提出DMDR框架，把强化学习纳入蒸馏过程，通过使用DMD损失作为强化学习中的正则项，实现蒸馏和RL的协同优化。同时，设计了动态分布引导和动态重噪声采样两种训练策略，提升初始蒸馏质量。

Result: 实验表明，DMDR在各种少步扩散模型中达到领先的视觉生成质量和提示一致性，甚至在一些指标上超越了原始多步教师模型的性能。

Conclusion: DMDR证明了强化学习与蒸馏的结合能有效解锁少步生成器的能力，是提升扩散模型推理效率和生成质量的有效方法，对扩散模型的应用推广具有重要意义。

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [289] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth 是一种专为地球观测数据设计的多模态时空基础模型，能在多项任务上达到最先进性能，并已公开源代码与模型。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据具有空间性、序列性和多模态性，传统方法难以高效处理和泛化。现有基础模型无法很好地适应地球观测领域的复杂特点，亟需专门设计的新方法。

Method: 作者提出了 OlmoEarth 模型，采用创新的自监督学习方法、掩码策略和损失函数，针对地球观测数据特性优化。模型融合多种模态，兼具空间与时序建模能力。

Result: 在与 12 种基础模型的对比中，OlmoEarth 在 24 项任务中有 15 项的特征表达任务、29 项任务中有 19 项的微调任务上表现最佳。并被用于完整的数据采集与推断平台。

Conclusion: OlmoEarth 模型在地球观测领域取得了显著突破，为 NGO 和非营利组织开放，推动解决全球性问题。相关代码和预训练权重已开放下载。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [290] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: 本论文提出GS-Light，一种高效的基于文本引导、兼具位置感知能力的3D场景重光照（relighting）系统，适用于Gaussian Splatting场景，不需训练即可实现多视角重光照，显著提升图像质量及一致性。


<details>
  <summary>Details</summary>
Motivation: 在3D场景编辑，尤其是重光照任务中，如何根据用户的文本指令灵活、高效地调整光线参数（方向、颜色、强度等）是一个实际需求，但现有方法缺乏对多视角一致性和文本到照明意图的准确映射支持，效率及易用性有提升空间。

Method: GS-Light结合大规模视觉-语言模型解析用户提示词，利用深度、法线和语义分割信息融合，实现对照明先验和视角几何约束的联合建模，生成初始潜变量，并输入多视角扩散模型，产出高保真、多视角一致的重光照图像。最后，将3D Gaussian Splatting场景微调以达到全局一致重光照。

Result: GS-Light在室内和室外多场景下均优于SOTA基线，表现在多视角一致性、成像质量、美学分数、语义相似度等多项定量与用户主观评测指标上均获得提升。

Conclusion: GS-Light是一种无需训练即可实现根据文本指令灵活重光照3DGS场景的高效方法，显著提升效果与用户体验，为后续3D内容创作与编辑提供了新思路。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [291] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了TiViBench基准和VideoTPO策略，用于评估与提升视频生成模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型主要关注视觉质量与时序连贯性，但缺乏衡量模型物理与逻辑推理能力的基准工具。因此需要新的基准来系统性评估与提升视频生成模型的推理能力。

Method: 作者设计了TiViBench——一个分层基准，涵盖结构推理、空间/视觉模式推理、符号/逻辑推理、动作规划与任务执行四个维度，包含24个多样化任务、三个难度等级。同时，提出VideoTPO测试时优化策略，通过大语言模型对生成视频的自分析，增强推理表现，无需额外训练或数据。

Result: 实验发现，商业模型如Sora 2和Veo 3.1具有更强推理潜力；开源模型则因训练规模与数据多样性受限，潜力尚未激发。此外，使用VideoTPO后，推理性能显著提升。

Conclusion: TiViBench和VideoTPO为视频生成模型推理能力的评估与提升提供了有效工具，为未来该领域研究奠定基础。

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [292] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出了FFSE，一种3D感知的自回归方法，实现了真实图像中直观且物理合理的物体编辑。其在单轮和多轮编辑上均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本生成图像技术虽然提升了图像语义编辑能力，但在3D感知且真实一致的物体操作方面存在不足，如需要耗时且易错的3D重建或不能有效保持场景一致性。

Method: 作者提出FFSE框架，将编辑过程建模为一系列学习到的3D变换（包括平移、缩放、旋转等）。用户可在真实图像上进行灵活物体编辑，背景效果及整体场景一致性得以保持。引入了3DObjectEditor混合数据集，涵盖多轮编辑、不同物体及场景，用于辅助多轮编辑能力的训练与评估。

Result: 大量实验表明，FFSE在单轮与多轮3D感知编辑场景中均明显优于当前主流编辑方法，尤其是在保持背景和场景一致性方面表现突出。

Conclusion: FFSE为3D感知的真实图像物体编辑带来了突破，无需复杂3D重建，支持多轮灵活操作，并在现有方法中取得了最佳性能。

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [293] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: 本文提出了UnSAMv2方法，实现了无需人工标注即可对图像进行任意粒度的分割控制，并显著提升了SAM模型在多项任务和基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流的分割基础模型（如SAM）在分割粒度控制方面存在不足，用户常常需要手动调整才能达到理想的分割细致度。同时，同一提示词对应多个合理分割结果、获取全粒度密集标注的高昂成本，也限制了此类问题通过监督学习方式的解决。

Method: UnSAMv2在UnSAM提出的“分而治之”策略基础上，进一步自动挖掘了大量掩模-粒度对，并引入了创新性的粒度控制嵌入，实现对分割尺度的精准、连续控制。整个系统仅需6000张未标注图像和极少量的额外参数（0.02%），即可训练获得。

Result: UnSAMv2集成于SAM-2后，针对交互式、全图像和视频分割等多种任务，在超过11个公开基准测试中均获得显著提升。例如，NoC90从5.69降至4.75，1-IoU从58.0升至73.1，AR1000从49.6升至68.3。

Conclusion: UnSAMv2表明，借助粒度感知的自监督学习方案，少量未标注数据即可极大激发视觉基础模型的潜能，实现无需人工标注的任意粒度分割，拓展了大模型落地的应用边界。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [294] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种多镜头半监督视频目标分割（MVOS）新方法，包括一种过渡模拟数据增强策略（TMA）和Segment Anything Across Shots（SAAS）模型，并引入了新的Cut-VOS数据集，显著提升了跨镜头分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频目标分割方法主要关注于单镜头视频，无法有效处理镜头切换带来的困难，而多镜头高质量注释数据稀缺，限制了实际应用。

Method: 作者提出了TMA数据增强策略，使模型能用单镜头数据实现跨镜头泛化，并提出SAAS模型，能有效检测理解镜头转换。同时构建了新数据集Cut-VOS，包含密集注释、多样物体类别和高频过渡。

Result: 在YouMVOS和Cut-VOS数据集上，SAAS模型在复杂镜头切换下表现出色，达到当前最佳的性能。

Conclusion: 通过创新的数据增强和模型设计，本方法有效地提升了多镜头视频目标分割的实用性和准确性，为相关领域提供了新基准和工具。

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [295] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出直接预测高清图片而非预测噪声量的扩散模型（简称JiT），并证明该方法能在高维空间下有效生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型通常预测噪声而非干净图像，但从流形假设来看，天然图像应在低维流形而噪声量不符合这一假设。作者希望借助流形特性，改善生成过程。

Method: 作者提出“Just image Transformers”（JiT）方法，不用tokenizer，无需预训练，也不引入额外损失，直接用大patch的Transformer在像素层面对原始图像建模，并直接预测清晰图像。

Result: 在ImageNet的256和512分辨率下，采用大patch（16、32）的JiT模型取得了与现有方法有竞争力的结果，其中传统预测噪声的方法在高维下往往表现不佳，甚至失败。

Conclusion: 直接预测清晰图像结合大patch Transformer是一条有效途径，能够回归到数据流形本质，在高维空间表现优异，提供一种Transformer扩散模型的新范式。

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [296] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 本文推出了TimeStampEval基准，用于在长转录文本中，根据非逐字引用精准检索毫秒级时间戳。作者提出的两阶段方法在大幅提升检索准确率的同时，推理成本节省超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有的模糊匹配算法在句子有语义一致但语法不同的情形下效果不佳，这在官方文字记录与语音转文字转录对齐任务中非常常见。为了解决这个难题，并应用于自动化长播客内容的拼接，作者设计了新的评测标准和方法。

Method: 提出TimeStampEval基准数据集，并设计两阶段方法：先用RapidFuzz进行预筛选，再由大语言模型（LLM）对短片段做验证。同时，作者系统性评估了六种现代LLM在大规模转录文本（2800句/12万个token）上的表现，分析了提示设计、token预算和处理模糊匹配的效果。

Result: 1）查询位置及格式优化能提升准确率3-20个百分点，并减少30-40%的token消耗；2）边界偏移是关键错误类型，模型本质上能理解任务；3）适中token预算可将弱方案准确率从37%提升到77%，强方案准确率超过90%；4）‘Assisted Fuzzy’方法让模糊检索准确率大幅提高最多50个百分点，同时延迟和正确结果成本下降96%。在10个不同类型转录本上，方法对文本长度、词汇变化和领域变化具备很强鲁棒性，错误检索拒绝率达95-100%。

Conclusion: 提出的两阶段‘Assisted Fuzzy’方法解决了复杂跨文本语义相似但形式不同的检索难题，显著降低计算成本，并对不同转录本和领域均具鲁棒性，能有效支撑实际应用如国会记录拼接播客等场景。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [297] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一款强调“交互扩展”能力的开源研究智能体，系统性提升了模型与环境交互的深度和频率，可有效进行复杂工具调用与信息查找，在多个基准测试中表现优于以往开源模型。


<details>
  <summary>Details</summary>
Motivation: 以往的智能体多从模型规模或上下文长度入手提升性能，忽视了与外部环境“交互频率和深度”的潜力，因此作者提出将交互规模化作为性能提升的第三维度，以提升复杂研究任务性能。

Method: 引入“交互规模化”理念，通过系统训练让模型适应更深层次、更频繁的人机环境交互，并用强化学习优化，使模型在高达256K上下文和600次工具调用任务下依然高效、准确地完成多轮推理和信息检索。

Result: MiroThinker 72B在GAIA、HLE、BrowseComp、BrowseComp-ZH四项基准测试分别获得81.9%、37.7%、47.1%、55.6%的高分，优于先前开源智能体，接近商业模型GPT-5-high。实验证明，交互深度的提升可带来类似于模型规模和上下文扩展的性能增长。

Conclusion: “交互扩展”是下代开源研究智能体开发的第三关键要素，有助于突破仅靠模型扩容和上下文增长的性能瓶颈，应成为 agent 设计的新方向。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [298] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 本文对语言模型（LMs）是否真正具备“推理能力”展开探讨，指出当前相关定义与语言模型实际的训练与工作方式并不一致。作者提出LM输出貌似推理的结果其实源于统计规律，而非真正的逻辑机制。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域普遍宣称语言模型具备推理能力，但这些定义模糊且与语言模型的训练与推断过程不符，容易误导对其本质的理解。

Method: 作者采用理论分析的方式，将主流的transformer类语言模型视为隐式有限阶马尔可夫核，通过考察其预测分布，分析推理相关输出的统计本质。

Result: 分析表明，语言模型所谓“推理”输出实为统计模式匹配，依赖于训练数据中的统计不变性，而不涉及显式逻辑推理机制，因此难以保证逻辑一致性。

Conclusion: 语言模型更接近于“统计模式匹配器”而非真正的推理器。本文呼吁NLP领域在描述和分析模型时，明确区分统计规律与真正推理的区别，以促进更科学的讨论和创新。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [299] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 本文评估了7种不同参数规模的大型语言模型在水电监管文件信息抽取任务中的表现，并首次建立了资源-性能映射，为实际部署提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 由于监管文档处理需兼顾提取效果和计算资源消耗，如何选择合适的大语言模型既保证性能又兼顾低成本，是当前行业落地亟需解决的问题。

Method: 系统测试了0.6B到70B参数的7个开源大模型在水电许可文档信息抽取中的效果，聚焦于关键F1分数的变化点以及模型规模对验证方法有效性的影响，并分析了小模型的系统性幻觉（hallucination）问题。

Result: 在14B参数处，模型表现发生显著跃迁，有效验证后F1达到0.64；消费级设备支持模型可达64% F1，小模型最高51%；超大模型F1接近77%，但需企业级硬件。小模型出现完美召回但实际抽取失败的幻觉模式。此外，首次绘制了监管场景下开源模型资源与性能映射图。

Conclusion: 研究为水电合规及广泛信息抽取任务提供了模型选型实证和指导，强调了参数规模阈值和验证机制的重要性，对任务推广有参考意义。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [300] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探索用LLM进行自动形式化以验证LLM生成内容的准确性，初步实验证明其可用于一致性和逻辑验证。


<details>
  <summary>Details</summary>
Motivation: 当前大模型可将自然语言转为结构化输出，但缺乏验证这些输出准确性的方法。本文希望弥补这一空白，探索验证机制。

Method: 搭建LLM驱动的自动形式化管道，设计两项实验：一是检测不同表达的自然语言需求是否等价，二是检测LLM输出与原始需求的逻辑一致性。

Result: 实验一表明模型能识别不同措辞的等价需求，实验二发现了LLM输出与需求间的逻辑冲突，验证了该方法具备一致性检查和形式验证的初步能力。

Conclusion: 初步结果显示自动形式化有助于提升LLM生成内容的可靠性，为后续进一步研究这个方向奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [301] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 本文提出了一个针对电影剧本故事情感分析的自动化框架，通过情感弧线分析和角色语境扩展，辅助选取和理解叙事内容。


<details>
  <summary>Details</summary>
Motivation: 随着叙事数据量急剧增加，单靠人工分析已难以满足需求，因此亟需高效的自动化语义分析方法来理解和分析电影剧本等叙事文本。

Method: 利用基于NRC-VAD的Valence、Arousal和Dominance评分自定义词典，结合LabMTsimple模块进行基于词典的情感分析。情感弧线提取后，通过Ward层次聚类对相似情感轨迹的剧情进行分组，对角色相关情感和上下文进行更深入剖析。

Result: 在电影剧本数据集上实验证明，该分析框架能够有效辅助用户和读者在筛选故事时对内容与情感有更直观的理解与把握。

Conclusion: 该方法提升了电影剧本等叙事文本的自动理解和分析能力，为智能化内容筛选和推荐提供了有力工具，有望应用于更广泛的故事型语料。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [302] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 本文针对放射科随访影像判断任务，构建并标注了6393份报告数据集，系统比较了传统ML和多种大模型性能，发现GPT-4o等LLM在优化后接近人工水平，但传统方法仍有价值。


<details>
  <summary>Details</summary>
Motivation: 目前在医学影像领域，尤其是放射科的随访影像检测任务，缺乏权威、标注好的数据集，导致无法系统评测大语言模型（LLMs）在该领域的能力。作者希望填补此空白，并推动更好的自动化工具发展。

Method: 作者构建并手工标注了6393份来自586名患者的放射科报告，专注于随访影像状态的检测。采用了包括LR、SVM、Longformer、Llama3-8B-Instruct等传统与大模型，以及GPT-4o和开源GPT-OSS-20B，设置了基础和任务优化两种输入配置，并通过改进提示优化模型推理。评测采用精度、召回、F1和自助法统计置信区间。

Result: 互标一致性极高（F1=0.846）。在优化输入下，GPT-4o表现最好（F1=0.832），GPT-OSS-20B表现接近（F1=0.828），传统LR和SVM表现也很强（F1≈0.776），提示经过优化后，LLM已接近人工水平。

Conclusion: 经过提示和输入优化的LLMs在放射科随访检测任务上能接近人类水平，但传统、易解释的方法依然为基线具有实际意义。构建的数据集对于领域内模型评测和发展具有重要价值。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [303] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文提出并公开了首个大规模巴西葡萄牙语医疗问答语料库MedPT，包含38万余条真实医患对话，为葡萄牙语医疗领域大模型发展提供关键资源，并在医疗专业分诊任务上取得了94%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要聚焦高资源语言，忽视了其他语种的医疗应用，简单翻译不能充分反映跨语言的临床与文化特殊性，亟需原生语料支持葡萄牙语等低资源语言的医疗AI发展。

Method: 作者构建并细致筛选了一个包含384,095条真实医患问答的大型葡萄牙语医疗数据集，通过混合定量与定性分析去噪，并借助大模型辅助为问题划分7种语义意图，提升数据集质量和层次。

Result: 数据分析显示该数据集主题广泛（3200类）、独具医患沟通的语言属性。在20类分诊任务上，微调的1.7B参数模型F1分数达94%；错误多因临床实际模糊而非模型失效，反映数据深层语义丰富性。

Conclusion: 此次公开的MedPT语料库可显著促进葡萄牙语医疗AI公平、精确与本地适应性发展，有重要影响力和推广价值。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [304] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 该论文提出了一种名为ClinStructor的管道，利用大语言模型将临床自由文本转化为结构化、任务相关的问题-答案对，从而提升模型可解释性和泛化能力，在ICU死亡率预测任务上仅略微降低性能。


<details>
  <summary>Details</summary>
Motivation: 临床记录通常为非结构化文本，存在偏见、跨平台泛化能力差以及可解释性弱等问题。亟需更透明、可靠并能广泛适用的建模方法。

Method: 作者提出ClinStructor，先用大语言模型把自由文本结构化为特定任务的问题-答案对，然后再做预测建模，增加了建模流程的结构化和可控性。

Result: 在ICU病人死亡率预测任务中，ClinStructor方法的AUC仅比直接微调方法降低2-3%，但显著提高了模型的透明度和可控性。

Conclusion: ClinStructor为临床环境下构建可靠、可解释、泛化性强的机器学习模型奠定了基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [305] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 本文提出使用监督微调（SFT）和强化学习（RL）方法优化GPT-2生成更具情感和情境感知的治疗性对话，实验结果显示该方法显著提升了对话质量和情感识别能力。


<details>
  <summary>Details</summary>
Motivation: 精神健康问题带来重大社会经济负担，且疫情加剧了获取心理健康服务的困难，因此亟需高效、可及的人工智能辅助治疗对话系统，但现有大型语言模型在情境理解和情感把控方面存在不足。

Method: 通过重构输入格式，使模型能同时处理上下文和情绪信息，对GPT-2模型进行监督微调和基于多组件奖励函数的强化学习，促使生成结果对齐于专业治疗师和标注情绪。

Result: 强化学习后，模型在BLEU、ROUGE、METEOR等多项指标上均超越基线GPT-2，情感识别准确率达99.34%（基线为66.96%），模型输出具备高度情境相关性和专业性。

Conclusion: 强化学习显著提升了LLM在治疗性对话生成中的情感和情境把控能力，可作为治疗师的有益辅助工具，同时需要保持临床人工监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [306] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 本文提出了CALM框架，可实现可解释性的LLM临床文本分类，并保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在临床文本分类任务上具有先进性能，但其预测结果缺乏解释性，限制了在临床实际和研究中的应用。医生和研究人员需要明确知道哪些病历部分导致了风险信号。

Method: 提出了CALM（Classification with Additive Large Language Models）框架。该方法通过将输入文本划分为具有语义的组成部分，如病历条目中的不同部分，然后以各部分对预测结果的“加和”形式进行整体预测，使每部分贡献变得可解释。“加和”的结构可提供类似广义加性模型（GAM）的风险曲线等可视化展示，并且适用结构化或可半自动结构化抽取的文本。

Result: CALM在保持LLM基本分类性能的同时，实现了对每个组成部分贡献的直观解释。其解释能力有助于建立医务人员和研究者的信任，并可在模型开发和审核时揭示具有临床意义的模式。

Conclusion: CALM框架为临床文本分类任务提供了性能与可解释性的兼得方案，助力模型实际应用和可信度提升，并支持质量检测及风险可视化。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [307] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 本文提出了一种安全驱动的数据分析LLM新框架：不允许LLM直接生成代码或访问底层数据，只能通过预定义、安全的工具集与数据交互，并提出了评估多步推理能力的新数据集InData。


<details>
  <summary>Details</summary>
Motivation: 直接让大语言模型自动在数据库上生成与执行代码对敏感数据有很大安全隐患，因此需要一种更安全的数据分析方式。

Method: 作者提出限制LLM直接访问和代码生成，只允许其通过严格定义和验证的工具进行交互。此外，建立了新的数据集InData，分为三个难度，用于系统化评估LLM通过工具集进行多步推理做数据分析的能力，并测试了15种开源大模型。

Result: 大型模型在简单任务表现优秀，但在复杂多步任务中准确率显著下降（如gpt-oss-120b在简单任务97.3%，难任务仅69.6%），说明现有模型多步工具使用和推理能力有限。

Conclusion: 当前LLM尚不具备强健的多步工具推理能力。InData的发布有助于推动更安全、更智能的数据分析模型开发和更有效的评测。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [308] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 本文研究了知识图谱驱动的对话生成（KG-DG）任务中，大语言模型（LLM）对外部知识的利用问题。通过提出新的评估方法和实体匿名化技术，有效提升了LLM对知识图谱信息的融合能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种NLP任务上表现出色，但其在利用外部知识图谱生成对话的能力尚不清楚。现有LLM生成对话时往往依赖内部知识，与提供的知识图谱信息脱节。因此，需要研究提升LLM对外部知识依赖度的方法。

Method: 作者提出LLM-KAT评估流程，用于衡量生成回复中知识依赖程度。同时，提出实体匿名化方法，通过隐藏具体实体名称，促使模型关注于利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，实体匿名化等方法能使LLM生成的回复更好地利用外部知识，提高知识依赖度。

Conclusion: 本文的新评估方法和实体匿名化技术可显著提升LLM在知识图谱驱动对话生成任务中对外部知识的利用能力，对相关任务有积极意义。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [309] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 本文探讨了熵校准问题，即语言模型生成文本的熵与其在人类文本上的对数损失是否一致。作者理论和实证分析了当前模型在生成长文本时熵表现及其扩展性。


<details>
  <summary>Details</summary>
Motivation: 动机源于现有自回归语言模型在生成长文本时熵不断增加、文本质量降低。常见做法通过截断分布提高质量，但牺牲了多样性。研究意在探究模型规模能否自然改善熵校准，及理论上是否存在无权衡的完美校准方法。

Method: 作者先从简化理论视角分析数据分布的幂律指数对模型熵校准可扩展性的影响，并据此对大中小（0.5B到70B参数）语言模型的实际熵校准情况开展实证测量。

Result: 理论分析发现，当数据分布的幂律指数接近1时，随着数据规模的增加，熵校准的提升极为缓慢。实证结果显示，各尺寸模型的熵校准能力相近，误差累积速度几乎一致，实际表现符合理论预测。

Conclusion: 模型规模扩展难以显著提升熵校准能力，因此截断虽常用但非理想方案；理论上，如果存在能拟合未来文本熵的黑盒预测器，则有可能在保持log loss的同时降低熵，实现真正校准。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [310] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 本文提出了一种新的命名实体识别（NER）推理框架ReasoningNER，将模型从传统的隐式模式匹配转变为显式可验证的推理过程，显著提升了模型在零样本和低资源场景下的泛化能力，并取得了领先于GPT-4的大幅性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有生成式大语言模型（LLMs）通过指令微调来提升NER能力，擅长基于语义模式匹配生成实体，但缺乏显式且可验证的推理机制。这导致在需要基于有限上下文推理的零样本或低资源场景中，泛化性能变差且容易出现错误。该工作旨在通过引入推理链，提升NER的推理能力和泛化表现。

Method: 本文提出三阶段推理框架：1）首先构建包含任务相关推理链（CoT）的新数据集，用于NER任务；2）利用该数据集对模型进行推理微调，使其在预测前能够生成合理推理链；3）在推理增强阶段，引入奖励信号优化推理质量，确保结果显式可验证。

Result: ReasoningNER在实验中展现了卓越的推理能力，在多项NER任务上取得了有竞争力的性能。在零样本设定下，F1分数超越GPT-4达12.3个百分点。此外，分析结果显示其在面向推理信息抽取研究中的巨大潜力。

Conclusion: 通过显式推理链的引入与微调，本文提出的ReasoningNER大幅提升了命名实体识别任务的推理及泛化能力，尤其在零样本、低资源等传统方法易失效的场景表现突出，对面向推理的信息抽取研究有重要推动作用。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [311] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 链式思维（CoT）解释可以提升透明度，但同时会引发确认偏误，使用户过于信任模型，即使模型推理存在错误。


<details>
  <summary>Details</summary>
Motivation: 解释通常被视为提升人工智能系统透明度的工具，但实际应用中解释可能助长用户对AI输出的盲目信任。作者希望揭示链式思维（CoT）解释在多模态道德情景中的双刃剑作用。

Method: 作者通过系统性地修改推理链，并操纵模型输出的语气，对视觉-语言模型（VLMs）中的推理错误、用户信任和用户发现错误的能力进行实验分析。

Result: （1）用户更倾向于通过结果一致性而产生信任，即使模型推理实际有误用户仍依赖；（2）自信的语气降低了错误被发现的概率，同时用户持续保持信赖，说明输出风格对正确性的影响超过实际推理。

Conclusion: CoT 解释既能提升透明度，也可能误导用户。NLP系统应提供能激发用户质疑与批判性思维的解释，而非助长盲从。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [312] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一套新颖的文化能力测试基准，并用新评价体系全面评估大型语言模型在多元文化情境下的表现。结果显示，传统“薄”评价方法高估了模型的文化理解力，而“厚”评价能更准确、稳定地揭示模型在文化推理上的优劣。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型文化能力的评测多为“去情境化”或强制选择，忽视了模型对真实、多元文化情境的理解和推理能力。为弥补这一不足，需要更真实、更细致的方法来评估模型的文化推理与理解。

Method: 作者设计了一套新的评测基准，通过提出带有现实情境的任务，要求模型做出符合具体文化语境的推理。同时，除常规的“准确匹配”外，还引入了覆盖度、具体性、内涵和连贯性四个维度的评价指标。

Result: 实验结果显示，传统基于表层对错的“薄”评价体系在衡量文化能力时高估模型表现且评估结果波动大。而新提出的“厚”评价法能区分模型的推理深度，降低评估方差，提供更稳定且易解释的文化理解能力判据。

Conclusion: 厚评估法比传统薄评估更能真实反映大型语言模型对于多元文化环境下任务的表现，有助于未来模型在文化多样性和理解力上的提升和优化。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [313] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 本文结合反向翻译（BT）与微调（FT）方法，显著提升了在小型日语数据集上的神经机器翻译表现。


<details>
  <summary>Details</summary>
Motivation: 当前神经机器翻译在低资源语言对（如英日）场景下表现有限，尤其是在可用平行语料很少的情况下，有效提升翻译质量的方法需求迫切。

Method: 以基础英→日模型为起点，先利用日语单语语料通过反向翻译方式生成合成数据进行训练，再用真实小型中英平行新闻及文学数据集对模型微调。进一步，将反向翻译生成样本和原始数据集整合，然后整体微调。

Result: 仅用反向翻译，COMET评分小幅提升（0.460→0.468）；仅用微调时，大幅提升（0.460→0.589）；结合两者后，表现进一步提升，最高达0.597。

Conclusion: 即使训练数据有限，反向翻译和微调协同使用，也能显著提升英日翻译质量，优于各自单独应用，为低资源语种模型提升提供了一种高效策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [314] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: 本文提出了一种名为LLMLagBench的新基准，专用于评测大语言模型训练数据的时间边界和模型对新近事件的了解程度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）受限于其预训练数据的时间截止点，导致对于时效性强的问题无法回答最新信息。当用户不知道训练时间边界时，模型还可能混入过时的信息，影响推理准确性，因此急需一种系统方法评估模型的时间边界和新知识把握能力。

Method: 作者提出了LLMLagBench基准，通过评估LLM对最新事件的知识掌握度，系统识别其训练数据的最早时间边界，并将该基准应用于大量主流LLM，涵盖声明和未声明数据截止点的模型。同时，基准有效性通过人工校验和与公开信息对比验证。

Result: 使用LLMLagBench，作者成功评测了多种LLM的时间知识边界，且通过人工和公开信息对比表明基准评测具有很高的可靠性。

Conclusion: LLMLagBench为分析和比较LLMs的训练数据时效性和新知识掌握能力提供了客观、有效的工具，能帮助用户合理预期模型回溯信息的准确性。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [315] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 现有多模态对话立场识别研究存在伪多模态和用户同质化问题。本文提出U-MStance数据集和PRISM模型，通过引入用户画像和多模态上下文推理，显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态立场检测研究中的视觉信息仅限于主贴，评论区只有文本；且忽视了用户个性对立场表达的影响，导致结果与真实社交场景脱节。为此，需构建更加贴近真实互动和个性差异的数据集和模型。

Method: 构建了U-MStance大规模用户中心化多模态立场检测数据集。提出PRISM模型，从用户历史行为中抽取长时序用户画像，融合评论区多模态信息，通过链式思考方法对齐多模态语义和语用；引入任务互助机制，实现立场检测与立场感知回复生成的联合优化。

Result: 在U-MStance数据集上的实验证明，PRISM模型性能显著超越强基线模型，充分体现了用户画像建模和多模态、有上下文推理的重要性。

Conclusion: 将用户个性和全流程多模态信息引入对话立场检测，能显著增强现实场景下社交媒体立场理解的效果，推动该领域更加贴近真实应用。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [316] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 本文提出了AI-Salesman框架，创新性地用于电销等说服性对话任务。该方法在真实数据集TeleSalesCorpus上，结合了贝叶斯强化学习和动态脚本指导，在多个评测指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 说服性对话（如电话推销）需要多轮规划和事实准确性，但现有大模型很难兼顾这两点，且缺乏领域专用数据。此外，直接应用LLM容易产生策略僵化及虚假内容，需要新方法有效提升模型能力。

Method: 1）构建并发布了真实领域的电销对话数据集TeleSalesCorpus；2）提出双阶段AI-Salesman框架，包括训练阶段的贝叶斯监督强化学习（从真实噪声对话中学习鲁棒策略），推理阶段的DOGA（动态脚本库逐轮指导）；3）引入结合细致能力指标与LLM自动评审的综合评测框架。

Result: 实验结果显示，AI-Salesman在自动化指标和人工综合评价上均大幅优于各种基线方法，能更有效应对复杂的说服场景。

Conclusion: AI-Salesman框架结合数据、方法和评测创新，为复杂说服型对话带来新的进展，在电销等领域具备落地和推广价值。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [317] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: 本文提出了VBackChecker，一种无需参考的多模态大语言模型（MLLMs）幻觉检测框架，并建立了新的评测基准。该方法有效提升了幻觉检测能力，表现优于现有复杂方案。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在实际应用中由于幻觉现象不可靠，准确检测幻觉成为提升模型可靠性的关键。当前检测方法要么依赖参考数据，要么对复杂场景处理能力不足，且缺乏解释性。

Method: 提出VBackChecker，该系统基于像素级Grounding LLM，结合推理与指代分割功能，验证多模态大模型输出与视觉输入的一致性。设计了新的指令微调数据生成流程（R-Instruct），包含丰富上下文、分割掩码和高难度负样本。构建了R^2-HalBench评测集，涵盖18个MLLM的真实世界、丰富场景数据与高质量标注。

Result: VBackChecker在R^2-HalBench数据集上表现优异，超越以往的复杂幻觉检测框架，在像素级定位任务上提升超10%，在幻觉检测能力上接近GPT-4o。

Conclusion: VBackChecker显著提升了MLLM在复杂场景下的幻觉检测能力，兼具高解释性和无需参考数据的优势，为实际应用中的MLLM可靠性提供新路径。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [318] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: 提出CriticSearch框架，利用细粒度、逐回合的反馈机制提升大语言模型与搜索引擎结合下的问答能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型集成搜索工具模式，依赖强化学习训练，但强化学习中的稀疏奖励导致训练效率低、稳定性差。急需改进奖励分配，提升搜索问答系统的训练效率和效果。

Method: 提出CriticSearch，通过冻结的异步评价大模型，对每一步推理过程进行回溯评判，利用全轨迹和标准答案信息，生成更密集、稳定的奖励信号，逐回合引导策略优化。

Result: 在多种多跳推理数据集上的实验显示，CriticSearch收敛速度更快、训练更稳定、性能显著提升，超越现有主流方法。

Conclusion: CriticSearch有效改善了工具整合推理中的奖励稀疏问题，为大模型依靠外部知识检索进行复杂推理提供了高效的训练范式。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [319] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: 提出了MME-RAG方法，将实体识别任务分为两步，通过模块化管理增强检索，显著提升了多领域任务型对话的实体识别表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在任务型对话中的细粒度实体识别，特别是在领域适应和检索可控性上表现不佳，影响了推理和决策能力。

Method: 提出MME-RAG框架，将实体识别任务分为两步：第一步由轻量级管理器做类别级判断，第二步由专家模型做实体区间级提取。每个专家配有KeyInfo检索器，在推理时引入语义对齐的少样本案例，无需额外训练即可适应不同领域。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和自建多领域客服数据集上，MME-RAG在多数领域优于最新基线方法。消融实验显示：层次化分解策略和KeyInfo检索机制是提升稳健性和跨领域泛化能力的关键。

Conclusion: MME-RAG是一种具备可扩展性和可解释性的自适应对话理解方案，有效解决细粒度实体识别的领域适应和检索可控性难题。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [320] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 该论文提出了一种高效检测大语言模型（LLMs）幻觉输出的新方法CONFACTCHECK，无需外部知识库，且资源消耗更低，准确率更高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常常生成事实不准确或与真实世界不符的文本（即“幻觉”），在医疗、金融等关键领域存在重大风险。而当前常用的检测手段要么需要多次API调用（增加成本和延迟），要么需要访问模型权重，但实际大部分情况下只能通过API调用，难以落地。

Method: 提出CONFACTCHECK方法，核心思想是在生成文本中进行事实性探查，并检测模型自身以及不同模型之间的响应是否一致，以此判断是否存在幻觉。该方法无需借助外部知识库，仅基于LLM本身输出进行一致性检测。

Result: 在多个数据集上与其他基线方法对比实验，CONFACTCHECK能以更少资源消耗、较快速度实现更高准确率，优于同条件下的现有方法。

Conclusion: CONFACTCHECK方法为API级、资源受限环境下的LLM幻觉检测提供了高效且易于部署的新思路，有望推动LLM安全可靠应用。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [321] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种新方法ViConBERT，通过结合对比学习和词义蒸馏提升越南语语义理解，达到当前最优性能，并发布了相关大规模评测数据集。


<details>
  <summary>Details</summary>
Motivation: 虽然上下文化词嵌入技术带来了语义任务的重大突破，但大多数仅限于高资源语言，越南语等低资源语言仍缺乏高效模型与评测资源。本文旨在填补该领域空白，提升越南语精细语义理解能力。

Method: 创新性地将SimCLR对比学习与基于词义 gloss 的知识蒸馏相结合，提出ViConBERT模型。同时，构建并发布了首个大型合成越南语语义理解评测数据集ViConWSD，涵盖词义消歧与上下文相似度任务。

Result: ViConBERT在越南语词义消歧（F1=0.87）、ViCon 语义评测集（AP=0.88）和ViSim-400（Spearman's rho=0.60）上均超越或媲美现有最佳模型，有效提升了模型在离散语义与连续语义的表达能力。

Conclusion: ViConBERT显著提升了越南语语义任务的表现，为低资源语言构建强大语义模型与评测体系提供了新思路与工具。相关开源资源有助于社区进一步优化和拓展越南语自然语言处理。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [322] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出了一种用小型大语言模型（LLM）压缩输入，再让大型LLM处理的新范式，并系统性评估多种模型作为“输入压缩器”的能力，最终提出新模型Cmprsr，在多场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 调用大型LLM成本高，因此希望利用较小的LLM对输入做压缩，减少调用大模型时的金钱与算力消耗。

Method: 系统性地评测了25个开源和闭源LLM作为压缩器，包括其语义保留及对用户压缩率要求的服从性；对表现最佳的小模型进行压缩提示优化（如Textgrad方法）；并在发现Qwen3-4B效果突出后，采用监督微调和GRPO后训练，追求压缩率控制与下游任务表现，最终形成Cmprsr模型。

Result: Cmprsr在长文本（MeetingBank, LongBench）及短文本（GSM8k）场景下，比其他抽取式和抽象式压缩方案性能更优，且对压缩率要求响应准确。

Conclusion: 通过用优化过的小LLM对输入压缩可大幅降低大LLM推理成本，同时保证压缩后文本高质量和高灵活性控制，具有高通用性。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [323] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种将法律判决文书的抽象式摘要转化为提取式摘要的自动化流程，旨在缓解人工标注的高成本，并丰富现有摘要数据集，提升法律文书自动摘要的研究与应用。


<details>
  <summary>Details</summary>
Motivation: 法律文书由于含有复杂的法律术语、上下文依赖性强且篇幅长，摘要生成对法律工作者造成了很大负担。尽管深度学习方法能生成抽象式摘要，但易误解法律术语或遗漏关键信息，因此越来越多研究倾向于使用提取式摘要。此外，人工标注高质量的提取式摘要成本极高，亟需自动化解决方案。

Method: 作者提出一种轻量级、透明的流程，将现有的抽象式黄金标准摘要自动转变为对应的提取式黄金标准摘要，并保持原有摘要中的专家观点。该方法用于增强七个包含抽象式摘要的法律判决数据集，通过添加提取式摘要丰富资源。还对新生成的提取式摘要与原有抽象式摘要在结构、词汇、语义等方面进行全面对比评估，并在领域信息层级上进行比对。

Result: 成功将七个判例摘要数据集中的抽象式摘要自动转为对应的提取式摘要，并进行了多维度评估，验证了新摘要的质量和有效性。即将公开发布有助于推动法律文书自动摘要研究。

Conclusion: 本文工作有效降低了提取式摘要人工标注的成本，提供了丰富的数据资源，为法律文书自动提取摘要提供了新思路，将有助于相关研究领域的进一步发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [324] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本文比较了大语言模型（LLMs）与人类在 buzzer 竞答环境下答题的难易差异，发现它们在部分题目上表现不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多自然语言处理任务中超越了人类，但尚不清楚对人类来说难的问题，对LLMs是否同样困难。作者希望通过对比分析，揭示LLMs与人类在答题难度上的异同。

Method: 作者收集了包含问题、答案和人类正确率的日语竞答数据集，并设计多种prompt让LLMs答题，随后从两个分析视角将LLMs与人类的正确率进行了对比。

Result: 实验发现，与人类相比，LLMs在Wikipedia中未覆盖的答案题，以及需要数值型答案的题目上表现更差。

Conclusion: LLMs与人类在答题难度上存在差异，特别是在超出其知识覆盖范围和涉及数值计算的问题上，LLMs表现仍有较大提升空间。

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [325] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文研究了否定指令（如“不要提到X”）下，大语言模型（LLM）是否会出现与人类类似的“反弹效应”（即原本要回避的内容反而被激活）。通过实验发现，否定后的反弹效应广泛存在，且与干扰内容类型和极性分离程度相关。


<details>
  <summary>Details</summary>
Motivation: 人类在接收到否定指令时，往往会无意中激活被禁止的内容，这称为“讽刺性反弹效应”。目前尚不清楚大语言模型在类似指令下，是否也会出现类似人类的认知偏差。探究这一问题有助于深入理解LLM的内部机制及模型可靠性。

Method: 作者设计了两个实验：（1）在给出否定指令后，引入不同类型的干扰内容（语义、句法、重复）来衡量反弹效应的强度；（2）检验模型对同一概念的中性与否定表达的区分能力，并关联于反弹的持续度。同时，进行注意力机制的电路追踪分析，发现模型中部分中层注意力头在放大被禁止词时有特殊作用。

Result: （1）否定指令后，模型普遍出现反弹，且随着干扰内容变得更长或有语义性时，反弹效应增强，而重复内容有助于抑制反弹。（2）模型区分中性和否定表达的能力越强，其反弹效应持续性越高。电路追踪表明，有些中层注意力头会放大对被禁止内容的关注，早期层则尝试抑制。

Conclusion: LLM在否定指令下存在与人类类似的讽刺性反弹效应，其机制体现为模型内部不同层和注意力头功能分化；这些结果加深了我们对LLM长上下文干扰与认知偏差的理解。作者还公开了ReboundBench数据集以促进后续研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [326] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: 本文提出了首个专为泰米尔语设计的语言评价基准ILAKKANAM，并用其系统评估了各类大语言模型在泰米尔语下的能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语种基准测试大多基于英文翻译数据，难以覆盖针对低资源、形态丰富语言（如泰米尔语）的真实语言和文化特征，导致对模型在这些语言上的表现了解甚少。

Method: 研究团队人工筛选并整理了820道来自斯里兰卡学校各年级的泰米尔语考试题目，涵盖五大语言学类别及事实知识类别，由专业语言学家标注，构建ILAKKANAM基准。随后，他们用标准评价框架测试了开源与闭源大语言模型在此基准下的表现，并按类别、年级进行详细分析。

Result: Gemini 2.5在总体表现最佳，开源模型明显落后，突显出在语义扎根方面的差距。所有模型在低年级题目表现良好，但随着语言复杂度提升，表现明显下降。另外，模型整体性能与其区分语言类别的能力没有强相关，说明模型表现可能更多源于数据曝光，而非真正理解。

Conclusion: 大语言模型在低资源、形态复杂的语言上仍有显著进步空间，当前表现多依赖于训练曝光而非深层理解，强调针对特定语言文化精细化评测基准的重要性。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [327] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 该论文提出了用于评估奖励模型的新方法，即通过探查偏好表示进行评估，并构建了多维奖励模型基准（MRMBench）以测试不同偏好维度下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型通常只在固定的成对排序测试集上进行评估，缺乏对各偏好维度表现的细致测量，难以全面反映奖励模型的能力。

Method: 作者开发了MRMBench，包括六个针对不同偏好维度的探查任务，引导奖励模型在多维度下捕捉偏好。此外，引入推理时探查分析方法，用于识别奖励预测过程中实际使用的偏好维度并增强可解释性。

Result: 实验表明，MRMBench的表现与大语言模型对齐效果高度相关，是奖励模型开发的可靠参考。分析结果显示，现有奖励模型在多维偏好捕捉上表现有限，且推理时探查方法能有效衡量奖励预测的可信度。

Conclusion: MRMBench和推理时探查为奖励模型的多维评估和优化提供了新工具和指标，有助于提升大语言模型的对齐水平和对奖励模型的深层理解。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [328] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 本文提出了关注“意外性”（serendipity）的知识图谱问答（KGQA）框架SerenQA，集成了相关性、新颖性和意外性度量，发现主流大模型仍难以发现有价值的新发现。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答系统多追求相关但可预测的答案，缺乏发现意外且有价值发现的能力。而在科学与临床等领域，意外发现对于创新至关重要，因此亟需衡量与促进大模型在该方向的能力。

Method: 作者正式定义了面向意外性的问题任务，提出SerenQA评测框架，包括相关性、新颖性和意外性三要素的评价指标，构建了以临床知识图为基础、专家标注的基准集（聚焦药物再定位），并设计了涵盖知识检索、子图推理和意外性探索的评估流程。

Result: 实验显示，主流大语言模型在知识检索环节表现优异，但在发掘真正令人惊喜和有价值的新发现方面，表现仍较为有限。说明在意外性方面还有显著提升空间。

Conclusion: 论文为KGQA引入了评测与促进模型挖掘“意外发现”的新框架，提供了公开资源和严格度量，为后续提升大模型科学创新能力奠定基础。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [329] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一种面向大语言模型（LLM）的轻量级安全防护机制，能检测有害内容和对抗性提示，在各类基准上表现优异，并公开发布。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，如何有效检测和防御有害或对抗性输入成为迫切需求。现有方法要么负载过重，要么误判率高，需要更高效且实用的解决方案。

Method: 作者提出SGuard-v1，包括两个专门子模型：ContentFilter用于识别语料中的安全风险，JailbreakFilter用于检测和防御对抗性提示。SGuard-v1构建于Granite-3.3-2B-Instruct（20亿参数），涵盖12种语言，采用140万条收集与合成数据进行指令微调，两组件训练目标不同，分别分配数据。

Result: 在多项公开和专有安全基准测试中，SGuard-v1在保持模型轻量化的同时实现了最先进的安全检测表现，同时误判率较低并提升了判别的可解释性。

Conclusion: SGuard-v1不仅为LLM提供了高效准确的安全防护，同时因其轻量和可解释性强，适合实际部署和后续AI安全领域的研究。代码和模型以Apache-2.0许可证发布，便于学界和业界采用。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [330] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 该论文提出了一种面向名词的QA语义分析框架（QA-Noun），用于细粒度语义分解，并与现有的谓词-论元QA方法结合，大幅提升了句子的事实分解粒度。


<details>
  <summary>Details</summary>
Motivation: 现有基于问答（QA）的语义分解方法主要关注于谓词-论元关系，较少涉及以名词为核心的语义建模，导致难以全面覆盖句子内的各种事实与语义角色。

Method: 作者设计了九种面向名词的问答模板，涵盖名词在句子中的显性语法角色和隐含语境角色，并据此建立注释指南、语料库，并训练模型，实现了与QA-SRL的统一语义分解。

Result: QA-Noun能全面覆盖AMR中的名词论元，同时发现额外的语境隐含关系。与QA-SRL结合后，句子的事实分解粒度提升130%以上，明显优于FactScore、DecompScore等方法。

Conclusion: QA-Noun有效补足了基于QA的语义分析框架，实现对句子的更细粒度、更全面的语义分解，为跨文本语义对齐和事实提取提供了可扩展的技术基础。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [331] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TAdaRAG的新型检索增强生成（RAG）框架，通过任务自适应地从外部知识构建知识图谱，并验证其在各领域和长文本任务中的优越效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因需要将外部知识切分为更小片段，导致信息丢失，进而引发模型幻觉与推理链断裂。此外，传统RAG常检索非结构化知识，带来无关细节，妨碍准确推理。

Method: 提出TAdaRAG框架，采用意图驱动的路由机制，将任务导向特定领域的抽取模板，再结合有监督微调与基于强化学习的隐式抽取方法，实现知识的简明、一致与无冗余集成。

Result: 在六个公开基准和一个真实商业场景（NowNewsQA）测试，使用三种主流模型，TAdaRAG在多领域和长文本任务中均超越现有方法，展现出良好泛化性和实用价值。

Conclusion: TAdaRAG有效解决了RAG中信息丢失和无关干扰问题，提升了大模型推理与生成表现，是实际应用中更优的RAG方案。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [332] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果推断的数据增强方法，有效缓解了RLHF奖励模型中普遍存在的长度偏差，提高了奖励分配的公平性和内容敏感度。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的RLHF过程中，奖励模型容易产生对长文本的偏爱，将啰嗦与高质量错误地等同，影响模型输出的实际质量和效率。

Method: 作者首次提出利用因果框架分析和缓解奖励模型的长度偏差。具体方法包括生成两类反事实响应对：其一是长度差异大但内容类似，另一类是内容差异大但长度相近。通过将这些样本加入奖励模型训练，使模型能独立判断内容质量，而不被长度所干扰。

Result: 实验证明，该方法有效降低了奖励模型对长文本的偏好，使政策模型输出结果更加简洁且关注内容本身。

Conclusion: 提出的因果反事实数据增强方法能够显著抑制奖励模型的长度偏差，提升其对内容的敏感性和整体健壮性，适用于RLHF流程的优化。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [333] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 本文针对任务型对话系统与实际应用中GUI界面及无定制API的现实割裂，提出并构建了新的多模态对话数据集MMWOZ，并以此设计了新基线模型MATE，推动多模态任务型对话系统发展。


<details>
  <summary>Details</summary>
Motivation: 传统任务型对话系统通常依赖于定制的后端API，然而在实际中前端GUI界面的普及和后端API难以获取，导致这些方法难以落地。因此需要新的方法和数据集以支持真实环境下的任务型对话系统。

Method: 作者首先开发Web风格GUI作为前端，并编写自动化脚本将MultiWOZ 2.3中对话状态及系统动作转为GUI操作指令，同时收集网页快照与操作指令，构建MMWOZ数据集。随后提出多模态基线模型MATE，并在新数据集上进行实验分析。

Result: 作者成功构建了MMWOZ多模态数据集，涵盖网页快照和操作指令。基线模型MATE在该数据集上实现了对任务型对话Agent的初步建模和实验评估。

Conclusion: MMWOZ数据集与MATE模型为多模态任务型对话（尤其在仅有前端GUI而无API的场景）提供了基础，实现了更贴合真实应用的对话Agent研究平台，推动了相关领域发展。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [334] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法GAPO，有效提升了大语言模型生成结果的多样性，避免过度重复。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型常出现模式崩溃问题，即在存在多个有效答案时，生成内容经常重复，缺乏多样性，影响了模型实际应用的效果。

Method: 作者提出Group-Aware Policy Optimization（GAPO），这是对现有Group Relative Policy Optimization（GRPO）的一种扩展。GAPO通过在整体组的层面上考虑奖励函数，使模型能够学习群体层面的属性如多样性和覆盖率。实验中采用基于生成概率的奖励机制，鼓励模型对有效答案进行均匀采样。

Result: 使用GAPO训练的大模型能产生更为均匀、有效且多样的回答。实验显示，该方法能够提升开放式任务和标准基准（如GSM8K、MATH、HumanEval、MMLU-Pro）上的输出多样性，同时准确率没有下降。

Conclusion: GAPO是一种简单而有效的方法，可以提升大语言模型结果的多样性，缓解模式崩溃，为实际应用提供更丰富的输出，且不损害原有准确性。作者还计划开源其代码。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [335] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0 是Lychee系列的全开源全模态大模型，主打跨模态（文字、图片、语音等）理解、推理和生成，性能大幅超越同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流多模态大模型在理解、推理和生成能力上受限，尤其在不同模态（如视频、音频、图片、文本等）间的统一建模和高效推理存在瓶颈。本文旨在通过改进模型架构和训练方法，提升多模态大模型的通用性及各项任务表现。

Method: 1) 采用动态容量的门控专家混合（MoE）结构，平衡效率与能力，支持10种模态输入；2) 引入Omni-Modality 3D RoPE提升模态间时空对齐；3) 创新训练流程，先跨模态预训练，再逐步监督微调激活专用专家，利用迭代GSPO-DPO方法稳定强化学习训练并增强推理；4) 精选7.5亿token的多模态开源数据，加入专为生成设计的token，实现条件生成。

Result: 在85项多模态基准评测中，多项表现超过同类顶级模型，对比Qwen2.5-Omni在76项任务中有50项表现更优，尤其在视频理解、全模态理解和视听觉推理等任务提升4-7%；长语音处理WER降幅达4.2%；在低级图像处理和可控生成等细分领域领先。

Conclusion: Uni-MoE 2.0作为高效、开源的全能多模态大模型，在多模态任务上实现了全方位领先，为统一大模型发展提供了新范例，并具有广泛的实际应用潜力。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [336] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出了一种针对航空通知（NOTAMs）的语义解析任务，并构建了高质量的标注数据集Knots，显著提升了NOTAM自动化理解与处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有NOTAM自动解析方法多停留在表层，如分类或实体识别，缺乏对其复杂语义和行业知识的深度理解，因此亟需发展更具推理和语义层次的自动化处理方法。

Method: 提出NOTAM语义解析新任务，结合航空领域知识、语义推理能力，生成结构化推理结果，并构建了包含12,347条NOTAM的高质量专家标注数据集Knots，支持多领域协作标注。系统评估了多种提示工程和模型适配技术。

Result: 通过系统性实验，提出的方法在航空文本理解和处理方面取得了显著进步，证明了所构建数据集和方法的有效性。

Conclusion: 该方法和数据集推动了自动NOTAM分析系统的进步，为航空领域重要文本的深度自动化处理提供了有力工具和见解。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [337] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出Reason-KE++，通过对大型语言模型(LLM)多跳推理任务中的推理过程级别对齐，弥补当前SFT方法存在的“忠实性缺口”，有效减少事实幻觉，并在复杂任务中刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT（监督微调）方法，被用于教会LLM更好地完成多跳推理任务，但其实它们更关注输出格式模仿而非真实推理逻辑，导致模型容易忽略事实编辑，发生错误推理和事实幻觉。

Method: 提出Reason-KE++，结合SFT和强化学习（RL）。创新地引入Stage-aware Reward机制，为推理过程中的每一步（如分解、子问题回答）加密集奖励监督，强化过程均衡而非只看最终答案。另外发现仅用最终正确率优化RL会导致推理质量被破坏。

Result: Reason-KE++在MQUAKE-CF-3k 数据集上表现优异，最终推理准确率为95.48%，比最优基线提升5.28%。并且过程带来的推理完整性(如Hop准确率)也大幅提升，显著减少了LLM对事实的“幻觉”。

Conclusion: 在多跳复杂推理任务中，只有对齐推理过程本身、加强中间思考步骤的监督，才能从根本上提升LLM的可信度和正确性，仅仅优化最终输出会掩盖推理过程的缺陷。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [338] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种直接的波斯语到英语语音翻译（S2ST）系统，通过合成并扩充平行语音数据显著提升了模型在低资源场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 直接语音到语音翻译模型因其简化流程和降低推理延迟，在理论上优于传统级联系统，但受限于低资源语言（如波斯语）缺乏大规模平行语音语料的现实问题。

Method: 提出了三部分组成的S2ST模型：1）基于conformer的编码器，利用自监督预训练；2）带有相对位置注意力的因果transformer解码器，将声学特征转为离散目标语音单元；3）基于单元的神经声码器生成语音波形。为扩充数据，通过大语言模型翻译和先进的语音合成系统，生成大量波斯-英语平行语音，扩充数据量至原有6倍。

Result: 在CVSS语料库的波斯-英语部分，所提模型在引入合成数据后，ASR BLEU分数较基线提升4.6分。

Conclusion: 自监督预训练、离散语音单元与合成数据的结合，有效提升了低资源语言对之间的直接语音翻译性能。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [339] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: 论文提出了EvoSynth，一个能自主进化破解算法的自动化攻防系统，从而超越了以往只能基于现有策略组合的自动攻防框架。实验显示其对强健大模型如Claude-Sonnet-4.5的攻击成功率高达85.5%，同时生成的攻击方式更加多样。


<details>
  <summary>Details</summary>
Motivation: 当前自动化大模型攻防（red teaming）框架仅能基于已有的攻击策略组合或改良，缺乏自主创新新的破解机制的能力，这严重限制了攻防测试的广度和深度。

Method: EvoSynth框架摒弃传统攻击提示的细化方法，采用多智能体系统自动化设计、演化和执行新颖、基于代码的攻击算法。其核心是代码级别的自我修正循环，能针对失败案例自动重写和优化自己的攻击逻辑。

Result: 在大量实验中，EvoSynth对如Claude-Sonnet-4.5等鲁棒性极强的大模型，取得了85.5%的攻击成功率。生成的攻击方式也比现有方法更加多样化，显著提升了攻击覆盖率和创新性。

Conclusion: EvoSynth不仅将大模型攻防自动化推向新的高度，也为攻击算法的进化合成开启了研究新方向，并已公开代码以促进后续研究。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [340] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: 提出了一种称为Adaptive Focus Memory（AFM）的动态上下文管理方法，用于提升大语言模型在多轮对话中的效率与安全性。AFM通过智能分级存储历史消息，在显著降低计算成本的同时，保持关键信息不丢失。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话中的记忆策略要么简单但资源消耗大（如每次全量重放历史），要么容易丢失用户关键信息（如静态摘要或只重视近期消息）。特别在安全敏感场景（如过敏信息），这些缺陷可能造成严重后果，因此亟需一种高效且安全的信息管理方案。

Method: AFM为对话历史中的每条消息动态分配三种保真度级别（全量、压缩、占位），依据当前查询的语义相似性、时间权重（半衰期）及重要性分类综合决定。系统在受限token预算下，优先保留相关度最高的信息，同时尽量记录完整交互痕迹。实现上，AFM提供了面向OpenAI兼容API的Python模块，支持离线运行。

Result: 在用户具有严重花生过敏并计划前往泰国的安全场景基准测试中，AFM能够在短至中等长度对话中保持对过敏信息的记忆；其安全性表现与全量重放策略相当，但平均token消耗降低了66%。

Conclusion: AFM能显著降低推理成本而不牺牲对话安全性或事实连续性，非常适合多轮对话系统对高效与安全的同时需求。其开源实现便于实际部署和推广。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [341] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型（LLMs）在集合理解方面存在明显脆弱性，即使面对极为基础的集合成员判断任务，仍然会出现不可预测的错误和表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 近年来LLMs在复杂推理任务上表现卓越，但对简单任务却常常失误，这一矛盾引发了人们对其可靠性和可解释性的担忧。作者希望通过基础任务系统性地揭示模型潜在的失效模式。

Method: 作者设计了极为简单的集合成员查询任务，例如“苹果是否属于集合{梨、李子、苹果、覆盆子}”，并在大规模实验中考察了不同的提示、语义结构、元素顺序和模型类型对结果的影响。

Result: 大规模实验证明，LLMs在集合成员判断这类基础任务上的表现高度不稳定且不可预测。所有变量（提示、结构、顺序等）都会引发性能的显著波动，说明模型对集合的理解非常零散和混乱。

Conclusion: 该研究通过简单任务的大规模实验，系统描绘了LLMs的失效模式，强调了这种方法对于未来模型评测和机制理解的巨大价值。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [342] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 本文探讨了语言模型训练过程中相变现象是否不仅存在于大型模型，也可在小模型、线性训练空间及训练早期观察到。作者提出了一套基于词汇统计的新指标，发现即使在较小的transformer语言模型中，相变依然存在并能在传统损失曲线外被检测到。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，大型语言模型在规模达到某临界点后会出现突发能力提升（即相变现象）。但尚不清楚小型模型是否也有类似的现象，以及这种现象能否在训练曲线的线性空间与训练早期检测。

Method: 作者训练了一个小型的GPT风格的transformer模型，用字符级语料进行训练，同时追踪单词平均长度、正确/错误词数、词汇多样性，并采用Poisson与sub-Poisson统计法分析词的组织方式，系统检测训练期间这些指标的变化。

Result: 作者发现，在训练的某个阶段词汇统计指标会出现明显的转变点。这些转变无法通过传统损失或验证曲线发现，但通过新设计的基于词汇和统计的方法能被清楚揭示。

Conclusion: 相变组织是语言模型训练的普遍特征，不局限于大型模型，在小型模型及训练早期也能在训练线性空间直接观测到。结果强调了开发专门指标和探针检测训练动态的重要性，有助于理解模型能力突现的机制。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [343] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 本论文提出通过在用户输入中定期插入控制句子（称为“中断”）来提升大型语言模型的对齐能力，尤其是应对输入过长时易被越狱的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的对齐研究主要关注提升模型在对抗性攻击和异常行为下的鲁棒性，但随着用户输入长度增加，越狱概率也随之上升，且缺乏随输入规模增长而有效的对齐强化方法。

Method: 论文提出了一种基于“中断”的方法：即每隔约x个token在用户输入中加入控制性句子，以此抑制模型偏离期望轨道的行为。同时建议这种做法可通用于链式思维（Chain-of-Thought）处理过程，防止模型策划不当行为。

Result: 尚未给出具体实验结果，仅提出了“中断”作为解决输入长度随越狱概率上升问题的初步解决方案。

Conclusion: “中断”机制有望成为提升LLM对齐能力且可扩展到长输入场景的有效方式，未来需进一步具体实验验证其效果。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [344] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLMs）在自动形式化任务中对同义改写自然语言输入的鲁棒性，发现输入表达方式的微小变动会显著影响模型输出的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动形式化领域表现出色，但它们在生成可靠、可验证的表达时仍存在困难。已有研究显示，LLM输出容易因自然语言输入的同义改写而产生较大波动，因此本论文希望验证在生成形式化证明任务中，这一现象是否依然成立。

Method: 作者选用了MiniF2F和Lean 4版的ProofNet两个形式化基准，以及两个主流的LLM。通过对自然语言陈述进行语义等价但措辞不同的改写，生成一系列输入，并交叉评估模型对这些改写输入的表现，主要度量语义和编译有效性。

Result: 实验结果表明，在不同的同义改写输入下，LLM输出的性能表现具有较大波动；即使输入语义非常接近，模型输出的有效性和可靠性也会显著变化。

Conclusion: 即便自然语言表述仅有细微的措辞变动，这些变化也会对LLM的自动形式化结果带来较大影响，揭示了当前大语言模型在这一领域的鲁棒性不足。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [345] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: 本文推出了BioMedJImpact数据集，系统性地分析生物医学期刊的合作强度、AI参与度如何共同影响期刊影响力。结果发现合作更紧密和AI参与度更高的期刊，引用影响力和期刊声望更高，同时验证了AI检测方案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 虽然期刊影响力评估对学术交流至关重要，但目前的公开数据资源很少能反映‘合作结构’与‘AI研究’对生物医学领域期刊声望的共同影响。因此，作者希望填补这一空白，推动更细致的期刊层面学术影响与AI参与的关联分析。

Method: 构建BioMedJImpact数据集，涵盖174万篇来自2744个期刊的生物医学文章，整合了文献计量指标、合作属性和大模型提取的AI参与语义特征。AI参与特征采用作者提出的三阶段LLM流程自动提取，并通过人工评估进行有效性验证。随后，利用数据集分析2016-2019与2020-2023疫情前后合作强度和AI参与度对科学影响的作用。

Result: 分析发现：1）高合作强度，特别是作者团队更大、更具多样性的期刊获得更高引用影响力；2）AI参与度与期刊声望（如分区排名）的关联日益增强。三阶段LLM检测AI参与度的机器人标注与人工标注一致性良好。

Conclusion: BioMedJImpact数据集和方法论为生物医学与AI交叉领域提供了全面、可扩展、内容感知的科学计量研究工具，有助于深入理解科学影响与创新动态；其AI特征标注方法也经验证可靠。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [346] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 论文提出用激活工程方法，引导LLaMA 3.1-8B展现更细腻、更类人的情感表达。通过分析关键激活点并应用情感表达向量，模型在对话中表现出更高的情感特征和更强的个人参与感。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型对话能力越来越强，但如何让其具备细腻、类似人类的情感表达仍然很难。现有方法大多只针对表层输出或需要大量微调。作者希望找到不用大规模微调也能提升模型情感表达的方法。

Method: 首先通过attribution patching方法定位对情感表达有因果影响的神经元，然后用反差文本对（如情绪正向和负向的例子）求激活差异，得到情感表达向量。将这些向量应用到新的对话提示词中，实现情感特征的增强。

Result: 采用该方法后，模型生成的回复更具正向情感（如喜悦、信任），并且使用更多第一人称代词，表现出更高层次的个人参与，更接近人类对话风格。

Conclusion: 本文提出了一种精确且可解释的新方法，为对话AI的情感表达和人性化增强提供了新方向和工具。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [347] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文探讨了主题模型（以LDA为代表）在多次运行时结果不一致的问题。作者提出了一种新的稳定性度量方法，并使用LDA生成带有“真实”主题的新语料，通过多次实验评估LDA输出的稳定性和准确性。结果发现LDA内部一致性较好但未必能找出真实主题。


<details>
  <summary>Details</summary>
Motivation: 当涉及到大规模文本分析时，主题建模是一种常用工具，但由于其概率本质，每次运行可能产生不同的主题分布，从而影响研究的可复现性和解释性。作者关注主题模型输出的可靠性，并希望量化其稳定性。

Method: 1. 定义一种结合准确性和一致性的主题模型稳定性新度量方法。
2. 利用LDA的生成特性，生成带有已知主题分布的合成语料。
3. 在该语料上对LDA进行50次独立运行，统计输出主题的波动性与准确度。

Result: 实验证明LDA能够较好地确定文档中主题数目。多次运行后LDA输出的主题较为一致，但这些主题与实际'真实'主题仍有偏差。

Conclusion: LDA主题模型在内部一致性方面表现良好，不易出现剧烈波动，但生成的主题未必具有真实语义意义，因此在实际解读时需谨慎。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [348] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: 该论文介绍了NeuroLex，一个专为脑电图（EEG）报告设计的轻量级领域自适应语言模型，实现了比通用模型更优的文本理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用语言模型难以有效捕捉EEG报告的专业表达和诊断特征，导致在医学文本分析和多模态神经解码等应用中表现有限。作者因此开发专门适用于EEG报告的语言模型。

Method: 作者提出NeuroLex模型，仅利用哈佛脑电数据库内的EEG报告文本进行预训练，并采用span-corruption策略和说明式微调，覆盖报告润色、段落摘要与术语问答等核心任务，从而学习EEG领域特定的语法和推理模式。

Result: 实验显示，NeuroLex在困惑度、信息提取、摘要准确性、标签效率及对否定和事实幻觉的鲁棒性等衡量指标上均优于同等规模的通用模型。

Conclusion: NeuroLex作为EEG报告的领域自适应语言模型，推动了生物医学文本建模，并为基于语言的神经解码应用奠定了坚实的解释性基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [349] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链（MCoT）在提升多模态大模型复杂推理能力方面的研究进展，包括方法、评测、应用及前景展望。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在感知任务取得进展后，其复杂推理能力仍有待提升，现有模型存在推理过程不透明与泛化能力不足等问题。单模思维链（CoT）已在语言模型展现出提升推理可解释性和透明度的效果，因此有必要在多模态领域进行扩展和系统梳理。

Method: 本文首先从技术演进和任务需求分析MCoT的提出背景与理论动因，然后从思维链范式、后训练阶段、推理阶段三方面介绍主流MCoT方法及其机制。此外，总结了相关评测基准与指标，并探讨了MCoT的应用场景。

Result: 现有多模态思维链方法不断涌现，评测体系逐步完善，MCoT在提升推理透明性和模型解释性方面展现出潜力，应用场景逐步扩展。

Conclusion: 多模态思维链是提升MLLM推理能力的重要方向，仍面临挑战如方法泛化、推理链自适应性等，未来需在理论与应用层面进一步探索。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [350] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的文本希望表达分类方法，并比较分析了BERT、GPT-2和DeBERTa三种架构在二分类和多分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 希望表达是心理健康和社会媒体分析中的重要情绪特征，现有方法难以精准识别不同类型的希望表达，需要利用最新的自然语言处理技术进行有效的自动化检测。

Method: 设计并实现了BERT、GPT-2和DeBERTa三种transformer架构，对文本进行“希望/非希望”二分类与五类希望相关情感的多分类。同时进行准确率与计算资源消耗的全面比较，并通过误差分析揭示不同模型的特点。

Result: BERT在二分类（84.49%）和多分类（72.03%）准确率均优于GPT-2和DeBERTa，同时训练所需资源最少。GPT-2整体准确率最低，但对讽刺性希望的检测效果最好（召回率92.46%）。DeBERTa表现居中但计算代价很高。

Conclusion: 对于专门化的情绪检测任务，合适的模型架构比大型模型更重要。本文为希望表达的计算分析提供了框架，并在心理健康和社交媒体研究等领域具有应用潜力。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [351] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 本论文针对Google Search中的AI Overviews（AIO）与Featured Snippets（FS）呈现的AI生成内容，审查其在婴儿护理和孕期相关搜索中的信息质量与一致性，发现两者在33%的情况下内容不一致，并且普遍缺乏医学安全保障，提示AI健康信息控管需加强。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在搜索引擎中的广泛应用，越来越多用户通过AIO和FS获取健康相关信息，但是这些内容的质量和一致性尚未被充分评估，尤其是在医学安全性方面可能存在风险。因此，需要系统性审查这些AI信息展示的质量及其对用户健康信息获取的影响。

Method: 作者对1,508个真实婴儿护理和怀孕相关的搜索查询进行了系统算法审计。通过多维度的严谨评估框架，对AIO与FS的答案一致性、相关性、医学安全保障、信息来源类别以及情感倾向等进行了细致分析和比较。

Result: 结果显示，AIO与FS在同一搜索结果页面呈现的信息有33%存在不一致，且两者的医学安全保障措施比例极低（AIO为11%，FS为7%）。虽然答案相关性较高，但FS较AIO更常链接至商业来源，而健康和保健网站为主要信息来源类别。

Conclusion: AIO与FS存在信息一致性和医学安全保障不足等问题，可能影响用户获取准确、可靠健康信息，建议加强AI健康信息的质量管控。此外，论文提出的审计方法可应用于其它高风险领域的AI系统质量评估。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [352] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 论文提出了“视觉房间”理论，认为多模态大模型（MLLMs）能精准描述视觉细节但未必理解其背后的情感和意图，并设计了新的层级化基准测试视觉-认知对齐能力。实验显示模型感知强于认知，认知与感知推理并非因果相关，认知能力随规模提升感知却无明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态语言模型虽然能描述视觉内容，但其是否真正"理解"所见，仍有争议。为探讨“看见≠理解”这一命题，作者借用经典哲学思想，将其引入多模态领域。

Method: 作者建立了Visual Room 2.0基准，模拟人类不同层次的感知-认知过程，将模型的任务分为低、中、高三个层次的17个细分任务，涵盖属性识别、场景理解、文本蕴含、因果与社交推理。数据集包含350多模态样本，每个样本有6个层级提问，总计2100个问题，对10个主流MLLMs进行了评测。

Result: 实验结果表明：1）模型感知能力普遍优于认知能力；2）认知推理在逻辑上不依赖感知推理；3）模型规模提升对认知能力有正效应，但对感知能力提升不明显。

Conclusion: 论文以实证数据检验并支持“看见≠理解”的观点，提出了多模态感知-认知对齐的评测新标准，有助于推动下一代具备更强理解能力MLLMs的发展。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [353] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 本文提出了一种高效恢复大语言模型（LLMs）诚实性的技术HCNR，通过仅对关键神经元进行修复，大幅提升了模型在安全关键任务中的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被应用于高风险领域，模型的诚实性变得极为重要，而常用的监督微调（SFT）会严重削弱模型表达诚实的能力，现有修复方法成本高、效率低。

Method: 作者观察到SFT主要损伤了LLM表达边界认知的能力而不是边界认知本身。据此，提出Honesty-Critical Neurons Restoration (HCNR)：1）识别并将表达能力相关的关键神经元恢复到预训练状态；2）通过海森矩阵引导补偿，保持与任务相关神经元的协同。

Result: 在四个QA任务和五个LLM家族的实验中，HCNR恢复了33.25%因微调损失的诚实性，同时比现有方法速度提升2.23倍，数据需求减少十倍以上。

Conclusion: HCNR能高效、低成本地恢复LLM的诚实表达，是实现可信赖LLM部署的实际可行方案。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [354] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: 本文提出了AA-Omniscience基准，用于更科学地评估大语言模型的事实回忆能力及自知之明（知识校准能力），以满足跨领域实际应用的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法侧重于泛能力的衡量，但在实际使用时，对事实准确性和对知识盲点的识别更加重要。因此，提出一种既能考查事实回忆，又能考查模型知之为知之、不知为不知能力的新评测方法。

Method: 作者设计了AA-Omniscience基准，包括6000道问题，覆盖42个经济相关话题和6个领域，来源权威。采用Omniscience Index指标（-100到100），该指标兼顾了答对和因不确定而选择不答的情况，对答错（幻想）进行惩罚，对合理选择不答进行奖励。

Result: 在多种前沿大模型的评测下，Claude 4.1 Opus得分最高（4.8），仅有三款模型得分高于0。整体结果显示，即便是最强模型，事实性和校准能力仍存在明显不足，不同领域下模型表现存在较大差异。

Conclusion: 前沿大模型在事实准确性和知识自知方面还有较大提升空间，且模型的选择应基于具体场景和领域的重要性，而非仅看通用指标。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [355] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 论文分析了多语种与单语种词嵌入对齐的优劣，并提出了更合理的比对方式。


<details>
  <summary>Details</summary>
Motivation: 虽然多语种嵌入当前被广泛采用，能处理多语言和混合语言文本，但是否在所有情况下都优于单语种对齐模型，及其较高计算成本是否总是合理，仍值得探究。

Method: 作者以双语词表归纳（BLI）作为主要评估指标，系统比较传统的嵌入对齐方法、新的多语种嵌入模型及其结合方式在高/低资源语种下的表现，并考察语言家族的影响。此外，提出了基于词干的BLI新方法，以及用于多语种模型的词表剪枝技术。

Result: 部分情况下，BLI并不能真实反映嵌入空间的对齐程度。联合方法通常更好，但多语种嵌入在低资源语言下效果更佳。新提出的词干BLI和词表剪枝提高了评估效果。

Conclusion: 多语种和单语种对齐模型各有优劣，单一指标难以完全衡量对齐，并需根据实际场景、语言资源和需求选择合适模型或结合方法。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [356] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: 本文提出了Spark-Prover-X1，一种7B参数量的自动定理证明大模型，通过三阶段训练，显著提升了中等规模LLM的数学推理能力，并引入了新的ExamFormal-Bench基准数据集。


<details>
  <summary>Details</summary>
Motivation: 高质量、形式化数学数据匮乏限制了LLM在自动定理证明中的进展，亟需能提升推理性能且可获取的中型模型。

Method: 创新性提出三阶段训练框架：先在大规模数学语料上进行持续预训练并加入创新性任务（如CoT-augmented state prediction），再通过专家迭代SFT微调Spark-Prover-X1-7B和Spark-Formalizer-X1-7B，最后采用目标性Group Relative Policy Optimization优化提升难题表现。此外，构建了包含402题的ExamFormal-Bench作为评测基准。

Result: Spark-Prover-X1-7B在同等规模开源模型中表现最佳，平均通过率达37.0%。在PutnamBench和CombiBench等算竞难题上表现突出，分别解决27题和通过率24.0%。

Conclusion: 多样化训练数据与分阶段优化训练流程有效提升了轻量级大模型的形式化数学推理能力。相关模型与数据集已公开，推动领域发展。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [357] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了BeDiscovER，这是一个评估当前主流大语言模型（LLM）篇章理解能力的综合性基准套件，涵盖多种篇章层次与类型任务，并对多个开源及前沿模型进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各类自然语言处理任务中表现优异，但其对篇章层次知识与推理能力的全面分析和系统评估仍然不足，缺少专门针对此类能力的基准。因此，作者希望建立一个覆盖广泛、丰富且具挑战性的标准评测体系。

Method: BeDiscovER基准集合兼容5类公开篇章理解任务，涵盖词汇、句子、多句及文档级的52个子数据集，包括常见的篇章分析、时序关系抽取等任务，以及如篇章粒子消歧等新型挑战。同时，还包含了多语种、多框架的篇章关系分类共享任务。作者使用BeDiscovER对Qwen3系列、DeepSeek-R1、GPT-5-mini等开源及前沿LLM进行了系统评估。

Result: 评测结果显示，当前代表性大语言模型在时序推理的算术层面表现良好，但在全文推理、修辞关系识别等复杂篇章与语义现象上仍表现不足。

Conclusion: 尽管当前大语言模型在篇章推理基础任务上已有进步，但在更细致、深入的篇章层次推理和语义理解方面仍有较大发展空间，需要进一步优化与创新。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [358] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 该论文评估了大型语言模型（LLM）对随机对照试验（RCT）遵循CONSORT 2010报告标准的自动识别能力。目前LLM只能作为初步筛查工具，尚难以取代人工评审。


<details>
  <summary>Details</summary>
Motivation: 传统人工核查RCT是否符合CONSORT标准耗时且费力，成为同行评审和循证医学中的瓶颈。研究团队希望评估最新LLM在无需训练的情况下，自动检测RCT报告规范性的能力和局限性。

Method: 研究建立了一个包含150篇不同医学领域已发表RCT的黄金标准数据集。采用零样本学习设定，测试多种主流LLM模型对三分类任务（合规、不合规、不适用）的准确性，主指标为宏平均F1分数，并补充项级分析和定性误差分析。

Result: 表现最佳的模型（Gemini-2.5-Flash和DeepSeek-R1）宏F1分数仅为0.634，与专家一致性较低（Cohen’s Kappa ≈ 0.28）。模型能高准确识别合规项（F1>0.85），但对不合规和不适用项表现不佳（F1<0.4），GPT-4o等知名模型表现也未达预期。

Conclusion: 主流LLM在识别RCT报告规范性方面有一定潜力，尤其对合规项初筛有效。但目前难以准确发现报告缺漏或方法学缺陷，尚不足以胜任关键性质量评估工作，不能替代人工专家审查。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [359] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体框架（Agent-Event-Coder, AEC）用于零样本事件抽取，将事件抽取任务流程化、代码化，从而提升大模型在该任务上的表现，实验结果优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 零样本事件抽取是大语言模型的难题，直接提示往往导致结果不完整或不符合规范（如触发词误判、漏掉参数、结构错误等），需要新的更系统的解决方式。

Method: 提出AEC，将事件抽取分解为检索、规划、编码、验证四个子任务，分别由不同的大语言模型智能体完成。事件的结构用可执行的类定义表示，以验证智能体反馈结果准确性，并可循环迭代优化抽取质量。整体流程类似软件工程中的代码生成。

Result: 在五个领域和六种大语言模型上实验，AEC在所有任务中均优于现有零样本事件抽取方法。

Conclusion: 将事件抽取流程化、代码化并引入多智能体合作机制，有效提升了零样本场景下大模型的事件抽取准确度和结构一致性，具备较强实用价值。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [360] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 本文系统比较了循环神经网络（ConvLSTM）和注意力机制（Transformer）两种架构在手语识别中的表现，结果显示Transformer模型明显优于ConvLSTM。


<details>
  <summary>Details</summary>
Motivation: 随着手语识别技术的发展，选择合适的神经网络结构以兼顾识别准确率和资源消耗成为关键问题。本文旨在通过实验对比，为实际应用中架构选择提供依据。

Method: 作者在Azerbaijani手语数据集（AzSLD）和美国手语词级数据集（WLASL）上，分别实现并评估了代表性循环神经网络（ConvLSTM）和注意力机制模型（Vanilla Transformer），对比了二者在Top-1、Top-5准确率及计算效率等方面的表现。

Result: 实验结果表明，Transformer模型在两个数据集上的Top-1准确率分别高达76.8%和88.3%，均优于ConvLSTM。尽管ConvLSTM在计算效率上表现更优，但识别准确率尤其在小数据集上明显落后。

Conclusion: Attention-based Transformer适合追求高识别率和签名者泛化能力的应用，而ConvLSTM适合计算资源有限或对时序建模有特殊需求的场景。本文分析了这两种架构的优劣和适用范围，为手语识别系统的模型选择提供了参考。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [361] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 本论文提出了一种无需人工标注、利用大型语言模型（LLM）进行零样本语法能力评估的新方法，并展示了其高效性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 口语语法能力评估因其自发性、非结构性和不流畅性带来额外困难。构建准确的语法评分模型传统上依赖大量专家标注，难以满足大规模数据需求，亟需自动化、低资源的替代方法。

Method: 提出零样本语法能力评估框架，使用LLM根据语法能力评分标准生成预测作为伪标签，对无标注数据进行训练。设计新型训练框架，有效应对标签噪声，利用transformer模型进行学习。同时分析了不同LLM选择及干净样本与噪声样本比例对性能的影响。

Result: 实验结果表明，所提方法能以高精度预测语法能力分数。对误差强度及分数预测进行定性分析，证实了方法的鲁棒性和可解释性。

Conclusion: 该方法为实现大规模、低资源的语法能力自动评估提供了新途径，显著降低了对人工标注的依赖。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [362] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文针对孟加拉语ASR文本中难以区分无意重复与语法性重叠（词语重复）的问题，提出了手动标注的2万条数据集，并基于主流多语种大模型和专用微调模型进行了效果验证，为孟加拉语文本标准化任务提供了基线。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语等低资源语言的ASR文本里，词语重复既可能是系统错误（非流畅性）也可能是语法现象（重叠构词法），而以往的方法不能正确区分，误删了有价值的信息，因此需要构建有区分标注的数据集并探索有效识别方法。

Method: 作者手工标注了一个2万条的孟加拉语ASR数据集，显式区分非流畅性重复与语法性重叠，分别用多语种大语言模型（LLM）和针对性微调的编码器模型（如BanglaBERT）进行实验比较。

Result: 多语种LLM在少样本提示下表现良好（最高82.68%准确率），但经过专门微调的孟加拉语BERT模型表现更优，准确率达到84.78%，F1得分0.677。

Conclusion: 该研究首次公开了孟加拉语ASR区分两类重复的语料资源，并建立了强有力的基线，为后续孟加拉语包含语义保留的文本标准化等应用打下基础。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [363] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: 本文提出了TCM-5CEval，这是一个针对大语言模型（LLMs）在中医领域表现的五维基准评测工具，能够更细致地考察模型的多方面能力。利用此工具评估的15个主流LLM显示，虽然在基础知识回忆方面表现良好，但在古文理解与推理稳定性上表现较差，且普遍存在位置偏置问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在一般领域已显示出强大能力，但在包含复杂文化内涵和专业知识的中医领域，尚缺乏系统、细致的评测手段，且已知存在知识结构不全和对文化背景理解不足的问题。该工作旨在以更全面细致的方式检测和揭示模型在中医领域的能力差异和短板。

Method: 作者设计了TCM-5CEval基准，从核心知识、古文素养、临床决策、中药知识及临床非药物治疗五个维度，对15种主流LLM进行系统性测试。同时，还引入了排列一致性测试，用以考查模型对选项顺序变化的鲁棒性。

Result: 评测结果揭示，不同LLM在中医领域表现存在显著差距。部分模型如deepseek_r1与gemini_2_5_pro在总体上表现较好，但所有模型在古文文本解读和对问题选项顺序的敏感性方面都有明显不足，推理稳定性较差。

Conclusion: TCM-5CEval为评估大语言模型在中医领域的多方面能力提供了更为细致和全面的测评工具，并详细揭示了当前模型在知识应用和推理稳定性上的核心弱点。模型社区可借助该基准推动模型改进与同行标准化评测。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [364] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化评估机器翻译模型性能的新方法—翻译熵（translation entropy），应用于多个主流翻译模型，并揭示了其可用于不同翻译器的客观排名。


<details>
  <summary>Details</summary>
Motivation: 目前主流的基于深度学习的翻译系统缺乏客观、定量的性能评估手段，主要因为单一语言的信息熵尚未知。论文旨在填补该评估方法的空白。

Method: 提出翻译熵的计算方法：以某一基准句为底本，仅更改其中一个token，数据统计分析不同token替换后译文不变的概率，这些概率即为该token的熵值，平均后得整体翻译熵。还扩展到两个token替换情况，并用该方法评测了MarianMT、T5-Base、NLLB-200等公开翻译模型。

Result: 该方法能量化评判多个主流翻译器，并能揭示双向翻译熵是否对称。还发现：两个token替换对熵的影响存在乘法效应，且译文去生成性与token分别的去生成性成正比。

Conclusion: 首次证明了翻译熵可以作为可测、可量化的特性，为人工翻译器的客观基准评测提供了新工具。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [365] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 本文评估了多种大语言模型（LLM）在恢复罗马尼亚语文本中变音符号（diacritic）能力，比较其准确率及表现差异。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语等含有大量变音符号的语言在文本处理时面临符号恢复的技术难题，现有自动方法常存在准确率瓶颈。随着大语言模型发展，评估其在该任务上的潜力和优势有实际意义。

Method: 作者使用包含丰富变音符号的罗马尼亚语语料库，分别用不同大语言模型（如GPT-3.5、GPT-4、GPT-4o、Gemini 1.0 Pro、Llama 2/3、Mixtral 8x7B、airoboros 70B、RoLlama 2 7B）并通过不同提示模板（零样本、复杂多样本等）对变音符号恢复任务进行测试和对比。

Result: 结果显示GPT-4o等模型在恢复变音符号任务上准确率很高，明显优于基线方法，而Meta的Llama系列等模型表现波动较大。各模型架构、训练数据和提示设计对结果影响显著。

Conclusion: 大模型在处理变音符号丰富语言的自动恢复任务方面展现出巨大潜力，模型选择与设计（尤其是架构、训练数据和提示）至关重要，为未来相关NLP工具优化指明方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [366] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文研究了视觉语言模型（VLMs）理解和解释语音频谱图与波形图的能力。结果发现，无论零样本还是微调模型在识别语音表征任务上表现均未超过随机水平。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在多模态任务中表现突出，但其是否具备专业性地解释语音的能力（如专业语音学家）尚未被系统评测，作者希望探究VLMs对语音频谱和波形等视觉表征的理解深度。

Method: 作者构建了一个包含4000多个英文孤立单词的全新数据集，每个单词配有一致风格的频谱图和波形图。通过多项选择题测试VLMs，要求模型从3个通过音素编辑距离生成的干扰项和1个真实转写中选出正确的音素或字母转写。实验涵盖零样本和微调场景。

Result: 无论是零样本还是经过微调的VLMs，在该任务上的准确率都很低，几乎未超过随机选择水平。

Conclusion: 结果表明，仅通过配对示例，VLMs尚不足以掌握频谱图、波形图等声学图形的专业解释，需要更具针对性的参数化知识以胜任此类任务。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [367] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型融合方法，通过对专家模型进行加权平均，提升了大模型多领域性能并在多项基准测试中取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）能力强大，但其训练过程耗时且昂贵。模型融合（model souping）作为提升性能的低成本方式，但普遍采用简单均匀平均，未充分利用各模型在不同类别上的专长。因此，如何更高效地融合模型以提升多领域表现，是当前亟需解决的问题。

Method: 提出了“类别专家汤”（SoCE）的方法：首先用基准任务划分出表现低相关性的类别；再为每类别挑选表现最佳的“专家”模型；最后对这些专家模型权重采用非均匀加权平均，而非传统的均匀平均，以实现更优融合效果。

Result: SoCE方法在多领域任务（如多语言能力、工具调用、数学能力）上相较均匀融合和单一模型均有显著提升，并在Berkeley Function Calling排行榜刷新了最新最好成绩，体现了性能和鲁棒性的双重增强。

Conclusion: SoCE通过有针对性地权重融合“专家”模型，实现了大模型性能的有效提升，为模型融合技术提供了更高效、实用的新思路，具有广阔应用前景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [368] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: 本文提出了一种用于EaaS模型版权保护的新型语义水印方法RegionMarker，有效提升了对多种攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: Embedding-as-a-Service虽方便高效，但易受模型提取攻击，导致模型被盗用，造成经济损失。现有水印方案针对性强，难以全面防御复杂多样的攻击，尤其在面对语义扰动（如释义、降维）时保护有限。迫切需要更具通用性和鲁棒性的水印机制，全面增强EaaS版权防护。

Method: 提出RegionMarker框架：在低维子空间中定义触发区域，将水印嵌入相关文本嵌入向量。使用私有降维矩阵投影并随机选择触发区域，增加水印移除难度。水印分布于整个触发区域，并以文本嵌入作为水印，使方法能抵抗释义与维度扰动攻击。

Result: 在多种数据集上进行了大量实验，表明RegionMarker能有效抵御多种模型提取、释义和降维攻击，显著提升了EaaS模型的版权保护能力。

Conclusion: RegionMarker框架为EaaS提供了更强大、全面的版权保护，抵御多类攻击，优于现有水印技术，具备实际部署价值。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [369] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 本文介绍了阿拉伯世界酒店业领域情感分析的挑战，并提出了一个多方言情感检测的共享任务，通过多方言人工标注酒店评论数据集，推动针对阿拉伯方言的情感分析技术发展。


<details>
  <summary>Details</summary>
Motivation: 随着客户反馈在阿拉伯酒店业中的重要性提升，行业亟需适用于阿拉伯多方言的情感分析工具。然而，现有资源和技术多以标准阿拉伯语为主，缺乏对不同地方方言的支持，本研究旨在填补这一空白。

Method: 策划了一项情感分析共享任务，使用原为现代标准阿拉伯语的酒店评论，人工翻译至沙特和摩洛哥（Darija）方言，经母语者校验，收集平衡三类情感（正、中、负）共538条。参赛系统基于该多方言数据集进行训练和评测。

Result: 共有40多个团队注册，12个团队提交了系统，表现最佳的系统F1分数为0.81，展示了当前多方言情感分析的可行性及面临的技术难题。

Conclusion: 该研究为阿拉伯多方言情感分析提供了重要数据资源和基准，验证了方言差异对情感分析的影响，指出了发展更强大、适应多方言的NLP系统的必要性。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [370] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型(LLMs)在某一任务或语言上的改进对其他任务和语言组合的转移影响，发现任务一致情况下跨语言转移有利，而跨任务常导致性能下降，并揭示了任务/语言之间存在稳定的供体接受体关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多任务、多语言上表现突出，但尚不清楚对某一任务或语言的优化会如何影响其它任务和语言及其组合，因此有必要系统性考察这种转移机制。

Method: 在多个开源LLM家族和不同规模下，采用PEFT/LoRA微调，每次只用单一的任务-语言组合对模型进行训练，随后评估模型在全部其它任务-语言组合上的表现变化，并将转移分解为三个情形：同任务跨语言、同语言跨任务、任务和语言均不同。

Result: 发现两大模式：（1）同任务跨语言转移通常正向有益，而跨任务经常带来性能损失，表现出明显的本任务与非本任务的不对称；（2）存在稳定的“供体-受体”结构，即某些任务/语言稳定提升（供体），某些易受损（受体）。

Conclusion: 研究结果为细粒度微调和风险规避型模型专化提供了指导，可更合理地安排LLM的多任务多语言应用和开发。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [371] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: 本文提出PEDIASBench框架，系统评估大语言模型(LLMs)在儿科真实临床环境中的表现。结果显示当前LLMs虽有基础医学知识，但在复杂推理、动态决策和人文关怀方面存在显著短板，尚不能独立承担儿科临床工作。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医学领域的快速发展，亟需系统评估其在儿科临床实践中的实际能力与安全性，以指导其在医疗中的应用和改进。

Method: 作者提出PEDIASBench评测框架，涵盖基础知识应用、动态诊疗能力、儿科医疗安全与伦理三大维度，对12个主流LLM在19个儿科亚专科和211种典型疾病上的表现进行系统评估。采用选择题、案例推理和伦理安全任务进行多维度打分。

Result: 部分模型（如Qwen3-235B-A22B）在基础知识题中表现优异，但遇到更复杂任务时准确率下降约15%。多项选择题暴露了其综合推理和知识召回能力的不足。动态诊疗场景下，尽管DeepSeek-R1表现相对较好，但大多数模型难以适应病情变化。在医学伦理和安全领域，Qwen2.5-72B表现最佳，但人文关怀能力不足。

Conclusion: 现有儿科领域大语言模型在动态决策和人文关怀方面存在明显不足，独立医疗尚不可行。未来需注重多模态集成与临床反馈闭环，加强安全性、可解释性与人机协作。但其在辅助决策、医学教育和患者沟通领域具有重要潜力，为智慧儿科医疗系统奠定基础。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [372] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文提出了PAL-Bench基准，用于评估服务型对话助手在长期用户交互中的个性化能力，并提出了H²Memory记忆框架，实现更精准的个性化响应。通过合成和人工标注，构建了首个包含中文多轮对话历史的PAL-Set数据集，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 智能设备广泛普及推动了人机对话助理的发展，但现有方法往往忽视长期交互中的用户主观特征及个性化需求，缺乏能真实评测系统个性化能力的工具与中文长历史数据集。

Method: 构建了PAL-Bench个性化基准和PAL-Set数据集（多轮、多会话、中文用户日志与对话历史），利用多步大模型生成与人工校验。提出H²Memory层次异构记忆机制，结合检索增强生成以提升个性化服务能力。

Result: 在PAL-Bench和外部数据集上的实验结果显示，所提出的H²Memory框架能显著提升服务型助手的个性化响应表现，优于现有其它方法。

Conclusion: PAL-Bench和PAL-Set为服务型会话系统个性化研究提供了全面基准与数据基础，H²Memory框架在提升个性化长对话上的有效性得到验证，对今后智能对话系统个性化方向具有推动作用。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [373] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 本文提出了一种用于翻译质量评估（TQE）的新型非线性评分模型，以更贴近人类对不同长度文本中翻译错误容忍度的真实感知。


<details>
  <summary>Details</summary>
Motivation: 当前主流的基于MQM的TQE方法将错误线性映射到惩罚分数，且校准样本多为1000-2000词。线性外推会导致对于短文本处罚过严、长文本处罚过轻，这种与专家直觉不符，影响评估的公正性和准确性。

Method: 作者基于Multi-Range框架提出对误差可接受数量与样本长度成对数关系的假设，结合心理物理和认知理论（如Weber-Fechner定律、认知负荷理论）提出双参数对数模型E(x)=a*ln(1+b*x)。该模型以两个容忍点进行标定，通过一维求根获得参数，可动态调整容忍度，并兼容现有评估流程。

Result: 在三家大型企业级环境的数据实证中，发现可接受错误数与样本长度成对数关系。新模型不仅提升了评分可解释性，也增强了评估的公平性和评估者之间的一致性，无论是AI还是人工翻译。对线性模型的相对误差做了明确的约束。

Conclusion: 该非线性评分模型把TQE推向了更符合人类感知和文档规模的评估方式，提高了评价的可扩展性和准确性，并为AI驱动的文档级评估提供更坚实的理论基础。讨论了对CAT/LQA系统的实现建议及对人类与AI生成文本评估的影响。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [374] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出了一种基于方面的情感分析方法，用于破解泰国语年报中的情感遮蔽，建立了专门的标注指南并构建数据集，测试了多种模型，并通过事件研究分析情感对股价的影响。


<details>
  <summary>Details</summary>
Motivation: 财报中的文本常对不利信息进行美化或隐晦表达，导致市场参与者难以准确判断真实情感倾向，该研究旨在解决这一情感遮蔽问题，提高市场行为分析的准确性。

Method: 作者开发了适用于泰国语财报情感遮蔽标注的细化指南，对百余篇年报进行了分面情感标注，并应用多种文本分类模型进行实验，最终通过事件研究方法探索情感分析结果与股价波动之间的关联。

Result: 多种文本分类模型在新构建的数据集上展现出较强的分类能力，事件研究发现，市场对财报中特定方面的情感表现有选择性反应。

Conclusion: 财报文本情感分析复杂，尤其是在存在遮蔽语言时。分析和识别这些隐晦表达，对于准确评估市场情感极为关键。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [375] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型(LLMs)自动化公共叙事定性标注的新框架，并验证其接近专家水平的表现，展示了在更大规模数据和政治演讲中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 公共叙事(PNs)对于领导力提升和公民动员至关重要，但因主观性强和专家标注成本高，系统化分析一直存在挑战。作者希望解决高效、客观进行大规模PNs分析的问题。

Method: 作者提出了基于代码本和专家知识的大语言模型自动标注框架，与专家结果对比，通过F1分数评价。随后将方法扩展到更大叙事数据集，并应用于政治演讲分析。

Result: 大语言模型标注与专家表现接近，在8个叙事和14种标签上平均F1分数达到0.80。方法成功扩展到22个故事和政治演讲中，揭示PN元素在不同语境下的表现。

Conclusion: LLM辅助的叙事分析具备规模化潜力，有助于深入理解公民叙事与政治修辞，但仍存在局限与改进空间，未来研究需进一步完善和验证此类方法。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [376] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 本文介绍了两个为匈牙利语自动语音识别（ASR）领域构建的新数据集，填补了该语言领域自发和对话语料不足的空白，并提供了基线实验和评测结果。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在ASR领域有丰富的数据支持，而匈牙利语等低资源语言缺乏自发和对话性语料，制约了相关技术发展。研究旨在填补这一领域空白，加速匈牙利语语音技术进步。

Method: 研究团队基于原有BEA匈牙利语语音库中未加工部分，构建了BEA-Large和BEA-Dialogue两个新数据集：BEA-Large包含255小时自发语音及详细元数据，BEA-Dialogue包含85小时自然对话并支持说话人区分。团队使用公开的ASR模型（如Fast Conformer）进行微调，建立基线，并开展说话人分离实验。

Result: 微调后的Fast Conformer在自发语音和重复语音上分别取得14.18%和4.8%的词错误率（WER），说话人分离实验的错误率（DER）在13.05%至18.26%之间。这些结果为未来研究提供了基线。

Conclusion: 研究表明对话ASR仍有较大挑战，尤其在处理口语非流畅、重叠和非正式语言时困难。新数据集的发布不仅有助于匈牙利语语音技术发展，也为其他语言自发及对话语音基准制定提供了方法框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [377] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种基于细致分类体系的新方法，构建了更具多样性和覆盖性的Text-to-SQL数据集SQL-Synth，并验证了其有效性，对提升数据集与LLM模型性能分析具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL数据集覆盖面有限，难以真实反映实际应用中的多样性，影响模型评估与训练效果，因此需要新的分类体系和生成方法。

Method: 作者首先提出了一个包含核心意图、语句类型、语法结构和关键动作等维度的新分类法，对现有数据集进行了系统分析。随后，结合该分类法与LLM，设计了一个引导式数据集合成流程，生成SQL-Synth数据集。

Result: 实验证明，SQL-Synth相比主流基准集具有更广泛的场景覆盖和多样性，同时揭示现有LLM在多样性和复杂性上表现有限，但通过微调可显著提升其表现。

Conclusion: 新提出的分类法和数据集增强了对Text-to-SQL场景的覆盖，对于评估和提升LLM能力有重要作用，可指导未来训练数据的构建和模型评测。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [378] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为O-Mem的新型记忆框架，通过主动用户画像实现更高效的个性化响应，并在多个基准上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）驱动的智能体在人类对话和复杂环境下的长期交互中，因上下文一致性和动态个性化能力有限，表现仍有不足，现有记忆系统的检索方式也可能遗漏关键用户信息。

Method: O-Mem框架基于主动用户画像，从用户与智能体的主动互动中动态抽取和更新用户特征及事件记录，支持分层检索用户属性和主题相关上下文，以提升个性化和连贯性。

Result: O-Mem在LoCoMo基准上达到了51.76%，比前SOTA（LangMem）提升近3%；在PERSONAMEM基准上达62.99%，比前SOTA（A-Mem）提升3.5%。此外，在响应速度和效率上也有提升。

Conclusion: O-Mem有效提升了记忆框架的个性化和响应效率，为未来高效贴近人类的个性化AI助手研发提供新路径。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [379] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLM）将机器学习分类器中难以理解的特征，转化为人类可理解的语言现象，从而提升鉴别虚假评论的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习分类器虽然能有效区分虚假评论和真实评论，但其提取到的特征难以被人类理解和解释，限制了实际应用与人为判断。

Method: 作者提出借助LLM，将机器学习模型中学习到的词汇线索转化为人类可解释的语言现象，并验证这些现象在数据中有实际基础、可跨领域泛化并具备较强预测能力。

Result: 作者发现这种转化得到的语言现象比LLM的先验知识或通过in-context learning得到的现象预测力更强，且能更好地解释虚假评论与真实评论的差异。

Conclusion: 该工作提出的方法有助于提升人类对虚假评论的鉴别能力，尤其是在缺乏自动鉴别工具的环境下，能够增强判断评论可信度的能力。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [380] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 本文提出了一个融合翻译与图像生成的TAI框架，通过大语言模型和扩散模型促进印度诗歌的跨文化理解与传播。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌深具文化意蕴和语言复杂性，但因多义性、文化典故和复杂语法，难以为非母语者所理解，且相关研究对印度本土语言诗歌关注甚少。

Method: 提出TAI框架，包括：（1）基于赔率比偏好对齐算法的高精度翻译模块，将印度多形态诗歌译为英文；（2）基于语义图的图像生成模块，将诗歌意象及其隐喻直观地可视化。实验采用人工与定量评估指标。

Result: TAI Diffusion在诗歌图像生成任务中显著优于已有方法。同时，作者还构建了包含21种低资源印度语言、共1570首诗歌的MorphoVerse数据集。

Conclusion: 该方法提高了印度本土诗歌的翻译及视觉理解水平，推动了诗歌的全球可及性和跨文化欣赏，对优质教育与减少文化鸿沟具有积极意义。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [381] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本论文提出Lang1系列模型，专为医疗操作决策而设计，通过在大规模电子健康记录(EHR)和网络文本上进行预训练，并在实际任务上评估其性能，证明了针对医疗场景的专用大模型具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型虽具备优秀的医学知识和对话能力，但在医院运营决策相关的实际专业任务中缺乏专门知识。针对医疗运营（如病患流转、费用、质量）需提升模型的实际预测力，因此需要开发领域专用模型并验证其实用性。

Method: 作者构建了Lang1模型（100M-7B参数），在来自纽约大学Langone Health的电子健康记录（EHR）的80B医疗词元与网络的627B词元上预训练。为了在真实环境下评估，提出了ReMedE基准，基于66万余份EHR笔记，涵盖30天再入院预测、30天死亡率预测、住院时长、共病编码、保险索赔拒绝预测等五个关键任务，并在零样本和微调后与通用大模型进行对比。

Result: 在零样本模式下，主流通用模型和特化模型在五项任务中四项表现都不佳（AUROC仅36.6%-71.7%），只有死亡率预测例外。经监督微调后，Lang1-1B比体量大70倍的通用微调模型和671倍的零样本大模型表现更佳，AUROC提升3.64%-6.75%和1.66%-23.66%。联合多任务微调还能使多任务表现均获提升，并具有良好泛化能力。

Conclusion: 医院运营预测需要专门的有监督微调，且基于领域数据的预训练显著提升效率。实验证明，专用医疗大模型可于专业任务与通用模型竞争。高效的医疗AI需结合领域预训练、监督微调以及真实场景评价，方能达到实际应用要求。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [382] [Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance](https://arxiv.org/abs/2511.11616)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: 本文提出了一种分层式架构，实现大规模多无人机系统（multi-UAV）的高效、可扩展、抗攻击和隐私保护的碰撞规避方案。通过三层智能分布架构和自适应差分隐私机制，显著提升系统性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机系统在碰撞规避时，需兼顾实时性、抗对抗能力和隐私保护。但现有方案多为单体式，计算复杂度高且缺乏拜占庭容错。这些缺陷阻碍了系统于大规模场景的实际应用。

Method: 作者提出三层架构：本地层采用稠密图注意力机制并实现极低延迟；区域层用稀疏注意力配合异步联邦学习及坐标修剪均值聚合降低复杂度；全局层引入轻量化Hashgraph协议。同时，设计自适应差分隐私机制，根据信威胁实时动态调整噪声，提升隐私-效用平衡。分布式哈希表（DHT）实现轻量审计替代区块链共识以降低延迟和资源消耗。

Result: 该架构可扩展至500架无人机，碰撞率低于2%，实现拜占庭容错能力（容忍f < n/3错误节点），并能在各规模集群中平均于50毫秒内生成可靠决策（达到95分位数判决延迟）。

Conclusion: 该框架兼顾可扩展性、低延迟、隐私性与安全性，为多无人机大规模实时协作提供了切实可行且高效的方案，有助于推动无人机协作系统的实际部署和应用。

Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(ε\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.

</details>


### [383] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: 提出了一种基于机械臂的衣物触觉数据采集系统，利用模拟手指滑动，系统地收集不同运动状态下的多模态触觉数据，并通过机器学习验证了运动参数对衣物触感识别效果的提升。


<details>
  <summary>Details</summary>
Motivation: 衣物的触觉感受影响穿着舒适度，但目前缺乏系统性、可量化的方法来采集和分析衣物在实际运动（如摩擦、滑动）中的触觉数据。该研究旨在寻找决定衣物舒适度的物理属性。

Method: 设计了一个机械臂系统，带有模拟手指，通过精确控制速度和方向，对完整衣物进行滑动测量，收集多模态触觉数据（如音频、加速度等），并加入运动相关参数进行标签化。进一步用机器学习评估运动参数对触觉识别的作用。

Result: 实验表明，加入运动参数标签后，音频和加速度数据的衣物识别准确率均有提升；验证了运动参数对衣物触觉表征的重要性。

Conclusion: 该系统为衣物触觉数据的自动化、无损采集提供了可扩展的方法，有助于后续织物感知与仿真研究。

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [384] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: 本论文提出了一种基于图像的方法，用于分析缠绕植物卷须受到不同部位机械刺激后的形态变化，能够高精度重建卷须的三维形状，并发现卷须顶端更具响应性，为植物生物力学和仿生机器人设计提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管关于缠绕植物的研究已久，但如何系统提取卷须形态时序变化、触发事件与接触位置之间的关系仍具挑战性。为此，作者希望开发一种高效且解释性强的方法来深入理解这些关系，推动生物力学及仿生工程发展。

Method: 作者提出了一种基于图像的时间序列分析方法，通过3D分段Clothoid模型对受到不同部位机械刺激后的卷须形状进行几何重建。该方法与深度学习等对比，在数据需求、计算成本与可解释性上具有优势。

Result: 所提出的几何方法在卷须三维形态重建上的准确率达R2 > 0.99，展现出高度的稳健性和可靠性。分析结果显示，卷须的顶端（apical segment）对机械刺激表现出更高的响应性，可能与该区域组织的灵活性和感受性相关。

Conclusion: 本研究不仅为揭示植物卷须力学响应机制提供了有效工具，也为基于缠绕植物特性的智能仿生机器人系统的设计与开发打下了基础。

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [385] [ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts](https://arxiv.org/abs/2511.11740)
*Haowen Jiang,Xinyu Huang,You Lu,Dingji Wang,Yuheng Cao,Chaofeng Sha,Bihuan Chen,Keyu Chen,Xin Peng*

Main category: cs.RO

TL;DR: 本文提出了一种基于专家混合体（MoE）结构的新型自动驾驶系统框架ExpertAD，通过提升关键感知特征和减少任务干扰，实现更安全高效的自动驾驶决策。实验显示，碰撞率降低20%，推理延时下降25%。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统在感知和规划方面虽具潜力，但复杂路况下的语义干扰、任务干预和较高推理延迟等问题仍未解决，制约了系统的可靠决策和安全性。

Method: 作者提出ExpertAD框架，在架构上引入了感知适配器（PA）以加强与任务相关的特征表达，并采用稀疏专家混合（MoSE）模块，减轻多任务间的相互干扰，从而实现场景理解和高效预测规划。

Result: 实验表明ExpertAD在碰撞率和推理时延等核心指标上优于现有方法，并在罕见场景下（如事故或避让紧急车辆）展现了更好的多技能泛化能力。

Conclusion: ExpertAD提升了自动驾驶系统的安全性、响应效率和复杂场景适应能力，为端到端自动驾驶提供了更具实用价值的新方案。

Abstract: Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.

</details>


### [386] [Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review](https://arxiv.org/abs/2511.11777)
*Vinit Mehta,Charu Sharma,Karthick Thiyagarajan*

Main category: cs.RO

TL;DR: 本文综述了大语言模型（LLMs）与3D视觉结合在机器人感知领域的前沿进展、核心方法、应用场景与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI与机器人技术的快速发展，仅依赖单一感知或语言能力的机器人难以胜任复杂环境中的任务。将LLMs与3D视觉融合有望弥补语言与空间感知的鸿沟，提升机器人理解和操作真实世界的能力。

Method: 文章首先介绍LLMs与3D数据基础及关键3D感知技术，然后系统分析了在场景理解、文本到3D生成、目标定位及具身智能体等领域的最新研究，包括零样本3D分割、动态场景合成和语言引导操作等。还讨论了将3D数据与触觉、听觉、热感等多模态输入集成的LLMs。文中同时整理了相关数据集与评测指标。

Result: 当前方法在跨模态场景理解、智能感知和自主决策方面取得积极进展，例如通过语言提示自动实现3D分割和操作，提升了机器人对复杂环境的适应性和交互能力。

Conclusion: 未来研究需突破自适应模型结构、跨模态对齐和实时处理等难题，从而打造更智能、环境自适应、真正自主的机器人感知系统。

Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.

</details>


### [387] [LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles](https://arxiv.org/abs/2511.11840)
*Shuangyu Xie,Kaiyuan Chen,Wenjing Chen,Chengyuan Qian,Christian Juette,Liu Ren,Dezhen Song,Ken Goldberg*

Main category: cs.RO

TL;DR: 本文提出了一种名为LAVQA的新框架，使共享无人驾驶车辆在高不确定环境下能够更安全、有效地与远程人类操作者协作。


<details>
  <summary>Details</summary>
Motivation: 在不确定性高和无线网络延迟下，无人驾驶车辆面对复杂场景时容易停滞。远程人类介入虽可提升安全，但决策时间和延迟难控，因此亟需提升人车协作决策的效率和安全性。

Method: 提出LAVQA，一个感知延迟的共享自治框架，将视觉问答(VQA)与时空风险可视化结合，设计出LICOM(延迟引起的碰撞地图)，动态展示随着延迟变化的风险区域，帮助远程操作者更直观、及时地了解车辆安全态势和作出指导。

Result: 在CARLA自动驾驶仿真中，LAVQA相较忽略延迟的基线方法，能将碰撞率降低8倍以上，表现出显著优势。

Conclusion: LAVQA框架有效利用风险可视化和延迟敏感机制，增强了远程人机协作下自动驾驶车辆的安全性和响应能力，为提升复杂环境下的共享自治水平提供了新的思路。

Abstract: When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.

</details>


### [388] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: 本文提出了一种结合SLAM与Soar认知架构的自主水下认知系统，可在深海复杂环境中实现自适应导航。系统集成了多传感器数据和认知模块，提升了感知与智能决策能力，显著提高水下探测的安全性与自主性。


<details>
  <summary>Details</summary>
Motivation: 深海探测面临定向困难、通信中断和导航失效等难题，现有系统难以适应动态复杂的水下环境。本文旨在开发更智能、自适应的水下认知系统，提高探测效率与安全性。

Method: 作者提出的AUCS系统融合了声纳、激光雷达、惯导和多普勒测速仪等多传感器数据，通过Soar认知架构整合认知推理模块（包括感知、注意、规划和学习），实现动态环境下的语义理解、自适应传感与基于记忆的学习。相较传统SLAM，AUCS能区分动态与静态物体，减少错误闭环，提高地图一致性。

Result: 实验结果显示，该系统在感知-认知-行动-学习的完整闭环中有效提升了自主水下航行器的感知、推理和自适应能力，显著降低了错误识别与导航失效现象，提升了深海任务的安全性和可靠性。

Conclusion: 本文为新一代认知型水下自主系统奠定基础，展示了其在复杂海洋环境下的优越表现，有助于推动深海探测装备向更安全、可靠与智能方向发展。

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [389] [MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy](https://arxiv.org/abs/2511.11931)
*Saida Liu,Nikolay Atanasov,Shumon Koga*

Main category: cs.RO

TL;DR: 本文提出了一种用于多目标主动跟踪的新型控制策略MATT-Diff，能在未知目标数量及状态下实现有效跟踪，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多目标主动跟踪任务需要智能体在探索未知目标与跟踪已知但不确定目标间寻求平衡。现有方法在处理目标数量、状态未知时影响跟踪效果，且难以灵活切换多种行为模式。

Method: 作者设计了一种结合视觉Transformer和注意力机制的控制策略，使用扩散模型学习生成多模态动作序列，融合来自三种专家规划器（基于前沿线、基于不确定性切换、基于检测时间切换）生成的数据，以高效处理探索与跟踪的动态权衡。

Result: 实验表明，MATT-Diff在不同目标动态场景下的跟踪性能优于专家规划器与行为克隆等基线方法，能更好地适应目标状态的变化。

Conclusion: MATT-Diff 通过扩散模型和多模态学习，实现了灵活有效的主动多目标跟踪，为面对不确定环境下的目标数目、状态等信息缺失问题提供了新思路，在实际多目标追踪任务中具有较大应用潜力。

Abstract: This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.

</details>


### [390] [Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media](https://arxiv.org/abs/2511.11958)
*Derek Chen,Zoe Samuels,Lizzie Peiros,Sujaan Mukherjee,Michael C. Yip*

Main category: cs.RO

TL;DR: 本论文系统研究了螺旋推进系统在多种介质（干沙、湿沙、饱和沙和水）下的运动性能，发现某些设计参数对性能影响显著，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 螺旋推进系统作为两栖移动方式具备巨大潜力，但当前在跨越水体、颗粒物和过渡环境时存在运动优化的难题。

Method: 采用以原理为先的系统分析方法，测试不同螺旋结构在不同介质中的运动表现，并引入散热器设计中的优化参数进行性能分类和对比。

Result: 实验证明，不同介质中对性能影响最显著的参数各有不同，借助从散热器设计获得的优化参数能有效归纳性能表现。

Conclusion: 研究为螺旋外壳设计和适应性运动控制提出了针对性的建议，对提升螺旋推进系统在多环境下的应用具有参考价值。

Abstract: Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.

</details>


### [391] [Bootstrapped LLM Semantics for Context-Aware Path Planning](https://arxiv.org/abs/2511.11967)
*Mani Amani,Behrad Beheshti,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLM）与传统规划器的方法，使机器人能够根据自然语言提示在复杂语义环境中进行安全高效的运动规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注“执行什么任务”，而忽视了“如何在语义丰富的人类环境中安全、有效地执行任务”。为此，作者希望提升机器人理解自然语言指令后动态调整运动路径的能力。

Method: 将LLM作为概率语义传感器，结合贝叶斯自助法，用多次LLM“危险”评判来估计环境中各类风险的后验分布，据此生成路径规划中的代价函数，实现机器人对指令和语境的动态响应。

Result: 实验在仿真环境和支持BIM的数字孪生环境中进行，结果表明该方法能使机器人根据显式和隐式的语义信息灵活调整运动策略，并通过定性和定量指标验证了方法效果。

Conclusion: 该框架能够有效结合LLM与传统运动规划器，使机器人在语义复杂的人类空间中更安全、智能地依自然语言提示行动，拓展了机器人理解和执行自然语言指令的能力。

Abstract: Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM "danger" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.

</details>


### [392] [ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot](https://arxiv.org/abs/2511.11970)
*Sara Wickenhiser,Lizzie Peiros,Calvin Joyce,Peter Gavrilrov,Sujaan Mukherjee,Syler Sylvester,Junrong Zhou,Mandy Cheung,Jason Lim,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: ARCSnake V2是一款专为极端环境设计的两栖螺旋驱动蛇形机器人，可适应陆地、松散介质及水域多种地形，具备多模式运动能力。实验验证了其水下机动性、通信鲁棒性及受力调节性能。


<details>
  <summary>Details</summary>
Motivation: 现有的轮式或腿式机器人在复杂多变的极端环境（如洞穴、海洋、行星表面）运动时易受地形限制，亟需一种能适应多样地形的新型机器人。

Method: 提出ARCSnake V2，结合蛇形机器人的高机动性与阿基米德螺旋推进技术，并采用密封机械结构、串联螺旋与关节驱动、一体化浮力控制系统，以及与运动学匹配的手持控制器，实现平滑切换螺旋、轮式及侧弯多种运动模式。

Result: 通过大量实验，验证了ARCSnake V2在水下环境中的机动性、通信稳定性及受力可调节性。多模式运动能力表现出色。

Conclusion: ARCSnake V2具备极好的地形适应性和多域作业能力，可为极端环境的探索、搜救与环境监测提供强有力的平台。

Abstract: Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.

</details>


### [393] [SBAMP: Sampling Based Adaptive Motion Planning](https://arxiv.org/abs/2511.12022)
*Anh-Quan Pham,Kabir Ram Puri,Shreyas Raorane*

Main category: cs.RO

TL;DR: 本文提出了一种结合采样与自适应控制的新运动规划方法（SBAMP），既具备全局最优路径生成能力，又能实时适应动态障碍与扰动。


<details>
  <summary>Details</summary>
Motivation: 以往采样方法（如RRT*）难以应对动态快速变化场景，而基于学习的动力系统需要事先演示数据，泛化能力有限。因此，需要一种无需预训练、既能全局规划又能本地自适应的方法。

Method: 将RRT*用于全局路径采样规划，结合SEDS本地控制器，实现路径点间的连续、稳定自适应轨迹跟踪。引入Lyapunov方法保证系统稳定，无需预训练数据。

Result: 在仿真和真实RoboRacer机器人上测试，新方法在动态障碍、扰动恢复和急转弯等场景中性能优越，能实时适应并保持路径最优。

Conclusion: SBAMP为动态、非结构化环境下的自主机器人导航提供了一种可扩展、无需经验数据的实时适应性运动规划方案。

Abstract: Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.

</details>


### [394] [Decoupled Action Head: Confining Task Knowledge to Conditioning Layers](https://arxiv.org/abs/2511.12101)
*Jian Zhou,Sihao Lin,Shuai Fu,Qi WU*

Main category: cs.RO

TL;DR: 论文提出了一种机器人操作的行为克隆新训练方法，通过将通用动作头的预训练和任务特定适应解耦，利用（几乎零成本的）运动学轨迹提升数据利用率和训练效率。实验显示该方法在分布内和分布外任务均有效，并提出了大幅减少参数量和训练成本的新模型DP-MLP。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Policy及主流行为克隆方法受限于配对训练数据稀缺和模型原理不明，导致泛化能力与设计科学性不足。该论文旨在解决数据瓶颈和模型理解问题，并提升训练效率。

Method: 提出解耦训练方案：首先利用运动学生成的大量轨迹无监督预训练动作头（action generator），随后冻结该部分，通过特征调制针对新任务进行适配。进一步提出用参数量大幅减少的MLP替代原有巨型U-Net骨干网络。

Result: 实验结果表明，解耦方法在分布内和分布外任务均有效，训练效率显著提升（DP-C解耦训练加速41%）；DP-MLP模型仅为4M参数，训练速度提升至83.9%-89.1%且几乎不损失性能。

Conclusion: 通用动作生成骨干在机器人操作中作用有限，关键为任务特定条件；解耦训练能高效利用无配对数据，也利于更小基础模型的应用，为高效、大规模行为克隆提供了新方向。

Abstract: Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.

</details>


### [395] [Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies](https://arxiv.org/abs/2511.12148)
*Advik Sinha,Akshay Arjun,Abhijit Das,Joyjit Mukherjee*

Main category: cs.RO

TL;DR: 本文提出了一种高效的解决方案，使仿生蛇形机器人能在障碍密集的环境中进行避障跟踪控制。方法通过进化神经网络（NEAT）为蛇形运动生成动态步态参数，实现路径跟踪。实验表明，该方法在大规模障碍环境下计算资源占用低，性能优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 仿生蛇形机器人在复杂障碍环境中避障和路径跟踪是一大挑战，现有方法常计算资源高或效果有限。本文旨在提升机器人环境适应性和资源利用率，实现更高效的避障路径跟踪控制。

Method: 采用NEAT算法，根据实时传感器（如LiDAR与其他机器人状态传感器）采集的障碍与目标信息，演化神经网络控制蛇形机器人的步态（频率与偏移角），以动态调整运动。同时，根据终点位置和时间参数化奖励函数，并通过模拟环境（PyBullet）验证与对比性能。

Result: 提出的方法在仿真大规模高障碍环境下表现出色，计算效率高。性能优于多数已有方法，在部分新型方法（如CBRL）下结果相当，但计算开销更低。

Conclusion: NEAT驱动的动态步态控制为仿生蛇形机器人的复杂环境自主运动提供了高效、实用的方案，有望提升实际环境下机器人的适应能力和应用价值。

Abstract: This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot

</details>


### [396] [Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)](https://arxiv.org/abs/2511.12160)
*Wenbin Mai,Minghui Liwang,Xinlei Yi,Xiaoyu Xia,Seyyedali Hosseinalipour,Xianbin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新型多智能体运动规划框架（RE-DPG），融合了可达性分析与动态势博弈理论，实现了在不确定环境下的安全、鲁棒与可扩展决策。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在动态且不确定环境中运动规划面临交互复杂性、随机扰动及模型不确定性，现有方法难以兼顾计算效率、分布式决策与安全性保障。

Method: 提出了RE-DPG框架，将多智能体协调形式化为动态势博弈，Nash均衡给出各智能体的最优控制策略。为提升扩展性与分布式执行，设计了ND-iBR算法，能有限步收敛到ε-Nash均衡。安全层面，将多智能体前向可达集(MA-FRS)纳入代价函数，显式建模不确定性传播与避碰约束。

Result: 通过仿真与二维、三维实际实验，验证了RE-DPG在多种操作场景下的有效性，包括安全性、鲁棒性和可扩展性表现。

Conclusion: RE-DPG能够在动态不确定环境下实现高效、分布式、安全的多智能体运动规划，并具有理论收敛性和广泛适用性。

Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.

</details>


### [397] [Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance](https://arxiv.org/abs/2511.12184)
*Jun Huo,Kehan Xu,Chengyao Li,Yu Cao,Jie Zuo,Xinxing Chen,Jian Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种用于多余机器人腿（SRL）的人机交互高效变阻抗控制方法，可在内外部扰动下保障力控的安全性。通过设计混合位置/力阻抗控制器与参数实时生成网络，实现了灵活与刚性状态的动态切换，提升了系统的稳定性与适应性。


<details>
  <summary>Details</summary>
Motivation: SRL作为浮基机器人，内部扰动强，控制难度高。为提升SRL在人机系统中的安全性，需开发能适应未知扰动且保证力控稳定性的控制方法。

Method: 1. 构建浮基SRL的动力学模型；2. 设计匹配动态力矩输入的混合位置/力阻抗控制器；3. 提出高效变阻抗控制（VIC）方法，通过动态调整阻抗参数提升刚柔切换能力；4. 设计实时、保障稳定性的阻抗参数生成网络。

Result: 仿真与实验证明，所提系统在弹性状态下信号切换平滑，在刚性状态下能提供强支撑力，实现了对环境扰动的适应与冲击缓解。

Conclusion: 该方法有效提升了SRL在人机交互中的安全性和适应性，同时也为个体步态差异下的互动提供了实用解决方案，推进了人机系统的实际应用进展。

Abstract: In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.

</details>


### [398] [Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization](https://arxiv.org/abs/2511.12186)
*Jun Huo,Jian Huang,Jie Zuo,Bo Yang,Zhongzheng Fu,Xi Li,Samer Mohammed*

Main category: cs.RO

TL;DR: 本文针对多功能超额机器人肢体（SRLs）设计中的难点，提出了一种多目标优化（MOO）设计理论，通过创新方法提升设备性能，并在实验中获得显著改善。


<details>
  <summary>Details</summary>
Motivation: 现有SRLs需满足康复与功能增强等多样化需求，但上肢与下肢功能差异大，缺乏统一且高效的设计理论，导致设备通用性及性能受限。

Method: 作者提出结合抓取与行走工作空间相似性、坐立支撑力及质量-惯性等多目标优化理论，采用椭球空间向量量化简化工作空间描述，并引入多子群体萤火虫算法以优化模型收敛效果，最终将优化结论用于设备原型设计并进行实测。

Result: 通过实验，优化后的SRL装置在抓取成功率上提升7.2%，步行及坐立任务的肌肉活动分别降低12.7%和25.1%，显示出设计理论的有效性。

Conclusion: 提出的多目标优化设计理论简化了SRL设计流程，显著提升了设备的功能性和效率，为多功能机器人肢体开发提供了新的高效思路。

Abstract: Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.

</details>


### [399] [Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps](https://arxiv.org/abs/2511.12203)
*Antony Thomas,Fulvio Mastrogiovanni,Marco Baglietto*

Main category: cs.RO

TL;DR: 本文提出了一种统一的约束移位问题方法，通过移动障碍物帮助机器人找到可行路径。方法分为两步：首先规划碰撞轨迹，后通过移动障碍物使其变为可行路径。实验证明方法适用不同类型问题。


<details>
  <summary>Details</summary>
Motivation: 复杂环境下机器人常因障碍物或约束限制难以找到可行路径。传统路径规划侧重于在障碍物间寻找路径，而现实中，可以通过主动移动障碍物来拓展可行空间，提高任务完成率。本文旨在为此类问题建立统一解决方法。

Method: 方法分为两阶段：第一阶段，通过在障碍物间计算一条轨迹，同时最小化设定的目标函数；第二阶段，根据这条轨迹调整障碍物位置，使轨迹不再发生碰撞，即成为实际可行路径。

Result: 通过多个实例展示，所提方法能在两类不同的约束移位问题中成功找到可行路径，证明了其通用性和有效性。

Conclusion: 该方法为机器人在约束复杂、环境变化的场景中提供了一种高效可行的路径规划新思路，对实际机器人操作与任务执行具有较大应用潜力。

Abstract: We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.

</details>


### [400] [SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation](https://arxiv.org/abs/2511.12232)
*Lingfeng Zhang,Erjia Xiao,Xiaoshuai Hao,Haoxiang Fu,Zeying Gong,Long Chen,Xiaojun Liang,Renjing Xu,Hangjun Ye,Wenbo Ding*

Main category: cs.RO

TL;DR: 该论文提出了一种无需环境特定训练的社交导航框架SocialNav-Map，大幅降低了机器人与人的碰撞率，并显著优于现有RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的社交导航方法训练时间极长（2000+小时），且泛化能力差，不易直接推广到新环境，实际应用受限。

Method: 提出SocialNav-Map，通过将目标位置变换到地图坐标系统，结合动态轨迹预测和动态占用图，实现更好的人类动态建模。具体通过历史轨迹和朝向两种方式预测行人运动，将其作为动态障碍体现在占用图中，从而提升避障与导航效率。

Result: 在Social-HM3D和Social-MP3D数据集上进行大量实验，SocialNav-Map在人类碰撞率上比SOTA RL方法降低了10%以上，并且无需针对新环境训练，整体性能更优。

Conclusion: SocialNav-Map通过消除环境特定训练需求，提升了安全高效的社交导航能力，为复杂真实环境下的大规模机器人部署奠定了基础。

Abstract: Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.

</details>


### [401] [Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration](https://arxiv.org/abs/2511.12237)
*Alysson Ribeiro da Silva,Luiz Chaimowicz*

Main category: cs.RO

TL;DR: 提出了一种多机器人探索方法，结合通信约束与间歇连接，在未知环境下生成与跟踪会合计划。方法通过MILP优化与简单跟踪机制提升在大规模仿真中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人探索受限于通信条件，尤其在事先未知环境中（如水下），既有方法要么牺牲效率，要么难以解释计划，且需依赖环境信息，限制实际应用。

Method: 定义了多机器人探索与通信约束间歇连接（MRE-CCIC）问题，并提出基于混合整数线性规划（MILP）生成会合计划；配合提出的RTUS跟踪机制，实现机器人在实际未知环境中跟随计划。算法在Gazebo仿真平台大规模测试。

Result: 方法可有效地生成并跟踪会合计划，实现任务高效完成。同时提供了开源的MILP计划生成器和测试环境。

Conclusion: 所提方法提升了多机器人在通信受限和环境不确定情况下的探索能力，兼具计划可执行性和实际适用性，为实际部署提供了新的解决思路。

Abstract: Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.

</details>


### [402] [SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty](https://arxiv.org/abs/2511.12361)
*Leroy D'Souza,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: 本文提出了一种基于Soft Actor-Critic（SAC）框架的专家混合体（MoE）方法SAC-MoE，用于处理包含潜在、不可观测模式切换的混合动态系统。该方法在仿真测试中实现了对新环境的更好泛化能力，并显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 混合动态系统广泛存在于如机器人、赛车等应用中，其面临的主要挑战是：系统模式和触发事件常常不可观测，导致标准控制方法难以适应，且模式切换剧烈时常规强化学习方法容易失效。因此亟需新方法提升面对这些不确定性时的泛化能力和鲁棒性。

Method: 作者提出SAC-MoE，将SAC中的actor建模为专家混合体，并利用可学习的路由器对专家进行自适应选择。此外，提出基于课程学习的训练策略——通过在高难度情形中优先采集数据，提升模型泛化至未见模式和切换情形的能力。

Result: 在混合自主赛车和多足机器人行走等仿真环境下，SAC-MoE对未见环境的零样本泛化性能显著优于其他基线（最高达6倍）。课程学习策略也提升了所有评价的策略表现。路由器的可解释性分析显示其能针对不同隐含模式激活不同专家。

Conclusion: SAC-MoE有效提升了混合动态系统在不可观测模式切换条件下的泛化能力和鲁棒性。课程训练进一步巩固了模型的普适性和性能，具有广泛的实际应用价值。

Abstract: Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.
  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.

</details>


### [403] [Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots](https://arxiv.org/abs/2511.12380)
*Nicholas Gunter,Heiko Kabutz,Kaushik Jayaram*

Main category: cs.RO

TL;DR: 本论文提出多层压电PVDF驱动器，兼具高力和柔性，能大幅提升软体微型机器人性能。


<details>
  <summary>Details</summary>
Motivation: 目前高力驱动器如PZT堆叠装置易碎，而软聚合物驱动器尽管柔性好但带宽低，难以兼顾两者。因此需要拥有更好力学性能和频率响应的新型驱动器。

Method: 作者开发了多层PVDF压电驱动器，实现并联电压分布并系统考察层数和厚度对驱动性能影响，结合理论建模和实验验证其效果。

Result: 驱动器可实现>3mm自由挠曲、>20mN阻挡力、≥500Hz驱动频率，并且工作电压仅150V。同时将该驱动器集成于微型机器人，展示其在大扰动下依然能够通过共振实现运动。

Conclusion: 多层PVDF驱动器在力学性能、响应频率和柔性间取得了良好平衡，在软体微型机器人领域有重要应用潜力。

Abstract: Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.

</details>


### [404] [Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks](https://arxiv.org/abs/2511.12383)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: 本文评估了MAML-TRPO在多样化机器人操作任务（MetaWorld ML10基准）的快速自适应能力，发现其在一些任务上能实现一次性自适应，但在泛化能力和任务适应性上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现实机器人系统常常需要在最少的数据下快速适应新任务。如何有效实现这种小样本自适应能力，是元学习领域的重要问题，尤其是在操作任务多样化的背景下。本文选取MetaWorld ML10这个具挑战性的基准，旨在评估当前主流的元学习方法在实际机器人多任务适应中的表现与局限。

Method: 本文将模型无关元学习（MAML）与信赖域策略优化（TRPO）结合，针对MetaWorld ML10的十种不同机器人操作任务进行实验测试，分析MAML-TRPO在学习通用初始化方面的能力，实现跨不同操作行为的few-shot自适应。

Result: MAML-TRPO在训练任务上最终成功率达21.0%，在未见测试任务上为13.2%。一次梯度更新便有明显性能提升，但测试任务的性能在元训练过程中易现停滞，泛化表现逊于训练任务。不同任务之间自适应效果差异大，成功率从0%到80%不等。

Conclusion: 梯度型元学习方法对多样机器人操作有潜力，但泛化能力有限且任务适应性波动大。未来应关注任务感知自适应与更为结构化的策略架构，以提升多任务的泛化和适应能力。

Abstract: Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.

</details>


### [405] [Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control](https://arxiv.org/abs/2511.12390)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经网络的虚拟现实（VR）遥操作框架，用于控制仿人机器人，实现更自然、鲁棒的操作。


<details>
  <summary>Details</summary>
Motivation: 现有VR遥操作系统依赖于逆运动学（IK）和手动调参的PD控制器，难以应对外力干扰、用户多样性与动态任务，导致动作不自然及适应性不足。

Method: 提出用深度强化学习训练的神经策略，直接将VR控制器输入映射为机器人关节命令。利用IK遥操作演示作为初始化，在仿真中进一步加入外力扰动与轨迹平滑奖励进行微调。

Result: 在Unitree G1仿人机器人上测试显示，该策略相较IK方案，跟踪误差降低34%，运动平滑度提升45%，对外力干扰适应性更强，并保持50Hz实时控制。

Conclusion: 基于学习的方法能显著提升仿人机器人遥操作的自然性与鲁棒性，对复杂操控任务（如抓取、开门、双手协调等）表现优异。

Abstract: Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.

</details>


### [406] [RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation](https://arxiv.org/abs/2511.12436)
*Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Yanbiao Ma,Yunfeng Diao,Ziyu Jia,Wenbo Ding,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: 本文研发了RoboAfford++大规模多模态可供性学习数据集及评估基准，实现对机器人操控与导航的更好理解与推理。通过实验验证，VLMs经该数据集微调后可有效提升对物体及空间可供性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）虽擅长高层次任务规划和场景理解，但缺乏物体及空间可供性（affordance）细粒度注释，难以推理具体可交互点（如抓取/放置位置），限制其实用性。

Method: 提出RoboAfford++数据集，包含86万余张图片和200万QA注释，涵盖三个关键任务：物体可供性识别、物体可供性预测及空间可供性定位。此外，构建了RoboAfford-Eval基准，评估模型在三项现实任务上的表现。

Result: 实验显示，现有VLMs在可供性推理方面表现不足，但经RoboAfford++微调后，推理能力显著提升，能更好识别和定位物体与空间的可交互区域。

Conclusion: RoboAfford++作为高质量可供性数据集，显著促进了VLMs在机器人操控与导航领域的实际推理能力，揭示并提升了模型对物体及空间可供性的理解。

Abstract: Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

</details>


### [407] [ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps](https://arxiv.org/abs/2511.12479)
*Navin Sriram Ravie,Keerthi Vasan M,Bijo Sebastian*

Main category: cs.RO

TL;DR: 提出了一种新方法ClutterNav用于在密集杂乱环境中高效取出目标物品，方法既高效又接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 在密集堆叠的杂乱环境中移除目标物体时，现有方法要么计算复杂（基于规则），要么泛化性和可解释性差（端到端强化学习），亟需既高效又灵活的新方案。

Method: 提出ClutterNav框架，将目标检索视为连续强化学习任务，利用演示数据训练的removability critic结合积分梯度，基于场景的几何和空间特征实时评估每一步的移除代价，并动态平衡即时可移除性与长远目标暴露性，无需预设启发式规则。

Result: 在模拟和真实场景中进行了大量实验，ClutterNav能够进行实时、感知遮挡的决策，表现接近人类水准。

Conclusion: ClutterNav在无需预定义规则的情况下，实现了高效、可解释且泛化性强的密集混杂物体检索，推动了该领域的技术进展。

Abstract: Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.

</details>


### [408] [Botany Meets Robotics in Alpine Scree Monitoring](https://arxiv.org/abs/2511.12526)
*Davide De Benedittis,Giovanni Di Lorenzo,Franco Angelini,Barbara Valle,Marina Serena Borgatti,Paolo Remagnino,Marco Caccianiga,Manolo Garabini*

Main category: cs.RO

TL;DR: 本论文提出利用腿式机器人（ANYmal C）辅助植物学家进行高山碎石栖息地的监测，通过深度学习实现植物物种识别，提升监测效率，改善数据采集，推动环境科学中的机器人应用。


<details>
  <summary>Details</summary>
Motivation: 高山碎石栖息地因物种独特且常含濒危植物，正面临气候变化等严重威胁。传统监测方式需科学家亲自到偏远、危险地区，既耗时又费力，亟需高效、可持续的新方法。

Method: 研究团队在意大利阿尔卑斯生物区部署了腿式机器人ANYmal C，连续两年进行两轮实地监测，通过深度学习算法对关键植物物种进行自动检测与分类，并结合传统植物群落学调查。

Result: 结果显示，腿式机器人能高效通过复杂地形，提升碎石栖息地监测的频率和效率。与传统调查结合，可优化野外工作流程，加强数据采集、存储与利用。

Conclusion: 本研究证明机器人可以大幅提升栖息地监测的可行性和可持续性，推动环境科学领域的数字化和自动化进程，为生态保护提供更优解决方案。

Abstract: According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.

</details>


### [409] [EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones](https://arxiv.org/abs/2511.12618)
*Jordan Leyva,Nahim J. Moran Vera,Yihan Xu,Adrien Durasno,Christopher U. Romero,Tendai Chimuka,Gabriel O. Huezo Ramirez,Ziqian Dong,Roberto Rojas-Cessa*

Main category: cs.RO

TL;DR: 本文提出了EcoFlight算法，用于无人机（UAV）在有障碍物的三维空间中进行能耗最优路径规划。该算法考虑了无人机动力系统与飞行动力学，实现低能耗飞行，并通过多种障碍物密度下的仿真，证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前无人机路径规划很少考虑实际存在的障碍物，且避障往往会带来较高的能耗，这对点对点航线的效率构成挑战。因此迫切需要兼顾避障与能效的路径规划方法。

Method: 提出EcoFlight算法，将障碍物环境下的三维路径规划与无人机动力系统、飞行动力学模型结合，搜索最低能耗的飞行路径，并对比直接飞行和最短距离等方法进行仿真评测。

Result: 仿真结果显示，EcoFlight算法在各种障碍物密度下均能找到能耗更低的路径，尤其在高密度障碍环境中表现突出。同时，合理的飞行速度设置能进一步提升节能效果。

Conclusion: EcoFlight可有效降低无人机飞行中的能耗，提升点对点航线效率，特别适用于复杂障碍环境，具有应用前景。

Abstract: Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.

</details>


### [410] [Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning](https://arxiv.org/abs/2511.12650)
*Arvind Kumar Mishra,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: 本文探索了在二维机械臂形态优化问题中，使用强化学习（RL）方法能否通过奖励反馈自主找到最优结构，实验结果验证了RL在已知与未知解析解场景下均可高效收敛至最优解。


<details>
  <summary>Details</summary>
Motivation: 大多数机械臂形态优化问题缺乏闭式解析解，而传统的网格搜索或启发式方法在高维空间下计算开销极大。因此需寻找更高效且可扩展的形态优化方法。

Method: 以追踪特定轨迹（圆、椭圆、矩形）的二维机械臂为例，采用三种强化学习算法（SAC、DDPG、PPO）作为形态优化器，将机械臂结构参数映射为RL的动作空间，与网格搜索和黑盒优化方法进行对比，考察其在有无解析解场景下的收敛与效果。

Result: 三种RL方法在已知解析解的圆轨迹任务中均成功重发现最优解，且在椭圆和矩形等非解析解场景下也能可靠收敛，而传统网格/黑盒优化需更高评估成本。

Conclusion: 强化学习是一种高效且可扩展的机械臂形态优化手段，既能恢复已知最优解，也能解决无解析解的复杂优化任务。

Abstract: In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.
  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.

</details>


### [411] [Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL](https://arxiv.org/abs/2511.12755)
*Aleesha Khurram,Amir Moeini,Shangtong Zhang,Rohan Chandra*

Main category: cs.RO

TL;DR: 本文提出了一种基于推理时少样本提示（few-shot prompt）驱动的领域自适应新方法，通过ICRL实现端到端自动驾驶在恶劣天气条件下的自适应，无需更新模型参数或收集额外数据。实验在CARLA仿真环境中取得较当前提示驱动方法更安全高效的结果。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶端到端系统难以适应新领域（如恶劣天气），常用的再训练或采集新数据方法在实践上不可扩展，因此需要一种无需额外训练与数据即可适应新领域的方法。

Method: 提出了一种推理时、利用少数状态-动作轨迹进行提示的领域自适应方法，利用ICRL实现闭环自动驾驶，允许在不变更模型参数、不收集新数据情况下，仅用推理时观测到的通用轨迹进行领域自适应。

Result: 在CARLA仿真环境下，所提ICRL方法相比现有基于提示的领域自适应基线，能在目标域（恶劣天气）实现更安全、更高效、更舒适的驾驶策略。

Conclusion: 本文方法打破当前提示驱动领域自适应仅限感知任务与需专家样本的局限，推动其在端到端自动驾驶决策控制中的应用，并在仿真实验验证其有效性。

Abstract: Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.

</details>


### [412] [DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation](https://arxiv.org/abs/2511.12778)
*Vignesh Rajagopal,Kasun Weerakoon Kulathun Mudiyanselage,Gershom Devake Seneviratne,Pon Aswin Sankaralingam,Mohamed Elnoor,Jing Liang,Rohan Chandra,Dinesh Manocha*

Main category: cs.RO

TL;DR: 本文提出了DR.Nav，一种用于无人地图和结构化场景下应对死胡同问题的机器人导航新方法，利用RGB-LiDAR融合和贝叶斯推断提升自主导航的安全与效率。实验显示DR.Nav在检测准确性和路径效率上都显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在无结构环境以及复杂障碍（如拐角、植被遮挡、路口堵塞）下，传统导航系统难以及时发现并恢复死胡同，导致机器人易陷入困境。现有方法仅编码可通行性，缺乏对死胡同风险与恢复机制的深度融合。

Method: DR.Nav通过跨模态RGB-LiDAR数据融合与注意力机制对每个网格单元进行死胡同概率和恢复点的实时估计，利用贝叶斯推断持续更新。核心创新是将死胡同感知、预测和恢复风险统一映射到动态语义代价地图，实现对不安全区域的前瞻性避让和路径重规划。

Result: DR.Nav在多种复杂室内外环境下测试，死胡同检测准确率提升83.33%，到达目标的时间减少52.4%，在性能上大幅超越DWA、MPPI、Nav2 DWB等先进路径规划器。

Conclusion: DR.Nav显著提升了无人地图或复杂场景中的机器人自主导航鲁棒性和路径安全性，为后续导航系统的实用部署奠定基础。

Abstract: We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions

</details>


### [413] [ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model](https://arxiv.org/abs/2511.12795)
*Boshu Lei,Wen Jiang,Kostas Daniilidis*

Main category: cs.RO

TL;DR: 本文提出了一种基于能量模型的抓取姿态生成方法，并通过主动视角选择提高机器人在高密度杂乱环境中的抓取成功率。实验表明，所提方法在有限视角下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中，机器人的抓取任务难度较大。以往方法往往忽视了抓取分布的信息增益估计，或者只用抓取分布的投影，未能充分利用SE(3)流形上的抓取姿态结构。作者希望改进信息获取方式，提高抓取效率和成功率。

Method: 作者提出了一种经过校准的基于能量的抓取姿态分布建模方法，通过与真实抓取成功率对齐，对多模态的抓取分布进行建模。并提出一种基于抓取分布估计信息增益的主动视角选择方法，引导机器人高效探索并获取目标物体关键区域视角。

Result: 在模拟环境和真实机器人实验中，所提方法在有限视角预算情况下，能成功完成高效抓取任务，性能超越了现有主流方法。此外，论文还搭建了可复现的平台以促进后续相关研究。

Conclusion: 基于能量分布建模和主动视角选择的新方法，提升了机器人在杂乱环境下有限视角的抓取能力，对后续主动抓取研究具有促进作用。

Abstract: Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.

</details>


### [414] [Structured Imitation Learning of Interactive Policies through Inverse Games](https://arxiv.org/abs/2511.12848)
*Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: 本文提出了一种结构化模仿学习框架，结合生成式单智能体策略学习与博弈论结构，实现无显式交流下的多智能体协作策略学习。实验表明，该方法在多智能体互动情境下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法在学习复杂运动技能方面取得进展，但学习需要多智能体协作、且无显式交流的互动策略仍较为困难，因为行为复杂性大大提高。

Method: 提出将模仿学习分为两个阶段：首先利用标准模仿学习从多智能体演示中学习个体行为；然后通过求解逆向博弈问题，结构化学习智能体间的依赖关系。

Result: 在合成的5智能体社交导航任务中，该方法显著优于非互动策略，并在仅用50个演示的情况下，其表现接近真实的互动策略。

Conclusion: 结构化模仿学习方法在多智能体互动任务中具有巨大潜力，为无显式交流下的高复杂互动策略学习提供了有效解决方案。

Abstract: Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

</details>


### [415] [Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos](https://arxiv.org/abs/2511.12882)
*Taiyi Su,Jian Zhu,Yaxuan Li,Chong Ma,Zitai Huang,Yichen Zhu,Hanli Wang,Yi Xu*

Main category: cs.RO

TL;DR: 本文提出了一种结合多视角轨迹视频的新型具身世界模型MTV-World，能有效提升机器人运动与物理交互预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有具身世界模型难以通过仅靠低层级动作（如关节位置）实现对未来真实物理交互的准确建模，导致模拟与实际动作不一致。此问题亟需通过更具空间表达力的控制信号加以改进。

Method: 作者提出用融合了相机内外参数与笛卡尔变换后的多视角轨迹视频作为控制信号，取代直接使用低层级动作。为弥补3D到2D投影造成的空间信息损失，采用多视图框架，保证输出对物理世界高一致性。还提出了一套自动评估流程，结合多模态大模型与视频目标分割方法，并用Jaccard指数量化空间一致性。

Result: 实验表明，在复杂双臂操作场景下，MTV-World相比已有方法在机器人运动精度和物体交互准确性方面实现了显著提升。

Conclusion: MTV-World方法能够有效利用多视角轨迹视频作为控制条件，提升具身世界模型的动作精度与交互一致性，为实现更准确、自然的人机物理交互奠定了基础。

Abstract: Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

</details>


### [416] [Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction](https://arxiv.org/abs/2511.12896)
*Jun Huo,Hongge Ru,Bo Yang,Xingjian Chen,Xi Li,Jian Huang*

Main category: cs.RO

TL;DR: 本文提出了一种基于硅胶气室的软性六轴力/力矩传感器，具备较高的力检测精度与安全性，并通过结构创新实现高效解耦，实验结果表明其性能优良。


<details>
  <summary>Details</summary>
Motivation: 随着机器人与人机交互等领域对安全柔性力控的需求增长，实现高精度、低耦合、柔性的多轴力/力矩传感器成为重要难题。传统刚性六轴传感器存在交叉轴干扰和柔软性不足的问题。

Method: 设计了一种内置16通道气压计的硅胶气室结构六轴力/力矩传感器。采用刚-软分层结构，将六轴信号解耦为两个三轴子问题，以降低交叉干扰，并通过有限元仿真和实验性能测试验证。

Result: 传感器原型可检测±50N力与±1Nm力矩，平均偏差4.9%、重复性2.7%、非线性5.8%、迟滞6.7%。实验表明静态、动态载荷响应良好，传感器保持柔软特性。

Conclusion: 所提出的软性六轴传感器在实现软体、全自由度力/力矩检测的同时，展示出优良的解耦性能和测量精度，具备良好的实用前景。

Abstract: Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\%$, 2.7$\%$, 5.8$\%$ and 6.7$\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.

</details>


### [417] [TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints](https://arxiv.org/abs/2511.12910)
*Yong Li,Yujun Huang,Yi Chen,Hui Cheng*

Main category: cs.RO

TL;DR: 本文提出了一种专为差速驱动轮式机器人（DWR）设计的时间最优路径参数化（TOPP）算法（TOPP-DWR），能够更好地兼顾实际运动约束并提升路径执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有TOPP方法通常忽略了DWR的角速度及关节速度约束，导致在实际控制中性能下降。因此，亟需一种能系统性处理多种约束并适用于DWR的TOPP算法。

Method: 1. 用非均匀B样条表示初始轨迹。2. 将角速度、关节速度、线速度、线加速度等多种约束，统一转化为线速度约束。3. 通过松弛变量，将优化问题重构为二阶锥规划（SOCP），提高数值计算效率。4. 实验对比不同方法，评估新算法优越性，并进行实际导航测试。

Result: 实验证明，所提TOPP-DWR方法在满足全部约束条件的前提下，实现了时间最优路径参数化，并且在定量指标上优于对比方法。实际导航实验也表明该方法具有良好的应用价值。

Conclusion: TOPP-DWR算法能够有效处理DWR及类似移动机器人常见的多种约束，在仿真和真实环境中均表现出色，为移动机器人路径规划和高性能控制提供了可行且实用的解决方案。

Abstract: Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.

</details>


### [418] [DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping](https://arxiv.org/abs/2511.12912)
*Yingting Zhou,Wenbo Cui,Weiheng Liu,Guixing Chen,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种名为DiffuDepGrasp的新框架，通过生成更真实的深度图噪声实现了仿真到现实无缝的机械臂抓取政策迁移，显著减少了对真实数据的需求和部署复杂度，并在多个抓取任务中取得高成功率。


<details>
  <summary>Details</summary>
Motivation: 用深度图像在模拟环境中训练的端到端机械臂抓取策略，在实际应用时会因传感器噪声等现实因素（如噪点和空洞）导致迁移效果不佳。现有方法噪声建模不自然且数据效率低、对配对数据要求高或部署成本大。因此亟需一种数据高效、部署灵活的sim2real策略。

Method: 提出DiffuDepGrasp框架，核心是Diffusion Depth Generator，包括两个模块：1）Diffusion Depth Module利用条件扩散模型采用时序几何先验，能高效学习复杂的真实传感器噪声分布；2）Noise Grafting Module在注入感知伪影的同时保持深度数据真实度。整个系统仅需使用原始深度输入，即可直接在现实中部署，无需额外计算开销。

Result: 在无需针对真实数据再训练（zero-shot transfer）的情况下，用模拟训练的抓取策略直接在现实环境测试，12种物品抓取平均成功率达95.7%，且对新物体具备较强泛化能力。

Conclusion: DiffuDepGrasp能够高效缓解深度图sim2real迁移难题，实现了无数据迁移下的高效机械臂抓取，极大降低了数据采集和部署门槛。

Abstract: Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.

</details>


### [419] [GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving](https://arxiv.org/abs/2511.12941)
*Chunyong Hu,Qi Luo,Jianyun Xu,Song Wang,Qiang Li,Sheng Yang*

Main category: cs.RO

TL;DR: 本文提出了GUIDE框架，通过3D高斯分布进行实例检测和占用预测，并兼具强大的跟踪能力，实验在nuScenes数据集上显示出显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的3D边界框方法难以精准表达实际道路中各种不规则形状的障碍物，因此需要更细致、表达力更强的表示方法，以提升自动驾驶中的感知和决策能力。

Method: 提出使用3D高斯分布对障碍物进行表示，采用稀疏的高斯-to-体素投影（Gaussian-to-Voxel Splatting）策略，既能获取细粒度的实例级占用信息，又降低了稠密体素网格所需的计算量，同时集成了鲁棒的跟踪功能。

Result: 在nuScenes数据集上的实例占用mAP达到21.61，比现有方法提升了50%，并具备有竞争力的跟踪能力。

Conclusion: GUIDE在自动感知系统中设立了新的基准，兼顾了高精准度和计算效率，更好地应对了自动驾驶环境的复杂挑战。

Abstract: In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.

</details>


### [420] [SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models](https://arxiv.org/abs/2511.12972)
*Siddarth Narasimhan,Matthew Lisondra,Haitong Wang,Goldie Nejat*

Main category: cs.RO

TL;DR: 本文提出了SplatSearch，一种结合稀疏视角3D高斯投影重建和多视角扩散模型的方法，用于提升机器人在未知环境中基于单一目标图片导航与搜索目标的能力。实验结果显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: Instance Image Goal Navigation（IIN）任务挑战在于机器人需通过单一目标图片，在未知且信息稀疏的新环境下检索特定目标，尤其在视角不同与场景重建稀疏时更为困难。当前方法在特征匹配和场景探索上仍受限，因此需要新的策略提升导航精度和效率。

Method: 提出SplatSearch架构：利用稀疏在线的3D高斯投影（3DGS）生成候选目标周边多视角渲染图像；再通过多视角扩散模型补全缺失区域，实现目标图片的鲁棒特征匹配。此外，设计了一种利用合成视点视觉和目标语义信息共同评估前沿区域优先级的探索策略，提高搜索效率和相关性。

Result: 在高度仿真的家庭和真实环境实验中，SplatSearch在任务成功率和路径最优性方面均超越现有最新方法。消融实验进一步验证了所提出各设计组件的有效性。

Conclusion: SplatSearch将稀疏3D重建、多视角补全和语义驱动探索有效结合，为基于实例图片的目标导航提供了更高效、可靠的解决方案，推动了该领域的技术进步。

Abstract: The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.

</details>


### [421] [CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner](https://arxiv.org/abs/2511.12984)
*Miryeong Park,Dongjin Cho,Sanghyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: 本文提出了一种整合地形可通行性和置信度感知的行星探测机器人路径规划框架，有效提升了高不确定性地形的探索安全性和地图可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有行星机器人探索方法虽考虑可行性约束，但对复杂地形（如陨石坑）高程估计不确定性应对不足，缺乏针对不确定性减少的主动探索策略，且未系统评估不确定性对导航安全和地图质量的影响。

Method: 论文提出集成安全路径生成、自适应置信度更新和置信度感知探索的一体化框架。利用基于Kalman滤波的高程估计，生成地形可通行性与置信度得分，再结合图优化探索（GBP），优先探索可通行但置信度低的区域。

Result: 通过模拟月球环境验证，采用新颖的低置信度区域占比指标，与基线GBP方法比，不确定性降低69%，任务成功率提升至100%（基线为0%）。

Conclusion: 所提方法显著改善了行星机器人在高不确定性地形中的探索安全性与地图可靠性，优于现有路径规划方法。

Abstract: Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.

</details>


### [422] [APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation](https://arxiv.org/abs/2511.13042)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: 本文针对A*等图搜索路径规划算法产生的路径存在非最短、锯齿等问题，提出了一种高效的后处理算法（APP），显著优化路径长度、平滑性与实际导航效果。


<details>
  <summary>Details</summary>
Motivation: 现有A*及其变种的图搜索规划方法，受限于节点扩展方向，往往生成路径非最短，且会在无障碍的开阔区域产生不必要的转向甚至锯齿线条，这与人类在开阔区域期望的笔直路径不符，因此亟需对其优化。

Method: 提出了一套基于代价地图的A*路径后处理算法（APP），包括：1）双向顶点简化算法，通过正反向处理和全局捷径策略，减少路径冗余及转向；2）迭代局部路径扰动算法，进一步提升路径平滑性。并与现有方法做了对比实验与实地验证。

Result: 实验结果表明，APP在规划耗时、路径长度和减少不必要转向方面均优于现有方法，定量指标表现突出。此外，APP在实际机器人导航中也展现出良好可行性。

Conclusion: APP算法能有效后处理A*等图搜索路径，使路径更短、更直、更平滑，对服务机器人实际部署具有显著意义。

Abstract: Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.

</details>


### [423] [Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments](https://arxiv.org/abs/2511.13048)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: 本文提出了一种针对半结构化环境中清洁机器人全局路径规划的通用方法，通过融合交通规则约束和路径最优性，实现更高效且安全的导航。


<details>
  <summary>Details</summary>
Motivation: 现有全局路径规划方法要么忽视交通规则，导致高频再规划和碰撞风险，要么严格遵守道路网络，导致路径过长，影响导航效率。因此，需平衡路径长度与道路约束，提高半结构化环境中的实际导航性能。

Method: 方法包括：1) 构建单向道路网络表达半结构化环境中的交通约束；2) 提出一种混合策略，允许在起点和终点穿越道路以缩短路径；3) 针对复杂交叉口，设计了双层势场图，确保规划结果的性能。

Result: 通过对比实验，定量结果显示该方法在路径长度与对道路网络一致性之间取得了更优的平衡，优于现有方法。

Conclusion: 提出的方法有效提升了半结构化环境下清洁机器人路径规划的实用性，兼顾了路径效率和交通规则一致性，为该领域的导航系统应用提供了新思路。

Abstract: Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.

</details>


### [424] [Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers](https://arxiv.org/abs/2511.13071)
*Michal Levin,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出了一种无需已知传感器方向和旋转操作的、基于学习的微型加速度计偏置校准方法，在实验中该方法误差比传统方法降低52%以上。


<details>
  <summary>Details</summary>
Motivation: 低成本微机电（MEMS）加速度计在导航、机器人和消费电子等领域广泛应用，但其性能常因偏置误差而降低。现有校准方法需要已知传感器方向或复杂的依赖姿态的步骤，操作繁琐且难以现场快速部署。为解决这一实际问题，作者提出了新方法。

Method: 该方法是一种‘无模型’（model-free）的、基于学习的校准方法，在传感器静止时估算加速度计偏置，无需已知其方向，也无需旋转操作。实验采用13.39小时的数据集验证了该方法的有效性。

Result: 实验结果表明，该方法相比传统技术，在误差水平上取得了超过52%的显著降低，显示出较强的准确性和适用性。

Conclusion: 本工作提出的校准方法可以在无需已知方向的场景下快速部署，提升了低成本惯性传感器的可靠性，具有广泛的科学与工业应用潜力，且消除了对‘水平’校准的需求。

Abstract: Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.

</details>


### [425] [ResAlignNet: A Data-Driven Approach for INS/DVL Alignment](https://arxiv.org/abs/2511.13096)
*Guy Damari,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种基于1D ResNet-18的深度学习方法（ResAlignNet）用于自主水下航行器（AUV）的传感器对准，显著提升了对准速度，无需复杂机动与外部辅助。


<details>
  <summary>Details</summary>
Motivation: 现有的惯性导航和测速仪集成方法对传感器对准存在收敛慢、依赖规定运动和外部传感器等问题，限制了AUV的实际操作灵活性。

Method: 将对准问题转化为基于1D ResNet-18网络的学习优化，仅需AUV自带传感器数据，无需外部定位或特殊运动，支持Sim2Real迁移，在仿真数据上训练、实地数据上部署。

Result: 在Snapir AUV上的实验表明，ResAlignNet仅需25秒数据便达到了0.8°的对准精度，比标准方法收敛时间缩短65%。

Conclusion: 该方法免除了对特定运动模式和外部辅助的依赖，实现了高效、传感器无关的对准，便于AUV即时部署，推动了水下导航系统灵活性和实用性。

Abstract: Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8° using only 25 seconds of data collection, representing a 65\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.

</details>


### [426] [Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing](https://arxiv.org/abs/2511.13100)
*Xuecheng Chen,Jingao Xu,Wenhua Ding,Haoyang Wang,Xinyu Luo,Ruiyang Duan,Jialong Chen,Xueqian Wang,Yunhao Liu,Xinlei Chen*

Main category: cs.RO

TL;DR: 本文提出了利用事件相机对无人机螺旋桨转速进行高效、精准感知的方法，极大提升了地面对空中无人机的非接触式感测能力。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用普及，如何从地面精确、无接触地感知无人机状态（特别是螺旋桨转速）已成为基础且关键的技术难题，因为这直接影响无人机监控、指挥和安全等各种场景。

Method: 提出了基于事件相机的系统\sysname，包含两个核心组件：一是“Count Every Rotation”，针对事件相机对环境噪声极为敏感的问题，设计了专门算法实现对螺旋桨转速的准确实时估计；二是“Every Rotation Counts”，基于精确转速数据进一步推断无人机的内部和外部动态。

Result: 在真实无人机配送场景下，\sysname可实现3ms感知延迟，转速估计误差仅为0.23%。推断无人机飞行指令的准确率达96.5%，与其他传感器融合后无人机追踪精度提升超过22%。

Conclusion: 以螺旋桨转速为核心特征并结合事件相机技术，极大提升了无人机地面感知的速度与精度，为无人机监控和交互等应用带来突破性进展。

Abstract: As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \sysname. \sysname features two components: \textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\%. Additionally, \sysname infers drone flight commands with 96.5\% precision and improves drone tracking accuracy by over 22\% when combined with other sensing modalities. \textit{ Demo: {\color{blue}https://eventpro25.github.io/EventPro/.} }

</details>


### [427] [Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design](https://arxiv.org/abs/2511.13120)
*Trevor Exley,Anderson Brazil Nardin,Petr Trunin,Diana Cafiso,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种单体单元（MU），将驱动、结构和传感功能集成于一体，推动软体机器人结构与传感一体化设计。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人中，驱动、结构、传感通常分散设计，集成度低，难以复制与扩展。作者希望开发一种高集成、易复制和可扩展的软体机器人基础单元。

Method: 引入参数化设计框架，将驱动腔室与结构晶格单元的尺寸建立确定性规则，通过实验同质化获得结构材料参数，用有限元模拟进行传感器优化布局，并通过实验验证机械性能及传感能力。

Result: 优化后的单元成功制造并实现嵌入式传感且保持原有机械性能，此外方法可扩展至更大单元及夹爪演示，验证该单元的普适性。

Conclusion: 本文展现了一种新型单体单元设计理念，结合确定性设计规则与仿真优化，实现可复制、可扩展、传感集成的软体机器人构建，为软体机器人设计提供了一种通用框架。

Abstract: This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.

</details>


### [428] [Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control](https://arxiv.org/abs/2511.13188)
*Osama Al Sheikh Ali,Sotiris Koutsoftas,Ze Zhang,Knut Akesson,Emmanuel Dean*

Main category: cs.RO

TL;DR: 本文提出了一种集成人工环境表示、轨迹生成和模型预测控制（MPC）的自主移动机器人导航框架，并通过实验验证在复杂环境中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 目前，AMRs在复杂环境下自主导航仍面临环境表示、碰撞检测和高效轨迹规划等多重挑战。以往方法往往分散处理，难以统一优化导航全流程，且直接在MPC中编码障碍物的计算开销较大。

Method: 方法组件包括：1）以四叉树构建从占据栅格图生成对齐坐标轴的无碰撞区域；2）用这些区域统一进行安全通道构建和线性约束MPC；3）构建连通图，采用样条曲线平滑轨迹，形成一体化导航系统。

Result: 实验证明，所提出的方法在复杂环境下相对基线方法有更高的成功率与更优性能表现。

Conclusion: 将环境分区、避障约束与规划控制有机结合，可大幅提升移动机器人导航的效率、可靠性与适用性。

Abstract: This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.

</details>


### [429] [PIGEON: VLM-Driven Object Navigation via Points of Interest Selection](https://arxiv.org/abs/2511.13207)
*Cheng Peng,Zhenzhe Zhang,Cheng Chi,Xiaobao Wei,Yanhao Zhang,Heng Wang,Pengwei Wang,Zhongyuan Wang,Jing Liu,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于兴趣点和视觉语言模型（VLM）的对象导航方法PIGEON，实现未知环境下的高效对象导航，并在多个基准上获得了最先进的零样本迁移效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有对象导航方法在决策频率和智能水平之间难以平衡、决策前瞻性差和动作不连续的问题，提高机器人导航效率和智能性。

Method: PIGEON方法在探索过程中维护轻量且语义对齐的快照记忆，并作为后续探索策略的语义输入。使用大型视觉语言模型PIGEON-VL从探索中形成的兴趣点中选择目标，再由低层规划器执行具体动作，提高决策频率。同时，基于兴趣点的决策生成可验证奖励的强化学习数据，提升模型泛化和推理能力。

Result: 在经典对象导航基准测试中，提出的零样本迁移方法取得了最先进的表现。引入可验证奖励的强化学习进一步增强了模型的语义引导和实时推理能力。

Conclusion: PIGEON方法有效提升了未知环境下对象导航的性能，实现了高语义感知和推理能力，推动了智能体在真实导航任务中的应用。

Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.

</details>


### [430] [GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry](https://arxiv.org/abs/2511.13216)
*Chiyun Noh,Sangwoo Jung,Hanjun Kim,Yafei Hu,Laura Herlant,Ayoung Kim*

Main category: cs.RO

TL;DR: 该论文提出一个用于足式机器人高精度里程计估计的新框架GaRLILEO，主要提升机器人在楼梯、坡道等复杂环境下的垂直定位精度，无需依赖激光雷达或摄像头。


<details>
  <summary>Details</summary>
Motivation: 现有足式机器人里程计方法依赖腿部运动传感与IMU，易受到冲击、滑动、震动等影响，尤其在垂直方向精度低且存在漂移，而引入激光雷达或相机又受限于稀疏或重复场景。急需更鲁棒、更精确的里程计方法。

Method: 提出GaRLILEO框架：一方面利用SoC雷达的多普勒信息与腿部运动学建立连续时间速度样条，解耦IMU的速度；另一方面引入软S2约束重力因子，精准估算重力向量，提高垂直方向姿态估计，无需LiDAR或相机辅助。

Result: 在自采集的真实室内外数据集上评估，GaRLILEO框架在楼梯、坡道等场景的垂直里程计精度达到最先进水平。数据集和算法均已开源。

Conclusion: GaRLILEO显著提升了足式机器人在复杂地形下的垂直定位精度，是无需外部感知(如LiDAR/相机)的高精度机器人里程计新方案。

Abstract: Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO

</details>


### [431] [EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation](https://arxiv.org/abs/2511.13312)
*Jonas Bode,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 本论文提出将扩散模型应用于融合视觉与文本输入的机器人控制策略，实现更精确的机器人操作任务。通过在CALVIN数据集上的实验，验证了该方法在多任务和长时序任务上的优越性。


<details>
  <summary>Details</summary>
Motivation: 在实际人类环境中，通用机器人需要能够理解自然语言并据此执行具体任务。目前将视觉和文本信息结合的机器人控制面临策略泛化性、准确性及多任务执行等挑战。作者希望借助扩散模型提升机器人理解和执行复杂操控任务的能力。

Method: 作者将扩散模型引入视觉-动作策略框架，融合视觉和文本输入，用参考演示数据训练模型，在给定文本命令时生成机器人运动轨迹。方法从图像生成领域借鉴扩散模型的相关技术，同时优化嵌入表示。

Result: 在CALVIN数据集上进行评测，实验显示该方法在多种操控任务上表现更佳，尤其在连续执行多项任务（长时序任务）时成功率明显提高。

Conclusion: 本方法证明了扩散模型在机器人多任务操控中的有效性，为通用多任务机器人操作提供了新的方向。

Abstract: Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.

</details>


### [432] [ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning](https://arxiv.org/abs/2511.13327)
*Juntao Jian,Yi-Lin Wei,Chengjie Mou,Yuhao Lin,Xing Zhu,Yujun Shen,Wei-Shi Zheng,Ruizhen Hu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为ZeroDexGrasp的零样本任务导向灵巧抓取方法，通过多模态大语言模型和抓取精炼，实现无需人工标注数据的泛化抓取。


<details>
  <summary>Details</summary>
Motivation: 目前面向任务的灵巧抓取高度依赖标注数据，难以泛化到多样物体和复杂任务。本研究旨在解决现有方法泛化能力不足和数据标注成本高的问题。

Method: 提出ZeroDexGrasp框架，采用基于提示的多阶段语义推理，利用多模态大语言模型从任务和物体语义推断初始抓取位姿和接触信息，再结合接触引导的抓取优化，实现抓取位姿的物理可行性和任务对齐。

Result: 实验表明，ZeroDexGrasp方法能够在多种新颖物体类别和复杂任务需求下，实现高质量的零样本灵巧抓取。

Conclusion: ZeroDexGrasp推进了更具泛化性和智能性的机器人抓取能力，是朝着通用机器人抓取迈出的重要一步。

Abstract: Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.

</details>


### [433] [Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness](https://arxiv.org/abs/2511.13459)
*Bingkun Huang,Yuhe Gong,Zewen Yang,Tianyu Ren,Luis Figueredo*

Main category: cs.RO

TL;DR: 该论文提出了一种结合PPO和运动基元的任务空间能量安全强化学习框架，针对富含接触的机械臂操作任务，显著提升了在复杂3D环境下的成功率、轨迹平滑性及能量安全性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习多基于MDP且在机器人关节空间实施，忽略了任务空间和与环境接触安全等因素，难以满足复杂任务对轨迹一致性、安全性、能耗等多重需求。

Method: 文章采用PPO算法结合运动基元生成任务空间轨迹，并引入能量感知的笛卡尔阻抗控制目标，实现机器人与环境之间的能量安全与可靠交互。

Result: 实验表明，新框架在多种表面类型的3D环境下表现优于已有方法，达到了更高的任务完成率、更平滑的轨迹及更安全的能耗控制。

Conclusion: 该方法能有效提升复杂环境下机器人操作任务的安全性与成功率，为任务空间强化学习和安全控制提供了有效策略。

Abstract: Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.

</details>


### [434] [Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety](https://arxiv.org/abs/2511.13530)
*Vesna Poprcova,Iulia Lefter,Matthias Wieser,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 本文提出了一种用于反映人在与社交机器人互动时社会焦虑的多模态数据集采集协议，目的是支持社会焦虑的鲁棒检测与分析。


<details>
  <summary>Details</summary>
Motivation: 人类社会焦虑普遍存在，但在人工智能与机器人领域中相关多模态数据集稀缺，严重限制了社会焦虑研究和应用进展。因此，开发新的数据集成为亟需解决的问题。

Method: 作者设计一种多模态数据集采集协议，涉及至少70名根据社会焦虑水平分组的参与者，在控制实验条件下与社交机器人Furhat进行约10分钟Wizard-of-Oz角色扮演互动。整个过程中同步采集音频、视频和生理信号，并补充收集个体情境数据。

Result: 将建立包含音频、视频、生理信号以及情境相关信息的多模态社会焦虑数据集，可反映不同社会焦虑水平下的人际行为表现。

Conclusion: 该多模态数据集协议有助于推动人-机互动情感自适应研究，促进对社会焦虑的鲁棒多模态检测与理解。

Abstract: Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.

</details>


### [435] [OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving](https://arxiv.org/abs/2511.13707)
*Xiaoyu Liang,Ziang Liu,Kelvin Lin,Edward Gu,Ruolin Ye,Tam Nguyen,Cynthia Hsu,Zhanxin Wu,Xiaoman Yang,Christy Sum Yu Cheung,Harold Soh,Katherine Dimitropoulou,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: OpenRoboCare是一个多模态的机器人护理数据集，收集职业治疗师在执行日常生活活动（ADL）时的演示数据，包含多种传感器信息，旨在助力机器人在复杂护理任务中的感知和行动能力提升。


<details>
  <summary>Details</summary>
Motivation: 目前机器人学习在护理任务方面虽然有所进展，但缺乏大规模、多样化且由专家主导、能反映真实护理流程的数据集。因此，研究团队希望通过公开一个高质量、丰富的数据集，推动机器人辅助护理技术的发展。

Method: 本项目邀请21名职业治疗师在两个实验假人上完成15项ADL任务，采集了包括RGB-D视频、姿态跟踪、眼动追踪、任务与动作标注、触觉传感等五种模态的数据，并对护理专家的原则和策略进行了分析。

Result: 该数据集不仅提供了护理动作和交互的多维度信息，还通过实验表明，对现有最先进的机器人感知及人类活动识别方法提出了新的挑战，有助于发现并弥补机器人辅助护理中的技术短板。

Conclusion: OpenRoboCare数据集为机器人辅助护理研究提供了宝贵资源，有望推动更高效、更安全、更具适应性的辅助机器人发展，对AI护理领域具有重要意义。

Abstract: We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.

</details>


### [436] [From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands](https://arxiv.org/abs/2511.13710)
*Jianglong Ye,Lai Wei,Guangqi Jiang,Changwei Jing,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种联合优化多指灵巧手硬件设计与控制策略的方法，通过微调指尖几何形状，同时优化相应控制，使机器人手能够兼顾稳定的强力抓握与精细的精准操作，大幅提升多指灵巧手的精细操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有多指机器人手在强力抓握上效果较好，但在精细操作中表现不足；而平行夹持器虽擅长精细操作，却缺乏多功能性。如何让同一灵巧手同时实现强力与精准操作，是当前机器人手设计的一大挑战。

Method: 作者通过对机器人手指尖几何做轻量化接触面修改，并将其参数与控制联合优化。控制策略根据任务动态切换，实现强力抓握和精准夹持（简化为拇指-食指的平行运动），并设计可迁移至现实的控制方法。在设计环节，借助大规模仿真和可微分神经物理代理模型优化指尖形状。

Result: 方法在模拟到现实任务的精准抓握上，针对未见过的物体取得82.5%的成功率，在实际夹持面包等复杂任务上达93.3%成功率。

Conclusion: 所提联合硬件和控制共设计框架能在不影响强力抓握的前提下，显著提升多指灵巧手的精细操作能力，为多功能机器人手的发展提供有效方案。

Abstract: Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision

</details>
